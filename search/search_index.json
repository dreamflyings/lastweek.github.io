{
    "docs": [
        {
            "location": "/", 
            "text": "Hi! I\nm Yizhou Shan, a PhD student at \nUCSD CSE\n,\nadvised by Prof. \nYiying Zhang\n.\nI\nm a member of \nUCSD Wuklab\n and \nUCSD SysNet\n.\nI was a \nPurdue Boilermaker\n.\n\nCV\n.\n\n\nContact: ys AT ucsd DOT edu\n\n\nNews\n[\nApr 2020\n] \nDisaggregated Persistent Memory\n accepted to \nATC\n20\n[\nSep 2019\n] Moved to UCSD.\n[\nMay 2019\n] Intern at \nVMware Research\n, with \nMarcos K. Aguilera\n[\nApr 2019\n] \nStorm\n accpeted to \nSYSTOR\n19\n. Awarded Best Paper.\n[\nJan 2019\n] Short paper on \nDisaggregated Persistent Memory\n accpeted to \nNVMW\n19\n[\nJul 2018\n] \nLegoOS\n accepted to \nOSDI\n18\n. Awarded Best Paper.\n[\nMay 2018\n] Intern at \nVMware Research\n, with \nStanko Novakovic\n.\nResearch\n\n\nI work on distributed system, networking, operating system, FPGA, and their intersections.\nMy recent research focus is on disaggregated datacenter, designing its hardware, OS, and upper layer applications.\n\n\nPublications\n\n\n\n\n\n\nTowards Low-Cost, Fast, and Scalable Disaggregated Persistent Memory Systems\n\n  \n Shin-Yeh Tsai, \nYizhou Shan\n, Yiying Zhang\n  \n \nATC 2020\n (\nto appear\n)\n\n\n\n\n\n\nStorm: a fast transactional dataplane for remote data structures\n\n  \n Stanko Novakovic, \nYizhou Shan\n, Aasheesh Kolli, Michael Cui, Yiying Zhang, Haggai Eran, Liran Liss, Michael Wei, Dan Tsafrir, Marcos Aguilera\n  \n \nSYSTOR 2019\n \n(Best Paper)\n\n\n\n\n\n\nLegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation\n\n  \n \nYizhou Shan\n, Yutong Huang, Yilun Chen, Yiying Zhang\n  \n \nOSDI 2018\n \n(Best Paper)\n \n[Code]\n\n\n\n\n\n\nDistributed Shared Persistent Memory\n\n  \n \nYizhou Shan\n, Shin-Yeh Tsai, Yiying Zhang\n  \n \nSoCC 2017\n \n[Code]\n\n\n\n\n\n\nWorkshops\n\n\n\n\n\n\nChallenges in Building and Deploying Disaggregated Persistent Memory\n\n  \n \nYizhou Shan\n, Yutong Huang, Yiying Zhang\n  \n \n10\nth\n Annual Non-Volatile Memories Workshop (\nNVMW 2019\n)\n\n\n\n\n\n\nDisaggregating Memory with Software-Managed Virtual Cache\n\n  \n \nYizhou Shan\n, Yiying Zhang\n  \n \n2018 Workshop on Warehouse-scale Memory Systems (\nWAMS 2018\n) (co-located with ASPLOS \n18)\n\n\n\n\n\n\nDistributed Shared Persistent Memory\n\n  \n \nYizhou Shan\n, Shin-Yeh Tsai, Yiying Zhang\n  \n \n9\nth\n Annual Non-Volatile Memories Workshop (\nNVMW 2018\n)\n\n\n\n\n\n\nDisaggregated Operating System\n\n  \n Yiying Zhang, \nYizhou Shan\n, Sumukh Hallymysore\n  \n \n17\nth\n International Workshop on High Performance Transaction Systems (\nHPTS 2017\n)\n\n\n\n\n\n\nPosters\n[P2] \nLego: A Distributed, Decomposed OS for Resource Disaggregation\n\n\n \nYizhou Shan\n, Yilun Chen, Yutong Huang, Sumukh Hallymysore, Yiying Zhang\n\n \nPoster at the 26\nth\n ACM Symposium on Operating Systems Principles (\nSOSP 2017\n)\n[P1] \nDisaggregated Operating System\n\n\n \nYizhou Shan\n, Sumukh Hallymysore, Yutong Huang, Yilun Chen, Yiying Zhang\n\n \nPoster at the ACM Symposium on Cloud Computing 2017 (\nSoCC 2017\n)\nSocial\n\n\n \n \n \n  \n\n\n\n\nGoogle Scholar\n\n\nGithub\n\n\nTwitter\n\n\nLinkedIn", 
            "title": "Home"
        }, 
        {
            "location": "/#research", 
            "text": "I work on distributed system, networking, operating system, FPGA, and their intersections.\nMy recent research focus is on disaggregated datacenter, designing its hardware, OS, and upper layer applications.", 
            "title": "Research"
        }, 
        {
            "location": "/#publications", 
            "text": "Towards Low-Cost, Fast, and Scalable Disaggregated Persistent Memory Systems \n    Shin-Yeh Tsai,  Yizhou Shan , Yiying Zhang\n     ATC 2020  ( to appear )    Storm: a fast transactional dataplane for remote data structures \n    Stanko Novakovic,  Yizhou Shan , Aasheesh Kolli, Michael Cui, Yiying Zhang, Haggai Eran, Liran Liss, Michael Wei, Dan Tsafrir, Marcos Aguilera\n     SYSTOR 2019   (Best Paper)    LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation \n     Yizhou Shan , Yutong Huang, Yilun Chen, Yiying Zhang\n     OSDI 2018   (Best Paper)   [Code]    Distributed Shared Persistent Memory \n     Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang\n     SoCC 2017   [Code]", 
            "title": "Publications"
        }, 
        {
            "location": "/#workshops", 
            "text": "Challenges in Building and Deploying Disaggregated Persistent Memory \n     Yizhou Shan , Yutong Huang, Yiying Zhang\n     10 th  Annual Non-Volatile Memories Workshop ( NVMW 2019 )    Disaggregating Memory with Software-Managed Virtual Cache \n     Yizhou Shan , Yiying Zhang\n     2018 Workshop on Warehouse-scale Memory Systems ( WAMS 2018 ) (co-located with ASPLOS  18)    Distributed Shared Persistent Memory \n     Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang\n     9 th  Annual Non-Volatile Memories Workshop ( NVMW 2018 )    Disaggregated Operating System \n    Yiying Zhang,  Yizhou Shan , Sumukh Hallymysore\n     17 th  International Workshop on High Performance Transaction Systems ( HPTS 2017 )    Posters [P2]  Lego: A Distributed, Decomposed OS for Resource Disaggregation    Yizhou Shan , Yilun Chen, Yutong Huang, Sumukh Hallymysore, Yiying Zhang   Poster at the 26 th  ACM Symposium on Operating Systems Principles ( SOSP 2017 ) [P1]  Disaggregated Operating System    Yizhou Shan , Sumukh Hallymysore, Yutong Huang, Yilun Chen, Yiying Zhang   Poster at the ACM Symposium on Cloud Computing 2017 ( SoCC 2017 )", 
            "title": "Workshops"
        }, 
        {
            "location": "/#social", 
            "text": "Google Scholar  Github  Twitter  LinkedIn", 
            "title": "Social"
        }, 
        {
            "location": "/blog/20200506-on-firmware-softwares/", 
            "text": "On Open-Source Firmware Systems Landscape\n\n\nVersion History\nDate\nDescription\nMay 6, 2020\nInitial Version\nFirmware makes people confusing, not to mention there\nare so many of them. To make it worse, they can be used/combined\nin a very flexible way.\n\n\nBottom-up:\n\n\n\n\nCoreboot/Libreboot/UEFI: for motherboard init, e.g., init memory controller.\n\n\nUEFI/BIOS\n\n\nGRUB2/U-Boto: Bootloader\n\n\nOS\n\n\n\n\nThe landscape:", 
            "title": "2020-05 On-Firmware-Softwares"
        }, 
        {
            "location": "/blog/20200506-on-firmware-softwares/#on-open-source-firmware-systems-landscape", 
            "text": "Version History Date Description May 6, 2020 Initial Version Firmware makes people confusing, not to mention there\nare so many of them. To make it worse, they can be used/combined\nin a very flexible way.  Bottom-up:   Coreboot/Libreboot/UEFI: for motherboard init, e.g., init memory controller.  UEFI/BIOS  GRUB2/U-Boto: Bootloader  OS   The landscape:", 
            "title": "On Open-Source Firmware Systems Landscape"
        }, 
        {
            "location": "/blog/20200501-on-graphic-softwares/", 
            "text": "On Unix Graphic Softwares\n\n\nVersion History\nDate\nDescription\nMay 1, 2020\nInitial Version\nFor work reason, I use VNC a lot recently. I need to login into our lab\ns servers and perform\nintensive graphic operations. Somehow I\nm not a fan of GUI-based systems,\nbut it really got me wonder: how VNC works? Or, how graphics/GUI works in general?\n\n\nSo I decided to look it up. The whole thing was very complex to me at the beginning.\nThere are numerous layers of systems, and it not clear who is doing what.\nAfter getting a better understanding, I realize it is \ndo one thing and do it well\n works at its best:\nEach layer of the graphic stack is doing what it is supposed to do, nothing more and nothing less.\nEven though the line blurred over the years (i think), the principle persists. I like it.\n\n\nI\nm no where near explaing the whole thing well (still a bit confused myself :)).\nBut you can find awesome references at:\n1) \nWiki Display Server\n,\n2) \nWayland Architecture\n\n3) \nStackExchange Difference between Xorg and Gnome/KDE/Xfce\n\n\nFollowing are some figures I drew to show the architecture of all these softwares.\nIn the graphic world, kernel\ns involvement is minimal, but a critical one.\nKernel mainly need to deliver mouse/keyboard events, render frames via graphic cards,\nhandle network. In other words, kernel provides a mechanism.\nThe policy is left to userspace stacks.\n\n\nAt the lowest level, we have Display Manager, or Display Server.\nDownstream, this layer interact with kernel, i.e., getting keyboard/mouse events from kernel evdev framework,\nrendering frames via DRM interfaces.\nUpstream, this layer accepts request from their clients (i.e., the widget layer) and make them happen in real displays.\nTypical systems at this layer are X.org server and Wayland. They follow the client-server model,\ncommunication has a certain protocol and is via socket (I guess?). \n\n\nNext up, is the \nwidget toolkit\n, or UX library layer.\nThe famous GTK/Qt belong to this layer.\nWhat this layer is doing? So this one is a collection of widgets, like buttons, menu, dropdown,\ni.e., GUI elements. Both GTK/Qt offer a lot such stuff (if you are using GNOME desktop, try run \ngtk3-widget-factory\n).\nThis layer ask the display manager layer (e.g. X.org server) to display stuff.\n\n\nGNOME/KDE are \ndesktop envionment\n,\nthey present the whole desktop experience, it includes many applications built based on GTK and Qt, respectively.\nYou probably have seen \ngnome-shell\n, yup, this is GNOME\ns main program.\n\n\nThe highest layer is user applications, like Chrome (which \nby default uses GTK on linux\n, code on \nui/gtk\n).\nAll these linux GUI applications, they are usually built on top of either GTK or Qt\ns libraries.\nThat being said, if you want to develop GUI-based apps on Linux, chances are, you\nwill use either of the libraries.\n\n\nThis is a landscape overview:\n\n\n\nBut how \nVNC\n fits into the big picture?\nIn short, VNC sits in the middle between X and GTK/Qt.\nOn one hand, VNC appears as a client of X. On the other, VNC appears as an X server to GTK/Qt.\nMiddleman works at its best lol. There are, however, many different implementation choices.\nIf you have used TigerVNC, which in turn uses Xvnc, its \nman page\n says:\n\nXvnc is the X VNC (Virtual Network Computing) server. It is based on a standard X server, but it has a \nvirtual\n screen rather than a physical one. X applications display themselves on it as if it were a normal X display, but they can only be accessed via a VNC viewer - see vncviewer(1). So Xvnc is really two servers in one. To the applications it is an X server, and to the remote VNC users it is a VNC server.\n\n\nThus it looks like this with VNC:\n\n\n\nA machine have multiple such instances, thus multiple virtual and physical display can coexist.\nAnd for that, I think it\ns all because of the clear separation of layers and good engineering\n(man, those graphic framebuffer code is monstrous):\n\n\n\nThis post remind me of \nWhat happens when you type google.com into your browser and press enter?\n?\n\n\nAs always, hope you enjoyed this blog.", 
            "title": "2020-05 On-Unix-Graphic-Softwares"
        }, 
        {
            "location": "/blog/20200501-on-graphic-softwares/#on-unix-graphic-softwares", 
            "text": "Version History Date Description May 1, 2020 Initial Version For work reason, I use VNC a lot recently. I need to login into our lab s servers and perform\nintensive graphic operations. Somehow I m not a fan of GUI-based systems,\nbut it really got me wonder: how VNC works? Or, how graphics/GUI works in general?  So I decided to look it up. The whole thing was very complex to me at the beginning.\nThere are numerous layers of systems, and it not clear who is doing what.\nAfter getting a better understanding, I realize it is  do one thing and do it well  works at its best:\nEach layer of the graphic stack is doing what it is supposed to do, nothing more and nothing less.\nEven though the line blurred over the years (i think), the principle persists. I like it.  I m no where near explaing the whole thing well (still a bit confused myself :)).\nBut you can find awesome references at:\n1)  Wiki Display Server ,\n2)  Wayland Architecture \n3)  StackExchange Difference between Xorg and Gnome/KDE/Xfce  Following are some figures I drew to show the architecture of all these softwares.\nIn the graphic world, kernel s involvement is minimal, but a critical one.\nKernel mainly need to deliver mouse/keyboard events, render frames via graphic cards,\nhandle network. In other words, kernel provides a mechanism.\nThe policy is left to userspace stacks.  At the lowest level, we have Display Manager, or Display Server.\nDownstream, this layer interact with kernel, i.e., getting keyboard/mouse events from kernel evdev framework,\nrendering frames via DRM interfaces.\nUpstream, this layer accepts request from their clients (i.e., the widget layer) and make them happen in real displays.\nTypical systems at this layer are X.org server and Wayland. They follow the client-server model,\ncommunication has a certain protocol and is via socket (I guess?).   Next up, is the  widget toolkit , or UX library layer.\nThe famous GTK/Qt belong to this layer.\nWhat this layer is doing? So this one is a collection of widgets, like buttons, menu, dropdown,\ni.e., GUI elements. Both GTK/Qt offer a lot such stuff (if you are using GNOME desktop, try run  gtk3-widget-factory ).\nThis layer ask the display manager layer (e.g. X.org server) to display stuff.  GNOME/KDE are  desktop envionment ,\nthey present the whole desktop experience, it includes many applications built based on GTK and Qt, respectively.\nYou probably have seen  gnome-shell , yup, this is GNOME s main program.  The highest layer is user applications, like Chrome (which  by default uses GTK on linux , code on  ui/gtk ).\nAll these linux GUI applications, they are usually built on top of either GTK or Qt s libraries.\nThat being said, if you want to develop GUI-based apps on Linux, chances are, you\nwill use either of the libraries.  This is a landscape overview:  But how  VNC  fits into the big picture?\nIn short, VNC sits in the middle between X and GTK/Qt.\nOn one hand, VNC appears as a client of X. On the other, VNC appears as an X server to GTK/Qt.\nMiddleman works at its best lol. There are, however, many different implementation choices.\nIf you have used TigerVNC, which in turn uses Xvnc, its  man page  says: Xvnc is the X VNC (Virtual Network Computing) server. It is based on a standard X server, but it has a  virtual  screen rather than a physical one. X applications display themselves on it as if it were a normal X display, but they can only be accessed via a VNC viewer - see vncviewer(1). So Xvnc is really two servers in one. To the applications it is an X server, and to the remote VNC users it is a VNC server.  Thus it looks like this with VNC:  A machine have multiple such instances, thus multiple virtual and physical display can coexist.\nAnd for that, I think it s all because of the clear separation of layers and good engineering\n(man, those graphic framebuffer code is monstrous):  This post remind me of  What happens when you type google.com into your browser and press enter? ?  As always, hope you enjoyed this blog.", 
            "title": "On Unix Graphic Softwares"
        }, 
        {
            "location": "/blog/20200404-on-read-once/", 
            "text": "On READ_ONCE and Compiler Opts\n\n\nVersion History\nDate\nDescription\nApr 13, 2020\nInitial Version\nI decide to write this blog after I once again got tricked by GCC optimizations.\nI was designing a simple single-producer-single-consumer ring buffer.\nSince there is a small time gap between slot-being-allocated and slot-being-usable (i.e., data filled),\nthe producer will set a non-atomic flag once the data is filled thus usable.\nThe consumer, running on a seperate CPU, will repeatly checking the usable flag\nafter it has grabbed the slot.\n\n\nSimple, right? Yet I ran into a lot random stuck during testing.\nI didn\nt even check the ring buffer design as I was so confident.\nThere was no timeout checking either. After some digging,\nI realized I missed using \nREAD_ONCE\n when consumer thread is polling for the usable flag.\n\n\nYeah, once again, \ngcc -O2\n tricked me:\nit will optmize away repeated memory accesses\nif it thinks the accessed variable/data is thread-local.\nFor instance, the following code snippet shows how gcc -O2 removes the memory access part.\nWithout -O2, a simple assembly loop is generated. With -O2, gcc generates a deadlock itself.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n          \nOriginal\n \nC\n                        \nAssembly\n                 \nAssembly\n\n                                            \n(\ngcc\n \n-\nS\n)\n               \n(\ngcc\n \n-\nS\n \n-\nO2\n)\n\n\nint\n \nx\n;\n                           \n|\n                            \n|\n\n                                 \n|\n \n.\nL2\n:\n                       \n|\n \n.\nL2\n:\n\n\n/* Spin until x becomes true */\n  \n|\n     \nmovl\n    \nx\n(\n%\nrip\n),\n \n%\neax\n  \n|\n     \njmp\n \n.\nL2\n\n\nvoid\n \nwait_for_x\n(\nvoid\n)\n            \n|\n     \ncmpl\n    \n$\n1\n,\n \n%\neax\n       \n|\n\n\n{\n                                \n|\n     \nje\n      \n.\nL2\n            \n|\n\n        \nwhile\n \n(\nx\n \n==\n \n1\n)\n           \n|\n                            \n|\n\n                \n;\n                \n|\n                            \n|\n\n\n}\n                                \n|\n                            \n|\n\n\n\n\n\n\n\nWhy this is happening?\n Because gcc thinks vairable \nx\n is thread-local and will not be accessed\nby multiple threads at the same time. Thus gcc thinks the above \nwhile (x == 1) ;\n check will never break,\nso generating an assembly deadlock jmp loop.\n\n\nWhy does this matter?\n Assume \nx\n is a shared variable.\nIn the following code snippet, there are two threads, A and B.\nThread A wait until B change \nx\n to 1.\nIf we compile with -O2, thread A will deadlock.\nAnd this was my bug above.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nint\n \nx\n;\n \n/* a global shared variable*/\n\n\n           \nThread\n \nA\n                         \nThread\n \nB\n\n\n\n/* Spin until x becomes true */\n  \n|\n   \n/* Set x at some point */\n\n\nvoid\n \nwait_for_x\n(\nvoid\n)\n            \n|\n   \nx\n \n=\n \n1\n;\n\n\n{\n                                \n|\n \n        \nwhile\n \n(\nx\n \n==\n \n1\n)\n           \n|\n \n                \n;\n                \n|\n \n\n}\n                                \n|\n \n\n\n\n\n\n\nThe common approach, is to add \nvolatile\n modifier, to explicitly express the concurrency issue.\nBut \nvolatile is considered harmful\n by linux kernel, and I agree with it.\n\n\nI generally use \nREAD_ONCE\n, \nWRITE_ONCE\n, \nACCESS_ONCE\n macros.\nThey \ntell\n gcc that the particualr variable is a shared global variable,\nthus for each time a C statment is running, the variable should be accessed once and exactly once.\nThe fix for above case is: \nwhile (READ_ONCE(x == 1)) ;\n.\n\n\nI will not go into details about why and how those macros are implemented.\nFor more information, refers to \nsource code\n, \nktsan wiki\n.\n\n\nHope you enjoyed this simple bug-documentation blog.", 
            "title": "2020-04 On-Read-Once-and-Compiler-Opts"
        }, 
        {
            "location": "/blog/20200404-on-read-once/#on-read_once-and-compiler-opts", 
            "text": "Version History Date Description Apr 13, 2020 Initial Version I decide to write this blog after I once again got tricked by GCC optimizations.\nI was designing a simple single-producer-single-consumer ring buffer.\nSince there is a small time gap between slot-being-allocated and slot-being-usable (i.e., data filled),\nthe producer will set a non-atomic flag once the data is filled thus usable.\nThe consumer, running on a seperate CPU, will repeatly checking the usable flag\nafter it has grabbed the slot.  Simple, right? Yet I ran into a lot random stuck during testing.\nI didn t even check the ring buffer design as I was so confident.\nThere was no timeout checking either. After some digging,\nI realized I missed using  READ_ONCE  when consumer thread is polling for the usable flag.  Yeah, once again,  gcc -O2  tricked me:\nit will optmize away repeated memory accesses\nif it thinks the accessed variable/data is thread-local.\nFor instance, the following code snippet shows how gcc -O2 removes the memory access part.\nWithout -O2, a simple assembly loop is generated. With -O2, gcc generates a deadlock itself.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10            Original   C                          Assembly                   Assembly \n                                             ( gcc   - S )                 ( gcc   - S   - O2 )  int   x ;                             |                              | \n                                  |   . L2 :                         |   . L2 :  /* Spin until x becomes true */    |       movl      x ( % rip ),   % eax    |       jmp   . L2  void   wait_for_x ( void )              |       cmpl      $ 1 ,   % eax         |  {                                  |       je        . L2              | \n         while   ( x   ==   1 )             |                              | \n                 ;                  |                              |  }                                  |                              |    Why this is happening?  Because gcc thinks vairable  x  is thread-local and will not be accessed\nby multiple threads at the same time. Thus gcc thinks the above  while (x == 1) ;  check will never break,\nso generating an assembly deadlock jmp loop.  Why does this matter?  Assume  x  is a shared variable.\nIn the following code snippet, there are two threads, A and B.\nThread A wait until B change  x  to 1.\nIf we compile with -O2, thread A will deadlock.\nAnd this was my bug above.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 int   x ;   /* a global shared variable*/ \n\n            Thread   A                           Thread   B  /* Spin until x becomes true */    |     /* Set x at some point */  void   wait_for_x ( void )              |     x   =   1 ;  {                                  |  \n         while   ( x   ==   1 )             |  \n                 ;                  |   }                                  |     The common approach, is to add  volatile  modifier, to explicitly express the concurrency issue.\nBut  volatile is considered harmful  by linux kernel, and I agree with it.  I generally use  READ_ONCE ,  WRITE_ONCE ,  ACCESS_ONCE  macros.\nThey  tell  gcc that the particualr variable is a shared global variable,\nthus for each time a C statment is running, the variable should be accessed once and exactly once.\nThe fix for above case is:  while (READ_ONCE(x == 1)) ; .  I will not go into details about why and how those macros are implemented.\nFor more information, refers to  source code ,  ktsan wiki .  Hope you enjoyed this simple bug-documentation blog.", 
            "title": "On READ_ONCE and Compiler Opts"
        }, 
        {
            "location": "/blog/20200404-on-net-transport/", 
            "text": "On Hardware-based Network Transport Design and Implementation\n\n\nTODO", 
            "title": "2020-04 On-Hardware-based-Network-Transport"
        }, 
        {
            "location": "/blog/20200404-on-net-transport/#on-hardware-based-network-transport-design-and-implementation", 
            "text": "TODO", 
            "title": "On Hardware-based Network Transport Design and Implementation"
        }, 
        {
            "location": "/notes/source_code/", 
            "text": "Source Code Study\n\n\nVersion History\nDate\nDescription\nApr 26, 2020\nAdd wayland, X, gnome, gtk etc\nApr 10, 2020\nadd graphics section\nApr 6, 2020\nadd verbs perftes\nMar 3, 2020\nadd FreeBSD, some fpga stuff\nFeb 4, 2020\nadd io_uring, firecracker\nJan 31, 2020\nAdd some good stuff\nJan 18, 2020\nInitial\nBeautiful code is art.\nRecently I started forking good open source code\ninto my own Github account and started casual reading and taking notes.\nThis page links to all of them.\n\n\nHappy hacking!\n\n\nMisc\n\n\nProjects supporting our day-to-day work.\n\n\n\n\nGNU glibc: libc, elf, and dynamic linker\n\n\nIt is the default C library used by almost everyone\n\n\nIt includes \nld.so\n, the dynamic linker\n\n\nI wrote some notes about GOT/PLT and explains what has happend before main() is called.\n\n\n\n\n\n\nGNU binutils: gas, static linker, and more\n\n\nThis repo has a lot commands like \nas\n, \nld\n, \nobjdump\n, \nnm\n and so on\n\n\nld\n is static linker and I like the magic of its linker script\n\n\nI guess another useful repo is \nelfutils\n\n\n\n\n\n\nstrace\n\n\nSystem call tracer at userspace\n\n\nI\nve designed \none\n for LegoOS in kernel space\n\n\n\n\n\n\nUnix Commands\n\n\nOf course almost all other listed repos in this section have some sort of commands.\n  But they are not essential. The following repos have the essential UNIX commands like ls, cat.\n  It\ns not possible to go through all of them. But rather, I think they serve as references\n  when we want to know how certain things are implemented (e.g., how dmesg get kernel log).\n\n\nBusyBox\n\n\nGNU Coreutils\n\n\nutil-linux\n\n\nFreeBSD and its friends\n\n\n\n\n\n\nTools\n\n\ntmux\n\n\ngit\n\n\n\n\n\n\nEditors\n\n\nvim\n\n\nneovim\n\n\n\n\n\n\nNetwork\n\n\niperf3\n is a TCP, UDP, and SCTP network bandwidth measurement tool\n\n\ntcpdump\n\n\nOpenSSH\n is our ssh!\n\n\nscapy\n: Python-based interactive packet manipulation program \n library. Very neat\n\n\nAlso checkout \nFreeBSD\n as it has tools like \nifconfig\n, \nif\n. \n\n\n\n\n\n\nC for life\n\n\nSome small and useful C projects\n\n\ncJSON\n\n\nA lightweight JSON parser in C.\n\n\nI think iperf3 is using it.\n\n\n\n\n\n\n\n\n\n\nOutliers\n\n\nCRIU: Checkpoint and Restore in Userspace\n\n\nThe reason I love this repo is because it has so many interesting pieces\n  on how to interact with kernel, save states, and restore them. In addition,\n  it shows how to properly use many less well known syscalls.\n\n\n\n\n\n\nGRUB2: bootloader\n\n\nLearn how modern bootloader works.\n\n\nDetailed analysis of Linux booting sequence (how it transit from\n  real-mode to protected mode, and finally to 64-bit mode,\n  how to navigate Linux source code etc.)\n\n\n\n\n\n\nFFmpeg\n\n\nFFmpeg project is famous for its clean and neat C code.\n\n\nBesides, this project is used by a lot online video service companies\n\n\n\n\n\n\nio_uring\n\n\nuser liburing\n\n\nkernel io_uring.c\n\n\n\n\n\n\n\n\n\n\n\n\nOperating Systems\n\n\n\n\nLinux 0.0.1\n\n\nPlan 9 OS\n\n\nMSR Singularity.\n\n\nillumos\n\n        - This is a fork of Oracle Solaris OS.\n\n\nseL4 Microkernel\n\n\nMacOS Darwin\n\n\nBSD: these repos have everything you can think of\n\n\nIf you ever wondered how X is done, how to get Y from OS, this is where you look into.\n\n\nFreeBSD\n\n\nOpenBSD\n\n\nNetBSD\n\n\nTrueOS\n\n\n\n\n\n\nUnikernel\n\n\nOSv. A lightweight unikernel.\n\n\nIncludeOS\n\n\nRumprun\n\n\nSolo5. Unikernel as processes!\n\n\n\n\n\n\n\n\n\n(Image source: \nhttps://commons.wikimedia.org/wiki/File:Unix_timeline.en.svg\n)\n\n\nVirtualization\n\n\n\n\nlibvirt: virsh and more\n\n\nQEMU\n\n\nCheck my \nnotes\n\n\n\n\n\n\nFirecracker\n\n\nrust-vmm\n\n\ncloud-hypervisor\n\n\n\n\nCompilers\n\n\n\n\nGNU GCC\n\n\nClang, LLVM, in C++\n\n\nThis is a collection of projects. Clang is the frontend,\ncompiles C/C++ code into LLVM\ns own IR format.\nThe the backend LLVM will take multiple Passes to optimize\nthe IR and the finally generate the assembly.\n\n\nThe beauty of Clang and LLVM is that they can be used\nas libraries, and we could invoke them to manipulate the\ncompilation results, to do source-to-source transforms,\nmodify Pass\ns IR etc. I found this super interesting!\n\n\n\n\n\n\nRustc, in Rust\n\n\nPHP, in C\n\n\nPython, in C\n\n\nGoogle V8, in C++\n\n\nApple Swift, in C++\n\n\nTCL, in C\n\n\nPerl 5, in C\n\n\n\n\nFirmware\n\n\nI\nm obsessed with firmware projects, maybe because that\ns where I got started.\nFirst it\ns SeaBIOS, the default one used by QEMU. Then UEFI, something I have never used (!).\n\n\n\n\nCoreboot and Libreboot\n\n\nSeaBIOS: the default BIOS used by QEMU\n\n\nqboot: an alternative and lightweight BIOS for QEMU\n\n\nThose are massive hackers, respect.\n\n\nMy experience about BIOS is calling them while the kernel (LegoOS) is running at 16-bit.\n  BIOS \nis\n the OS for a just-booted kernel. I remember the lower 1MB is never cleared,\n  maybe we could invoke the BIOS at 32 or 64-bit mode?\n\n\n\n\n\n\nUEFI\n\n\nUEFI EDK II \n\n\nEDK II is a firmware development environment for the UEFI and UEFI Platform Initialization (PI) specifications\n\n\nPart of the \nTianoCore\n project, an open-source UEFI platform\n\n\nThe Unified Extensible Firmware Interface (UEFI) is a specification that\n  defines a software interface between an operating system and platform firmware.\n  UEFI is designed to replace the Basic Input/Output System (BIOS) firmware interface.\n\n\nOVMF\n: OVMF is an EDK II based project to enable UEFI support for Virtual Machines. OVMF contains sample UEFI firmware for QEMU and KVM.\n\n\n\n\n\n\nMicrosoft Project Mu, a separate fork of EDK II\n\n\nProject Mu is a modular adaptation of TianoCore\ns edk2 tuned for building\nmodern devices using a scalable, maintainable, and reusable pattern\n\n\nIt\ns homepage explains the motivation behind it.\n\n\n\n\n\n\nA book: \nBeyond BIOS Developing with the Unified Extensible Firmware Interface\n.\n\n\n\n\n\n\nThen boot loaders such as GRUB and U-Boot\n\n\n\n\nThe open-source firmware landscape:\n\n\n\nFPGA\n\n\n\n\nMy own Collection\n\n\nMy own Paper Readings\n\n\nPartial Reconfiguration\n\n\nPartial Reconfiguration Building Framework\n\n\nIntepret Xilinx Bitstream\n\n\nHLS-based \nICAP\n Controller\n\n\n\n\n\n\nNetwork\n\n\nCorundum: an FPGA-based NIC\n\n\nThis is THE BEST network stack out there.\n\n\nThis is not simply a network stack, it is a NIC.\n\n\nSo what makes a NIC? First, PHY and MAC are basic.\nSecond, PCIe connection between host and board.\nThird, DMA using PCIe, for TX and RX packets between host and board.\nFourth, a host NIC driver;\nFifth, some opt modules at NIC.\n\n\nThis project has it all. Most amazingly, it works on so many boards.\n\n\nThey have an FCCM\n20 paper (finally!) describing the small modules inside.\n\n\n\n\n\n\nVerilog-Ethernet\n\n\nSelf-made PHY, MAC IPs, ARP, IP, UDP stack\n\n\nThis is also used by the Corundum project.\n\n\n\n\n\n\nLimago, HLS-based 100 GbE TCP/IP\n\n\nFPGA Network Stack\n\n\nThis one came from ETH as well.\n\n\nThis one is used by many papers, as far as i know, StRoM, EuroSys\n20.\n\n\nIt\ns mostly HLS-based. And has ETH/IP/UDP/TCP, RoCE v2 stack.\n\n\n\n\n\n\n\n\n\n\nSimulation, Synthesis, and P\nR\n\n\nIcarus iverilog\n.\n  iverilog is a compiler that translates Verilog source code into\n  executable programs for simulation, or other netlist formats for further processing \nman page\n.\n\n\nVMware Cascade\n.\n  Just-in-time compilation for Verilog, what a brilliant idea.\n\n\nVerilog-to-routing\n.\n\n\nSynthesis (\nODIN II\n)\n\n\nLogic Optimization \n Technology Mapping (\nABC\n)\n\n\nPlacement and Route (\nVPR\n)\n\n\n\n\n\n\n\n\n\n\n\n\nGraphics\n\n\nAs of today Apr 26, 2020, I sort of understand how evdev, X/Wayland, GTK/QT, and GNOME/KDE\nlayers on top of each other, and how they interact with each other.\nBut I still don\nt know how VNC comes into the picture. Let\ns figure out sometime!\nAnd draw a picture like I did for DPDK/libibverbs.\nAs of today May 1, 2020, I now know how VNC fits into the big picture.\n\n\n\n\nX Server\n and \nWayland\n\n\nX is being replaced by Wayland now..\n\n\nWayland code seems clean\n\n\n\n\n\n\nxvnc\n\n\nxvnc and its friends, are sitting on top of display manager (i.e., X/Wayland).\n  They are clients of X/Wayland, but they act as X/Wayland servers for upper layer\n  application such as GTK/Qt.\n\n\nIt\ns a middleman, bringing network between X and GTK.\n\n\nTigerVNC, TurboVNC and so on.\n\n\n\n\n\n\nGNOME Shell and GTK\n\n\nGTK\ns default backend is X.\n\n\nGNOME shell is a layer on top of GTK+. Similar for KDE/Qt.\n\n\n\n\n\n\nxRDP, an RDP server. In C\n\n\nFreeRDP, client and server. In C\n\n\nTook a brief read of the code, it\ns super neat. Should take a serious look sometime.\n\n\n\n\n\n\n\n\nThis figure shows the software landscape:\n\n\n\nWeb Servers\n\n\n\n\nApache httpd\n\n\nnginx\n\n\n\n\nKey Value Stores\n\n\nPoint of interests:\n1) in-memory, and can it extend to use disk/ssd?\n2) persistence support\n3) network support\n\n\n\n\nRocksDB: A persistent KVS for Flash and RAM Storage. C++\n\n\nLevelDB. C++\n\n\nMemcached. C\n\n\nRedis. C\n\n\netcd: Distributed reliable KVS. Go\n\n\n\n\nDatabases\n\n\n\n\nMySQL\n\n\nPostgresSQL\n\n\nYugabyte, distributed SQL\n\n\n\n\nRDMA and More\n\n\n\n\nRPC\n\n\ngRPC\n\n\neRPC, NSDI\n19\n\n\n\n\n\n\nMellanox libvma\n\n\nAn userspace IB verbs based layer providing POSIX socket APIs.\n  In other words, a library like SocketDirect, SIGCOMM\n19.\n\n\n\n\n\n\nverbs perftest\n\n\nThe collection contains a set of bandwidth and latency benchmark such as:\n\n\nSend        - \nib_send_bw\n and \nib_send_lat\n\n\nRDMA Read   - \nib_read_bw\n and \nib_read_lat\n\n\nRDMA Write  - \nib_write_bw\n and \nib_wriet_lat\n\n\nRDMA Atomic - \nib_atomic_bw\n and \nib_atomic_lat\n\n\nNative Ethernet (when working with MOFED2) - \nraw_ethernet_bw\n, \nraw_ethernet_lat\n\n\n\n\n\n\nrdma-core\n\n\nUserspace IB verbs library (e.g., libibverbs)\n\n\nLearn how userspace IB layer communicate with kernel, but also bypass kernel.\n  The technique replies on \nioctl()\n and \nmmap()\n, standard.\n  But the ABI interface (i.e., data structures) are quite complex.\n\n\nlibibverbs/example\n\n\nasyncwatch.c\n\n\ndevice_list.c\n\n\ndevinfo.c\n\n\npingpong.c\n\n\nrc_pingpong.c\n\n\nsrq_pingpong.c\n\n\nuc_pingpong.c\n\n\nud_pingpong.c\n\n\nxsrq_pingpong.c\n\n\n\n\n\n\ninfiniband-diags\n\n\nibv_devinfo    \n\n\niblinkinfo    \n\n\nibping    \n\n\nibaddr\n\n\n\n\n\n\nKernel Infiniband stack\n\n\n\n\n\n\nDPDK\n\n\nDPDK uses VFIO to directly access physical device.\nJust like how we directly assign device to guest OS in QEMU.\n\n\nEven though both DPDK and RDMA bypass kernel, their control\npath is very different. For DPDK, there is a complete device\ndriver in the user space, and this driver communicate with the device via MMIO.\nAfter VFIO ioctls, all data and control path bypass kernel.\nFor rdma-core, a lot control-path IB verbs (e.g., create_pd, create_cq) communicate with kernel via Infiniband device file ioctl.\nAnd you can see all those uverb hanlders in \ndrivers/infiniband/core/uverbs.c\n\nThose control verbs will mmap some pages between user and kernel,\nso all following datapath IB verbs (e.g., post_send) will just bypass kernel\nand talk to device MMIO directly. Although rdma-core also has some vendor-specific\n\ndrivers\n, but this is really different from the above DPDK\ns userspace PCIe driver, per se.\nUserspace \nrdma-core\n vendor-driver deals with the kernel devel vendor-level driver details.\n\n\nFWIW, if you are using a Mellanox VPI card in Ethernet mode (e.g. CX3-5),\n  DPDK will use its built-in mlx driver, which further use libibverbs,\n  which further relies on kernel IB stack. It\ns not a complete user solution somehow.\n  Note that DPDK built-in mlx driver uses RAW_PACKET QPs.", 
            "title": "Open-Source-Code-Study"
        }, 
        {
            "location": "/notes/source_code/#source-code-study", 
            "text": "Version History Date Description Apr 26, 2020 Add wayland, X, gnome, gtk etc Apr 10, 2020 add graphics section Apr 6, 2020 add verbs perftes Mar 3, 2020 add FreeBSD, some fpga stuff Feb 4, 2020 add io_uring, firecracker Jan 31, 2020 Add some good stuff Jan 18, 2020 Initial Beautiful code is art.\nRecently I started forking good open source code\ninto my own Github account and started casual reading and taking notes.\nThis page links to all of them.  Happy hacking!", 
            "title": "Source Code Study"
        }, 
        {
            "location": "/notes/source_code/#misc", 
            "text": "Projects supporting our day-to-day work.   GNU glibc: libc, elf, and dynamic linker  It is the default C library used by almost everyone  It includes  ld.so , the dynamic linker  I wrote some notes about GOT/PLT and explains what has happend before main() is called.    GNU binutils: gas, static linker, and more  This repo has a lot commands like  as ,  ld ,  objdump ,  nm  and so on  ld  is static linker and I like the magic of its linker script  I guess another useful repo is  elfutils    strace  System call tracer at userspace  I ve designed  one  for LegoOS in kernel space    Unix Commands  Of course almost all other listed repos in this section have some sort of commands.\n  But they are not essential. The following repos have the essential UNIX commands like ls, cat.\n  It s not possible to go through all of them. But rather, I think they serve as references\n  when we want to know how certain things are implemented (e.g., how dmesg get kernel log).  BusyBox  GNU Coreutils  util-linux  FreeBSD and its friends    Tools  tmux  git    Editors  vim  neovim    Network  iperf3  is a TCP, UDP, and SCTP network bandwidth measurement tool  tcpdump  OpenSSH  is our ssh!  scapy : Python-based interactive packet manipulation program   library. Very neat  Also checkout  FreeBSD  as it has tools like  ifconfig ,  if .     C for life  Some small and useful C projects  cJSON  A lightweight JSON parser in C.  I think iperf3 is using it.      Outliers  CRIU: Checkpoint and Restore in Userspace  The reason I love this repo is because it has so many interesting pieces\n  on how to interact with kernel, save states, and restore them. In addition,\n  it shows how to properly use many less well known syscalls.    GRUB2: bootloader  Learn how modern bootloader works.  Detailed analysis of Linux booting sequence (how it transit from\n  real-mode to protected mode, and finally to 64-bit mode,\n  how to navigate Linux source code etc.)    FFmpeg  FFmpeg project is famous for its clean and neat C code.  Besides, this project is used by a lot online video service companies    io_uring  user liburing  kernel io_uring.c", 
            "title": "Misc"
        }, 
        {
            "location": "/notes/source_code/#operating-systems", 
            "text": "Linux 0.0.1  Plan 9 OS  MSR Singularity.  illumos \n        - This is a fork of Oracle Solaris OS.  seL4 Microkernel  MacOS Darwin  BSD: these repos have everything you can think of  If you ever wondered how X is done, how to get Y from OS, this is where you look into.  FreeBSD  OpenBSD  NetBSD  TrueOS    Unikernel  OSv. A lightweight unikernel.  IncludeOS  Rumprun  Solo5. Unikernel as processes!     \n(Image source:  https://commons.wikimedia.org/wiki/File:Unix_timeline.en.svg )", 
            "title": "Operating Systems"
        }, 
        {
            "location": "/notes/source_code/#virtualization", 
            "text": "libvirt: virsh and more  QEMU  Check my  notes    Firecracker  rust-vmm  cloud-hypervisor", 
            "title": "Virtualization"
        }, 
        {
            "location": "/notes/source_code/#compilers", 
            "text": "GNU GCC  Clang, LLVM, in C++  This is a collection of projects. Clang is the frontend,\ncompiles C/C++ code into LLVM s own IR format.\nThe the backend LLVM will take multiple Passes to optimize\nthe IR and the finally generate the assembly.  The beauty of Clang and LLVM is that they can be used\nas libraries, and we could invoke them to manipulate the\ncompilation results, to do source-to-source transforms,\nmodify Pass s IR etc. I found this super interesting!    Rustc, in Rust  PHP, in C  Python, in C  Google V8, in C++  Apple Swift, in C++  TCL, in C  Perl 5, in C", 
            "title": "Compilers"
        }, 
        {
            "location": "/notes/source_code/#firmware", 
            "text": "I m obsessed with firmware projects, maybe because that s where I got started.\nFirst it s SeaBIOS, the default one used by QEMU. Then UEFI, something I have never used (!).   Coreboot and Libreboot  SeaBIOS: the default BIOS used by QEMU  qboot: an alternative and lightweight BIOS for QEMU  Those are massive hackers, respect.  My experience about BIOS is calling them while the kernel (LegoOS) is running at 16-bit.\n  BIOS  is  the OS for a just-booted kernel. I remember the lower 1MB is never cleared,\n  maybe we could invoke the BIOS at 32 or 64-bit mode?    UEFI  UEFI EDK II   EDK II is a firmware development environment for the UEFI and UEFI Platform Initialization (PI) specifications  Part of the  TianoCore  project, an open-source UEFI platform  The Unified Extensible Firmware Interface (UEFI) is a specification that\n  defines a software interface between an operating system and platform firmware.\n  UEFI is designed to replace the Basic Input/Output System (BIOS) firmware interface.  OVMF : OVMF is an EDK II based project to enable UEFI support for Virtual Machines. OVMF contains sample UEFI firmware for QEMU and KVM.    Microsoft Project Mu, a separate fork of EDK II  Project Mu is a modular adaptation of TianoCore s edk2 tuned for building\nmodern devices using a scalable, maintainable, and reusable pattern  It s homepage explains the motivation behind it.    A book:  Beyond BIOS Developing with the Unified Extensible Firmware Interface .    Then boot loaders such as GRUB and U-Boot   The open-source firmware landscape:", 
            "title": "Firmware"
        }, 
        {
            "location": "/notes/source_code/#fpga", 
            "text": "My own Collection  My own Paper Readings  Partial Reconfiguration  Partial Reconfiguration Building Framework  Intepret Xilinx Bitstream  HLS-based  ICAP  Controller    Network  Corundum: an FPGA-based NIC  This is THE BEST network stack out there.  This is not simply a network stack, it is a NIC.  So what makes a NIC? First, PHY and MAC are basic.\nSecond, PCIe connection between host and board.\nThird, DMA using PCIe, for TX and RX packets between host and board.\nFourth, a host NIC driver;\nFifth, some opt modules at NIC.  This project has it all. Most amazingly, it works on so many boards.  They have an FCCM 20 paper (finally!) describing the small modules inside.    Verilog-Ethernet  Self-made PHY, MAC IPs, ARP, IP, UDP stack  This is also used by the Corundum project.    Limago, HLS-based 100 GbE TCP/IP  FPGA Network Stack  This one came from ETH as well.  This one is used by many papers, as far as i know, StRoM, EuroSys 20.  It s mostly HLS-based. And has ETH/IP/UDP/TCP, RoCE v2 stack.      Simulation, Synthesis, and P R  Icarus iverilog .\n  iverilog is a compiler that translates Verilog source code into\n  executable programs for simulation, or other netlist formats for further processing  man page .  VMware Cascade .\n  Just-in-time compilation for Verilog, what a brilliant idea.  Verilog-to-routing .  Synthesis ( ODIN II )  Logic Optimization   Technology Mapping ( ABC )  Placement and Route ( VPR )", 
            "title": "FPGA"
        }, 
        {
            "location": "/notes/source_code/#graphics", 
            "text": "As of today Apr 26, 2020, I sort of understand how evdev, X/Wayland, GTK/QT, and GNOME/KDE\nlayers on top of each other, and how they interact with each other.\nBut I still don t know how VNC comes into the picture. Let s figure out sometime!\nAnd draw a picture like I did for DPDK/libibverbs.\nAs of today May 1, 2020, I now know how VNC fits into the big picture.   X Server  and  Wayland  X is being replaced by Wayland now..  Wayland code seems clean    xvnc  xvnc and its friends, are sitting on top of display manager (i.e., X/Wayland).\n  They are clients of X/Wayland, but they act as X/Wayland servers for upper layer\n  application such as GTK/Qt.  It s a middleman, bringing network between X and GTK.  TigerVNC, TurboVNC and so on.    GNOME Shell and GTK  GTK s default backend is X.  GNOME shell is a layer on top of GTK+. Similar for KDE/Qt.    xRDP, an RDP server. In C  FreeRDP, client and server. In C  Took a brief read of the code, it s super neat. Should take a serious look sometime.     This figure shows the software landscape:", 
            "title": "Graphics"
        }, 
        {
            "location": "/notes/source_code/#web-servers", 
            "text": "Apache httpd  nginx", 
            "title": "Web Servers"
        }, 
        {
            "location": "/notes/source_code/#key-value-stores", 
            "text": "Point of interests:\n1) in-memory, and can it extend to use disk/ssd?\n2) persistence support\n3) network support   RocksDB: A persistent KVS for Flash and RAM Storage. C++  LevelDB. C++  Memcached. C  Redis. C  etcd: Distributed reliable KVS. Go", 
            "title": "Key Value Stores"
        }, 
        {
            "location": "/notes/source_code/#databases", 
            "text": "MySQL  PostgresSQL  Yugabyte, distributed SQL", 
            "title": "Databases"
        }, 
        {
            "location": "/notes/source_code/#rdma-and-more", 
            "text": "RPC  gRPC  eRPC, NSDI 19    Mellanox libvma  An userspace IB verbs based layer providing POSIX socket APIs.\n  In other words, a library like SocketDirect, SIGCOMM 19.    verbs perftest  The collection contains a set of bandwidth and latency benchmark such as:  Send        -  ib_send_bw  and  ib_send_lat  RDMA Read   -  ib_read_bw  and  ib_read_lat  RDMA Write  -  ib_write_bw  and  ib_wriet_lat  RDMA Atomic -  ib_atomic_bw  and  ib_atomic_lat  Native Ethernet (when working with MOFED2) -  raw_ethernet_bw ,  raw_ethernet_lat    rdma-core  Userspace IB verbs library (e.g., libibverbs)  Learn how userspace IB layer communicate with kernel, but also bypass kernel.\n  The technique replies on  ioctl()  and  mmap() , standard.\n  But the ABI interface (i.e., data structures) are quite complex.  libibverbs/example  asyncwatch.c  device_list.c  devinfo.c  pingpong.c  rc_pingpong.c  srq_pingpong.c  uc_pingpong.c  ud_pingpong.c  xsrq_pingpong.c    infiniband-diags  ibv_devinfo      iblinkinfo      ibping      ibaddr    Kernel Infiniband stack    DPDK  DPDK uses VFIO to directly access physical device.\nJust like how we directly assign device to guest OS in QEMU.  Even though both DPDK and RDMA bypass kernel, their control\npath is very different. For DPDK, there is a complete device\ndriver in the user space, and this driver communicate with the device via MMIO.\nAfter VFIO ioctls, all data and control path bypass kernel.\nFor rdma-core, a lot control-path IB verbs (e.g., create_pd, create_cq) communicate with kernel via Infiniband device file ioctl.\nAnd you can see all those uverb hanlders in  drivers/infiniband/core/uverbs.c \nThose control verbs will mmap some pages between user and kernel,\nso all following datapath IB verbs (e.g., post_send) will just bypass kernel\nand talk to device MMIO directly. Although rdma-core also has some vendor-specific drivers , but this is really different from the above DPDK s userspace PCIe driver, per se.\nUserspace  rdma-core  vendor-driver deals with the kernel devel vendor-level driver details.  FWIW, if you are using a Mellanox VPI card in Ethernet mode (e.g. CX3-5),\n  DPDK will use its built-in mlx driver, which further use libibverbs,\n  which further relies on kernel IB stack. It s not a complete user solution somehow.\n  Note that DPDK built-in mlx driver uses RAW_PACKET QPs.", 
            "title": "RDMA and More"
        }, 
        {
            "location": "/notes/program_advice/", 
            "text": "Programming and Writing Advice\n\n\nVersion History\nDate\nDescription\nMar 28, 2020\nStarted.\nFreeBSD\n\n\n\n\nQuote\n\n\nSource\n.\nOur ideology can be described by the following guidelines:\n\n\n\n\nDo not add new functionality unless an implementor cannot complete a real application without it.\n\n\nIt is as important to decide what a system is not as to decide what it is. Do not serve all the world\ns needs; rather, make the system extensible so that additional needs can be met in an upwardly compatible fashion.\n\n\nThe only thing worse than generalizing from one example is generalizing from no examples at all.\n\n\nIf a problem is not completely understood, it is probably best to provide no solution at all.\n\n\nIf you can get 90 percent of the desired effect for 10 percent of the work, use the simpler solution.\n\n\nIsolate complexity as much as possible.\n\n\nProvide mechanism, rather than policy. In particular, place user interface policy in the client\ns hands.\n\n\n\n\nFrom Scheifler \n Gettys: \nX Window System\n\n\n\n\nProf. John Ousterhout\ns Favorite Sayings\n\n\n\n\nQuote\n\n\nSource\n:\n\n\n\n\nThe greatest performance improvement of all is when a system goes from not-working to working\n\n\nUse your intuition to ask questions, not to answer them\n\n\nThe most important component of evolution is death\n\n\nFacts precede concepts\n\n\nIf you don\nt know what the problem was, you haven\nt fixed it\n\n\nIf it hasn\nt been used, it doesn\nt work\n\n\nThe only thing worse than a problem that happens all the time is a problem that doesn\nt happen all the time\n\n\nThe three most powerful words for building credibility are \nI don\nt know\n\n\nCoherent systems are inherently unstable\n\n\n\n\n\n\nButler W. Lampson\ns Hints for Computer System Design\n\n\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf\n\n\nPerformance Evaluation\n\n\n\n\nSystems Benchmarking Crimes\n\n\nTim Harris: designing experiments for understanding performance\n\n\nVery practical and useful suggestions.\n\n\n\n\n\n\n\n\nWriting Tips\n\n\n\n\nPlain Writing Act of 2010, \nFederal Plain Language Guidelines.\n.\n\n\nDash Writing Tips", 
            "title": "Programming-Guidelines-and-Advice"
        }, 
        {
            "location": "/notes/program_advice/#programming-and-writing-advice", 
            "text": "Version History Date Description Mar 28, 2020 Started.", 
            "title": "Programming and Writing Advice"
        }, 
        {
            "location": "/notes/program_advice/#freebsd", 
            "text": "Quote  Source .\nOur ideology can be described by the following guidelines:   Do not add new functionality unless an implementor cannot complete a real application without it.  It is as important to decide what a system is not as to decide what it is. Do not serve all the world s needs; rather, make the system extensible so that additional needs can be met in an upwardly compatible fashion.  The only thing worse than generalizing from one example is generalizing from no examples at all.  If a problem is not completely understood, it is probably best to provide no solution at all.  If you can get 90 percent of the desired effect for 10 percent of the work, use the simpler solution.  Isolate complexity as much as possible.  Provide mechanism, rather than policy. In particular, place user interface policy in the client s hands.   From Scheifler   Gettys:  X Window System", 
            "title": "FreeBSD"
        }, 
        {
            "location": "/notes/program_advice/#prof-john-ousterhouts-favorite-sayings", 
            "text": "Quote  Source :   The greatest performance improvement of all is when a system goes from not-working to working  Use your intuition to ask questions, not to answer them  The most important component of evolution is death  Facts precede concepts  If you don t know what the problem was, you haven t fixed it  If it hasn t been used, it doesn t work  The only thing worse than a problem that happens all the time is a problem that doesn t happen all the time  The three most powerful words for building credibility are  I don t know  Coherent systems are inherently unstable", 
            "title": "Prof. John Ousterhout's Favorite Sayings"
        }, 
        {
            "location": "/notes/program_advice/#butler-w-lampsons-hints-for-computer-system-design", 
            "text": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf", 
            "title": "Butler W. Lampson's Hints for Computer System Design"
        }, 
        {
            "location": "/notes/program_advice/#performance-evaluation", 
            "text": "Systems Benchmarking Crimes  Tim Harris: designing experiments for understanding performance  Very practical and useful suggestions.", 
            "title": "Performance Evaluation"
        }, 
        {
            "location": "/notes/program_advice/#writing-tips", 
            "text": "Plain Writing Act of 2010,  Federal Plain Language Guidelines. .  Dash Writing Tips", 
            "title": "Writing Tips"
        }, 
        {
            "location": "/notes/virt/", 
            "text": "Notes about Virtualization\n\n\nVersion History\nDate\nDescription\nFeb 4, 2020\nAdd VFIO stuff\nJan 26, 2020\nMinor adjustment\nJan 25, 2020\nInitial Document\n\n\nWe read the source code of QEMU and KVM, try to understand their interactions and the function flow.\nEnd of the day, you should be able to map your knowledge onto real codes.\nThe document was orginally written in a Google Document, the following presentation\nis just an embedded version.\nFor better readibility, you can also check out the:\n\n\n\n\nGoogle Doc Version\n\n\nPDF Version", 
            "title": "Virtualization-Software"
        }, 
        {
            "location": "/notes/virt/#notes-about-virtualization", 
            "text": "Version History Date Description Feb 4, 2020 Add VFIO stuff Jan 26, 2020 Minor adjustment Jan 25, 2020 Initial Document  We read the source code of QEMU and KVM, try to understand their interactions and the function flow.\nEnd of the day, you should be able to map your knowledge onto real codes.\nThe document was orginally written in a Google Document, the following presentation\nis just an embedded version.\nFor better readibility, you can also check out the:   Google Doc Version  PDF Version", 
            "title": "Notes about Virtualization"
        }, 
        {
            "location": "/notes/cache_coherence/", 
            "text": "Practical Cache Coherence Implementation\n\n\nVersion History\nDate\nDescription\nFeb 24, 2020\nKobe and Gigi. Add Intel CCIP.\nOct 3, 2019\nAdd FPGA related discussion\nJun 28, 2019\nInitial draft\n\n\n\n\nPractical Cache Coherence\n\n\nSummary and Thoughs\n\n\nReadings\n\n\nMisc Facts\n\n\nCase Study\n\n\nIntel\n\n\nAMD\n\n\nARM\n\n\nOpenCAPI and CCIX\n\n\nOpenPiton\n\n\nFPGA\n\n\n\n\n\n\n\n\n\n\n\n\nA general collection of resources on cache coherence.\nI started this when I was having a hard time optimizing lock delegation.\nThis note is not about acadamic new ideas, but rather for\na concrete understanding of current cache coherence implementations.\n\n\nSummary and Thoughs\n\n\n\n\nThe textbooks tough us the basic concept of MESI. And realizations\n  like snoop and directory. But what usually missing is the implementation\n  details when it comes to: 1) conflicts, 2) no single shared bus.\n\n\nModern processors have Network-on-Chip (NoC). Cores, cache slices,\n  and memory controllers are connected via an on-chip network.\n  The model is no different from a datacenter cluster connected by real network.\n\n\nCache requests generated by MESI protocols should appear \natomic\n to cores.\n  Given the distributed nature of all resources, those cache requests\n  will have to be implemented like \ndistributed transactions\n.\n\n\nFor example, the MESIF is the cache coherence protocol used by Intel.\n  When a read is made to an invalid line, the corresponding cache\n  will perform a \ncache read transaction\n to read the data from\n  either other caches or memory. This transaction consists multiple\n  steps such as: send requests, collect responses, send ACKs.\n\n\nThose transactions will conflict if multiple reads and writes\n  happen at the same time. Someone has to resolve it.\n  It can be resolved by different cache controllers, or by a single\n  serialization point like home agent.\n\n\nJust like you can have many ways to implement transactions\n  for distributed systems, there are also many ways to do\n  cache coherence transactions. And there are many.\n\n\nAtomic Read-Modify-Write (RMW) instructions will make cache coherence\n  implementations even more complex. Those instructions include\n  \nread-and-inc\n, \ntest-and-set\n, and \nlock;\n-prefixed.\n  I think, there will some \nlock the bus\n, or \nlocked state\n at the\n  home agent per cache line. Having atomic RMW instructions\n  will add more complexity to the overall transaction design.\n\n\nWhile reading Intel related cache coherence diagrams/transactions,\n  you might find many different descriptions. Don\nt panic. They are\n  just different implementations proposed by Intel. Different\n  implementations will have different trade-offs and performance,\n  you can check \nFrank\ns post\n\n  for more details.\n\n\nDirectory-based cache coherence protocol and implementation will\n  be the future for multicore machines. Because it incurs much less\n  coherence traffic than snoop-based ones, thus more scalable.\n  The trend is confirmed by recent Intel UPI directory-based approach.\n  Related readings:\n\n\n[1]: \nWhy On-Chip Cache Coherence Is Here to Stay\n\n\n[2]: \nQPI 1.1 Invovled\n\n\n[3]: \nPaper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA \n99\n\n\n[4]: \nPaper: Using Destination-Set Prediction to Improve the Latency/Bandwidth Tradeoff in Shared-Memory Multiprocessors, ISCA\n03\n\n\n[5]: The trade-off: \n\n\n\n\n\n\n\n\nLeft questions:\n- Do cache coherence implementations ensure \nfairness\n among cores?\n\n\nReadings\n\n\n\n\nThe Architecture of the Nehalem Processor and Nehalem-EP SMP Platforms\n, chapter 5.2 Cache-Coherence Protocol for Multi-Processors.\n\n\nIntel: Performance Analysis Guide for Intel\u00ae Core\u2122 i7 Processor and Intel\u00ae Xeon\u2122 5500 processors\n\n\nBlog: NUMA Deep Dive Part 3: Cache Coherency\n\n\nBy far the BEST blog I\nve seen on the topic of \nIntel snoop models\n. Frank\ns other articles are also amazing.\n\n\nIntel is using MESIF cache coherence protocol, but it has multiple cache coherence implementations.\n  The first one is \nSource Snoop\n (or \nEarly Snoop\n), which is more like a traditional snoop-based\n  cache coherence implementation. Upon miss, the caching agent will broadcast to other agents.\n  The second one is \nHome Snoop\n, which is more like a directory-based cache coherence implementation.\n  Upon miss, the caching agent will contact home agent, and then the home agent will send requests\n  to other caching agents who have the requested cache line.\n  There are other implementations like Cluster-on-Die.\n  Intel UPI gets rid of all this complexity, it is only using directory-based, in the hope to reduce\n  cache coherence traffic, which make sense.\n\n\nRelated: \nBroadwell EP Snoop Models\n\n\nRelated: \nSkylay UPI\n\n\n\n\n\n\nPaper: MESIF: A Two-Hop Cache Coherency Protocol for Point-to-Point Interconnects (2009)__\n\n\nA MUST read.\n\n\nThis paper has the most extensive description of the MESIF protocol implementation.\n  It has many \ntiming diagrams\n than describe how cache requests actually proceed.\n  Those diagrams can help us understand what is needed to finish a cache request.\n\n\nTheir \nslides\n\n  has more timing diagrams.\n\n\nBut do note: the implementation described by this paper is different from\n  what \nIntel QPI\n\n  has in products. The difference is discussed at chapter 4. MESIF and QPI, namely,\n  other caching agents will send responses to Home agent rather than to requesting agent.\n  QPI relies on Home agent to solve conflict.\n\n\nAlso note: this is just one of the possible implementations to realize MESIF protocol.\n  There could be many other ways, e.g., QPI source snooping, QPI home snooping.\n  But all of them share the essential and general concepts and ideas.\n\n\n\n\n\n\nAppendix I: Large-Scale Multiprocessors and Scientific Applications\n,\n  chapter 7 Implementing Cache Coherence.\n\n\nThis is probably some most insightful discussion about real implementation of cache coherence.\n  With the distributed nature and Network-on-Chip, implementing cache coherence in modern\n  processors is no different than implementing a distributed transaction protocol.\n\n\nCache activities like read miss or write miss have multi-step operations, but they\n  need to appear as \natomic\n to users. Put in another way, misses are like transactions,\n  they have multiple steps but they must be atomic. They can be retried.\n\n\nHaving directory for cache coherence will make implementation easier. Because\n  the place (e.g., L3) where directory resides can serve as the serialization point.\n  They can solve write races.\n\n\nHome directory controller\n and \ncache controller\n will exchange messages like a set of distributed machines.\n  In fact, with NoC, they are actually distributed system.\n\n\n\n\n\n\nIntel: An Introduction to the Intel\u00ae QuickPath Interconnect\n,\n  page 15 MESIF.\n\n\nHotChips slide\n, has timing diagrams.\n\n\nIt explains the \nHome Snoop\n and \nSource Snoop\n used by Intel.\n\n\nBased on their explanation, it seems both \nHome Snoop\n and \nSource Snoop\n are using a combination of\n    snoop and directory. The Processor#4 (pg 17 and 18) maintains the directory.\n\n\nAnd this is a perfect demonstration of the details described in \nAppendix I: Large-Scale Multiprocessors and Scientific Applications\n.\n\n\nRelated patent: \nExtending a cache coherency snoop broadcast protocol with directory information\n\n\n\n\n\n\nPaper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA \n99\n\n\nA hybrid snoop and directory cache coherence implementation. The insight is snoop\n  cause too much bandwidth, directory incurs longer latency.\n\n\nSo this paper proposed \nMulticast snoop\n, where it multicasts coherence transactions\n  to selected processors, lowering the address bandwidth required for snooping.\n\n\n\n\n\n\nPaper: Why On-Chip Cache Coherence Is Here to Stay, Communications of ACM\n02\n\n\nThis paper discusses why cache coherence can scale. A nice read.\n\n\nR1: Coherence\u2019s interconnection network traffic per miss scales\n      when precisely tracking sharers. (Okay increased directory bits,\n  what about those storage cost? See R2).\n\n\nR2: Hierarchy combined with inclusion enables efficient scaling\n      of the storage cost for exact encoding of sharers.\n\n\nR3: private evictions should send explicit messages to shared cache\n      to enable precise tracking. Thus the recall (\nback invalidation\n) traffic can be\n  reduced when shared cache is evicting (assume inclusion cache).\n\n\nR4: Latencies of cache request can be amortized.\n\n\n\n\n\n\nBook: Parallel Computer Organization and Design\n, Chapter 7.\n\n\nLinks coherence and consistency together. This chapter uses detailed graphs to show\n  how different cache coherence implementations affect consistency.\n\n\n\n\n\n\nBook: A Primer on Memory Consistency and Cache Coherence\n\n\nBest book for this topic.\n\n\n\n\n\n\nDr.Bandwidth on explaining core-to-core communication transactions!\n\n\nSeriously, it\ns so good!\n\n\nAlthough, I just feel there are so many unpublished details about the exact coherence transactions.\n  Dr.Bandwidth himself used a lot \nmaybe\n, and listed possible actions.\n\n\n\n\n\n\nTransactional Memory Coherence and Consistency, ISCA\n04\n\n\nProgramming with Transactional Coherence and Consistency (TCC)\n\n\nSlide1\n\n\nAwarded the most influential paper at ISCA 2019. I took a read today (Jul 21, 2019).\n\n\nI feels like it\ns using the \nbatch\n optimization for all time. The TCC design,\n  kind of combines both cache coherence and memory consistency: how transactions\n  commit or orders, determins the coherence and consistency.\n\n\nIt seems the load/store speculative execution used in their context is so similar\n  to what Dr.Bandwidth said about Intel\ns implementation. Basically, the processor\n  might read some data from L1/L2 and continue execution, but there is a chance,\n  that the data is modifed by others, and the L3 caching agent or home agent\n  could decide to revoke it. Once receiving such revoke message,\n  the processor must cancel all executions that use the speculatively read data. \n\n\nIt mentions couple Thread-Level Speculation papers, I think they should on this topic.\n\n\n\n\n\n\n\n\nMisc Facts\n\n\n\n\nIntel Caching Agent (Cbox) is per core (or per LLC slice). Intel Home Agent is per memory controller.\n\n\nThe LLC coherence engine (CBo) manages the interface between the core and the last\nlevel cache (LLC). All core transactions that access the LLC are directed from the core\nto a CBo via the ring interconnect. The CBo is responsible for managing data delivery\nfrom the LLC to the requesting core. It is also responsible for maintaining coherence\nbetween the cores within the socket that share the LLC; generating snoops and\ncollecting snoop responses from the local cores when the MESIF protocol requires it.\n\n\nEvery physical memory address in the system is uniquely associated with a single Cbox\n  instance via a proprietary hashing algorithm that is designed to keep the distribution of\n  traffic across the CBox instances relatively uniform for a wide range of possible address patterns.\n\n\nRead more \nhere, chapter 2.3\n.\n\n\nStarting from Intel UPI, Caching Agent and Home Agent are combined as CHA.\n\n\n\n\n\n\nA good \ndiscussion\n about why QPI gradually drop \nSource Snoop\n and solely use \nHome Snoop\n.\n\n\nThe motivation is scalability. It turns out the new UPI only supports directory-based protocol.\n\n\nThis makes sense because 1) inter socket bandwidth is precious, 2) snoop will consume a lot bandwidth.\n\n\n\n\n\n\nIntel UPI is using directory-based home snoop coherency protocol\n\n\nIntel\u00ae Xeon\u00ae Processor Scalable Family Technical Overview\n\n\n\n\n\n\nTo provide sufficient bandwidth, shared caches are typically interleaved\n  by addresses with banks physically distributed across the chip.\n\n\n\n\nCase Study\n\n\nIntel\n\n\nIntel does not disclose too much details about their cache coherence implementations.\nThe most valuable information is extracted from uncore PMU manuals, and discussions\nfrom Dr. Bandwidth. According to Dr. Bandwidth, the Intel CPU could dynamically\nadapt its coherence strategy during runtime according to workload. There won\nt\nbe one fixed cache coherence implementation, there will be many. It depends on\nworkload which one is used at runtime.\n\n\nList below might not be completely true. Just my understanding.\n\n\n\n\nPhysical addresses are uniquely hashed into L3 slices. That means each individual\n  physical address belongs to a L3 slice, and also belongs to a home agent.\n\n\nUpon L2 miss, it will send requests to corresponding L3 slice. If the L3 slice\n  is in the local socket, the request can be delievered within the same socket.\n  If the L3 slice belongs to another remote socket, the L2 miss request will\n  be sent over QPI/UPI. Also note that the L2 controller will not send snoop requests.\n  (This is answering the question of \nwhy using local memory is faster than remote\n\n   from the cache coherence perspective.)\n\n\nAt L3, when received the request from a L2,\n\n\nIf it\ns in source snoop model, it will send \nsnoop messages\n to other sockets.\n\n\nIf it\ns in home snoop model, it will send \nread message\n to other sockets.\n  The another socket will generate snoop and collect responses. (R3QPI or home?)\n\n\nQuote Dr. Bandwidth: Maintaining consistency is easier if the data is sent\n  to the L3 first, and then to the requesting core, but it is also possible to\n  send to both at the same time (e.g., \nDirect2Core\n). In recent processors,\n  these return paths are chosen dynamically based on undocumented states and\n  settings of the processor.\n\n\nI\nm not sure who will ACK L2 at last. L3 or home agent? Both are possible.\n\n\n\n\n\n\nI think both L3 and home agent have directory information. They know where\n  to send snoop/read messages. And both of them can serialize coherence transactions!\n  It\ns just undocumented who is doing what.\n\n\nIn generall, we need to bear the fact that we cannot just figure out how Intel\n  cache coherence works underlying. We maybe just need to \nvaguely\n know the fact that:\n\n\nBoth directory and snoop will be used in combination.\n\n\nL3/home agent will serialize conflicting transactions\n\n\nL3/home agent will send data to requesting core\n\n\nL3/home agent will send final ACK to requesting L2\n\n\nA coherence transaction is a multi-step distributed transaction.\n  It involes sending requests, serialize conflicts, receiving responses/ACKs.\n\n\n\n\n\n\n\n\nWhen in doubt, read the \ndiscussion\n posted by Dr. Bandwidth.\n\n\nAMD\n\n\n\n\nAMD HyperTransport Assit for Cache Coherence\n\n\nSlide\n\n\nSlide\n\n\n\n\n\n\n\n\nARM\n\n\n\n\nAMBA CHI Specifications\n\n\nThis is probabaly the most comprehensive document I\nve ever seen about cache coherence.\n  Although terms used by ARM differs from the ones used by Intel, still, you can map them.\n  Chapter 5 Interconnect Protocol Flows has a lot timing diagrams regarding read/write/atomic\n  coherence transactions.\n\n\nIt\ns a good reference to know, but it would be hard to actually understand the details.\n\n\n\n\n\n\n\n\nOpenCAPI and CCIX\n\n\n\n\nCCIX White Paper\n\n\nOpenCAPI\n\n\n\n\nOpenPiton\n\n\n\n\nOpenPiton Microarchitecture Specification\n\n\nDirectory-based MESI\n\n\nThis spec has detailed coherence message packet format and type. Unfortunately,\n  it does not say anything about how they deal with coherence transaction conflicts.\n  E.g., some timeline diagrams like Figrue \n in this \npaper\n.\n\n\n\n\n\n\n\n\nFPGA\n\n\n\n\nAnalysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA Device, FPL\n19\n\n\nLEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM\n14.\n\n\nThis work is built on their earlier work, which basically add the data caching\n  concept to FPGA: using BRAM as L1, on-board DRAM as L2, host or remote DRAM as L3.\n\n\nIn their earlier work, each FPGA application (or bitstream) has a private L1 cache.\n\n\nIn this work, the add MESI coherence to these private L1 caches, as in they can make\n  multiple L1 cache cache-coherent.\n\n\nThe techniques and protocols from this paper are similar to the exisiting ones. For example,\n  1) they use a global serializing point to serialize transactions, 2) they designed a lot\n  messaging types such as INV, RESP and so on.\n\n\n\n\n\n\nVMware Research Project PBerry\n\n\nA very interesting and promising project.\n\n\n\n\n\n\nIntel FPGA PAC\n\n\nIntel itself is building a FPGA-CPU cache coherent setting. They use the Intel UPI interconnect\n  to natually the spectrum. The FPGA shell has some modules to handle this.\n\n\n\n\n\n\n\n\nIntel FPGA CCIP\n\n\n\n\nMaybe the ASPLOS\n20 Optimus paper is uing CCIP-related research platform?\n\n\n\n\n\n\n\n\nAlso some pointer chasing related stuff\n\n\n\n\nA Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA\n16\n\n\n\n\n\n\n\n\nFormal Verification\n\n\nTODO\nFIll me in.", 
            "title": "Practical-Cache-Coherence"
        }, 
        {
            "location": "/notes/cache_coherence/#practical-cache-coherence-implementation", 
            "text": "Version History Date Description Feb 24, 2020 Kobe and Gigi. Add Intel CCIP. Oct 3, 2019 Add FPGA related discussion Jun 28, 2019 Initial draft   Practical Cache Coherence  Summary and Thoughs  Readings  Misc Facts  Case Study  Intel  AMD  ARM  OpenCAPI and CCIX  OpenPiton  FPGA       A general collection of resources on cache coherence.\nI started this when I was having a hard time optimizing lock delegation.\nThis note is not about acadamic new ideas, but rather for\na concrete understanding of current cache coherence implementations.", 
            "title": "Practical Cache Coherence Implementation"
        }, 
        {
            "location": "/notes/cache_coherence/#summary-and-thoughs", 
            "text": "The textbooks tough us the basic concept of MESI. And realizations\n  like snoop and directory. But what usually missing is the implementation\n  details when it comes to: 1) conflicts, 2) no single shared bus.  Modern processors have Network-on-Chip (NoC). Cores, cache slices,\n  and memory controllers are connected via an on-chip network.\n  The model is no different from a datacenter cluster connected by real network.  Cache requests generated by MESI protocols should appear  atomic  to cores.\n  Given the distributed nature of all resources, those cache requests\n  will have to be implemented like  distributed transactions .  For example, the MESIF is the cache coherence protocol used by Intel.\n  When a read is made to an invalid line, the corresponding cache\n  will perform a  cache read transaction  to read the data from\n  either other caches or memory. This transaction consists multiple\n  steps such as: send requests, collect responses, send ACKs.  Those transactions will conflict if multiple reads and writes\n  happen at the same time. Someone has to resolve it.\n  It can be resolved by different cache controllers, or by a single\n  serialization point like home agent.  Just like you can have many ways to implement transactions\n  for distributed systems, there are also many ways to do\n  cache coherence transactions. And there are many.  Atomic Read-Modify-Write (RMW) instructions will make cache coherence\n  implementations even more complex. Those instructions include\n   read-and-inc ,  test-and-set , and  lock; -prefixed.\n  I think, there will some  lock the bus , or  locked state  at the\n  home agent per cache line. Having atomic RMW instructions\n  will add more complexity to the overall transaction design.  While reading Intel related cache coherence diagrams/transactions,\n  you might find many different descriptions. Don t panic. They are\n  just different implementations proposed by Intel. Different\n  implementations will have different trade-offs and performance,\n  you can check  Frank s post \n  for more details.  Directory-based cache coherence protocol and implementation will\n  be the future for multicore machines. Because it incurs much less\n  coherence traffic than snoop-based ones, thus more scalable.\n  The trend is confirmed by recent Intel UPI directory-based approach.\n  Related readings:  [1]:  Why On-Chip Cache Coherence Is Here to Stay  [2]:  QPI 1.1 Invovled  [3]:  Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA  99  [4]:  Paper: Using Destination-Set Prediction to Improve the Latency/Bandwidth Tradeoff in Shared-Memory Multiprocessors, ISCA 03  [5]: The trade-off:      Left questions:\n- Do cache coherence implementations ensure  fairness  among cores?", 
            "title": "Summary and Thoughs"
        }, 
        {
            "location": "/notes/cache_coherence/#readings", 
            "text": "The Architecture of the Nehalem Processor and Nehalem-EP SMP Platforms , chapter 5.2 Cache-Coherence Protocol for Multi-Processors.  Intel: Performance Analysis Guide for Intel\u00ae Core\u2122 i7 Processor and Intel\u00ae Xeon\u2122 5500 processors  Blog: NUMA Deep Dive Part 3: Cache Coherency  By far the BEST blog I ve seen on the topic of  Intel snoop models . Frank s other articles are also amazing.  Intel is using MESIF cache coherence protocol, but it has multiple cache coherence implementations.\n  The first one is  Source Snoop  (or  Early Snoop ), which is more like a traditional snoop-based\n  cache coherence implementation. Upon miss, the caching agent will broadcast to other agents.\n  The second one is  Home Snoop , which is more like a directory-based cache coherence implementation.\n  Upon miss, the caching agent will contact home agent, and then the home agent will send requests\n  to other caching agents who have the requested cache line.\n  There are other implementations like Cluster-on-Die.\n  Intel UPI gets rid of all this complexity, it is only using directory-based, in the hope to reduce\n  cache coherence traffic, which make sense.  Related:  Broadwell EP Snoop Models  Related:  Skylay UPI    Paper: MESIF: A Two-Hop Cache Coherency Protocol for Point-to-Point Interconnects (2009)__  A MUST read.  This paper has the most extensive description of the MESIF protocol implementation.\n  It has many  timing diagrams  than describe how cache requests actually proceed.\n  Those diagrams can help us understand what is needed to finish a cache request.  Their  slides \n  has more timing diagrams.  But do note: the implementation described by this paper is different from\n  what  Intel QPI \n  has in products. The difference is discussed at chapter 4. MESIF and QPI, namely,\n  other caching agents will send responses to Home agent rather than to requesting agent.\n  QPI relies on Home agent to solve conflict.  Also note: this is just one of the possible implementations to realize MESIF protocol.\n  There could be many other ways, e.g., QPI source snooping, QPI home snooping.\n  But all of them share the essential and general concepts and ideas.    Appendix I: Large-Scale Multiprocessors and Scientific Applications ,\n  chapter 7 Implementing Cache Coherence.  This is probably some most insightful discussion about real implementation of cache coherence.\n  With the distributed nature and Network-on-Chip, implementing cache coherence in modern\n  processors is no different than implementing a distributed transaction protocol.  Cache activities like read miss or write miss have multi-step operations, but they\n  need to appear as  atomic  to users. Put in another way, misses are like transactions,\n  they have multiple steps but they must be atomic. They can be retried.  Having directory for cache coherence will make implementation easier. Because\n  the place (e.g., L3) where directory resides can serve as the serialization point.\n  They can solve write races.  Home directory controller  and  cache controller  will exchange messages like a set of distributed machines.\n  In fact, with NoC, they are actually distributed system.    Intel: An Introduction to the Intel\u00ae QuickPath Interconnect ,\n  page 15 MESIF.  HotChips slide , has timing diagrams.  It explains the  Home Snoop  and  Source Snoop  used by Intel.  Based on their explanation, it seems both  Home Snoop  and  Source Snoop  are using a combination of\n    snoop and directory. The Processor#4 (pg 17 and 18) maintains the directory.  And this is a perfect demonstration of the details described in  Appendix I: Large-Scale Multiprocessors and Scientific Applications .  Related patent:  Extending a cache coherency snoop broadcast protocol with directory information    Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA  99  A hybrid snoop and directory cache coherence implementation. The insight is snoop\n  cause too much bandwidth, directory incurs longer latency.  So this paper proposed  Multicast snoop , where it multicasts coherence transactions\n  to selected processors, lowering the address bandwidth required for snooping.    Paper: Why On-Chip Cache Coherence Is Here to Stay, Communications of ACM 02  This paper discusses why cache coherence can scale. A nice read.  R1: Coherence\u2019s interconnection network traffic per miss scales\n      when precisely tracking sharers. (Okay increased directory bits,\n  what about those storage cost? See R2).  R2: Hierarchy combined with inclusion enables efficient scaling\n      of the storage cost for exact encoding of sharers.  R3: private evictions should send explicit messages to shared cache\n      to enable precise tracking. Thus the recall ( back invalidation ) traffic can be\n  reduced when shared cache is evicting (assume inclusion cache).  R4: Latencies of cache request can be amortized.    Book: Parallel Computer Organization and Design , Chapter 7.  Links coherence and consistency together. This chapter uses detailed graphs to show\n  how different cache coherence implementations affect consistency.    Book: A Primer on Memory Consistency and Cache Coherence  Best book for this topic.    Dr.Bandwidth on explaining core-to-core communication transactions!  Seriously, it s so good!  Although, I just feel there are so many unpublished details about the exact coherence transactions.\n  Dr.Bandwidth himself used a lot  maybe , and listed possible actions.    Transactional Memory Coherence and Consistency, ISCA 04  Programming with Transactional Coherence and Consistency (TCC)  Slide1  Awarded the most influential paper at ISCA 2019. I took a read today (Jul 21, 2019).  I feels like it s using the  batch  optimization for all time. The TCC design,\n  kind of combines both cache coherence and memory consistency: how transactions\n  commit or orders, determins the coherence and consistency.  It seems the load/store speculative execution used in their context is so similar\n  to what Dr.Bandwidth said about Intel s implementation. Basically, the processor\n  might read some data from L1/L2 and continue execution, but there is a chance,\n  that the data is modifed by others, and the L3 caching agent or home agent\n  could decide to revoke it. Once receiving such revoke message,\n  the processor must cancel all executions that use the speculatively read data.   It mentions couple Thread-Level Speculation papers, I think they should on this topic.", 
            "title": "Readings"
        }, 
        {
            "location": "/notes/cache_coherence/#misc-facts", 
            "text": "Intel Caching Agent (Cbox) is per core (or per LLC slice). Intel Home Agent is per memory controller.  The LLC coherence engine (CBo) manages the interface between the core and the last\nlevel cache (LLC). All core transactions that access the LLC are directed from the core\nto a CBo via the ring interconnect. The CBo is responsible for managing data delivery\nfrom the LLC to the requesting core. It is also responsible for maintaining coherence\nbetween the cores within the socket that share the LLC; generating snoops and\ncollecting snoop responses from the local cores when the MESIF protocol requires it.  Every physical memory address in the system is uniquely associated with a single Cbox\n  instance via a proprietary hashing algorithm that is designed to keep the distribution of\n  traffic across the CBox instances relatively uniform for a wide range of possible address patterns.  Read more  here, chapter 2.3 .  Starting from Intel UPI, Caching Agent and Home Agent are combined as CHA.    A good  discussion  about why QPI gradually drop  Source Snoop  and solely use  Home Snoop .  The motivation is scalability. It turns out the new UPI only supports directory-based protocol.  This makes sense because 1) inter socket bandwidth is precious, 2) snoop will consume a lot bandwidth.    Intel UPI is using directory-based home snoop coherency protocol  Intel\u00ae Xeon\u00ae Processor Scalable Family Technical Overview    To provide sufficient bandwidth, shared caches are typically interleaved\n  by addresses with banks physically distributed across the chip.", 
            "title": "Misc Facts"
        }, 
        {
            "location": "/notes/cache_coherence/#case-study", 
            "text": "", 
            "title": "Case Study"
        }, 
        {
            "location": "/notes/cache_coherence/#intel", 
            "text": "Intel does not disclose too much details about their cache coherence implementations.\nThe most valuable information is extracted from uncore PMU manuals, and discussions\nfrom Dr. Bandwidth. According to Dr. Bandwidth, the Intel CPU could dynamically\nadapt its coherence strategy during runtime according to workload. There won t\nbe one fixed cache coherence implementation, there will be many. It depends on\nworkload which one is used at runtime.  List below might not be completely true. Just my understanding.   Physical addresses are uniquely hashed into L3 slices. That means each individual\n  physical address belongs to a L3 slice, and also belongs to a home agent.  Upon L2 miss, it will send requests to corresponding L3 slice. If the L3 slice\n  is in the local socket, the request can be delievered within the same socket.\n  If the L3 slice belongs to another remote socket, the L2 miss request will\n  be sent over QPI/UPI. Also note that the L2 controller will not send snoop requests.\n  (This is answering the question of  why using local memory is faster than remote \n   from the cache coherence perspective.)  At L3, when received the request from a L2,  If it s in source snoop model, it will send  snoop messages  to other sockets.  If it s in home snoop model, it will send  read message  to other sockets.\n  The another socket will generate snoop and collect responses. (R3QPI or home?)  Quote Dr. Bandwidth: Maintaining consistency is easier if the data is sent\n  to the L3 first, and then to the requesting core, but it is also possible to\n  send to both at the same time (e.g.,  Direct2Core ). In recent processors,\n  these return paths are chosen dynamically based on undocumented states and\n  settings of the processor.  I m not sure who will ACK L2 at last. L3 or home agent? Both are possible.    I think both L3 and home agent have directory information. They know where\n  to send snoop/read messages. And both of them can serialize coherence transactions!\n  It s just undocumented who is doing what.  In generall, we need to bear the fact that we cannot just figure out how Intel\n  cache coherence works underlying. We maybe just need to  vaguely  know the fact that:  Both directory and snoop will be used in combination.  L3/home agent will serialize conflicting transactions  L3/home agent will send data to requesting core  L3/home agent will send final ACK to requesting L2  A coherence transaction is a multi-step distributed transaction.\n  It involes sending requests, serialize conflicts, receiving responses/ACKs.     When in doubt, read the  discussion  posted by Dr. Bandwidth.", 
            "title": "Intel"
        }, 
        {
            "location": "/notes/cache_coherence/#amd", 
            "text": "AMD HyperTransport Assit for Cache Coherence  Slide  Slide", 
            "title": "AMD"
        }, 
        {
            "location": "/notes/cache_coherence/#arm", 
            "text": "AMBA CHI Specifications  This is probabaly the most comprehensive document I ve ever seen about cache coherence.\n  Although terms used by ARM differs from the ones used by Intel, still, you can map them.\n  Chapter 5 Interconnect Protocol Flows has a lot timing diagrams regarding read/write/atomic\n  coherence transactions.  It s a good reference to know, but it would be hard to actually understand the details.", 
            "title": "ARM"
        }, 
        {
            "location": "/notes/cache_coherence/#opencapi-and-ccix", 
            "text": "CCIX White Paper  OpenCAPI", 
            "title": "OpenCAPI and CCIX"
        }, 
        {
            "location": "/notes/cache_coherence/#openpiton", 
            "text": "OpenPiton Microarchitecture Specification  Directory-based MESI  This spec has detailed coherence message packet format and type. Unfortunately,\n  it does not say anything about how they deal with coherence transaction conflicts.\n  E.g., some timeline diagrams like Figrue   in this  paper .", 
            "title": "OpenPiton"
        }, 
        {
            "location": "/notes/cache_coherence/#fpga", 
            "text": "Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA Device, FPL 19  LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM 14.  This work is built on their earlier work, which basically add the data caching\n  concept to FPGA: using BRAM as L1, on-board DRAM as L2, host or remote DRAM as L3.  In their earlier work, each FPGA application (or bitstream) has a private L1 cache.  In this work, the add MESI coherence to these private L1 caches, as in they can make\n  multiple L1 cache cache-coherent.  The techniques and protocols from this paper are similar to the exisiting ones. For example,\n  1) they use a global serializing point to serialize transactions, 2) they designed a lot\n  messaging types such as INV, RESP and so on.    VMware Research Project PBerry  A very interesting and promising project.    Intel FPGA PAC  Intel itself is building a FPGA-CPU cache coherent setting. They use the Intel UPI interconnect\n  to natually the spectrum. The FPGA shell has some modules to handle this.     Intel FPGA CCIP   Maybe the ASPLOS 20 Optimus paper is uing CCIP-related research platform?     Also some pointer chasing related stuff   A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA 16", 
            "title": "FPGA"
        }, 
        {
            "location": "/notes/cache_coherence/#formal-verification", 
            "text": "TODO\nFIll me in.", 
            "title": "Formal Verification"
        }, 
        {
            "location": "/notes/xperf/", 
            "text": "x86 Ring Switch Overhead (Page Fault version)\n\n\nVersion History\nDate\nDescription\nFeb 2, 2020\nMove github content to here\nAug 7, 2019\nInitial draft\n\n\nThis page describes the mechanisms to measure the pure\nx86 ring switch overhead, i.e., from ring 3 to ring 0 and back.\n\n\nIt is not straightforward to measure this in Linux kernel.\nBecause when a user program traps from user space to kernel space,\nkernel will first run some assembly instructions\nto save the registers and load some new ones for kernel usage\n(i.e., \nsyscall\n,\n \ncommon IDT\n,\n and some \ndirectly registered\n).\nAnd only then, the kernel will run the C code.\nThus if we place the measurement code\nin the first C function that will run (e.g., \ndo_syscall_64\n), it will be much larger than the actual ring switch overhead.\n\n\nMy proposed solutions hacks the \nentry_64.S\n and tries to save a timestamp as soon as possible.\nThe first version centers around page fault handler,\nwhose trapping mechanism is different from syscalls.\nHowever, I think it could be easily ported.\nThe code is \nhere\n.\n\n\nTakeaways:\n\n\n\n\nIt ain\nt cheap! It usually take ~400 cycles to trap from user to kernel space.\n\n\nUser-to-kernel crossing is more expansive than kernel-to-user crossing!\n\n\nVirtilization adds more overhead\n\n\n\n\nThe following content is adopted from the Github repo.\n\n\nNumbers\n\n\nThe numbers reported by this repo are slightly larger than the\nreal crossing overhead because some instructions are needed in between\nto do bookkeeping. Check below for details.\n\n\nSome preliminary numbers measured on top of Intel Xeon E5-v3 2.4GHz\n\n\n\n\n\n\n\n\nPlatform\n\n\nUser to Kernel (Cycles)\n\n\nKernel to User (Cycles)\n\n\n\n\n\n\n\n\n\n\nVM\n\n\n~600\n\n\n~370\n\n\n\n\n\n\nBare-metal\n\n\n~440\n\n\n~270\n\n\n\n\n\n\n\n\nMechanisms\n\n\nFiles changed\n\n\nThe whole patch is \nxperf.patch\n\n\n\n\narch/x86/entry/entry_64.S\n\n\narch/x86/mm/fault.c\n: save u2k_k to user stack\n\n\nxperf/xperf.c\n: userspace test code\n\n\n\n\nUser to kernel (u2k)\n\n\nAt a high-level, the flow is:\n\n\n\n\nUser save TSC into stack\n\n\nUser pgfault\n\n\nCross to kernel, get TSC, and save to user stack\n\n\n\n\nBut devil is in the details, especially this low-level assembly code.\nThere are several difficulties:\n\n\n\n\nOnce in kernel, we need to save TSC without corrupting any other\n      registers and memory content. Any corruption leads to panic etc.\n      The challenge is to find somewhere to save stuff.\n      Options are: kernel stack, user stack, per-cpu. Using user stack\n      is dangerous, because we can\nt use safe probe in this assembly (i.e., copy_from/to_user()).\n      Using kernel stack is not flexible because we need to manually\n      find a spot above pt_regs, and this subject to number of \ncall\n invoked.\n\n\nWe need to ensure the measuring only applied to measure program,\n      but not all user program. We let user save a MAGIC on user stack.\n\n\n\n\nThe approach:\n\n\n\n\nentry_64.S\n: Save rax/rdx into kernel stack, because they are known to be good\n      if the exceptions came from user space.\n\n\nentry_64.S\n: Save TSC into a per-cpu area. With swapgs surrounded.\n\n\nentry_64.S\n: Restore rax/rdx\n\n\nfault.c\n: use \ncopy_to_user\n to save \nu2k_k\n in user stack.\n\n\n\n\nEnable/Disable:\n\n\n\n\nentry_64.S\n: Change \nxperf_idtentry\n back to \nidtentry\n for both \npage_fault\n and \nasync_page_fault\n.\n\n\n\n\nNote: u2k hack is safe because we don\nt probe user virtual address directly in assembly.\nUserspace accessing is done via \ncopy_from_user()\n.\n\n\nKernel to user (k2u)\n\n\nAt a high-level, the flow is:\n\n\n\n\nKernel save TSC into user stack\n\n\nKernel IRET\n\n\nCross to user, get TSC, and calculate latency\n\n\n\n\nThis is relatively simpiler than measuring u2k because we can safely use kernel stack.\nThe approach:\n\n\n\n\nSave scratch %rax, %rdx, %rcx into kernel stack\n\n\nCheck if MAGIC match\n\n\nrdtsc\n\n\nsave to user stack\n\n\nrestore scratch registers\n\n\n\n\nEnable/Disable:\n\n\n\n\nentry_64.S:\n There is a \nxperf_return_kernel_tsc\n code block.\n\n\n\n\nNote: k2u hack is \nNOT SAFE\n because we probe user virtual address directly in assembly,\ni.e., \nmovq    %rax, (%rcx)\n in our hack. During my experiments, sometimes it will crash,\nbut not always.\n\n\nxperf/xperf.c\n\n\nThis user program will report both u2k and k2u crossing numbers.\nAfter compilation, use \nobjdump xperf.o -d\n to check assembly,\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n  \nmfence\n \n  \nrdtsc\n                 \n-\n \nu2k_u\n\n\n  \nshl\n    \n$\n0x20\n,\n%\nrdx\n\n  \nor\n     \n%\nrdx\n,\n%\nrax\n\n  \nmov\n    \n%\nrax\n,(\n%\nrdi\n)\n            \n-\n \nsave\n \nto\n \nuser\n \nstack\n\n\n  \nmovl\n   \n$\n0x12345678\n,(\n%\nrsi\n)\n     \n-\n \npgfault\n\n\n  \nrdtsc\n                 \n-\n \nk2u_u\n\n  \nmfence\n \n\n\n\n\n\nThe user stack layout upon pgfault is:\n\n1\n2\n3\n4\n5\n  \n|\n \n..\n       \n|\n\n  \n|\n \n8\nB\n \nmagic\n \n|\n \n(\nfilled\n \nby\n \nuser\n)\n   \n+\n24\n\n  \n|\n \n8\nB\n \nu2k_u\n \n|\n \n(\nfilled\n \nby\n \nuser\n)\n   \n+\n16\n\n  \n|\n \n8\nB\n \nu2k_k\n \n|\n \n(\nfilled\n \nby\n \nkernel\n)\n \n+\n8\n\n  \n|\n \n8\nB\n \nk2u_k\n \n|\n \n(\nfilled\n \nby\n \nkernel\n)\n \n--\n \n%\nrsp\n\n\n\n\n\n\nTSC Measurement\n\n\nTSC will be reodered if no actions are taken. We use \nmfence\n to mimize runtime errors.\n\n\nIdeally, we want a test sequence like this:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n/*\n\n\n * User to Kernel \n\n\n *\n\n\n *          mfence\n\n\n *          rdtsc   \n- u2k_u\n\n\n * (user)\n\n\n * -------  pgfault  --------\n\n\n * (kernel)\n\n\n *          rdtsc   \n- u2k_k\n\n\n *          mfence\n\n\n */\n\n\n\n/*\n\n\n * Kernel to User\n\n\n *\n\n\n *          mfence\n\n\n *          rdtsc   \n- k2u_k\n\n\n * (kernel)\n\n\n * -------  IRET --------\n\n\n * (user)\n\n\n *          rdtsc   \n- k2u_k\n\n\n *          mfence\n\n\n */\n\n\n\n\n\n\nBut we need some instructions in between to do essential setup.\nSo the real instruction flow is:\n\n\nU2K\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n(\nUser\n)\n\n    mfence \n    rdtsc                   \n-\n u2k_u\n\n    shl    \n$\n0x20\n,\n%rdx\n\n\n    or     %\nrdx\n,\n%rax\n\n\n    mov    %\nrax\n,(\n%rdi)\n\n\n\n    movl   $0x12345678,(%\nrsi\n)\n\n       \n--------------------------------\n         Crossing\n\n(\nKernel\n)\n\n    testb   \n$\n3\n,\n CS\n-\nORIG_RAX\n(\n%rsp)\n\n\n    jz  1f\n\n\n\n    movq    %\nrax\n,\n \n-8\n(\n%rsp)\n\n\n    movq    %\nrdx\n,\n \n-16\n(\n%rsp\n)\n\n\n    rdtsc                   \n-\n u2k_k\n    mfence\n\n\n\n\n\nK2U\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n(\nKernel\n)\n\n    mfence\n    rdtsc                   \n-\n k2u_k\n\n    shl \n$\n32\n,\n \n%rdx\n\n\n    or  %\nrdx\n,\n \n%rax\n\n\n\n    movq    %\nrax\n,\n \n(\n%rcx)\n\n\n    popq    %\nrcx\n    popq    \n%rdx\n\n\n    popq    %\nrax\n\n    INTERRUPT_RETURN\n       \n--------------------------------\n         Crossing\n\n(\nUser\n)\n\n    rdtsc                   \n-\n k2u_u\n    mfence\n\n\n\n\n\nMisc\n\n\n\n\nFor VM scenario, the page fault entry point is \nasync_page_fault\n, not the \npage_fault\n.\n\n\n\n\nHOWTO Run\n\n\nFAT NOTE:\n\n\n\n\nEnabling k2u code might bring crash\n\n\nIt\ns not safe to disable KPTI\n\n\nSwitch back to normal kernel after testing\n\n\nMake sure if you have a way to reboot your machine!\n\n\n\n\nSteps:\n\n\n\n\nCopy your current kernel\ns .config into this repo\n\n\nmake oldconfig\n\n\nDisable \nCONFIG_PAGE_TABLE_ISOLATION\n\n\nCompile kernel and install.\n\n\nReboot into new kernel\n\n\nDisable hugepage\n\n\necho never \n /sys/kernel/mm/transparent_hugepage/enabled\n\n\nRun \nxperf/xperf.c\n, you will get a report.", 
            "title": "Measure-x86-Ring-Switch-Overhead"
        }, 
        {
            "location": "/notes/xperf/#x86-ring-switch-overhead-page-fault-version", 
            "text": "Version History Date Description Feb 2, 2020 Move github content to here Aug 7, 2019 Initial draft  This page describes the mechanisms to measure the pure\nx86 ring switch overhead, i.e., from ring 3 to ring 0 and back.  It is not straightforward to measure this in Linux kernel.\nBecause when a user program traps from user space to kernel space,\nkernel will first run some assembly instructions\nto save the registers and load some new ones for kernel usage\n(i.e.,  syscall ,\n  common IDT ,\n and some  directly registered ).\nAnd only then, the kernel will run the C code.\nThus if we place the measurement code\nin the first C function that will run (e.g.,  do_syscall_64 ), it will be much larger than the actual ring switch overhead.  My proposed solutions hacks the  entry_64.S  and tries to save a timestamp as soon as possible.\nThe first version centers around page fault handler,\nwhose trapping mechanism is different from syscalls.\nHowever, I think it could be easily ported.\nThe code is  here .  Takeaways:   It ain t cheap! It usually take ~400 cycles to trap from user to kernel space.  User-to-kernel crossing is more expansive than kernel-to-user crossing!  Virtilization adds more overhead   The following content is adopted from the Github repo.", 
            "title": "x86 Ring Switch Overhead (Page Fault version)"
        }, 
        {
            "location": "/notes/xperf/#numbers", 
            "text": "The numbers reported by this repo are slightly larger than the\nreal crossing overhead because some instructions are needed in between\nto do bookkeeping. Check below for details.  Some preliminary numbers measured on top of Intel Xeon E5-v3 2.4GHz     Platform  User to Kernel (Cycles)  Kernel to User (Cycles)      VM  ~600  ~370    Bare-metal  ~440  ~270", 
            "title": "Numbers"
        }, 
        {
            "location": "/notes/xperf/#mechanisms", 
            "text": "", 
            "title": "Mechanisms"
        }, 
        {
            "location": "/notes/xperf/#files-changed", 
            "text": "The whole patch is  xperf.patch   arch/x86/entry/entry_64.S  arch/x86/mm/fault.c : save u2k_k to user stack  xperf/xperf.c : userspace test code", 
            "title": "Files changed"
        }, 
        {
            "location": "/notes/xperf/#user-to-kernel-u2k", 
            "text": "At a high-level, the flow is:   User save TSC into stack  User pgfault  Cross to kernel, get TSC, and save to user stack   But devil is in the details, especially this low-level assembly code.\nThere are several difficulties:   Once in kernel, we need to save TSC without corrupting any other\n      registers and memory content. Any corruption leads to panic etc.\n      The challenge is to find somewhere to save stuff.\n      Options are: kernel stack, user stack, per-cpu. Using user stack\n      is dangerous, because we can t use safe probe in this assembly (i.e., copy_from/to_user()).\n      Using kernel stack is not flexible because we need to manually\n      find a spot above pt_regs, and this subject to number of  call  invoked.  We need to ensure the measuring only applied to measure program,\n      but not all user program. We let user save a MAGIC on user stack.   The approach:   entry_64.S : Save rax/rdx into kernel stack, because they are known to be good\n      if the exceptions came from user space.  entry_64.S : Save TSC into a per-cpu area. With swapgs surrounded.  entry_64.S : Restore rax/rdx  fault.c : use  copy_to_user  to save  u2k_k  in user stack.   Enable/Disable:   entry_64.S : Change  xperf_idtentry  back to  idtentry  for both  page_fault  and  async_page_fault .   Note: u2k hack is safe because we don t probe user virtual address directly in assembly.\nUserspace accessing is done via  copy_from_user() .", 
            "title": "User to kernel (u2k)"
        }, 
        {
            "location": "/notes/xperf/#kernel-to-user-k2u", 
            "text": "At a high-level, the flow is:   Kernel save TSC into user stack  Kernel IRET  Cross to user, get TSC, and calculate latency   This is relatively simpiler than measuring u2k because we can safely use kernel stack.\nThe approach:   Save scratch %rax, %rdx, %rcx into kernel stack  Check if MAGIC match  rdtsc  save to user stack  restore scratch registers   Enable/Disable:   entry_64.S:  There is a  xperf_return_kernel_tsc  code block.   Note: k2u hack is  NOT SAFE  because we probe user virtual address directly in assembly,\ni.e.,  movq    %rax, (%rcx)  in our hack. During my experiments, sometimes it will crash,\nbut not always.", 
            "title": "Kernel to user (k2u)"
        }, 
        {
            "location": "/notes/xperf/#xperfxperfc", 
            "text": "This user program will report both u2k and k2u crossing numbers.\nAfter compilation, use  objdump xperf.o -d  to check assembly,  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11    mfence  \n   rdtsc                   -   u2k_u \n\n   shl      $ 0x20 , % rdx \n   or       % rdx , % rax \n   mov      % rax ,( % rdi )              -   save   to   user   stack \n\n   movl     $ 0x12345678 ,( % rsi )       -   pgfault \n\n   rdtsc                   -   k2u_u \n   mfence     The user stack layout upon pgfault is: 1\n2\n3\n4\n5    |   ..         | \n   |   8 B   magic   |   ( filled   by   user )     + 24 \n   |   8 B   u2k_u   |   ( filled   by   user )     + 16 \n   |   8 B   u2k_k   |   ( filled   by   kernel )   + 8 \n   |   8 B   k2u_k   |   ( filled   by   kernel )   --   % rsp", 
            "title": "xperf/xperf.c"
        }, 
        {
            "location": "/notes/xperf/#tsc-measurement", 
            "text": "TSC will be reodered if no actions are taken. We use  mfence  to mimize runtime errors.  Ideally, we want a test sequence like this:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 /*   * User to Kernel    *   *          mfence   *          rdtsc    - u2k_u   * (user)   * -------  pgfault  --------   * (kernel)   *          rdtsc    - u2k_k   *          mfence   */  /*   * Kernel to User   *   *          mfence   *          rdtsc    - k2u_k   * (kernel)   * -------  IRET --------   * (user)   *          rdtsc    - k2u_k   *          mfence   */    But we need some instructions in between to do essential setup.\nSo the real instruction flow is:  U2K  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ( User ) \n    mfence \n    rdtsc                    -  u2k_u\n\n    shl     $ 0x20 , %rdx      or     % rdx , %rax      mov    % rax ,( %rdi)      movl   $0x12345678,(% rsi ) \n        --------------------------------          Crossing ( Kernel ) \n    testb    $ 3 ,  CS - ORIG_RAX ( %rsp)      jz  1f      movq    % rax ,   -8 ( %rsp)      movq    % rdx ,   -16 ( %rsp ) \n\n    rdtsc                    -  u2k_k\n    mfence   K2U  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ( Kernel ) \n    mfence\n    rdtsc                    -  k2u_k\n\n    shl  $ 32 ,   %rdx      or  % rdx ,   %rax      movq    % rax ,   ( %rcx)      popq    % rcx\n    popq     %rdx      popq    % rax\n\n    INTERRUPT_RETURN\n        --------------------------------          Crossing ( User ) \n    rdtsc                    -  k2u_u\n    mfence", 
            "title": "TSC Measurement"
        }, 
        {
            "location": "/notes/xperf/#misc", 
            "text": "For VM scenario, the page fault entry point is  async_page_fault , not the  page_fault .", 
            "title": "Misc"
        }, 
        {
            "location": "/notes/xperf/#howto-run", 
            "text": "FAT NOTE:   Enabling k2u code might bring crash  It s not safe to disable KPTI  Switch back to normal kernel after testing  Make sure if you have a way to reboot your machine!   Steps:   Copy your current kernel s .config into this repo  make oldconfig  Disable  CONFIG_PAGE_TABLE_ISOLATION  Compile kernel and install.  Reboot into new kernel  Disable hugepage  echo never   /sys/kernel/mm/transparent_hugepage/enabled  Run  xperf/xperf.c , you will get a report.", 
            "title": "HOWTO Run"
        }, 
        {
            "location": "/notes/paper_perf_shadows/", 
            "text": "Hiding In The Shadows\n\n\nVersion History\nDate\nDescription\nJul 11, 2019\nInitial draft\n\n\nThere are shadows under the sun.\n\nThere are shadows in your life.\n\nThere are shadows in your computer.  \n\n\nThis note is about \nlatency tolerance techniques\n.\n\nThis note is about how to get the most out of the otherwise-wasted resource.\n\n\nNanoseconds\n\n\nArchitecture solutions to attack nanosecond-level performance shadows\nthat are mostly created by lower level data and instruction cache misses.\nOoO and SMT are the base to hide these latencies, but they fall short\nwhen ROB is full (or some other reasons).\nWhen that happens, these academic ideas come in rescue.\n\n\nRunahead\n\n\n\n\nQuote\n\n\nIn runahead, once the processor stalls, it uses the instruction window to\ncontinue to fetch and execute operations. The goal of runahead is to generate\nnew cache misses, thereby turning subsequent demand requests into cache hits\ninstead of cache misses.[5]\n\n\n\n\nPapers\n\n\n\n\nImproving Data Cache Performance by Pre-executing Instructions Under a Cache Miss, ICS\u201997\n\n\nRunahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors, HPCA\u201903\n\n\nEfficient Runahead Execution: Power-Efficient Memory Latency Tolerance, IEEE Micro\u201906\n\n\nGood timeline graphs show the benefit of Runahead.\n\n\n\n\n\n\nRunahead Threads to Improve SMT Performance, HPCA\u201908\n\n\nQoS control policy.\n\n\n\n\n\n\nContinuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads, MICRO\u201916\n\n\nNice idea to tackle the issue that runahead does not get enough time to run.\n\n\nAlso has the notion of ideal runahead coverage.\n\n\n\n\n\n\n\n\nComments\n\n\n\n\nWe should separate mechanism and policy.\n\n\nRunahead is the mechanism. It includes:\n\n\nEnter runahead\n\n\nExecution in runahead context (most important thing is to maintain those INV bits and pseudo-retires)\n\n\nExit runahead\n\n\n\n\n\n\nPrefetch is one of the policy, a major one. It\ns the side effect of running instructions in the execution phase of runahead mode.\n\n\nQoS control is another policy. This means adding specific rules to the execution phase. More specifically: limit the core resource usage of the runahead thread, thus reduce the impact on the co-running HW thread.\n\n\n\n\nHelper Threads (or Precomputation)\n\n\n\n\nQuote\n\n\nA helper thread is a stripped down version of the main thread that\nonly includes the necessary instructions to generate memory accesses,\nincluding control flow instructions [10].\n\n\n\n\n\n\nQuote\n\n\nPrecomputation uses idle thread contexts in a multithreaded architecture\nto improve performance of single-threaded applications.\nIt attacks program stalls from data cache misses by\npre-computing future memory accesses in available thread\ncontexts, and prefetching these data.[1]\n\n\n\n\n\n\nQuote\n\n\nSuch pre-execution threads are\npurely speculative, and their instructions are never committed\ninto the main computation. Instead, the pre-execution\nthreads run code designed to trigger cache misses. As long\nas the pre-execution threads execute far enough in front of\nthe main thread, they effectively hide the latency of the\ncache misses so that the main thread experiences signicantly fewer memory stalls.[5]\n\n\n\n\nPapers\n\n\n\n\nSpeculative Precomputation: Long-range Prefetching of Delinquent Loads, ISCA\n01\n\n\nDynamic\n Speculative Precomputation, Micro\n01\n\n\nTake a step further by using HW to construct the offloaded code slice automatically.\n\n\n\n\n\n\nExecution-based Prediction Using Speculative Slices, ISCA\n01\n\n\nTolerating Memory Latency through \nSoftware-Controlled\n Pre-Execution in Simultaneous Multithreading Processors, ISCA\n01\n\n\nWhat\ns up with ISCA\n01? This paper proposed to use software to control\nwhen to start running precomputation and when to exit. It uses compiler\ns\nhelp to generate those code slices, and insert special start/end instructions.\nOn the contrast, hardware-controller precomputation relies on hints such\nas cache misses.\n\n\n\n\n\n\nDesign and Evaluation of Compiler Algorithms for PreExecution, ASPLOS\n02\n\n\n5.1 A Study of Source-Level Compiler Algorithms for Automatic Construction of Pre-Execution Code, TOCS\n04\n\n\n\n\n\n\nDynamic Helper Threaded Prefetching on the Sun UltraSPARC\u00ae CMP Processor, Micro\n05\n\n\nThe \nfunction table\n at helper thread seems nice and useful.\n\n\n\n\n\n\nAccelerating and Adapting Precomputation Threads for Effcient Prefetching, HPCA\n07\n\n\nDynamically construct precomputation code, called p-slices. They can adapt\nthe same program differently depending on the program\ns data input and the underlying\nhardware architecture.\n\n\n\n\n\n\nInter-core Prefetching for Multicore Processors Using \nMigrating Helper Threads\n, ASPLOS\n11\n\n\nPure software solution. I like the idea. But I don\nt think it will\nwork for realistic applications.\n\n\nLearned \nsetcontext(), getcontext(), and swapcontext()\n.\n\n\n\n\n\n\nBootstrapping: Using SMT Hardware to Improve Single-Thread Performance, ASPLOS\n19\n\n\nFreeway: Maximizing MLP for slice-out-of-order execution, HPCA\n19\n\n\nStrictly speaking this is not in this catogory. But it is this paper\n  that lead me to Runahead and Helper thread topic. I was doing\n  something similar so those techniques caught my eye.\n\n\n\n\n\n\n\n\nComments\n\n\n\n\nThe catch about precomputation is that it must create lightweight threads\n  that can actually proceed faster than the main thread, so that they\n  stay out in front.\n\n\nOther catch is: you also need to create the code slice that will\n  run on another core context. First of all, how is this code slice different\n  from the original code? The extracted code will be simplified in the sense\n  that it will only access memory without doing other computations.\n  The second question is how this code slice is extracted and then constructed?\n  There are many ways. You can handwrite, or use a static compiler to pre-generate\n  them (by using techniques in above papers), or use hardware to dynamically\n  generate them during runtime, or use software to dynamically generate them during runtime.\n  There are ways to it, but I don\nt think this is the core of precomputation.\n\n\nAlso, same thing here, we should separate mechanism and policies.\n  Helper thread (or precomputation) is mainly used as a vehicle\n  for speculatively generating data addresses and prefetching.\n\n\n\n\nThread-Level Speculation\n\n\nFill me in.\n\n\nLocks\n\n\nApplying the insight of \nget the most out of the otherwise-wasted resource\n\nto the lock area. I will wait for Sanidhya\ns SOSP\n19 paper. :-)\n\n\nMisc\n\n\n\n\nStretch: Balancing QoS and throughput for colocated server workloads on SMT cores (Best Paper), HPCA\n19\n\n\nKeyword: \nROB\n, \nCo-location QoS\n.\n\n\nThis paper tackles the perf interference when running co-running two SMT threads\n  on a single physical core, which is the common case in datacenters.\n  However co-running latency-sensitive jobs and batch jobs will\n  have huge impact on the perf of both.\n\n\nThis paper found: \nLatency-sensitive workloads show little benefit\n  from large ROB capacities in modern server processors .. because frequent\n  cache misses and data-dependent computation limit both instruction\n  and memory-level parallelisms (ILP and MLP). In contrast, many batch\n  workloads benefit from a large ROB that helps unlock higher ILP and MLP.\n\n\nSo they propose to have a ROB partition scheme rather than static equal\n  partition. Of course they also did some very extensive studies before\n  deciding to scale ROB. They first found shared ROB has the biggest\n  impact on perf interference than any other resources such as branch\n  predictor, cache, and so on. They further found that latency-sensitive\n  workload can tolerate some perf slack, which means they will not\n  violate their QoS even with a smaller ROB.\n\n\nAnyway, I think this is a very nice paper. Good reasoning, simple solution,\n  but works effectively.\n\n\n\n\n\n\n\n\nPut it all together\n\n\n\n\nBoth runahead and helper thread were proposed to do prefetch.\n  But they have a key difference. Runahead is invoked in the \nsame core\n,\n  and is invoked when ROB is full (not always though). Helper thread is\n  invoked at \nanother core\n. Besides, runahead can just fetch the\n  instructions and run, no need to cook another code slice. But for\n  helper thread, it needs to extract a code slice that will run on another core.\n\n\nI think the most important thing is to realize their insight.\n  In the most straightforward and plain way: they are trying to\n  get the most out of the otherwise-wasted resource. For example,\n  in runahead, they realize that with some help, the CPU is still\n  able to generate cache misses even if the instruction table is full.\n  For precomputation, obviously it is using the other idle cores.\n  The simple insight itself is not interesting enough, usually\n  where it\ns applied make things quite interesting.\n\n\n\n\nMicroseconds\n\n\nFill me in\n\n\nMilliseconds\n\n\nSleep. And wake me up when september ends. And this seems to be enough. ;-)\nThis is true for OS to handle slow HDD and slow network.", 
            "title": "Performance-Shadows"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#hiding-in-the-shadows", 
            "text": "Version History Date Description Jul 11, 2019 Initial draft  There are shadows under the sun. \nThere are shadows in your life. \nThere are shadows in your computer.    This note is about  latency tolerance techniques . \nThis note is about how to get the most out of the otherwise-wasted resource.", 
            "title": "Hiding In The Shadows"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#nanoseconds", 
            "text": "Architecture solutions to attack nanosecond-level performance shadows\nthat are mostly created by lower level data and instruction cache misses.\nOoO and SMT are the base to hide these latencies, but they fall short\nwhen ROB is full (or some other reasons).\nWhen that happens, these academic ideas come in rescue.", 
            "title": "Nanoseconds"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#runahead", 
            "text": "Quote  In runahead, once the processor stalls, it uses the instruction window to\ncontinue to fetch and execute operations. The goal of runahead is to generate\nnew cache misses, thereby turning subsequent demand requests into cache hits\ninstead of cache misses.[5]   Papers   Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss, ICS\u201997  Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors, HPCA\u201903  Efficient Runahead Execution: Power-Efficient Memory Latency Tolerance, IEEE Micro\u201906  Good timeline graphs show the benefit of Runahead.    Runahead Threads to Improve SMT Performance, HPCA\u201908  QoS control policy.    Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads, MICRO\u201916  Nice idea to tackle the issue that runahead does not get enough time to run.  Also has the notion of ideal runahead coverage.     Comments   We should separate mechanism and policy.  Runahead is the mechanism. It includes:  Enter runahead  Execution in runahead context (most important thing is to maintain those INV bits and pseudo-retires)  Exit runahead    Prefetch is one of the policy, a major one. It s the side effect of running instructions in the execution phase of runahead mode.  QoS control is another policy. This means adding specific rules to the execution phase. More specifically: limit the core resource usage of the runahead thread, thus reduce the impact on the co-running HW thread.", 
            "title": "Runahead"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#helper-threads-or-precomputation", 
            "text": "Quote  A helper thread is a stripped down version of the main thread that\nonly includes the necessary instructions to generate memory accesses,\nincluding control flow instructions [10].    Quote  Precomputation uses idle thread contexts in a multithreaded architecture\nto improve performance of single-threaded applications.\nIt attacks program stalls from data cache misses by\npre-computing future memory accesses in available thread\ncontexts, and prefetching these data.[1]    Quote  Such pre-execution threads are\npurely speculative, and their instructions are never committed\ninto the main computation. Instead, the pre-execution\nthreads run code designed to trigger cache misses. As long\nas the pre-execution threads execute far enough in front of\nthe main thread, they effectively hide the latency of the\ncache misses so that the main thread experiences signicantly fewer memory stalls.[5]   Papers   Speculative Precomputation: Long-range Prefetching of Delinquent Loads, ISCA 01  Dynamic  Speculative Precomputation, Micro 01  Take a step further by using HW to construct the offloaded code slice automatically.    Execution-based Prediction Using Speculative Slices, ISCA 01  Tolerating Memory Latency through  Software-Controlled  Pre-Execution in Simultaneous Multithreading Processors, ISCA 01  What s up with ISCA 01? This paper proposed to use software to control\nwhen to start running precomputation and when to exit. It uses compiler s\nhelp to generate those code slices, and insert special start/end instructions.\nOn the contrast, hardware-controller precomputation relies on hints such\nas cache misses.    Design and Evaluation of Compiler Algorithms for PreExecution, ASPLOS 02  5.1 A Study of Source-Level Compiler Algorithms for Automatic Construction of Pre-Execution Code, TOCS 04    Dynamic Helper Threaded Prefetching on the Sun UltraSPARC\u00ae CMP Processor, Micro 05  The  function table  at helper thread seems nice and useful.    Accelerating and Adapting Precomputation Threads for Effcient Prefetching, HPCA 07  Dynamically construct precomputation code, called p-slices. They can adapt\nthe same program differently depending on the program s data input and the underlying\nhardware architecture.    Inter-core Prefetching for Multicore Processors Using  Migrating Helper Threads , ASPLOS 11  Pure software solution. I like the idea. But I don t think it will\nwork for realistic applications.  Learned  setcontext(), getcontext(), and swapcontext() .    Bootstrapping: Using SMT Hardware to Improve Single-Thread Performance, ASPLOS 19  Freeway: Maximizing MLP for slice-out-of-order execution, HPCA 19  Strictly speaking this is not in this catogory. But it is this paper\n  that lead me to Runahead and Helper thread topic. I was doing\n  something similar so those techniques caught my eye.     Comments   The catch about precomputation is that it must create lightweight threads\n  that can actually proceed faster than the main thread, so that they\n  stay out in front.  Other catch is: you also need to create the code slice that will\n  run on another core context. First of all, how is this code slice different\n  from the original code? The extracted code will be simplified in the sense\n  that it will only access memory without doing other computations.\n  The second question is how this code slice is extracted and then constructed?\n  There are many ways. You can handwrite, or use a static compiler to pre-generate\n  them (by using techniques in above papers), or use hardware to dynamically\n  generate them during runtime, or use software to dynamically generate them during runtime.\n  There are ways to it, but I don t think this is the core of precomputation.  Also, same thing here, we should separate mechanism and policies.\n  Helper thread (or precomputation) is mainly used as a vehicle\n  for speculatively generating data addresses and prefetching.", 
            "title": "Helper Threads (or Precomputation)"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#thread-level-speculation", 
            "text": "Fill me in.", 
            "title": "Thread-Level Speculation"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#locks", 
            "text": "Applying the insight of  get the most out of the otherwise-wasted resource \nto the lock area. I will wait for Sanidhya s SOSP 19 paper. :-)", 
            "title": "Locks"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#misc", 
            "text": "Stretch: Balancing QoS and throughput for colocated server workloads on SMT cores (Best Paper), HPCA 19  Keyword:  ROB ,  Co-location QoS .  This paper tackles the perf interference when running co-running two SMT threads\n  on a single physical core, which is the common case in datacenters.\n  However co-running latency-sensitive jobs and batch jobs will\n  have huge impact on the perf of both.  This paper found:  Latency-sensitive workloads show little benefit\n  from large ROB capacities in modern server processors .. because frequent\n  cache misses and data-dependent computation limit both instruction\n  and memory-level parallelisms (ILP and MLP). In contrast, many batch\n  workloads benefit from a large ROB that helps unlock higher ILP and MLP.  So they propose to have a ROB partition scheme rather than static equal\n  partition. Of course they also did some very extensive studies before\n  deciding to scale ROB. They first found shared ROB has the biggest\n  impact on perf interference than any other resources such as branch\n  predictor, cache, and so on. They further found that latency-sensitive\n  workload can tolerate some perf slack, which means they will not\n  violate their QoS even with a smaller ROB.  Anyway, I think this is a very nice paper. Good reasoning, simple solution,\n  but works effectively.", 
            "title": "Misc"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#put-it-all-together", 
            "text": "Both runahead and helper thread were proposed to do prefetch.\n  But they have a key difference. Runahead is invoked in the  same core ,\n  and is invoked when ROB is full (not always though). Helper thread is\n  invoked at  another core . Besides, runahead can just fetch the\n  instructions and run, no need to cook another code slice. But for\n  helper thread, it needs to extract a code slice that will run on another core.  I think the most important thing is to realize their insight.\n  In the most straightforward and plain way: they are trying to\n  get the most out of the otherwise-wasted resource. For example,\n  in runahead, they realize that with some help, the CPU is still\n  able to generate cache misses even if the instruction table is full.\n  For precomputation, obviously it is using the other idle cores.\n  The simple insight itself is not interesting enough, usually\n  where it s applied make things quite interesting.", 
            "title": "Put it all together"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#microseconds", 
            "text": "Fill me in", 
            "title": "Microseconds"
        }, 
        {
            "location": "/notes/paper_perf_shadows/#milliseconds", 
            "text": "Sleep. And wake me up when september ends. And this seems to be enough. ;-)\nThis is true for OS to handle slow HDD and slow network.", 
            "title": "Milliseconds"
        }, 
        {
            "location": "/notes/rmap/", 
            "text": "Linux Reverse Map\n\n\n\n\nRead those carefully, you will understand:\n\n\n\n\nPDF: Object-based Reverse Mapping\n\n\nLWN: Virtual Memory II: the return of objrmap\n\n\nLWN: The object-based reverse-mapping VM\n\n\n\n\n\n\n\n\nI used to implement the basic \nPTE-chain based rmap for LegoOS\n.\nI can see the downsides of it. I tried to understand the\nlinux rmap before, somehow gave up because I couldn\nt fully\nunderstand one thing:\nfor a page that is shared among multiple processes\n VMAs, the source code\nsuggests it will always have same offset from the beginning of\n\nall\n VMA (i.e., \nvm_start\n). But does it actually works like this\nfor ALL cases? I just think it\ns possible that a page is mapped\nby an VMA which has a slightly different starting address.\n\n\nI still have doubt about it. But after accepting this assumption,\nit\ns just easy to understand. I will check later on.\n\n\nThe code suggests:\n\n\n\n\nThe offset of a page is saved in \npage-\nindex\n.\n\n\nFor anonmouys pages, the \npage-\nindex\n is saved by \npage_set_anon_rmap()\n.\n\n\nWhen doing rmap walk over multiple VMAs:\n\n\nFor \nanon\n: \nunsigned long address = vma_address(page, vma);\n\n\nFor \nfile\n: \nunsigned long address = vma_address(page, vma);\n\n\nAnd  \nvma_address()\n is basically \npage-\nindex\n\n\n\n\n1\n2\n3\n4\n5\n6\n    \nstatic\n \ninline\n \nunsigned\n \nlong\n\n    \n__vma_address\n(\nstruct\n \npage\n \n*\npage\n,\n \nstruct\n \nvm_area_struct\n \n*\nvma\n)\n\n    \n{\n\n        \npgoff_t\n \npgoff\n \n=\n \npage_to_pgoff\n(\npage\n);\n\n        \nreturn\n \nvma\n-\nvm_start\n \n+\n \n((\npgoff\n \n-\n \nvma\n-\nvm_pgoff\n)\n \n \nPAGE_SHIFT\n);\n\n    \n}\n\n\n\n\n\n\n\nCompared to basic PTE-chain based solution, object-based rmap:\n\n\nThe real benefit\n\n\n\n\nDuring page fault, we only need to set \npage-\nmapping\n to point to \nstruct anon_vma\n,\n  rather than allocating a new structure and insert.\n\n\n\n\nThe downside\n\n\n\n\nDuring rmap walk, we need extra computation to walk each VMA\ns page table\n  to make sure that the page is actually mapped within this specific VMA.\n\n\n\n\nAdding \nstruct anon_vma\n is really similar to the idea of reusing \naddress_space\n,\ni.e., having a data structure trampoline.\n\n\nSome more boring details:\n\n\n\n\nAll pages within a single VMA share just one \nanon_vma\n.\n  \nvma-\nanon_vma\n indicates if a VMA has attached or note.\n  Related function is \nanon_vma_prepare()\n within \ndo_anonymous_fault()\n \n1\n.\n\n\n\n\n\nYizhou Shan \n\nCreated: Jun 16, 2019\n\nLast Updated: Jun 17, 2019", 
            "title": "Linux Reverse Map (rmap)"
        }, 
        {
            "location": "/notes/rmap/#linux-reverse-map", 
            "text": "Read those carefully, you will understand:   PDF: Object-based Reverse Mapping  LWN: Virtual Memory II: the return of objrmap  LWN: The object-based reverse-mapping VM     I used to implement the basic  PTE-chain based rmap for LegoOS .\nI can see the downsides of it. I tried to understand the\nlinux rmap before, somehow gave up because I couldn t fully\nunderstand one thing:\nfor a page that is shared among multiple processes  VMAs, the source code\nsuggests it will always have same offset from the beginning of all  VMA (i.e.,  vm_start ). But does it actually works like this\nfor ALL cases? I just think it s possible that a page is mapped\nby an VMA which has a slightly different starting address.  I still have doubt about it. But after accepting this assumption,\nit s just easy to understand. I will check later on.  The code suggests:   The offset of a page is saved in  page- index .  For anonmouys pages, the  page- index  is saved by  page_set_anon_rmap() .  When doing rmap walk over multiple VMAs:  For  anon :  unsigned long address = vma_address(page, vma);  For  file :  unsigned long address = vma_address(page, vma);  And   vma_address()  is basically  page- index   1\n2\n3\n4\n5\n6      static   inline   unsigned   long \n     __vma_address ( struct   page   * page ,   struct   vm_area_struct   * vma ) \n     { \n         pgoff_t   pgoff   =   page_to_pgoff ( page ); \n         return   vma - vm_start   +   (( pgoff   -   vma - vm_pgoff )     PAGE_SHIFT ); \n     }    Compared to basic PTE-chain based solution, object-based rmap:  The real benefit   During page fault, we only need to set  page- mapping  to point to  struct anon_vma ,\n  rather than allocating a new structure and insert.   The downside   During rmap walk, we need extra computation to walk each VMA s page table\n  to make sure that the page is actually mapped within this specific VMA.   Adding  struct anon_vma  is really similar to the idea of reusing  address_space ,\ni.e., having a data structure trampoline.  Some more boring details:   All pages within a single VMA share just one  anon_vma .\n   vma- anon_vma  indicates if a VMA has attached or note.\n  Related function is  anon_vma_prepare()  within  do_anonymous_fault()   1 .   \nYizhou Shan  \nCreated: Jun 16, 2019 \nLast Updated: Jun 17, 2019", 
            "title": "Linux Reverse Map"
        }, 
        {
            "location": "/notes/userfaultfd/", 
            "text": "Userfaultfd\n\n\n(Notes based on linux 5.2-rc3)\n\n\n\n\nCode Layout\n\n\nMajor file: \nfs/userfaultfd.c\n, which has all the functions and callbacks.\n\n\nCallers spread across: \nmm/memory.c\n, \nmm/mremap.c\n, \nmm/mmap.c\n, and some others.\n\n\nThe userfaultfd code is not that hard to understand if you already know how waitqueue etc work. It\ns built center around the \nfile_ops\n, and couple callbacks for mm.\n\n\n\n\n\n\nhandle_userfault()\n, called by \nmm/memory.c\n:\n\n\nUserfaultfd callback only happens for anonymous pgfault\n\n\nUserfaultfd skip all the LRU, rmap, cgroup\n\n\nUserfaultfd does not use the shared global zero page\n\n\n\n\n\n\n\n\nuserfaultfd_unmap_prep(), userfaultfd_unmap_complete()\n, called by \nmm/mmap.c\n, and \nmm/mremap.c\n:\n\n\n\n\nUserfaultfd got notified if there are remap and unmap\n\n\nUserfaultfd deliver events via \nuserfaultfd_event_wait_completion()\n\n\nI found code in mmap.c and mremap.c is NOT skipping rmap/lru code. Since userfaultfd related pages don\nt have these setup during pgfault, I think those rmap/lru cleanup code will notice this and handle it well. \nIn conclusion, userfault skip the expansive rmap/lru setup/teardown.\n\n\n\n\n\n\n\n\nWhy userfaultfd?\n\n\n\n\nAt first developed to enhance VM migration: after migration, the destination QEMU can handle pgfault and bring pages from remote via network.\n\n\nSome databases also use it to have customized feature: \nhttp://tech.adroll.com/blog/data/2016/11/29/traildb-mmap-s3.html\n\n\nMy thought? The ideal use case is very similar to we did in Hotpot: get the faulting user address, and fetch it from remote. Due to kernel limitations and security constraints, the userfaultfd has to go through many layers and multiple kernel/user crossing. It would be interesting to use user eBPF code to handle pgfault.\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Jun 4, 2019\n\nLast Updated: Jun 4, 2019", 
            "title": "Linux Userfaultfd"
        }, 
        {
            "location": "/notes/userfaultfd/#userfaultfd", 
            "text": "(Notes based on linux 5.2-rc3)   Code Layout  Major file:  fs/userfaultfd.c , which has all the functions and callbacks.  Callers spread across:  mm/memory.c ,  mm/mremap.c ,  mm/mmap.c , and some others.  The userfaultfd code is not that hard to understand if you already know how waitqueue etc work. It s built center around the  file_ops , and couple callbacks for mm.    handle_userfault() , called by  mm/memory.c :  Userfaultfd callback only happens for anonymous pgfault  Userfaultfd skip all the LRU, rmap, cgroup  Userfaultfd does not use the shared global zero page     userfaultfd_unmap_prep(), userfaultfd_unmap_complete() , called by  mm/mmap.c , and  mm/mremap.c :   Userfaultfd got notified if there are remap and unmap  Userfaultfd deliver events via  userfaultfd_event_wait_completion()  I found code in mmap.c and mremap.c is NOT skipping rmap/lru code. Since userfaultfd related pages don t have these setup during pgfault, I think those rmap/lru cleanup code will notice this and handle it well.  In conclusion, userfault skip the expansive rmap/lru setup/teardown.     Why userfaultfd?   At first developed to enhance VM migration: after migration, the destination QEMU can handle pgfault and bring pages from remote via network.  Some databases also use it to have customized feature:  http://tech.adroll.com/blog/data/2016/11/29/traildb-mmap-s3.html  My thought? The ideal use case is very similar to we did in Hotpot: get the faulting user address, and fetch it from remote. Due to kernel limitations and security constraints, the userfaultfd has to go through many layers and multiple kernel/user crossing. It would be interesting to use user eBPF code to handle pgfault.     \nYizhou Shan \nCreated: Jun 4, 2019 \nLast Updated: Jun 4, 2019", 
            "title": "Userfaultfd"
        }, 
        {
            "location": "/notes/trace/", 
            "text": "Linux Tracing\n\n\n\n\nSome general notes about the various tracers inside Linux kernel.\n\n\nLink to my old notes about the profilers/tracers in LegoOS: \nnotes\n.\nThe \nprofile points\n,\nwhich is able to profile arbitray code piece is still my favorite thing.\n\n\nIn Linux kernel, we have:\n\n\n\n\nftrace\n\n\nkprobe\n\n\nuprobe\n\n\nperf_event\n\n\ntracepoints\n\n\neBPF\n\n\n\n\nI tend to think this way:\n\n\n\n\nTracing needs two parts, \n1)\n \nMechanims to do callback.\n This means we need a way to\n    let our tracing/profiling code got invoked on a running system. This can be static\n    or dynamic. Static means we added our tracing code to source code, like tracepoints.\n    Dynamic means we added our tracing code when system is running, like ftrace and kprobe.\n    \n2)\n \nDo stuff within callback.\n All of them provide some sort of handling. But eBPF is the\n    most extensive one.\n\n\nFor example, \nftrace\n, \nkprobe\n, and \nperf_event\n include the callback facilities,\n    although they are not just limited to this.\n    \nftrace\n has the \ncall mount\n way to do callback on every single function invocation.\n    \nkprobe\n dynamically patch instructions and to do callback within exception handlers.\n    \nperf_event\n can let CPU fire NMI interrupt. Those are all mechanisms to catch perf data.\n\n\nThe blog from Julia explains it well: \nLinux tracing systems \n how they fit together\n\n\n\n\nftrace\n:\n\n\n\n\nMechanism\n\n\nFor each un-inlined function, gcc inserts a \ncall mcount\n, or a \ncall fentry\n\ninstruction at the very beginning. This means whenever a function is called,\nthe \nmcount()\n or the \nfentry()\n callback will be invoked.\n\n\nHaving these \ncall\n instructions introduce a lot overheads. So by default kernel\nreplace \ncall\n with \nnop\n. Only after we \necho something \n setup_filter_functions\n\nwill the ftrace code replace \nnop\n with \ncall\n. Do note, Linux uses the linker\nmagic again here, check Steven\ns slides.\n\n\nYou can do a \nobjdump vmlinux -d\n, and able to see the following instructions for\nalmost all functions: \ncallq  ffffffff81a01560 \n__fentry__\n.\n\n\nx86 related code: \narch/x86/kernel/ftrace_64.S\n, \narch/x86/kernel/ftrace.c\n\n\nQuestions: it seems we can know when a function got called by using fentry, but\nhow can we know the function has returned? The trick is: the returning address\nis pushed to stack when a function got called. So ftrace, again, can replace\nthat return address, so it can catch the exit time, and calculate the latency\nof a function. Neat!!\n\n\n\n\n\n\nResources\n\n\nftrace internal from Steven\n\n\n\n\n\n\nUsage\n\n\nFiles under /sys/kernel/debug/tracing/*\n\n\nperf help ftrace\n\n\n\n\n\n\n\n\nkprobe\n:\n\n\n\n\nMechanism\n\n\nKprobe replaces the original assembly instruction with an \nint3\n trap instruction.\n  So when we ran into the PC of the original instruction, an int3 CPU exception will happen.\n  Within \ndo_in3()\n, kernel will callback to core kprobe layer to do \npre-handler\n.\n  After singlestep, CPU have debug exception. Kernel walks into \ndo_debug()\n,\n  where kprobe run \npost-handler\n.\n\n\nKprobe is powerful, because it\ns able to trace almost everything at instruction level.\n\n\nKprobe can NOT touch things inside \nentry.S\n. It needs a valid \npt_regs\n to operate.\n\n\n\n\n\n\nResources\n\n\nAn introduction to kprobes (LWN)\n\n\n\n\n\n\n\n\neBPF\n:\n\n\n\n\nMechanism\n\n\nI think the most important thing is to understand what\ns the relationship between\neBPF and the others.\n\n\nPart I: Hook. eBPF attach its program to kprobe/uprobe/ftrace/perf_event.\nYou can think eBPF of \na generic callback layer\n for kprobe/uprobe/ftrace/perf_event.\nIt\ns essentially the second part of tracing as we mentioned above.\n\n\nPart II Run: eBPF run program when the above hook got invoked. eBPF is event-driven.\nThe program can be user-written eBPF code. Other articles explained it well.\n\n\n\n\n\n\nResources\n\n\nBrendan D. Gregg Blog\n\n\nGithub: Awesome-eBPF\n\n\nCilium: BPF and XDP Reference Guide\n\n\nBlog: An eBPF overview\n\n\nGithub: Bcc\n\n\n\n\n\n\n\n\nperf\n:\n\n\n\n\nperf tool is simply amazing. It not only use CPU PMU, but also integrated with ftrace/kprobe/eBPF.\n\n\nperf is a tool to present data, but also a tool to collect data.\n\n\nGood references\n\n\nhttp://www.brendangregg.com/perf.html\n\n\nhttps://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/\n\n\nhttps://opensource.com/article/18/7/fun-perf-and-python\n\n\n\n\n\n\n\n\nTrace in real time:\n\n\nPrint the number of page faults happen in every one second:\n\n1\nperf stat -e \npage-faults\n -I \n1000\n -a -- sleep \n10\n\n\n\n\n\n\nPrint the numberf of \nmmap\n syscall happen in every one second:\n\n1\nperf stat -e \nsyscalls:sys_enter_mmap\n -I \n1000\n -a -- sleep \n10\n\n\n\n\n\n\nDynamically trace kernel functions:\n\n1\n2\n3\nperf probe --add do_anonymous_page\nperf stat -I \n5000\n -e \npage-faults,probe:do_anonymous_page\n -- sleep \n10\n\nperf probe --del\n=\nprobe:do_anonymous_page\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Jun 10, 2019\n\nLast Updated: Jul 21, 2019", 
            "title": "Linux Trace/Profile"
        }, 
        {
            "location": "/notes/trace/#linux-tracing", 
            "text": "Some general notes about the various tracers inside Linux kernel.  Link to my old notes about the profilers/tracers in LegoOS:  notes .\nThe  profile points ,\nwhich is able to profile arbitray code piece is still my favorite thing.  In Linux kernel, we have:   ftrace  kprobe  uprobe  perf_event  tracepoints  eBPF   I tend to think this way:   Tracing needs two parts,  1)   Mechanims to do callback.  This means we need a way to\n    let our tracing/profiling code got invoked on a running system. This can be static\n    or dynamic. Static means we added our tracing code to source code, like tracepoints.\n    Dynamic means we added our tracing code when system is running, like ftrace and kprobe.\n     2)   Do stuff within callback.  All of them provide some sort of handling. But eBPF is the\n    most extensive one.  For example,  ftrace ,  kprobe , and  perf_event  include the callback facilities,\n    although they are not just limited to this.\n     ftrace  has the  call mount  way to do callback on every single function invocation.\n     kprobe  dynamically patch instructions and to do callback within exception handlers.\n     perf_event  can let CPU fire NMI interrupt. Those are all mechanisms to catch perf data.  The blog from Julia explains it well:  Linux tracing systems   how they fit together   ftrace :   Mechanism  For each un-inlined function, gcc inserts a  call mcount , or a  call fentry \ninstruction at the very beginning. This means whenever a function is called,\nthe  mcount()  or the  fentry()  callback will be invoked.  Having these  call  instructions introduce a lot overheads. So by default kernel\nreplace  call  with  nop . Only after we  echo something   setup_filter_functions \nwill the ftrace code replace  nop  with  call . Do note, Linux uses the linker\nmagic again here, check Steven s slides.  You can do a  objdump vmlinux -d , and able to see the following instructions for\nalmost all functions:  callq  ffffffff81a01560  __fentry__ .  x86 related code:  arch/x86/kernel/ftrace_64.S ,  arch/x86/kernel/ftrace.c  Questions: it seems we can know when a function got called by using fentry, but\nhow can we know the function has returned? The trick is: the returning address\nis pushed to stack when a function got called. So ftrace, again, can replace\nthat return address, so it can catch the exit time, and calculate the latency\nof a function. Neat!!    Resources  ftrace internal from Steven    Usage  Files under /sys/kernel/debug/tracing/*  perf help ftrace     kprobe :   Mechanism  Kprobe replaces the original assembly instruction with an  int3  trap instruction.\n  So when we ran into the PC of the original instruction, an int3 CPU exception will happen.\n  Within  do_in3() , kernel will callback to core kprobe layer to do  pre-handler .\n  After singlestep, CPU have debug exception. Kernel walks into  do_debug() ,\n  where kprobe run  post-handler .  Kprobe is powerful, because it s able to trace almost everything at instruction level.  Kprobe can NOT touch things inside  entry.S . It needs a valid  pt_regs  to operate.    Resources  An introduction to kprobes (LWN)     eBPF :   Mechanism  I think the most important thing is to understand what s the relationship between\neBPF and the others.  Part I: Hook. eBPF attach its program to kprobe/uprobe/ftrace/perf_event.\nYou can think eBPF of  a generic callback layer  for kprobe/uprobe/ftrace/perf_event.\nIt s essentially the second part of tracing as we mentioned above.  Part II Run: eBPF run program when the above hook got invoked. eBPF is event-driven.\nThe program can be user-written eBPF code. Other articles explained it well.    Resources  Brendan D. Gregg Blog  Github: Awesome-eBPF  Cilium: BPF and XDP Reference Guide  Blog: An eBPF overview  Github: Bcc     perf :   perf tool is simply amazing. It not only use CPU PMU, but also integrated with ftrace/kprobe/eBPF.  perf is a tool to present data, but also a tool to collect data.  Good references  http://www.brendangregg.com/perf.html  https://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/  https://opensource.com/article/18/7/fun-perf-and-python     Trace in real time:  Print the number of page faults happen in every one second: 1 perf stat -e  page-faults  -I  1000  -a -- sleep  10    Print the numberf of  mmap  syscall happen in every one second: 1 perf stat -e  syscalls:sys_enter_mmap  -I  1000  -a -- sleep  10    Dynamically trace kernel functions: 1\n2\n3 perf probe --add do_anonymous_page\nperf stat -I  5000  -e  page-faults,probe:do_anonymous_page  -- sleep  10 \nperf probe --del = probe:do_anonymous_page   \nYizhou Shan \nCreated: Jun 10, 2019 \nLast Updated: Jul 21, 2019", 
            "title": "Linux Tracing"
        }, 
        {
            "location": "/notes/cgroup-swap/", 
            "text": "How cgroup Trigger SwAp\n\n\nNotes on how cgroup mm triggers swap on a user-defined \nlimit_in_bytes\n.\nThis notes assume you have adequate knowledge on overall linux mm code.\nFor more information about cgroup in general, please check the \ndocument from RedHat\n.\n\n\nThere are several cgroup callbacks at \nmm/memory.c\n. Those functions are called to check if cgroup can honor this page allocation.\nAll of these functions are located in \nmm/memcontrol.c\n\n\n\n\nmem_cgroup_try_charge()\n\n\nmem_cgroup_commit_charge()\n\n\nmem_cgroup_cancel_charge()\n\n\n\n\nSome \nfacts\n about the implementation (up to linux 5.2)\n\n\n\n\nEach memory cgroup has its own LRU list vector\n\n\nAll memory cgroup\ns LRU lists and even the global LRU lists, they share a global LRU lock on a per-node basis. (Weird! Why?).\n\n\n\n\nTake a closer look of \nmem_cgroup_try_charge()\n, whose behavior is actually\nquite similar to the case of a real OOM: check if we still available\nmemory (here means memory usage is smaller than \nlimit_in_bytes\n),\nif unfortunately we run out of memory, it will then try to reclaim\nform the memory cgroup\ns LRU lists. If that did not work either,\nfinal step would be do OOM actions.\n\n\n\n\n\n\nmem_cgroup_try_charge()\n\n\n\n\ntry_charge()\n\n\npage_counter_try_charge():\n\n\nCheck if we hit \nlimit_in_bytes\n counter.\n\n\nHierarchically charge pages, costly.\n\n\n\n\n\n\ntry_to_free_mem_cgroup_pages()\n\n\nCallback to \nmm/vmscan.c\n to shrink the list (\nBingo!\n)\n\n\n\n\n\n\nAlso, reclaimer will establish swap pte entries\n\n\nmem_cgroup_oom()\n\n\n\n\n\n\n\n\n\n\n\n\nmem_cgroup_lruvec()\n\n\n\n\nOther than the global zone-wide LRU lists vector, each cgroup has its own LRU lists vector.\nChoose the vector that will be passed down to do \nshrink_page_list()\n etc.\n\n\n\n\n\n\n\n\nLRU Lists Maintainence\n\n\nInsertion to LRU lists is performed as follows: first, it will be inserted into a\nper-cpu array (\nlru_add_pvec\n). Once the array is full (default 15 entries),\nit will do a batch insertion into proper LRU lists (depends on \nmem_cgroup_lruvec\n we mentioned above).\n\n\nWhy Linux is doing this way? To scale.\n\n\n\nYizhou Shan\n\nCreated: Dec 3, 2018\n\nLast Updated: Jul 30, 2019", 
            "title": "Linux Cgroup and Swap"
        }, 
        {
            "location": "/notes/cgroup-swap/#how-cgroup-trigger-swap", 
            "text": "Notes on how cgroup mm triggers swap on a user-defined  limit_in_bytes .\nThis notes assume you have adequate knowledge on overall linux mm code.\nFor more information about cgroup in general, please check the  document from RedHat .  There are several cgroup callbacks at  mm/memory.c . Those functions are called to check if cgroup can honor this page allocation.\nAll of these functions are located in  mm/memcontrol.c   mem_cgroup_try_charge()  mem_cgroup_commit_charge()  mem_cgroup_cancel_charge()   Some  facts  about the implementation (up to linux 5.2)   Each memory cgroup has its own LRU list vector  All memory cgroup s LRU lists and even the global LRU lists, they share a global LRU lock on a per-node basis. (Weird! Why?).   Take a closer look of  mem_cgroup_try_charge() , whose behavior is actually\nquite similar to the case of a real OOM: check if we still available\nmemory (here means memory usage is smaller than  limit_in_bytes ),\nif unfortunately we run out of memory, it will then try to reclaim\nform the memory cgroup s LRU lists. If that did not work either,\nfinal step would be do OOM actions.    mem_cgroup_try_charge()   try_charge()  page_counter_try_charge():  Check if we hit  limit_in_bytes  counter.  Hierarchically charge pages, costly.    try_to_free_mem_cgroup_pages()  Callback to  mm/vmscan.c  to shrink the list ( Bingo! )    Also, reclaimer will establish swap pte entries  mem_cgroup_oom()       mem_cgroup_lruvec()   Other than the global zone-wide LRU lists vector, each cgroup has its own LRU lists vector.\nChoose the vector that will be passed down to do  shrink_page_list()  etc.", 
            "title": "How cgroup Trigger SwAp"
        }, 
        {
            "location": "/notes/cgroup-swap/#lru-lists-maintainence", 
            "text": "Insertion to LRU lists is performed as follows: first, it will be inserted into a\nper-cpu array ( lru_add_pvec ). Once the array is full (default 15 entries),\nit will do a batch insertion into proper LRU lists (depends on  mem_cgroup_lruvec  we mentioned above).  Why Linux is doing this way? To scale.  \nYizhou Shan \nCreated: Dec 3, 2018 \nLast Updated: Jul 30, 2019", 
            "title": "LRU Lists Maintainence"
        }, 
        {
            "location": "/notes/proc/", 
            "text": "Special Files\n\n\n\n\n/sys/devices/system/\nname\n\n\nCreation: \nsubsys_system_register()\n, @ \ndrivers/base/bus.c\n\n\nNote that this subdirectory is a legacy. Newer stuffer are added into other folders inside \n/sys\n.\n\n\n/sys/devices/system/cpu/*\n, @ \ndrivers/base/cpu.c\n\n\nRoot Object is \ncpu_root_attrs\n. The \nonline\n file belongs to another \nsub-object\n\n\nAnd this \nregister_cpu()\n function is used to setup the directories for each cpu.\n\n\n\n\n\n\n\n\n\n\n\n\nMany applications use \n/sys/devices/system/cpu/online\n to get the number of available CPUs.\nAnd it\ns hard to change this behavior because it\ns usually encoded inside glibc.\nThus, if you want to \nhide\n certain CPUs from applications for some reason,\nyou can write a kernel module that use \nset_cpu_active(cpu, false)\n,\nand then use the following small patch. (Note that using \nset_cpu_online(cpu, false)\n\nwill confuse CPU idle routine and panic.)\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\ndiff --git a/drivers/base/cpu.c b/drivers/base/cpu.c\n\n\n--- a/drivers/base/cpu.c\n\n\n+++ b/drivers/base/cpu.c\n\n\n@@ -220,7 +220,8 @@ static ssize_t show_cpus_attr(struct device *dev,\n\n\n /* Keep in sync with cpu_subsys_attrs */\n static struct cpu_attr cpu_attrs[] = {\n\n-       _CPU_ATTR(online, \n__cpu_online_mask),\n\n\n+       _CPU_ATTR(online, \n__cpu_active_mask),\n\n        _CPU_ATTR(possible, \n__cpu_possible_mask),\n        _CPU_ATTR(present, \n__cpu_present_mask),\n };\n\n\n\n\n\n\n\n/proc/pressure\n\n\nhttps://lwn.net/Articles/759658/\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Jul 26, 2019\n\nLast Updated: Aug 03, 2019", 
            "title": "Linux Special Files"
        }, 
        {
            "location": "/notes/proc/#special-files", 
            "text": "/sys/devices/system/ name  Creation:  subsys_system_register() , @  drivers/base/bus.c  Note that this subdirectory is a legacy. Newer stuffer are added into other folders inside  /sys .  /sys/devices/system/cpu/* , @  drivers/base/cpu.c  Root Object is  cpu_root_attrs . The  online  file belongs to another  sub-object  And this  register_cpu()  function is used to setup the directories for each cpu.       Many applications use  /sys/devices/system/cpu/online  to get the number of available CPUs.\nAnd it s hard to change this behavior because it s usually encoded inside glibc.\nThus, if you want to  hide  certain CPUs from applications for some reason,\nyou can write a kernel module that use  set_cpu_active(cpu, false) ,\nand then use the following small patch. (Note that using  set_cpu_online(cpu, false) \nwill confuse CPU idle routine and panic.)  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 diff --git a/drivers/base/cpu.c b/drivers/base/cpu.c  --- a/drivers/base/cpu.c  +++ b/drivers/base/cpu.c  @@ -220,7 +220,8 @@ static ssize_t show_cpus_attr(struct device *dev, \n\n /* Keep in sync with cpu_subsys_attrs */\n static struct cpu_attr cpu_attrs[] = { -       _CPU_ATTR(online,  __cpu_online_mask),  +       _CPU_ATTR(online,  __cpu_active_mask), \n        _CPU_ATTR(possible,  __cpu_possible_mask),\n        _CPU_ATTR(present,  __cpu_present_mask),\n };    /proc/pressure  https://lwn.net/Articles/759658/     \nYizhou Shan \nCreated: Jul 26, 2019 \nLast Updated: Aug 03, 2019", 
            "title": "Special Files"
        }, 
        {
            "location": "/notes/linux-resource/", 
            "text": "Linux Kernel Resource\n\n\n\n\nVersion History\nDate\nDescription\nMar 13, 2020\nadd lkt, nice\nOct 7, 2019\nFeels like I need to start thi\nHigh-level\n\n\n\n\nLinux Kernel Teaching\n\n\nThe Linux Storage Stack Diagram\n\n\nLinux kernel map\n\n\n\n\nSched\n\n\n\n\nEvolution of the x86 context switch in Linux\n\n\n\n\nMisc\n\n\n\n\nWhy printk() is so complicated (and how to fix it)\n\n\nSlide\n.\n\n\n\n\n\n\nSpecial sections in Linux binaries, 2013\n\n\n\n\nStorage\n\n\n\n\ni10, good NVMe code", 
            "title": "Linux Resource"
        }, 
        {
            "location": "/notes/linux-resource/#linux-kernel-resource", 
            "text": "Version History Date Description Mar 13, 2020 add lkt, nice Oct 7, 2019 Feels like I need to start thi", 
            "title": "Linux Kernel Resource"
        }, 
        {
            "location": "/notes/linux-resource/#high-level", 
            "text": "Linux Kernel Teaching  The Linux Storage Stack Diagram  Linux kernel map", 
            "title": "High-level"
        }, 
        {
            "location": "/notes/linux-resource/#sched", 
            "text": "Evolution of the x86 context switch in Linux", 
            "title": "Sched"
        }, 
        {
            "location": "/notes/linux-resource/#misc", 
            "text": "Why printk() is so complicated (and how to fix it)  Slide .    Special sections in Linux binaries, 2013", 
            "title": "Misc"
        }, 
        {
            "location": "/notes/linux-resource/#storage", 
            "text": "i10, good NVMe code", 
            "title": "Storage"
        }, 
        {
            "location": "/notes/kvm-basic/", 
            "text": "Just some basics about KVM\n\n\nResources\n\n\n\n\nIntel Virtualisation: How VT-x, KVM and QEMU Work Together\n\n\n\n\nHacking Notes\n\n\nIf you are hacking some low-level stuff that is running as a VM,\npay close attention if KVM is involved. I started this note because\nI spent sometime twisting \npage_fault\n IDT entry, but it turns out\nKVM uses \nasync_page_fault\n. Oh, well.\n\n\n\n\nKVM page fault entry (\narch/x86/entry/entry_64.S\n)\n\n\nIt is \nidtentry async_page_fault       do_async_page_fault     has_error_code=1\n\n\n..not \nidtentry page_fault             do_page_fault           has_error_code=1\n\n\n\n\n\n\n\n\nMore on Virturlization\n\n\nWell. I swear I want to learn more about Virturlization..\n\n\n\n\nIntel SDM, volume 3, Chapter 23 - Chapter 33.\n\n\n\n\n\nYizhou Shan\n\nCreated: May 20, 2019\n\nLast Updated: Sep 11, 2019", 
            "title": "Linux KVM"
        }, 
        {
            "location": "/notes/kvm-basic/#just-some-basics-about-kvm", 
            "text": "", 
            "title": "Just some basics about KVM"
        }, 
        {
            "location": "/notes/kvm-basic/#resources", 
            "text": "Intel Virtualisation: How VT-x, KVM and QEMU Work Together", 
            "title": "Resources"
        }, 
        {
            "location": "/notes/kvm-basic/#hacking-notes", 
            "text": "If you are hacking some low-level stuff that is running as a VM,\npay close attention if KVM is involved. I started this note because\nI spent sometime twisting  page_fault  IDT entry, but it turns out\nKVM uses  async_page_fault . Oh, well.   KVM page fault entry ( arch/x86/entry/entry_64.S )  It is  idtentry async_page_fault       do_async_page_fault     has_error_code=1  ..not  idtentry page_fault             do_page_fault           has_error_code=1", 
            "title": "Hacking Notes"
        }, 
        {
            "location": "/notes/kvm-basic/#more-on-virturlization", 
            "text": "Well. I swear I want to learn more about Virturlization..   Intel SDM, volume 3, Chapter 23 - Chapter 33.   \nYizhou Shan \nCreated: May 20, 2019 \nLast Updated: Sep 11, 2019", 
            "title": "More on Virturlization"
        }, 
        {
            "location": "/misc/essential/", 
            "text": "System Developing Essentials\n\n\nMisc Advice\n\n\n\n\nElevator Pitches, John Wilkes\n\n\nCreating an effective poster, John Wilkes\n\n\nHow to Get a Paper Accepted at OOPSLA\n\n\n\n\nTools\n\n\n\n\nStack and Register Dumper\n\n\nNMI and software Watchdog\n\n\nTracepoint and Ring Buffer\n\n\nProfilers\n\n\nCounters\n\n\nWhiskey and Luck\n\n\n\n\nKeep in mind\n\n\n\n\nStress your system\n\n\nEvery single critical subsystem\n\n\nConfident with your base subsystem\n\n\nFix bug/Improve perf at early stage\n\n\n\n\n\n\nPlan ahead\n\n\nSingle thread, or thread pool?\n\n\nHow to avoid using \nlock\n?\n\n\nWhat lock to use?\n\n\nHow to reduce \nlock contention\n?\n\n\nDoes this data structure need \nreference counter\n?\n\n\nShould I use per-cpu data structures?\n\n\nShould I pad this lock $-line aligned to avoid pingpong?\n\n\n\n\n\n\n\n\nDecent Cleanup\n\n\n\n\nI\nm fucking hate a crap kernel module just kill my machine, either stuck or bug.\n\n\nFree buffer/structure\n\n\nRemove the \npointer\n from friends\n list/tree. If you forgot to do so, mostly you will have some silent memory corruption. So be kind, cleanup what you have done during intilization.\n\n\nReport error. Do not be SILENT.\n\n\n\n\n\n\n\n\nClever Buffer Management\n\n\n\n\nkmem_cache?\n\n\nstatic pre-allocated array?\n\n\nRing buffer?\n\n\nOther than kmem_cache, I used other two solutions to optimize various dynamic allocation in LegoOS. The motivation is very simple: some data structures will be allocated/free very very frequently at runtime. So we want to speed it up!\n\n\n\n\n\n\n\n\nSystem Building Advice\n\n\n\n\nJohn Ousterhout\n\n\nIf you don\nt know what the problem was, you haven\nt fixed it\n\n\nIf it hasn\nt been used, it doesn\nt work", 
            "title": "Essential"
        }, 
        {
            "location": "/misc/essential/#system-developing-essentials", 
            "text": "", 
            "title": "System Developing Essentials"
        }, 
        {
            "location": "/misc/essential/#misc-advice", 
            "text": "Elevator Pitches, John Wilkes  Creating an effective poster, John Wilkes  How to Get a Paper Accepted at OOPSLA", 
            "title": "Misc Advice"
        }, 
        {
            "location": "/misc/essential/#tools", 
            "text": "Stack and Register Dumper  NMI and software Watchdog  Tracepoint and Ring Buffer  Profilers  Counters  Whiskey and Luck", 
            "title": "Tools"
        }, 
        {
            "location": "/misc/essential/#keep-in-mind", 
            "text": "Stress your system  Every single critical subsystem  Confident with your base subsystem  Fix bug/Improve perf at early stage    Plan ahead  Single thread, or thread pool?  How to avoid using  lock ?  What lock to use?  How to reduce  lock contention ?  Does this data structure need  reference counter ?  Should I use per-cpu data structures?  Should I pad this lock $-line aligned to avoid pingpong?     Decent Cleanup   I m fucking hate a crap kernel module just kill my machine, either stuck or bug.  Free buffer/structure  Remove the  pointer  from friends  list/tree. If you forgot to do so, mostly you will have some silent memory corruption. So be kind, cleanup what you have done during intilization.  Report error. Do not be SILENT.     Clever Buffer Management   kmem_cache?  static pre-allocated array?  Ring buffer?  Other than kmem_cache, I used other two solutions to optimize various dynamic allocation in LegoOS. The motivation is very simple: some data structures will be allocated/free very very frequently at runtime. So we want to speed it up!", 
            "title": "Keep in mind"
        }, 
        {
            "location": "/misc/essential/#system-building-advice", 
            "text": "John Ousterhout  If you don t know what the problem was, you haven t fixed it  If it hasn t been used, it doesn t work", 
            "title": "System Building Advice"
        }, 
        {
            "location": "/notes/benchmark/", 
            "text": "Benchmarks\n\n\nVersion History\nDate\nDescription\nApr 2, 2020\nUpdate\nAug 13, 2019\nUpdate\nAug 03, 2019\nInitial draft\nAreas\n\n\nSynchronization/Concurrency Community\n\n\n\n\nPhoenix HPCA (heavy mmap/munmap, i.e., mm-sem usage)\n\n\nMOSBENCH/Metis (same as Phoenix)\n\n\nLevelDB (a popular workload)\n\n\nLinux locktorture\n\n\nFilesystems (fs)\n\n\nLiTL, ATC\n16, \nhttps://github.com/multicore-locks/litl\n\n\nReferences\n\n\nShuffleLock, SOSP\n19\n\n\nCompact NUMA-aware Locks, EuroSys\n19\n\n\nfill me in\n\n\n\n\n\n\n\n\nOS\n\n\n\n\nwill-it-scale\n\n\nlmbench\n\n\nsysbench\n\n\n\n\nFPGA\n\n\n\n\nRosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA\n18\n\n\nAmophOS has a lot more.\n\n\n\n\nMisc Information\n\n\n\n\nSystems Benchmarking Crimes", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/notes/benchmark/#benchmarks", 
            "text": "Version History Date Description Apr 2, 2020 Update Aug 13, 2019 Update Aug 03, 2019 Initial draft", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/notes/benchmark/#areas", 
            "text": "", 
            "title": "Areas"
        }, 
        {
            "location": "/notes/benchmark/#synchronizationconcurrency-community", 
            "text": "Phoenix HPCA (heavy mmap/munmap, i.e., mm-sem usage)  MOSBENCH/Metis (same as Phoenix)  LevelDB (a popular workload)  Linux locktorture  Filesystems (fs)  LiTL, ATC 16,  https://github.com/multicore-locks/litl  References  ShuffleLock, SOSP 19  Compact NUMA-aware Locks, EuroSys 19  fill me in", 
            "title": "Synchronization/Concurrency Community"
        }, 
        {
            "location": "/notes/benchmark/#os", 
            "text": "will-it-scale  lmbench  sysbench", 
            "title": "OS"
        }, 
        {
            "location": "/notes/benchmark/#fpga", 
            "text": "Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA 18  AmophOS has a lot more.", 
            "title": "FPGA"
        }, 
        {
            "location": "/notes/benchmark/#misc-information", 
            "text": "Systems Benchmarking Crimes", 
            "title": "Misc Information"
        }, 
        {
            "location": "/misc/cheatsheet/", 
            "text": "Cheatsheet\n\n\nPython\n\n\n\n\nf\n{0x0c180606:032b}\n\n\n\n\nVNC\n\n\n\n\n\n\nServer side: Start server on certain port with certain geometry:\n\n1\nvncserver\n \n:\n66\n \n-\ngeometry\n \n1920\nx1080\n\n\n\n\n\n\n\n\n\n\nClient side: for safety, use SSH tunnel.\n\n\n\n\n-p 22\n: ssh port is 22\n\n\n-L 7777:localhost:5966\n: Forward localhost\ns 7777 to server\ns 5966\n\n1\n2\n3\n4\n5\nStep 1)\nssh -p 22 -v -C -L 7777:localhost:5966 root@yourserver.com\n\nStep 2)\nUse VNC client to establish connection with localhost:7777\n\n\n\n\n\n\n\n\n\n\n\nvirsh\n\n\n\n\nPass commands to QEMU in the virsh bash:\n\n1\n# qemu-monitor-command guest_os_id --hmp \ninfo cpus\n\n\n\n\n\n\n\n\nMarkdown\n\n\n\n\nEmoji cheatsheet\n\n\n\n\ntmux\n\n\n\n\nInstall \ntmux-plugins\n, it makes your terminal bling bling.\n\n\n\n\nbash\n\n\n\n\n\n\nShow current git branch in PS1:\n\n1\n2\n3\n4\n5\nparse_git_branch\n()\n \n{\n\n     git branch \n2\n /dev/null \n|\n sed -e \n/^[^*]/d\n -e \ns/* \\(.*\\)/ git:(\\1)/\n\n\n}\n\n\n\nPS1\n=\n\\[\\e[32m\\][\\u@\\h: \\W\\e[33m\\]\\$(parse_git_branch)\\[\\033[32m\\]]\\[\\e[00m\\] \n$\n \n\n\n\n\n\n\n\n\n\n\nForward \nman\n pages to \nvim\n:\n\n1\n2\nvman\n()\n \n{\n man \n$*\n \n|\n col -b \n|\n vim -c \nset ft=man nomod nolist\n -\n;\n \n}\n    \n\nalias\n \nman\n=\nvman\n\n\n\n\n\n\n\n\n\n\nGit\n\n\n1\ngit log --pretty=\n%C(Yellow)%h %C(auto)%d (%C(Green)%cr%C(reset))%x09 %C(Cyan)%an: %C(reset)%s\n --date=short --graph\n\n\n\n\n\n\nQEMU\n\n\n\n\nRun standalone kernel:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n# Create a new directory to store the serial output from printk().\n\n\nOUTPUT_DIR\n=\ntest-output\n\n\nif\n \n[\n -e \n$OUTPUT_DIR\n \n]\n;\n \nthen\n\n        \nif\n \n[\n -f \n$OUTPUT_DIR\n \n]\n;\n \nthen\n\n                \necho\n \nERROR: \n$OUTPUT_DIR\n is not a directly\n\n                \nexit\n \n1\n\n        \nfi\n\n\nelse\n\n        mkdir -p \n$OUTPUT_DIR\n\n\nfi\n\n\n\nKERNEL\n=\narch/x86_64/boot/bzImage\n\n\nKERNEL_PARAM\n=\nconsole=ttyS0 earlyprintk=serial,ttyS0,115200\n\n\nSERIAL\n=\n-serial file:\n$OUTPUT_DIR\n/ttyS0 -serial file:\n$OUTPUT_DIR\n/ttyS1\n\n\n\n# -cpu Haswell,+tsc,+sse,+xsave,+aes,+avx,+erms,+pdpe1gb,+pge \\\n\n\n# Above -cpu option may not work with some kernels.\n\nqemu-system-x86_64 -s  \n\\\n\n        -nographic \n\\\n\n        -kernel \n$KERNEL\n -append \n$KERNEL_PARAM\n \n\\\n\n        -no-reboot \n\\\n\n        -d int,cpu_reset -D \n$OUTPUT_DIR\n/qemu.log \n\\\n\n        \n$SERIAL\n \n\\\n\n        -m 16G \n\\\n\n        -monitor stdio \n\\\n\n        -smp \ncpus\n=\n24\n,cores\n=\n12\n,threads\n=\n2\n,sockets\n=\n2\n \n\\\n\n        -numa node,cpus\n=\n0\n-11,mem\n=\n8G,nodeid\n=\n0\n \n\\\n\n        -numa node,cpus\n=\n12\n-23,mem\n=\n8G,nodeid\n=\n1\n\n\n\n\n\n\n\n\nInstall CentOS on Dell PowerEdge\n\n\n\n\nEnable \nSR-IOV\n for future usage\n\n\nPress \nF11 Boot Manager\n during boot\n\n\nFind \nIntegrated Devices\n\n\nEnable \nSR-IOV Global Enable\n\n\n\n\n\n\nPartition\n\n\n/boot\n: e.g, 50GB\n\n\nswap\n: e.g, 4G\n\n\n/\n: all left\n\n\n\n\n\n\nDon\nt forget to enable Network during installation.\n\n\nChange SSH port\n\n\nDisable \nfirewalld\n\n\nsystemctl stop firewalld\n\n\nsystemctl disable firewalld\n\n\n\n\n\n\nIf SELinux is enabled\n\n\nyum install policycoreutils-python\n\n\nsemanage port -a -t ssh_port_t -p tcp #PORTNUMBER\n\n\n\n\n\n\nChange \n/etc/ssh/sshd_config\n\n\nsystemctl restart sshd\n\n\n\n\n\n\n\n\nAvoid Typing SSH Password\n\n\n\n\nGenerate keys: \nssh-keygen -t rsa\n\n\nCopy to remote: \nssh-copy-id -i ~/.ssh/id_rsa.pub username@remotehost -p 22\n\n\n\n\nGRUB2 on Ubuntu\n\n\n\n\nNothing like grubby?! Shame on you.\n\n\nStep I: \ncat /boot/grub/grub.cfg | grep menuentry\n\n\n1\n2\nmenuentry \nUbuntu, with Linux 4.16.0\n --class ubuntu  ...\nmenuentry \nUbuntu, with Linux 4.9.92\n --class ubuntu  ...\n\n\n\n\n\nStep II: Open \n/etc/default/grub\n, change\n\n\nGRUB_DEFAULT=\nAdvanced options for Ubuntu\nUbuntu, with Linux 4.16.0\n\n\nGRUB_DEFAULT=\nAdvanced options for Ubuntu\nUbuntu, with Linux 4.9.92\n\n\n\n\n\n\nStep III: \nsudo update-grub\n\n\n\n\nMigrate to Ubuntu From MacOS\n\n\n\n\nDisable [Super+p]\n. This is my tmux prefix somehow.\n\n\nxmodmap to switch Super and CTRL. \n1", 
            "title": "Cheatsheet"
        }, 
        {
            "location": "/misc/cheatsheet/#cheatsheet", 
            "text": "", 
            "title": "Cheatsheet"
        }, 
        {
            "location": "/misc/cheatsheet/#python", 
            "text": "f {0x0c180606:032b}", 
            "title": "Python"
        }, 
        {
            "location": "/misc/cheatsheet/#vnc", 
            "text": "Server side: Start server on certain port with certain geometry: 1 vncserver   : 66   - geometry   1920 x1080      Client side: for safety, use SSH tunnel.   -p 22 : ssh port is 22  -L 7777:localhost:5966 : Forward localhost s 7777 to server s 5966 1\n2\n3\n4\n5 Step 1)\nssh -p 22 -v -C -L 7777:localhost:5966 root@yourserver.com\n\nStep 2)\nUse VNC client to establish connection with localhost:7777", 
            "title": "VNC"
        }, 
        {
            "location": "/misc/cheatsheet/#virsh", 
            "text": "Pass commands to QEMU in the virsh bash: 1 # qemu-monitor-command guest_os_id --hmp  info cpus", 
            "title": "virsh"
        }, 
        {
            "location": "/misc/cheatsheet/#markdown", 
            "text": "Emoji cheatsheet", 
            "title": "Markdown"
        }, 
        {
            "location": "/misc/cheatsheet/#tmux", 
            "text": "Install  tmux-plugins , it makes your terminal bling bling.", 
            "title": "tmux"
        }, 
        {
            "location": "/misc/cheatsheet/#bash", 
            "text": "Show current git branch in PS1: 1\n2\n3\n4\n5 parse_git_branch ()   { \n     git branch  2  /dev/null  |  sed -e  /^[^*]/d  -e  s/* \\(.*\\)/ git:(\\1)/  }  PS1 = \\[\\e[32m\\][\\u@\\h: \\W\\e[33m\\]\\$(parse_git_branch)\\[\\033[32m\\]]\\[\\e[00m\\]  $        Forward  man  pages to  vim : 1\n2 vman ()   {  man  $*   |  col -b  |  vim -c  set ft=man nomod nolist  - ;   }      alias   man = vman", 
            "title": "bash"
        }, 
        {
            "location": "/misc/cheatsheet/#git", 
            "text": "1 git log --pretty= %C(Yellow)%h %C(auto)%d (%C(Green)%cr%C(reset))%x09 %C(Cyan)%an: %C(reset)%s  --date=short --graph", 
            "title": "Git"
        }, 
        {
            "location": "/misc/cheatsheet/#qemu", 
            "text": "Run standalone kernel:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 # Create a new directory to store the serial output from printk().  OUTPUT_DIR = test-output  if   [  -e  $OUTPUT_DIR   ] ;   then \n         if   [  -f  $OUTPUT_DIR   ] ;   then \n                 echo   ERROR:  $OUTPUT_DIR  is not a directly \n                 exit   1 \n         fi  else \n        mkdir -p  $OUTPUT_DIR  fi  KERNEL = arch/x86_64/boot/bzImage  KERNEL_PARAM = console=ttyS0 earlyprintk=serial,ttyS0,115200  SERIAL = -serial file: $OUTPUT_DIR /ttyS0 -serial file: $OUTPUT_DIR /ttyS1  # -cpu Haswell,+tsc,+sse,+xsave,+aes,+avx,+erms,+pdpe1gb,+pge \\  # Above -cpu option may not work with some kernels. \nqemu-system-x86_64 -s   \\ \n        -nographic  \\ \n        -kernel  $KERNEL  -append  $KERNEL_PARAM   \\ \n        -no-reboot  \\ \n        -d int,cpu_reset -D  $OUTPUT_DIR /qemu.log  \\ \n         $SERIAL   \\ \n        -m 16G  \\ \n        -monitor stdio  \\ \n        -smp  cpus = 24 ,cores = 12 ,threads = 2 ,sockets = 2   \\ \n        -numa node,cpus = 0 -11,mem = 8G,nodeid = 0   \\ \n        -numa node,cpus = 12 -23,mem = 8G,nodeid = 1", 
            "title": "QEMU"
        }, 
        {
            "location": "/misc/cheatsheet/#install-centos-on-dell-poweredge", 
            "text": "Enable  SR-IOV  for future usage  Press  F11 Boot Manager  during boot  Find  Integrated Devices  Enable  SR-IOV Global Enable    Partition  /boot : e.g, 50GB  swap : e.g, 4G  / : all left    Don t forget to enable Network during installation.  Change SSH port  Disable  firewalld  systemctl stop firewalld  systemctl disable firewalld    If SELinux is enabled  yum install policycoreutils-python  semanage port -a -t ssh_port_t -p tcp #PORTNUMBER    Change  /etc/ssh/sshd_config  systemctl restart sshd", 
            "title": "Install CentOS on Dell PowerEdge"
        }, 
        {
            "location": "/misc/cheatsheet/#avoid-typing-ssh-password", 
            "text": "Generate keys:  ssh-keygen -t rsa  Copy to remote:  ssh-copy-id -i ~/.ssh/id_rsa.pub username@remotehost -p 22", 
            "title": "Avoid Typing SSH Password"
        }, 
        {
            "location": "/misc/cheatsheet/#grub2-on-ubuntu", 
            "text": "Nothing like grubby?! Shame on you.  Step I:  cat /boot/grub/grub.cfg | grep menuentry  1\n2 menuentry  Ubuntu, with Linux 4.16.0  --class ubuntu  ...\nmenuentry  Ubuntu, with Linux 4.9.92  --class ubuntu  ...   Step II: Open  /etc/default/grub , change  GRUB_DEFAULT= Advanced options for Ubuntu Ubuntu, with Linux 4.16.0  GRUB_DEFAULT= Advanced options for Ubuntu Ubuntu, with Linux 4.9.92    Step III:  sudo update-grub", 
            "title": "GRUB2 on Ubuntu"
        }, 
        {
            "location": "/misc/cheatsheet/#migrate-to-ubuntu-from-macos", 
            "text": "Disable [Super+p] . This is my tmux prefix somehow.  xmodmap to switch Super and CTRL.  1", 
            "title": "Migrate to Ubuntu From MacOS"
        }, 
        {
            "location": "/notes/paper_fpga/", 
            "text": "An FPGA Reading List\n\n\n\n\nVersion History\nDate\nDescription\nNov 30, 2019\nAdd a lot security papers\nOct 22, 2019\nShuffle scheduling section. More focused. Add two more recent fpga-virt papers\nOct 5, 2019\nMore on scheduling. Add NoC. Add Security.\nOct 4, 2019\nAdd more papers extracted from AmophOS\nOct 3, 2019\nInitial version from \nGithub\nThis is a list of \nacademic papers\n that cover all sorts of FPGA related topic,\nmore from a system researcher\ns point of view though.\n\n\n\n\nVirtualization\n\n\nScheduling\n\n\nNoC\n\n\nMemory Hierarchy\n\n\nDynamic Memory Allocation\n\n\nIntegrate with Host Virtual Memory\n\n\nIntegrate with Host OSs\n\n\nSecurity\n\n\nSummary\n\n\n\n\n\n\nLanguages, Runtime, and Framework\n\n\nXilinx HLS\n\n\nXilinx CAD\n\n\nHigh-Level Languages and Platforms\n\n\nIntegrate with Frameworks\n\n\nCloud Infrastructure\n\n\nMisc\n\n\n\n\n\n\nApplications\n\n\nProgrammable Network\n\n\nDatabase\n\n\nStorage\n\n\nMachine Learning\n\n\nGraph\n\n\nKey-Value Store\n\n\nBio\n\n\nConsensus\n\n\nVideo Processing\n\n\nBlockchain\n\n\nMicro-services\n\n\n\n\n\n\nFPGA Internal\n\n\nGeneral\n\n\nPartial Reconfiguration\n\n\nLogical Optimization and Technology Mapping\n\n\nPlace and Route\n\n\nRTL2FPGA\n\n\n\n\n\n\n\n\n\n\nVirtualization\n\n\nScheduling\n\n\nScheduling is big topic for FPGA. Unlike the traditional CPU scheduling,\nthere are more aspects to consider, e.g.,\n1) Partial reconfiguration (PR), 2) Dynamic self PR,\n3) Preemptive scheduling, 4) Relocation, 5) Floorplanning, and so on.\n\n\nPreemptive Scheduling\n\n\n\n\nPreemptive multitasking on FPGAs, 2000\n\n\nMultitasking on FPGA Coprocessors, 2000\n\n\nContext saving and restoring for multitasking in reconfigurable systems, 2005\n\n\nReconOS Cooperative multithreading in dynamically reconfigurable systems, FPL\n09\n\n\nBlock, drop or roll(back): Alternative preemption methods for RH multi-tasking, FCCM\n09\n\n\nHardware Context-Switch Methodology for Dynamically Partially Reconfigurable Systems, 2010\n\n\nOn-chip Context Save and Restore of Hardware Tasks on Partially Reconfigurable FPGAs, 2013\n\n\nHTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013\n\n\nPreemptive Hardware Multitasking in ReconOS, 2015\n\n\n\n\nPreemptive Reconfiguration\n\n\n\n\nPreemption of the Partial Reconfiguration Process to Enable Real-Time Computing, 2018\n\n\n\n\nBitstreams\n\n\n\n\nGithub 7-series bitmap reverse engineering\n\n\nPARBIT: A Tool to Transform Bitfiles to Implement Partial Reconfiguration of Field Programmable Gate Arrays (FPGAs), 2001\n\n\nBIL: A TOOL-CHAIN FOR BITSTREAM REVERSE-ENGINEERING, 2012\n\n\nBITMAN: A Tool and API for FPGA Bitstream Manipulations, 2017\n\n\n\n\nRelocation:\n\n\n\n\nContext saving and restoring for multitasking in reconfigurable systems, 2005\n\n\nREPLICA2Pro: Task Relocation by Bitstream Manipulation in Virtex-II/Pro FPGAs, 2006\n\n\nRelocation and Automatic Floor-planning of FPGA Partial Configuration Bit-Streams, MSR 2008\n\n\nInternal and External Bitstream Relocation for Partial Dynamic Reconfiguration, 2009\n\n\nPRR-PRR Dynamic Relocation, 2009\n\n\nHTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2003\n\n\nAutoReloc, 2016\n\n\nHTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013\n\n\n\n\nOthers\n\n\n\n\nhthreads: A hardware/software co-designed multithreaded RTOS kernel, 2005\n\n\nhthreads: Enabling a Uniform Programming Model Across the Software/Hardware Boundary, FCCM\n16\n\n\nTartan: Evaluating Spatial Computation for Whole Program Execution, ASPLOS\n06\n\n\nA virtual hardware operating system for the Xilinx XC6200, 1996\n\n\nThe Swappable Logic Unit: a Paradigm for Virtual Hardware, FCCM\n97\n\n\nRun-time management of dynamically reconfigurable designs, 1998\n\n\nAll above ones are early work on FPGA scheduling.\n\n\nWorth a read, but don\nt take some of their assumptions. Some have been changed after SO many years.\n\n\n\n\n\n\nS1. Reconfigurable Hardware Operating Systems: From Design Concepts to Realizations, 2003\n\n\nS2. Operating Systems for Reconfigurable Embedded Platforms: Online Scheduling of Real-Time Tasks, 2004\n\n\nVery fruitful discussion. The paper schedules bitstreams inside FPGA,\n  following a \nReal-Time sched policy (deadline)\n.\n\n\nDifferent from CPU sched, FPGA scheduling needs to consider \nareas\n. The chip is\na rectangle box, allocating areas needs great care to avoid fragmentation!\n\n\n\n\n\n\nContext saving and restoring for multitasking in reconfigurable systems, FPL\n05\n\n\nOptimizing deschedule perf.\n\n\nThis paper discusses ways to save and restore the state information of a hardware task.\n  There are generally three approachs: a) adding indirection. Let app use system API to read/write states.\n  b) yield-type API. c) use PR controller to read back bitstream.\n\n\nThis paper used ICAP to read the bitstream back and extract necenssay state information that must\nbe present at next bitstream resume.\n\n\n\n\n\n\nScheduling intervals for reconfigurable computing, FCCM\n08\n\n\nHardware context-switch methodology for dynamically partially reconfigurable systems, 2010\n\n\nOnline Scheduling for Multi-core Shared Reconfigurable Fabric, DATE\n12\n\n\nMulti-shape Tasks Scheduling for Online Multitasking on FPGAs, 2014\n\n\nAmophOS, OSDI\n18\n\n\nHardware context switching on FPGAs, 2014\n\n\nEfficient Hardware Context-Switch for Task Migration between Heterogeneous FPGAs, 2016\n\n\n\n\nNoC\n\n\nNetwork-on-Chip on FPGA.\n\n\n\n\nInterconnection Networks Enable Fine-Grain Dynamic Multi-Tasking on FPGAs, 2002\n\n\nLike the idea of separating computation from communication.\n\n\nAlso a lot discussions about possible NoC designs within FPGA.\n\n\n\n\n\n\nLEAP Soft connections: Addressing the hardware-design modularity problem, DAC\n09\n\n\nVirtual channel concept. Time-insensitive.\n\n\n\n\n\n\nLeveraging Latency-Insensitivity to Ease Multiple FPGA Design, FPGA\n12\n\n\nCONNECT: re-examining conventional wisdom for designing nocs in the context of FPGAs, FPGA\n12\n\n\nYour Programmable NIC Should be a Programmable Switch, HotNets\n18\n\n\n\n\nMemory Hierarchy\n\n\nPapers deal with BRAM, registers, on-board DRAM, and host DRAM.\n\n\n\n\nLEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA\n11\n\n\nMain design hierarchy: Use BRAM as L1 cache, use on-board DRAM as L2 cache, and host memory as the backing store. Everthing is abstracted away through their interface (similar to load/store). Programming is pretty much the same as if you are writing for CPU.\n\n\nAccording to sec 2.2.2, its scratchpad controller, is using simple segment-based mapping scheme. Like AmorphOS\ns one.\n\n\n\n\n\n\nLEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM\n14\n\n\nFollow up work on LEAP Scratchpads, extends the work to have cache coherence between multiple FPGAs.\n\n\nCoherent Scatchpads with MOSI protocol.\n\n\n\n\n\n\nMATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA\n15\n\n\nCoRAM: An In-Fabric Memory Architecture for FPGA-Based Computing\n\n\nCoRAM provides an interface for managing the on- and off-chip memory resource of an FPGA. It use \ncontrol threads\n enforce low-level control on data movement.\n\n\nSeriously, the CoRAM is just like Processor L1-L3 caches.\n\n\n\n\n\n\nCoRAM Prototype and evaluation of the CoRAM memory architecture for FPGA-based computing, FPGA\n12\n\n\nPrototype on FPGA.\n\n\n\n\n\n\nSharing, Protection, and Compatibility for Reconfigurable Fabric with AMORPHOS, OSDI\n18\n\n\nHull: provides memory protection for on-board DRAM using \nsegment-based\n address translation.\n\n\n\n\n\n\nVirtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\n17\n\n\n\n\nDynamic Memory Allocation\n\n\nmalloc()\n and \nfree()\n for FPGA on-board DRAM.\n\n\n\n\nA High-Performance Memory Allocator for Object-Oriented Systems, IEEE\n96\n\n\nSysAlloc: A Hardware Manager for Dynamic Memory Allocation in Heterogeneous Systems, FPL\n15\n\n\nHi-DMM: High-Performance Dynamic Memory Management in High-Level Synthesis, IEEE\n18\n\n\n\n\nIntegrate with Host Virtual Memory\n\n\nPapers deal with OS Virtual Memory System (VMS). Note that, all these papers introduce some form of MMU into the FPGA\nto let FPGA be able to work with host VMS.\nThis added MMU is similar to CPU\ns MMU and \nRDMA NIC\ns internal cache\n.\nNote that the VMS still runs inside Linux (include pgfault, swapping, TLB shootdown and so on. What could really stands out, is to implement VMS inside FPGA.)\n\n\n\n\nVirtual Memory Window for Application-Specific Reconfigurable Coprocessors, DAC\n04\n\n\nEarly work that adds a new MMU to FPGA to let FPGA logic access \non-chip DRAM\n. Note, it\ns not the system main memory. Thus the translation pgtable is different.\n\n\nHas some insights on prefetching and MMU CAM design.\n\n\n\n\n\n\nSeamless Hardware Software Integration in Reconfigurable Computing Systems, 2005\n\n\nFollow up summary on previous DAC\n04 Virtual Memory Window.\n\n\n\n\n\n\nA Reconfigurable Hardware Interface for a Modern Computing System, FCCM\n07\n\n\nThis work adds a new MMU which includes a 16-entry TLB to FPGA. FPGA and CPU shares the same user virtual address space, use the same physical memory. FPGA and CPU share memory at \ncacheline granularity\n, FPGA is just another core in this sense. Upon a TLB miss at FPGA MMU, the FPGA sends interrupt to CPU, to let \nsoftware to handle the TLB miss\n. Using software-managed TLB miss is not efficient. But they made cache coherence between FPGA and CPU easy.\n\n\n\n\n\n\nLow-Latency High-Bandwidth HW/SW Communication in a Virtual Memory Environment, FPL\n08\n\n\nThis work actually add a new MMU to FPGA, which works just like CPU MMU. It\ns similar to IOMMU, in some sense.\n\n\nBut I think they missed one important aspect: cache coherence between CPU and FPGA. There is not too much information about this in the paper, it seems they do not have cache at FPGA. Anyhow, this is why recently CCIX and OpenCAPI are proposed.\n\n\n\n\n\n\nMemory Virtualization for Multithreaded Reconfigurable Hardware, FPL\n11\n\n\nPart of the ReconOS project\n\n\nThey implemented a simple MMU inside FPGA that includes a TLB. On protection violation or page invalid access cases, their MMU just hand over to CPU pgfault routines. How is this different from the FPL\n08 one? Actually, IMO, they are the same.\n\n\n\n\n\n\nS4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\n17\n\n\nThis paper also implemented a hardware MMU, but the virtual memory system still run on Linux.\n\n\nAlso listed in \nCloud Infrastructure\n part.\n\n\n\n\n\n\nLightweight Virtual Memory Support for Many-Core Accelerators in Heterogeneous Embedded SoCs, 2015\n\n\nLightweight Virtual Memory Support for Zero-Copy Sharing of Pointer-Rich Data Structures in Heterogeneous Embedded SoCs, IEEE\n17\n\n\nPart of the PULP project.\n\n\nEssentially a software-managed IOMMU. The control path is running as a Linux kernel module. The datapath is a lightweight AXI transation translation.\n\n\n\n\n\n\n\n\nIntegrate with Host OSs\n\n\n\n\nA Virtual Hardware Operating System for the Xilinx XC6200, FPL\n96\n\n\nOperating systems for reconfigurable embedded platforms: online scheduling of real-time tasks, IEEE\n04\n\n\nhthreads: a hardware/software co-designed multithreaded RTOS kernel, 2005\n\n\nReconfigurable computing: architectures and design methods, IEE\n05\n\n\nBORPH: An Operating System for FPGA-Based Reconfigurable Computers. PhD Thesis.\n\n\nFUSE: Front-end user framework for O/S abstraction of hardware accelerators, FCCM\n11\n\n\nReconOS \u2013 an Operating System Approach for Reconfigurable Computing, IEEE Micro\n14\n\n\nInvoke kernel from FPGA. They built a shell in FPGA and delegation threads in CPU to achieve this.\n\n\nThey implemented their own MMU (using pre-established pgtables) to let FPGA logic to access system memory. \nRef\n.\n\n\nRead the \nOperating Systems for Reconfigurable Computing\n sidebar, nice summary.\n\n\n\n\n\n\nLEAP Soft connections: Addressing the hardware-design modularity problem, DAC\n09\n\n\nChannel concept. Good.\n\n\n\n\n\n\nLEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA\n11\n\n\nBRAM/on-board DRAM/host DRAM layering. Caching.\n\n\n\n\n\n\nLEAP Shared Memories: Automating the Construction of FPGA Coherent Memories\n\n\nAdd cache-coherence on top of previous work.\n\n\nAlso check out my note on \nCache Coherence\n.\n\n\n\n\n\n\nLEAP FPGA Operating System, FPL\n14.\n\n\nA Survey on FPGA Virtualization, FPL\n18\n\n\nZUCL 2.0: Virtualised Memory and Communication for ZYNQ UltraScale+ FPGAs, FSP\n19\n\n\n\n\nSecurity\n\n\nIf I were to recommend, I\nd suggest start from:\n\n\n\n\nRecent Attacks and Defenses on FPGA-based Systems, 2019\n\n\nPhysical Side-Channel Attacks and Covert Communication on FPGAs: A Survey, 2019\n\n\nFPGA security: Motivations, features, and applications, 2014\n\n\n\n\nThe whole list:\n\n\n\n\nFPGAhammer : Remote Voltage Fault Attacks on Shared FPGAs , suitable for DFA on AES\n\n\nFPGA-Based Remote Power Side-Channel Attacks\n\n\nCharacterization of long wire data leakage in deep submicron FPGAS\n\n\nProtecting against cryptographic Trojans in FPGAS\n\n\nFPGA Side Channel Attacks without Physical Access\n\n\nFPGA security: Motivations, features, and applications\n\n\nFPGA side-channel receivers\n\n\nSecurity of FPGAs in data centers\n\n\nSecure Function Evaluation Using an FPGA Overlay Architecture\n\n\nMitigating Electrical-level Attacks towards Secure Multi-Tenant FPGAs in the Cloud\n\n\nThe Costs of Confidentiality in Virtualized FPGAs\n\n\nTemporal Thermal Covert Channels in Cloud FPGAs\n\n\nCharacterizing Power Distribution Attacks in Multi-User FPGA Environments\n\n\nFASE: FPGA Acceleration of Secure Function Evaluation\n\n\nSecuring Cryptographic Circuits by Exploiting Implementation Diversity and Partial Reconfiguration on FPGAs\n\n\nMeasuring Long Wire Leakage with Ring Oscillators in Cloud FPGAs\n\n\nPhysical Side-Channel Attacks and Covert Communication on FPGAs: A Survey\n\n\nLeaky Wires: Information Leakage and Covert Communication Between FPGA Long Wires\n\n\nUsing the Power Side Channel of FPGAs for Communication\n\n\nAn Inside Job: Remote Power Analysis Attacks on FPGAs\n\n\nLeakier Wires: Exploiting FPGA Long Wires for Covert- and Side-channel Attacks\n\n\nVoltage drop-based fault attacks on FPGAs using valid bitstreams\n\n\nMoats and Drawbridges: An Isolation Primitive for Reconfigurable Hardware Based Systems\n\n\nSensing nanosecond-scale voltage attacks and natural transients in FPGAs\n\n\nHolistic Power Side-Channel Leakage Assessment:\n\n\nHiding Intermittent Information Leakage with Architectural Support for Blinking\n\n\nExamining the consequences of high-level synthesis optimizations on power side-channel\n\n\nRegister transfer level information flow tracking for provably secure hardware design\n\n\nA Protection and Pay-per-use Licensing Scheme for On-cloud FPGA Circuit IPs\n\n\nRecent Attacks and Defenses on FPGA-based Systems\n\n\nPFC: Privacy Preserving FPGA Cloud - A Case Study of MapReduce\n\n\nA Pay-per-Use Licensing Scheme for Hardware IP Cores in Recent SRAM-Based FPGAs\n\n\nFPGAs for trusted cloud computing\n\n\n\n\nSummary\n\n\nSummary on current FPGA Virtualization Status. Prior art mainly focus on: 1) How to virtualize on-chip BRAM (e.g., CoRAM, LEAP Scratchpad),\n2) How to work with host, specifically, how to use the host DRAM, how to use host virtual memory.\n3) How to schedule bitstreams inside a FPGA chip. 4) How to provide certain services to make FPGA programming easier (mostly work with host OS).\n\n\nLanguages, Runtime, and Framework\n\n\nInnovations in the toolchain space.\n\n\nXilinx HLS\n\n\n\n\nDesign Patterns for Code Reuse in HLS Packet Processing Pipelines, FCCM\n19\n\n\nA very good HLS library from Mellanox folks.\n\n\n\n\n\n\nTemplatised Soft Floating-Point for High-Level Synthesis, FCCM\n19\n\n\nST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA, FCCM\n18\n\n\nHLScope+: Fast and Accurate Performance Estimation for FPGA HLS, ICCAD\n17\n\n\nSeparation Logic-Assisted Code Transformations for Efficient High-Level Synthesis, FCCM\n14\n\n\nAn HLS design aids that analyze the original program at \ncompile time\n and perform automated code transformations. The tool analysis pointer-manipulating programs and automatically splits heap-allocated data structures into disjoint, independent regions.\n\n\nThe tool is for C++ heap operations.\n\n\nTo put in another way: the tool looks at your BRAM usage, found any false-dependencies, and make multiple independent regions, then your II is improved.\n\n\n\n\n\n\nMATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA\n15\n\n\nThis is an HLS toolchain aid.\n\n\nFollow-up work of the above FCCM\n14 one. This time they use LEAP scracchpads as the underlying caching block.\n\n\n\n\n\n\n\n\nXilinx CAD\n\n\n\n\nMaverick: A Stand-alone CAD Flow for Partially Reconfigurable FPGA Modules, FCCM\n19\n\n\n\n\nHigh-Level Languages and Platforms\n\n\n\n\nJust-in-Time Compilation for Verilog, ASPLOS\n19\n\n\nChisel: Constructing Hardware in a Scala Embedded Language, DAC\n12\n\n\nChisel is being actively improved and used by UCB folks.\n\n\n\n\n\n\nRosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA\n18\n\n\nFrom JVM to FPGA: Bridging Abstraction Hierarchy via Optimized Deep Pipelining, HotCloud\n18\n\n\nHeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing, FPGA\n19\n\n\nLINQits: Big Data on Little Clients, ISCA\n13\n\n\nFrom Microsoft, used to express SQL-like functions (thus big data) and runs on ZYNQ (thus little client),\n\n\nYou wrote C#, LINQits translate it to verilog, and run the whole thing at a ZYNQ (ARM+FPGA) board.\n\n\n\n\n\n\nLime: a Java-Compatible and Synthesizable Language for Heterogeneous Architectures, OOPSLA\n10\n\n\nLime is a Java-based programming model and runtime from IBM which aims to provide a single unified\n  language to program heterogeneous architectures, from FPGAs to conventional CPUs\n\n\n\n\n\n\nA line of work from Standord\n\n\nGenerating configurable hardware from parallel patterns, ASPLOS\n16\n\n\nPlasticine: A Reconfigurable Architecture For Parallel Patterns, ISCA\n17\n\n\nSpatial: A Language and Compiler for Application Accelerators, PLDI\n18\n\n\nSpatial generates \nChisel\n code along with C++ code which can be used on a host CPU to control the execution of the accelerator on the target FPGA.\n\n\nThis kind of academic papers must have a lot good ideas. But the truth is it will not be reliable because it\ns from academic labs.\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrate with Frameworks\n\n\n\n\nMap-reduce as a Programming Model for Custom Computing Machines, FCCM\n08\n\n\nThis paper proposes a model to translate MapReduce code written in C to code that could run on FPGA and GPU. Many details are omitted, and they don\nt really have the compiler.\n\n\nSingle-host framework, everything is in FPGA and GPU.\n\n\n\n\n\n\nAxel: A Heterogeneous Cluster with FPGAs and GPUs, FPGA\n10\n\n\nA distributed MapReduce Framework, targets clusters with CPU, GPU, and FPGA. Mainly the idea of scheduling FPGA/GPU jobs.\n\n\nDistributed Framework.\n\n\n\n\n\n\nFPMR: MapReduce Framework on FPGA, FPGA\n10\n\n\nA MapReduce framework on a single host\ns FPGA. You need to write Verilog/HLS for processing logic to hook with their framework. The framework mainly includes a data transfer controller, a simple schedule that enable certain blocks at certain time.\n\n\nSingle-host framework, everything is in FPGA.\n\n\n\n\n\n\nMelia: A MapReduce Framework on OpenCL-Based FPGAs, IEEE\n16\n\n\nAnother framework, written in OpenCL, and users can use OpenCL to program as well. Similar to previous work, it\ns more about the framework design, not specific algorithms on FPGA.\n\n\nSingle-host framework, everything is in FPGA. But they have a discussion on running on multiple FPGAs.\n\n\nFour MapReduce FPGA papers here, I believe there are more. The marriage between MapReduce and FPGA is not something hard to understand. FPGA can be viewed as another core with different capabilities. The thing is, given FPGA\ns reprogram-time and limited on-board memory, how to design a good scheduling algorithm and data moving/caching mechanisms. Those papers give some hints on this.\n\n\n\n\n\n\nUCLA: When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud\n16\n\n\nUCLA: Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale, SoCC\n16\n\n\nA system that hooks FPGA with Spark.\n\n\nThere is a line of work that hook FPGA with big data processing framework (Spark), so the implementation of FPGA and the scale-out software can be separated. The Spark can schedule FPGA jobs to different machines, and take care of scale-out, failure handling etc. But, I personally think this line of work is really just an extension to ReconOS/FUSE/BORPH line of work. The main reason is: both these two lines of work try to integrate jobs run on CPU and jobs run on FPGA, so CPU and FPGA have an easier way to talk, or put in another way, CPU and FPGA have a better division of labor. Whether it\ns single-machine (like ReconOS, Melia), or distributed (like Blaze, Axel), they are essentially the same.\n\n\n\n\n\n\nUCLA: Heterogeneous Datacenters: Options and Opportunities, DAC\n16\n\n\nFollow up work of Blaze. Nice comparison of big and wimpy cores.\n\n\n\n\n\n\n\n\nCloud Infrastructure\n\n\n\n\nHuawei: FPGA as a Service in the Cloud\n\n\nUCLA: Customizable Computing: From Single Chip to Datacenters, IEEE\n18\n\n\nUCLA: Accelerator-Rich Architectures: Opportunities and Progresses, DAC\n14\n\n\nReminds me of \nOmniX\n. Disaggregation at a different scale.\n\n\nThis paper actually targets single-machine case. But it can reflect a distributed setting.\n\n\n\n\n\n\nEnabling FPGAs in the Cloud, CF\n14\n\n\nPaper raised four important aspects to enable FPGA in cloud: Abstraction, Sharing, Compatibility, and Security. FPGA itself requires a shell (paper calls it service logic) and being partitioned into multiple slots. Things discussed in the paper are straightforward, but worth reading. They did not solve the FPGA sharing issue, which, is solved by AmorphOS.\n\n\n\n\n\n\nFPGAs in the Cloud: Booting Virtualized Hardware Accelerators with OpenStack, FCCM\n14\n\n\nUse OpenStack to manage FPGA resource. The FPGA is partitioned into multiple regions, each region can use PR. The FPGA shell includes: 1) basic MAC, and packet dispatcher, 2) memory controller, and segment-based partition scheme, 3) a soft processor used for runtime PR control. One very important aspect of this project is: they envision input to FPGA comes from Ethernet, which is very true nowadays. And this also makes their project quite similar to Catapult. It\ns a very solid paper, though the evaluation is a little bit weak. What could be added: migration, different-sized region.\n\n\nThe above CF and FCCM papers are similar in the sense that they are both building SW framework and HW shell to provide a unified cloud management system. They differ in their shell design: CF one take inputs from DMA engine, which is local system DRAM, FCCM one take inputs from Ethernet. The things after DMA or MAC, are essentially similar.\n\n\nIt seems all of them are using simple segment-based memory partition for user FPGA logic. What\ns the pros and cons of using paging here?\n\n\n\n\n\n\nS1 DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL\n14\n\n\nS2 Virtualized FPGA Accelerators for Efficient Cloud Computing, CloudCom\n15\n\n\nS3 Designing a Virtual Runtime for FPGA Accelerators in the Cloud, FPL\n16\n\n\nS4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access\n17\n\n\nThe above four papers came from the same group of folks. S1 developed a framework to use PCIe to do PR, okay. S2 is a follow-up on S1, read S2\ns chapter IV hardware architecture, many implementation details like internal FPGA switch, AXI stream interface. But no memory virtualization discussion. S3 is a two page short paper. S4 is the realization of S3. I was particularly interested if S4 has implemented their own virtual memory management. The answer is NO. S4 leveraged on-chip Linux, they just build a customized MMU (in the form of using BRAM to store page tables. This approach is similar to the papers listed in \nIntegrate with Virtual Memory\n). Many things discussed in S4 have been proposed multiple times in previous cloud FPGA papers since 2014.\n\n\n\n\n\n\nMS: A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services, ISCA\n14\n\n\nMS: A Cloud-Scale Acceleration Architecture, Micro\n16\n\n\nCatapult is unique in its shell, which includes the Lightweight Transport Layer (LTL), and Elastic Router(ER). The cloud management part, which the paper just briefly mentioned, actually should include everything the above CF\n14 and FCCM\n14 have. The LTL has congestion control, packet loss detection/resend, ACK/NACK. The ER is a crossbar switch used by FPGA internal modules, which is essential to connect shell and roles.\n\n\nThese two Catapult papers are simply a must read.\n\n\n\n\n\n\nMS: A Configurable Cloud-Scale DNN Processor for Real-Time AI, Micro\n18\n\n\nMS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI\n18\n\n\nMS: Direct Universal Access : Making Data Center Resources Available to FPGA, NSDI\n19\n\n\nCatapult is just sweet, isn\nt it?\n\n\n\n\n\n\nASIC Clouds: Specializing the Datacenter, ISCA\n16\n\n\nVirtualizating FPGAs in the Cloud, ASPLOS\n20\n, to appear.\n\n\n\n\nMisc\n\n\n\n\nA Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA\n16\n\n\n\n\nApplications\n\n\nProgrammable Network\n\n\n\n\nMS: ClickNP: Highly Flexible and High Performance Network Processing with Reconfigurable Hardware, SIGCOMM\n16\n\n\nMS: Multi-Path Transport for RDMA in Datacenters, NSDI\n18\n\n\nMS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI\n18\n\n\nMellanox. NICA: An Infrastructure for Inline Acceleration of Network Applications, ATC\n19\n\n\nThe Case For In-Network Computing On Demand, EuroSys\n19\n\n\nFast, Scalable, and Programmable Packet Scheduler in Hardware, SIGCOMM\n19\n\n\nHPCC: high precision congestion control, SIGCOMM\n19\n\n\nOffloading Distributed Applications onto SmartNICs using iPipe, SIGCOMM\n19\n\n\nNot necessary FPGA, but SmartNICs. The actor programming model seems a good fit. There is another paper from ATC\n19 that optimizes \ndistributed actor runtime\n.\n\n\n\n\n\n\n\n\nDatabase and SQL\n\n\n\n\nOn-the-fly Composition of FPGA-Based SQL Query Accelerators Using A Partially Reconfigurable Module Library, 2012\n\n\nAccelerating database systems using FPGAs: A survey, FPL\n18\n\n\n\n\nStorage\n\n\n\n\nCognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC\n19\n\n\nINSIDER: Designing In-Storage Computing System for Emerging High-Performance Drive, ATC\n19\n\n\nLightStore: Software-defined Network-attached Key-value Drives, ASPLOS\n19\n\n\nFIDR: A Scalable Storage System for Fine-Grain Inline Data Reduction with Efficient Memory Handling, MICRO\n19\n\n\nCIDR: A Cost-Effective In-line Data Reduction System for Terabit-per-Second Scale SSD Array, HPCA\n19\n\n\n\n\nMachine Learning\n\n\n\n\nTABLA: A Unified Template-based Framework for Accelerating Statistical Machine Learning, HPCA\n16\n\n\nOptimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks, FPGA\n15\n\n\nFrom High-Level Deep Neural Models to FPGAs, ISCA\n16\n\n\nDeep Learning on FPGAs: Past, Present, and Future, arXiv\n16\n\n\nAccelerating binarized neural networks: Comparison of FPGA, CPU, GPU, and ASIC, FPT\n16\n\n\nFINN: A Framework for Fast, Scalable Binarized Neural Network Inference, FPGA\n17\n\n\nIn-Datacenter Performance Analysis of a Tensor Processing Unit, ISCA\n17\n\n\nAccelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs, FPGA\n17\n\n\nA Configurable Cloud-Scale DNN Processor for Real-Time AI, ISCA\n18\n\n\nMicrosoft Project Brainware. Built on Catapult.\n\n\n\n\n\n\nA Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks, MICRO\n18\n\n\nDNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, ICCAD\n18\n\n\nFA3C : FPGA-Accelerated Deep Reinforcement Learning\uff0c ASPLOS\u201919\n\n\nCognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC\n19\n\n\n\n\nGraph\n\n\n\n\nA Scalable Processing-in-Memory Accelerator for Parallel Graph Processing, ISCA\n15\n\n\nEnergy Efficient Architecture for Graph Analytics Accelerators, ISCA\n16\n\n\nBoosting the Performance of FPGA-based Graph Processor using Hybrid Memory Cube: A Case for Breadth First Search, FPGA\n17\n\n\nFPGA-Accelerated Transactional Execution of Graph Workloads, FPGA\n17\n\n\nAn FPGA Framework for Edge-Centric Graph Processing, CF\n18\n\n\n\n\nKey-Value Store\n\n\n\n\nAchieving 10Gbps line-rate key-value stores with FPGAs, HotCloud\n13\n\n\nThin Servers with Smart Pipes: Designing SoC Accelerators for Memcached, ISCA\n13\n\n\nAn FPGA Memcached Appliance, FPGA\n13\n\n\nScaling out to a Single-Node 80Gbps Memcached Server with 40Terabytes of Memory, HotStorage\n15\n\n\nKV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC, SOSP\n17\n\n\nThis link is also useful for better understading \nMorning Paper\n\n\n\n\n\n\nUltra-Low-Latency and Flexible In-Memory Key-Value Store System Design on CPU-FPGA, FPT\n18\n\n\n\n\nBio\n\n\n\n\nWhen Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud\n16\n\n\nFPGA Accelerated INDEL Realignment in the Cloud, HPCA\n19\n\n\n\n\nConsensus\n\n\n\n\nConsensus in a Box: Inexpensive Coordination in Hardware, NSDI\n16\n\n\n\n\nVideo Processing\n\n\n\n\nQuantifying the Benefits of Dynamic Partial Reconfiguration for Embedded Vision Applications (FPL 2019)\n\n\nTime-Shared Execution of Realtime Computer Vision Pipelines by Dynamic Partial Reconfiguration (FPL 2018)\n\n\n\n\nFPGA Internal\n\n\nFPGA20: Highlighting Significant Contributions from 20 Years of the International Symposium on Field-Programmable Gate Arrays (1992\n2011)\n\n\nGeneral\n\n\n\n\nFPGA and CPLD architectures: a tutorial, 1996\n\n\nReconfigurable computing: a survey of systems and software, 2002\n\n\nReconfigurable computing: architectures and design methods\n\n\nFPGA Architecture: Survey and Challenges, 2007\n\n\nRead the first two paragraphs of each section and then come back to read all of that if needed.\n\n\n\n\n\n\nRAMP: Research Accelerator For Multiple Processors, 2007\n\n\nThree Ages of FPGAs: A Retrospective on the First Thirty Years of FPGA Technology, IEEE\n15\n\n\n\n\nPartial Reconfiguration\n\n\n\n\nFPGA Dynamic and Partial Reconfiguration: A Survey of Architectures, Methods, and Applications, CSUR\n18\n\n\nMust read.\n\n\n\n\n\n\nDyRACT: A partial reconfiguration enabled accelerator and test platform, FPL\n14\n\n\nA high speed open source controller for FPGA partial reconfiguration\n\n\nHardware context-switch methodology for dynamically partially reconfigurable systems, 2010\n\n\n\n\nLogical Optimization and Technology Mapping\n\n\n\n\nFlowMap: An Optimal Technology Mapping Algorithm for Delay Optimization in Lookup-Table Based FPGA Designs, 1994\n\n\nCombinational Logic Synthesis for LUT Based Field Programmable Gate Arrays, 1996\n\n\nDAOmap: A Depth-optimal Area Optimization Mapping Algorithm for FPGA Designs, 2004\n\n\n\n\nPlace and Route\n\n\n\n\nVPR: A New Packing, Placement and Routing Tool for FPGA Research, 1997\n\n\nVTR 7.0: Next Generation Architecture and CAD System for FPGAs, 2014\n\n\n\n\nRTL2FPGA\n\n\n\n\nA Case for FAME: FPGA Architecture Model Execution, 2010\n\n\nStrober: Fast and Accurate Sample-Based Energy Simulation for Arbitrary RTL, 2016\n\n\nEvaluation of RISC-V RTL with FPGA-Accelerated Simulation, 2017\n\n\nFireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud, 2018", 
            "title": "Papers"
        }, 
        {
            "location": "/notes/paper_fpga/#an-fpga-reading-list", 
            "text": "Version History Date Description Nov 30, 2019 Add a lot security papers Oct 22, 2019 Shuffle scheduling section. More focused. Add two more recent fpga-virt papers Oct 5, 2019 More on scheduling. Add NoC. Add Security. Oct 4, 2019 Add more papers extracted from AmophOS Oct 3, 2019 Initial version from  Github This is a list of  academic papers  that cover all sorts of FPGA related topic,\nmore from a system researcher s point of view though.   Virtualization  Scheduling  NoC  Memory Hierarchy  Dynamic Memory Allocation  Integrate with Host Virtual Memory  Integrate with Host OSs  Security  Summary    Languages, Runtime, and Framework  Xilinx HLS  Xilinx CAD  High-Level Languages and Platforms  Integrate with Frameworks  Cloud Infrastructure  Misc    Applications  Programmable Network  Database  Storage  Machine Learning  Graph  Key-Value Store  Bio  Consensus  Video Processing  Blockchain  Micro-services    FPGA Internal  General  Partial Reconfiguration  Logical Optimization and Technology Mapping  Place and Route  RTL2FPGA", 
            "title": "An FPGA Reading List"
        }, 
        {
            "location": "/notes/paper_fpga/#virtualization", 
            "text": "", 
            "title": "Virtualization"
        }, 
        {
            "location": "/notes/paper_fpga/#scheduling", 
            "text": "Scheduling is big topic for FPGA. Unlike the traditional CPU scheduling,\nthere are more aspects to consider, e.g.,\n1) Partial reconfiguration (PR), 2) Dynamic self PR,\n3) Preemptive scheduling, 4) Relocation, 5) Floorplanning, and so on.", 
            "title": "Scheduling"
        }, 
        {
            "location": "/notes/paper_fpga/#preemptive-scheduling", 
            "text": "Preemptive multitasking on FPGAs, 2000  Multitasking on FPGA Coprocessors, 2000  Context saving and restoring for multitasking in reconfigurable systems, 2005  ReconOS Cooperative multithreading in dynamically reconfigurable systems, FPL 09  Block, drop or roll(back): Alternative preemption methods for RH multi-tasking, FCCM 09  Hardware Context-Switch Methodology for Dynamically Partially Reconfigurable Systems, 2010  On-chip Context Save and Restore of Hardware Tasks on Partially Reconfigurable FPGAs, 2013  HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013  Preemptive Hardware Multitasking in ReconOS, 2015", 
            "title": "Preemptive Scheduling"
        }, 
        {
            "location": "/notes/paper_fpga/#preemptive-reconfiguration", 
            "text": "Preemption of the Partial Reconfiguration Process to Enable Real-Time Computing, 2018", 
            "title": "Preemptive Reconfiguration"
        }, 
        {
            "location": "/notes/paper_fpga/#bitstreams", 
            "text": "Github 7-series bitmap reverse engineering  PARBIT: A Tool to Transform Bitfiles to Implement Partial Reconfiguration of Field Programmable Gate Arrays (FPGAs), 2001  BIL: A TOOL-CHAIN FOR BITSTREAM REVERSE-ENGINEERING, 2012  BITMAN: A Tool and API for FPGA Bitstream Manipulations, 2017", 
            "title": "Bitstreams"
        }, 
        {
            "location": "/notes/paper_fpga/#relocation", 
            "text": "Context saving and restoring for multitasking in reconfigurable systems, 2005  REPLICA2Pro: Task Relocation by Bitstream Manipulation in Virtex-II/Pro FPGAs, 2006  Relocation and Automatic Floor-planning of FPGA Partial Configuration Bit-Streams, MSR 2008  Internal and External Bitstream Relocation for Partial Dynamic Reconfiguration, 2009  PRR-PRR Dynamic Relocation, 2009  HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2003  AutoReloc, 2016  HTR: on-chip Hardware Task Relocation For Partially Reconfigurable FPGAs, 2013", 
            "title": "Relocation:"
        }, 
        {
            "location": "/notes/paper_fpga/#others", 
            "text": "hthreads: A hardware/software co-designed multithreaded RTOS kernel, 2005  hthreads: Enabling a Uniform Programming Model Across the Software/Hardware Boundary, FCCM 16  Tartan: Evaluating Spatial Computation for Whole Program Execution, ASPLOS 06  A virtual hardware operating system for the Xilinx XC6200, 1996  The Swappable Logic Unit: a Paradigm for Virtual Hardware, FCCM 97  Run-time management of dynamically reconfigurable designs, 1998  All above ones are early work on FPGA scheduling.  Worth a read, but don t take some of their assumptions. Some have been changed after SO many years.    S1. Reconfigurable Hardware Operating Systems: From Design Concepts to Realizations, 2003  S2. Operating Systems for Reconfigurable Embedded Platforms: Online Scheduling of Real-Time Tasks, 2004  Very fruitful discussion. The paper schedules bitstreams inside FPGA,\n  following a  Real-Time sched policy (deadline) .  Different from CPU sched, FPGA scheduling needs to consider  areas . The chip is\na rectangle box, allocating areas needs great care to avoid fragmentation!    Context saving and restoring for multitasking in reconfigurable systems, FPL 05  Optimizing deschedule perf.  This paper discusses ways to save and restore the state information of a hardware task.\n  There are generally three approachs: a) adding indirection. Let app use system API to read/write states.\n  b) yield-type API. c) use PR controller to read back bitstream.  This paper used ICAP to read the bitstream back and extract necenssay state information that must\nbe present at next bitstream resume.    Scheduling intervals for reconfigurable computing, FCCM 08  Hardware context-switch methodology for dynamically partially reconfigurable systems, 2010  Online Scheduling for Multi-core Shared Reconfigurable Fabric, DATE 12  Multi-shape Tasks Scheduling for Online Multitasking on FPGAs, 2014  AmophOS, OSDI 18  Hardware context switching on FPGAs, 2014  Efficient Hardware Context-Switch for Task Migration between Heterogeneous FPGAs, 2016", 
            "title": "Others"
        }, 
        {
            "location": "/notes/paper_fpga/#noc", 
            "text": "Network-on-Chip on FPGA.   Interconnection Networks Enable Fine-Grain Dynamic Multi-Tasking on FPGAs, 2002  Like the idea of separating computation from communication.  Also a lot discussions about possible NoC designs within FPGA.    LEAP Soft connections: Addressing the hardware-design modularity problem, DAC 09  Virtual channel concept. Time-insensitive.    Leveraging Latency-Insensitivity to Ease Multiple FPGA Design, FPGA 12  CONNECT: re-examining conventional wisdom for designing nocs in the context of FPGAs, FPGA 12  Your Programmable NIC Should be a Programmable Switch, HotNets 18", 
            "title": "NoC"
        }, 
        {
            "location": "/notes/paper_fpga/#memory-hierarchy", 
            "text": "Papers deal with BRAM, registers, on-board DRAM, and host DRAM.   LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA 11  Main design hierarchy: Use BRAM as L1 cache, use on-board DRAM as L2 cache, and host memory as the backing store. Everthing is abstracted away through their interface (similar to load/store). Programming is pretty much the same as if you are writing for CPU.  According to sec 2.2.2, its scratchpad controller, is using simple segment-based mapping scheme. Like AmorphOS s one.    LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM 14  Follow up work on LEAP Scratchpads, extends the work to have cache coherence between multiple FPGAs.  Coherent Scatchpads with MOSI protocol.    MATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA 15  CoRAM: An In-Fabric Memory Architecture for FPGA-Based Computing  CoRAM provides an interface for managing the on- and off-chip memory resource of an FPGA. It use  control threads  enforce low-level control on data movement.  Seriously, the CoRAM is just like Processor L1-L3 caches.    CoRAM Prototype and evaluation of the CoRAM memory architecture for FPGA-based computing, FPGA 12  Prototype on FPGA.    Sharing, Protection, and Compatibility for Reconfigurable Fabric with AMORPHOS, OSDI 18  Hull: provides memory protection for on-board DRAM using  segment-based  address translation.    Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access 17", 
            "title": "Memory Hierarchy"
        }, 
        {
            "location": "/notes/paper_fpga/#dynamic-memory-allocation", 
            "text": "malloc()  and  free()  for FPGA on-board DRAM.   A High-Performance Memory Allocator for Object-Oriented Systems, IEEE 96  SysAlloc: A Hardware Manager for Dynamic Memory Allocation in Heterogeneous Systems, FPL 15  Hi-DMM: High-Performance Dynamic Memory Management in High-Level Synthesis, IEEE 18", 
            "title": "Dynamic Memory Allocation"
        }, 
        {
            "location": "/notes/paper_fpga/#integrate-with-host-virtual-memory", 
            "text": "Papers deal with OS Virtual Memory System (VMS). Note that, all these papers introduce some form of MMU into the FPGA\nto let FPGA be able to work with host VMS.\nThis added MMU is similar to CPU s MMU and  RDMA NIC s internal cache .\nNote that the VMS still runs inside Linux (include pgfault, swapping, TLB shootdown and so on. What could really stands out, is to implement VMS inside FPGA.)   Virtual Memory Window for Application-Specific Reconfigurable Coprocessors, DAC 04  Early work that adds a new MMU to FPGA to let FPGA logic access  on-chip DRAM . Note, it s not the system main memory. Thus the translation pgtable is different.  Has some insights on prefetching and MMU CAM design.    Seamless Hardware Software Integration in Reconfigurable Computing Systems, 2005  Follow up summary on previous DAC 04 Virtual Memory Window.    A Reconfigurable Hardware Interface for a Modern Computing System, FCCM 07  This work adds a new MMU which includes a 16-entry TLB to FPGA. FPGA and CPU shares the same user virtual address space, use the same physical memory. FPGA and CPU share memory at  cacheline granularity , FPGA is just another core in this sense. Upon a TLB miss at FPGA MMU, the FPGA sends interrupt to CPU, to let  software to handle the TLB miss . Using software-managed TLB miss is not efficient. But they made cache coherence between FPGA and CPU easy.    Low-Latency High-Bandwidth HW/SW Communication in a Virtual Memory Environment, FPL 08  This work actually add a new MMU to FPGA, which works just like CPU MMU. It s similar to IOMMU, in some sense.  But I think they missed one important aspect: cache coherence between CPU and FPGA. There is not too much information about this in the paper, it seems they do not have cache at FPGA. Anyhow, this is why recently CCIX and OpenCAPI are proposed.    Memory Virtualization for Multithreaded Reconfigurable Hardware, FPL 11  Part of the ReconOS project  They implemented a simple MMU inside FPGA that includes a TLB. On protection violation or page invalid access cases, their MMU just hand over to CPU pgfault routines. How is this different from the FPL 08 one? Actually, IMO, they are the same.    S4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access 17  This paper also implemented a hardware MMU, but the virtual memory system still run on Linux.  Also listed in  Cloud Infrastructure  part.    Lightweight Virtual Memory Support for Many-Core Accelerators in Heterogeneous Embedded SoCs, 2015  Lightweight Virtual Memory Support for Zero-Copy Sharing of Pointer-Rich Data Structures in Heterogeneous Embedded SoCs, IEEE 17  Part of the PULP project.  Essentially a software-managed IOMMU. The control path is running as a Linux kernel module. The datapath is a lightweight AXI transation translation.", 
            "title": "Integrate with Host Virtual Memory"
        }, 
        {
            "location": "/notes/paper_fpga/#integrate-with-host-oss", 
            "text": "A Virtual Hardware Operating System for the Xilinx XC6200, FPL 96  Operating systems for reconfigurable embedded platforms: online scheduling of real-time tasks, IEEE 04  hthreads: a hardware/software co-designed multithreaded RTOS kernel, 2005  Reconfigurable computing: architectures and design methods, IEE 05  BORPH: An Operating System for FPGA-Based Reconfigurable Computers. PhD Thesis.  FUSE: Front-end user framework for O/S abstraction of hardware accelerators, FCCM 11  ReconOS \u2013 an Operating System Approach for Reconfigurable Computing, IEEE Micro 14  Invoke kernel from FPGA. They built a shell in FPGA and delegation threads in CPU to achieve this.  They implemented their own MMU (using pre-established pgtables) to let FPGA logic to access system memory.  Ref .  Read the  Operating Systems for Reconfigurable Computing  sidebar, nice summary.    LEAP Soft connections: Addressing the hardware-design modularity problem, DAC 09  Channel concept. Good.    LEAP Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic, FPGA 11  BRAM/on-board DRAM/host DRAM layering. Caching.    LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories  Add cache-coherence on top of previous work.  Also check out my note on  Cache Coherence .    LEAP FPGA Operating System, FPL 14.  A Survey on FPGA Virtualization, FPL 18  ZUCL 2.0: Virtualised Memory and Communication for ZYNQ UltraScale+ FPGAs, FSP 19", 
            "title": "Integrate with Host OSs"
        }, 
        {
            "location": "/notes/paper_fpga/#security", 
            "text": "If I were to recommend, I d suggest start from:   Recent Attacks and Defenses on FPGA-based Systems, 2019  Physical Side-Channel Attacks and Covert Communication on FPGAs: A Survey, 2019  FPGA security: Motivations, features, and applications, 2014   The whole list:   FPGAhammer : Remote Voltage Fault Attacks on Shared FPGAs , suitable for DFA on AES  FPGA-Based Remote Power Side-Channel Attacks  Characterization of long wire data leakage in deep submicron FPGAS  Protecting against cryptographic Trojans in FPGAS  FPGA Side Channel Attacks without Physical Access  FPGA security: Motivations, features, and applications  FPGA side-channel receivers  Security of FPGAs in data centers  Secure Function Evaluation Using an FPGA Overlay Architecture  Mitigating Electrical-level Attacks towards Secure Multi-Tenant FPGAs in the Cloud  The Costs of Confidentiality in Virtualized FPGAs  Temporal Thermal Covert Channels in Cloud FPGAs  Characterizing Power Distribution Attacks in Multi-User FPGA Environments  FASE: FPGA Acceleration of Secure Function Evaluation  Securing Cryptographic Circuits by Exploiting Implementation Diversity and Partial Reconfiguration on FPGAs  Measuring Long Wire Leakage with Ring Oscillators in Cloud FPGAs  Physical Side-Channel Attacks and Covert Communication on FPGAs: A Survey  Leaky Wires: Information Leakage and Covert Communication Between FPGA Long Wires  Using the Power Side Channel of FPGAs for Communication  An Inside Job: Remote Power Analysis Attacks on FPGAs  Leakier Wires: Exploiting FPGA Long Wires for Covert- and Side-channel Attacks  Voltage drop-based fault attacks on FPGAs using valid bitstreams  Moats and Drawbridges: An Isolation Primitive for Reconfigurable Hardware Based Systems  Sensing nanosecond-scale voltage attacks and natural transients in FPGAs  Holistic Power Side-Channel Leakage Assessment:  Hiding Intermittent Information Leakage with Architectural Support for Blinking  Examining the consequences of high-level synthesis optimizations on power side-channel  Register transfer level information flow tracking for provably secure hardware design  A Protection and Pay-per-use Licensing Scheme for On-cloud FPGA Circuit IPs  Recent Attacks and Defenses on FPGA-based Systems  PFC: Privacy Preserving FPGA Cloud - A Case Study of MapReduce  A Pay-per-Use Licensing Scheme for Hardware IP Cores in Recent SRAM-Based FPGAs  FPGAs for trusted cloud computing", 
            "title": "Security"
        }, 
        {
            "location": "/notes/paper_fpga/#summary", 
            "text": "Summary on current FPGA Virtualization Status. Prior art mainly focus on: 1) How to virtualize on-chip BRAM (e.g., CoRAM, LEAP Scratchpad),\n2) How to work with host, specifically, how to use the host DRAM, how to use host virtual memory.\n3) How to schedule bitstreams inside a FPGA chip. 4) How to provide certain services to make FPGA programming easier (mostly work with host OS).", 
            "title": "Summary"
        }, 
        {
            "location": "/notes/paper_fpga/#languages-runtime-and-framework", 
            "text": "Innovations in the toolchain space.", 
            "title": "Languages, Runtime, and Framework"
        }, 
        {
            "location": "/notes/paper_fpga/#xilinx-hls", 
            "text": "Design Patterns for Code Reuse in HLS Packet Processing Pipelines, FCCM 19  A very good HLS library from Mellanox folks.    Templatised Soft Floating-Point for High-Level Synthesis, FCCM 19  ST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA, FCCM 18  HLScope+: Fast and Accurate Performance Estimation for FPGA HLS, ICCAD 17  Separation Logic-Assisted Code Transformations for Efficient High-Level Synthesis, FCCM 14  An HLS design aids that analyze the original program at  compile time  and perform automated code transformations. The tool analysis pointer-manipulating programs and automatically splits heap-allocated data structures into disjoint, independent regions.  The tool is for C++ heap operations.  To put in another way: the tool looks at your BRAM usage, found any false-dependencies, and make multiple independent regions, then your II is improved.    MATCHUP: Memory Abstractions for Heap Manipulating Programs, FPGA 15  This is an HLS toolchain aid.  Follow-up work of the above FCCM 14 one. This time they use LEAP scracchpads as the underlying caching block.", 
            "title": "Xilinx HLS"
        }, 
        {
            "location": "/notes/paper_fpga/#xilinx-cad", 
            "text": "Maverick: A Stand-alone CAD Flow for Partially Reconfigurable FPGA Modules, FCCM 19", 
            "title": "Xilinx CAD"
        }, 
        {
            "location": "/notes/paper_fpga/#high-level-languages-and-platforms", 
            "text": "Just-in-Time Compilation for Verilog, ASPLOS 19  Chisel: Constructing Hardware in a Scala Embedded Language, DAC 12  Chisel is being actively improved and used by UCB folks.    Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs, FPGA 18  From JVM to FPGA: Bridging Abstraction Hierarchy via Optimized Deep Pipelining, HotCloud 18  HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing, FPGA 19  LINQits: Big Data on Little Clients, ISCA 13  From Microsoft, used to express SQL-like functions (thus big data) and runs on ZYNQ (thus little client),  You wrote C#, LINQits translate it to verilog, and run the whole thing at a ZYNQ (ARM+FPGA) board.    Lime: a Java-Compatible and Synthesizable Language for Heterogeneous Architectures, OOPSLA 10  Lime is a Java-based programming model and runtime from IBM which aims to provide a single unified\n  language to program heterogeneous architectures, from FPGAs to conventional CPUs    A line of work from Standord  Generating configurable hardware from parallel patterns, ASPLOS 16  Plasticine: A Reconfigurable Architecture For Parallel Patterns, ISCA 17  Spatial: A Language and Compiler for Application Accelerators, PLDI 18  Spatial generates  Chisel  code along with C++ code which can be used on a host CPU to control the execution of the accelerator on the target FPGA.  This kind of academic papers must have a lot good ideas. But the truth is it will not be reliable because it s from academic labs.", 
            "title": "High-Level Languages and Platforms"
        }, 
        {
            "location": "/notes/paper_fpga/#integrate-with-frameworks", 
            "text": "Map-reduce as a Programming Model for Custom Computing Machines, FCCM 08  This paper proposes a model to translate MapReduce code written in C to code that could run on FPGA and GPU. Many details are omitted, and they don t really have the compiler.  Single-host framework, everything is in FPGA and GPU.    Axel: A Heterogeneous Cluster with FPGAs and GPUs, FPGA 10  A distributed MapReduce Framework, targets clusters with CPU, GPU, and FPGA. Mainly the idea of scheduling FPGA/GPU jobs.  Distributed Framework.    FPMR: MapReduce Framework on FPGA, FPGA 10  A MapReduce framework on a single host s FPGA. You need to write Verilog/HLS for processing logic to hook with their framework. The framework mainly includes a data transfer controller, a simple schedule that enable certain blocks at certain time.  Single-host framework, everything is in FPGA.    Melia: A MapReduce Framework on OpenCL-Based FPGAs, IEEE 16  Another framework, written in OpenCL, and users can use OpenCL to program as well. Similar to previous work, it s more about the framework design, not specific algorithms on FPGA.  Single-host framework, everything is in FPGA. But they have a discussion on running on multiple FPGAs.  Four MapReduce FPGA papers here, I believe there are more. The marriage between MapReduce and FPGA is not something hard to understand. FPGA can be viewed as another core with different capabilities. The thing is, given FPGA s reprogram-time and limited on-board memory, how to design a good scheduling algorithm and data moving/caching mechanisms. Those papers give some hints on this.    UCLA: When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud 16  UCLA: Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale, SoCC 16  A system that hooks FPGA with Spark.  There is a line of work that hook FPGA with big data processing framework (Spark), so the implementation of FPGA and the scale-out software can be separated. The Spark can schedule FPGA jobs to different machines, and take care of scale-out, failure handling etc. But, I personally think this line of work is really just an extension to ReconOS/FUSE/BORPH line of work. The main reason is: both these two lines of work try to integrate jobs run on CPU and jobs run on FPGA, so CPU and FPGA have an easier way to talk, or put in another way, CPU and FPGA have a better division of labor. Whether it s single-machine (like ReconOS, Melia), or distributed (like Blaze, Axel), they are essentially the same.    UCLA: Heterogeneous Datacenters: Options and Opportunities, DAC 16  Follow up work of Blaze. Nice comparison of big and wimpy cores.", 
            "title": "Integrate with Frameworks"
        }, 
        {
            "location": "/notes/paper_fpga/#cloud-infrastructure", 
            "text": "Huawei: FPGA as a Service in the Cloud  UCLA: Customizable Computing: From Single Chip to Datacenters, IEEE 18  UCLA: Accelerator-Rich Architectures: Opportunities and Progresses, DAC 14  Reminds me of  OmniX . Disaggregation at a different scale.  This paper actually targets single-machine case. But it can reflect a distributed setting.    Enabling FPGAs in the Cloud, CF 14  Paper raised four important aspects to enable FPGA in cloud: Abstraction, Sharing, Compatibility, and Security. FPGA itself requires a shell (paper calls it service logic) and being partitioned into multiple slots. Things discussed in the paper are straightforward, but worth reading. They did not solve the FPGA sharing issue, which, is solved by AmorphOS.    FPGAs in the Cloud: Booting Virtualized Hardware Accelerators with OpenStack, FCCM 14  Use OpenStack to manage FPGA resource. The FPGA is partitioned into multiple regions, each region can use PR. The FPGA shell includes: 1) basic MAC, and packet dispatcher, 2) memory controller, and segment-based partition scheme, 3) a soft processor used for runtime PR control. One very important aspect of this project is: they envision input to FPGA comes from Ethernet, which is very true nowadays. And this also makes their project quite similar to Catapult. It s a very solid paper, though the evaluation is a little bit weak. What could be added: migration, different-sized region.  The above CF and FCCM papers are similar in the sense that they are both building SW framework and HW shell to provide a unified cloud management system. They differ in their shell design: CF one take inputs from DMA engine, which is local system DRAM, FCCM one take inputs from Ethernet. The things after DMA or MAC, are essentially similar.  It seems all of them are using simple segment-based memory partition for user FPGA logic. What s the pros and cons of using paging here?    S1 DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL 14  S2 Virtualized FPGA Accelerators for Efficient Cloud Computing, CloudCom 15  S3 Designing a Virtual Runtime for FPGA Accelerators in the Cloud, FPL 16  S4 Virtualized Execution Runtime for FPGA Accelerators in the Cloud, IEEE Access 17  The above four papers came from the same group of folks. S1 developed a framework to use PCIe to do PR, okay. S2 is a follow-up on S1, read S2 s chapter IV hardware architecture, many implementation details like internal FPGA switch, AXI stream interface. But no memory virtualization discussion. S3 is a two page short paper. S4 is the realization of S3. I was particularly interested if S4 has implemented their own virtual memory management. The answer is NO. S4 leveraged on-chip Linux, they just build a customized MMU (in the form of using BRAM to store page tables. This approach is similar to the papers listed in  Integrate with Virtual Memory ). Many things discussed in S4 have been proposed multiple times in previous cloud FPGA papers since 2014.    MS: A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services, ISCA 14  MS: A Cloud-Scale Acceleration Architecture, Micro 16  Catapult is unique in its shell, which includes the Lightweight Transport Layer (LTL), and Elastic Router(ER). The cloud management part, which the paper just briefly mentioned, actually should include everything the above CF 14 and FCCM 14 have. The LTL has congestion control, packet loss detection/resend, ACK/NACK. The ER is a crossbar switch used by FPGA internal modules, which is essential to connect shell and roles.  These two Catapult papers are simply a must read.    MS: A Configurable Cloud-Scale DNN Processor for Real-Time AI, Micro 18  MS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI 18  MS: Direct Universal Access : Making Data Center Resources Available to FPGA, NSDI 19  Catapult is just sweet, isn t it?    ASIC Clouds: Specializing the Datacenter, ISCA 16  Virtualizating FPGAs in the Cloud, ASPLOS 20 , to appear.", 
            "title": "Cloud Infrastructure"
        }, 
        {
            "location": "/notes/paper_fpga/#misc", 
            "text": "A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA 16", 
            "title": "Misc"
        }, 
        {
            "location": "/notes/paper_fpga/#applications", 
            "text": "", 
            "title": "Applications"
        }, 
        {
            "location": "/notes/paper_fpga/#programmable-network", 
            "text": "MS: ClickNP: Highly Flexible and High Performance Network Processing with Reconfigurable Hardware, SIGCOMM 16  MS: Multi-Path Transport for RDMA in Datacenters, NSDI 18  MS: Azure Accelerated Networking: SmartNICs in the Public Cloud, NSDI 18  Mellanox. NICA: An Infrastructure for Inline Acceleration of Network Applications, ATC 19  The Case For In-Network Computing On Demand, EuroSys 19  Fast, Scalable, and Programmable Packet Scheduler in Hardware, SIGCOMM 19  HPCC: high precision congestion control, SIGCOMM 19  Offloading Distributed Applications onto SmartNICs using iPipe, SIGCOMM 19  Not necessary FPGA, but SmartNICs. The actor programming model seems a good fit. There is another paper from ATC 19 that optimizes  distributed actor runtime .", 
            "title": "Programmable Network"
        }, 
        {
            "location": "/notes/paper_fpga/#database-and-sql", 
            "text": "On-the-fly Composition of FPGA-Based SQL Query Accelerators Using A Partially Reconfigurable Module Library, 2012  Accelerating database systems using FPGAs: A survey, FPL 18", 
            "title": "Database and SQL"
        }, 
        {
            "location": "/notes/paper_fpga/#storage", 
            "text": "Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC 19  INSIDER: Designing In-Storage Computing System for Emerging High-Performance Drive, ATC 19  LightStore: Software-defined Network-attached Key-value Drives, ASPLOS 19  FIDR: A Scalable Storage System for Fine-Grain Inline Data Reduction with Efficient Memory Handling, MICRO 19  CIDR: A Cost-Effective In-line Data Reduction System for Terabit-per-Second Scale SSD Array, HPCA 19", 
            "title": "Storage"
        }, 
        {
            "location": "/notes/paper_fpga/#machine-learning", 
            "text": "TABLA: A Unified Template-based Framework for Accelerating Statistical Machine Learning, HPCA 16  Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks, FPGA 15  From High-Level Deep Neural Models to FPGAs, ISCA 16  Deep Learning on FPGAs: Past, Present, and Future, arXiv 16  Accelerating binarized neural networks: Comparison of FPGA, CPU, GPU, and ASIC, FPT 16  FINN: A Framework for Fast, Scalable Binarized Neural Network Inference, FPGA 17  In-Datacenter Performance Analysis of a Tensor Processing Unit, ISCA 17  Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs, FPGA 17  A Configurable Cloud-Scale DNN Processor for Real-Time AI, ISCA 18  Microsoft Project Brainware. Built on Catapult.    A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks, MICRO 18  DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, ICCAD 18  FA3C : FPGA-Accelerated Deep Reinforcement Learning\uff0c ASPLOS\u201919  Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval, ATC 19", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/notes/paper_fpga/#graph", 
            "text": "A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing, ISCA 15  Energy Efficient Architecture for Graph Analytics Accelerators, ISCA 16  Boosting the Performance of FPGA-based Graph Processor using Hybrid Memory Cube: A Case for Breadth First Search, FPGA 17  FPGA-Accelerated Transactional Execution of Graph Workloads, FPGA 17  An FPGA Framework for Edge-Centric Graph Processing, CF 18", 
            "title": "Graph"
        }, 
        {
            "location": "/notes/paper_fpga/#key-value-store", 
            "text": "Achieving 10Gbps line-rate key-value stores with FPGAs, HotCloud 13  Thin Servers with Smart Pipes: Designing SoC Accelerators for Memcached, ISCA 13  An FPGA Memcached Appliance, FPGA 13  Scaling out to a Single-Node 80Gbps Memcached Server with 40Terabytes of Memory, HotStorage 15  KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC, SOSP 17  This link is also useful for better understading  Morning Paper    Ultra-Low-Latency and Flexible In-Memory Key-Value Store System Design on CPU-FPGA, FPT 18", 
            "title": "Key-Value Store"
        }, 
        {
            "location": "/notes/paper_fpga/#bio", 
            "text": "When Apache Spark Meets FPGAs: A Case Study for Next-Generation DNA Sequencing Acceleration, HotCloud 16  FPGA Accelerated INDEL Realignment in the Cloud, HPCA 19", 
            "title": "Bio"
        }, 
        {
            "location": "/notes/paper_fpga/#consensus", 
            "text": "Consensus in a Box: Inexpensive Coordination in Hardware, NSDI 16", 
            "title": "Consensus"
        }, 
        {
            "location": "/notes/paper_fpga/#video-processing", 
            "text": "Quantifying the Benefits of Dynamic Partial Reconfiguration for Embedded Vision Applications (FPL 2019)  Time-Shared Execution of Realtime Computer Vision Pipelines by Dynamic Partial Reconfiguration (FPL 2018)", 
            "title": "Video Processing"
        }, 
        {
            "location": "/notes/paper_fpga/#fpga-internal", 
            "text": "FPGA20: Highlighting Significant Contributions from 20 Years of the International Symposium on Field-Programmable Gate Arrays (1992 2011)", 
            "title": "FPGA Internal"
        }, 
        {
            "location": "/notes/paper_fpga/#general", 
            "text": "FPGA and CPLD architectures: a tutorial, 1996  Reconfigurable computing: a survey of systems and software, 2002  Reconfigurable computing: architectures and design methods  FPGA Architecture: Survey and Challenges, 2007  Read the first two paragraphs of each section and then come back to read all of that if needed.    RAMP: Research Accelerator For Multiple Processors, 2007  Three Ages of FPGAs: A Retrospective on the First Thirty Years of FPGA Technology, IEEE 15", 
            "title": "General"
        }, 
        {
            "location": "/notes/paper_fpga/#partial-reconfiguration", 
            "text": "FPGA Dynamic and Partial Reconfiguration: A Survey of Architectures, Methods, and Applications, CSUR 18  Must read.    DyRACT: A partial reconfiguration enabled accelerator and test platform, FPL 14  A high speed open source controller for FPGA partial reconfiguration  Hardware context-switch methodology for dynamically partially reconfigurable systems, 2010", 
            "title": "Partial Reconfiguration"
        }, 
        {
            "location": "/notes/paper_fpga/#logical-optimization-and-technology-mapping", 
            "text": "FlowMap: An Optimal Technology Mapping Algorithm for Delay Optimization in Lookup-Table Based FPGA Designs, 1994  Combinational Logic Synthesis for LUT Based Field Programmable Gate Arrays, 1996  DAOmap: A Depth-optimal Area Optimization Mapping Algorithm for FPGA Designs, 2004", 
            "title": "Logical Optimization and Technology Mapping"
        }, 
        {
            "location": "/notes/paper_fpga/#place-and-route", 
            "text": "VPR: A New Packing, Placement and Routing Tool for FPGA Research, 1997  VTR 7.0: Next Generation Architecture and CAD System for FPGAs, 2014", 
            "title": "Place and Route"
        }, 
        {
            "location": "/notes/paper_fpga/#rtl2fpga", 
            "text": "A Case for FAME: FPGA Architecture Model Execution, 2010  Strober: Fast and Accurate Sample-Based Energy Simulation for Arbitrary RTL, 2016  Evaluation of RISC-V RTL with FPGA-Accelerated Simulation, 2017  FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud, 2018", 
            "title": "RTL2FPGA"
        }, 
        {
            "location": "/fpga/bitstream/", 
            "text": "FPGA Bitstream Explained\n\n\n\n\nVersion History\nDate\nDescription\nDec 20, 2019\nUpdate\nOct 24, 2019\nCreated\nIntroduction\n\n\nA bitstream can configure an FPGA.\nA bitstream includes the descriptions of the hardware logic, routing, and initial\nvalues of registers and on-chip memory.\nThe common impression is that a bitstream has vendor-specific format and cannot be reversed.\nThe answer is yes and no.\nThe fact is that the bitstream file is more than the bits to configure an FPGA,\nit also has certain human-readable fields to describe the contents.\nIn addition, it has a simple assembly-like instruction set to describe the FPGA configuration process.\nThis note is trying to walk through this.\n\n\nAt a high-level, a bitstream file is similar to an executable program,\nin the sense that it describes everything of a desired functionality.\nAnalogous to the ELF format, a bistream has its own format to describe\nthe contents. Note that, this file format is publicly documented \n1\n.\nThat means you can analyze the contents of a bitstream file.\nThe undocument part is the configuration bits: FPGA vendor\ndoes not document the format of the configuration bits,\nand how they may to hardware resources.\n\n\nAs a normal FPGA user, you mostly do not need to understand neither of these.\nThese understandings are only required if you plan to do bitstream readback, preemption scheduling and so forth.\n\n\nAfter reading this note, you will understand that a bitstream file is a sequence of instructions and data.\nThe FPGA has a simple state machine to parse the bitstream and then configure the chip.\nPart of the bistream file format is public, the mapping between the bitstream configuration bits\nand the actual physical resource is undocumented.\n\n\nBistream Related Files\n\n\nIn a normal flow, Vivado only generates a simple \n.bit\n file.\nWhen you click \nProgram Device\n, Vivado will use this file to configure your FPGA.\n\n\nIn addition to generating this file, Vivado is capable of generating a bunch other files.\nYou can find a complete coverage in this \nlink\n.\nWe give a high level summary here.\nMost of the files have the same content and have similar file size.\nFor instance, the difference between a \n.rbt\n and a \n.bit\n is that the former one is in ASCII format while the latter is in binary format,\nbut they have the \nsame\n contents. As for a \n.bit\n and a \n.bin\n file, the latter does not have some ASCII headers at the beginning of the file.\n\n\n.ll\n, the logical link file, is very interesting.\nIt tells you the mapping between user logic and the actual bit offset in the bistream file data section.\nThis file can be used to aid preemption scheduling.\nHowever, note that, this file only documents a very small part of the mapping.\nTo the best of my knowledge, I think only the registers, on-chip memory are documented, but the routing\ninformation is missing. Thus, this file can help reserve engineer bitstream data section to some extend, but not full of it.\n\nPrjxray\n is an open source project working on cracking everything on 7-series FPGA.\n\n\nDetails\n\n\nWe use \n.rbt\n and \n.bit\n to demonstrate the file format.\nNote that they are essentially the same thing, except the former in human-readable ASCII format.\n\n\nThe target board is VCU118, the one used by many cloud vendors.\n\n\nThe following snippt is the first few lines of the \n.rpt\n file.\nThe first few lines are human-readable ASCII contents describing some general information\nabout the bitstream. Starting from line 8 is the actual bitstream file contents.\nNote that the \n.bin\n file starts directly from line 8, no general header info is attached.\nThe interesting part is the 1s and 0s.\nUnless otherwise noted, when we refer to bitstream format, we focus on the 1s and 0s only\nand omit any general ASICC information headers.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nXilinx\n \nASCII\n \nBitstream\n\n\nCreated\n \nby\n \nBitstream\n \n2018\n.\n3\n \nSW\n \nBuild\n \n2405991\n \non\n \nThu\n \nDec\n  \n6\n \n23\n:\n36\n:\n41\n \nMST\n \n2018\n\n\nDesign\n \nname\n:\n    \nbase_mb_wrapper\n;\nUserID\n=\n0XFFFFFFFF\n;\nVersion\n=\n2018\n.\n3\n\n\nArchitecture\n:\n   \nvirtexuplus\n\n\nPart\n:\n           \nxcvu9p-flga2104-2L-e\n\n\nDate\n:\n           \nWed\n \nNov\n \n20\n \n04\n:\n13\n:\n05\n \n2019\n\n\nBits\n:\n           \n641272864\n\n\n11111111111111111111111111111111\n\n\n11111111111111111111111111111111\n\n\n11111111111111111111111111111111\n\n\n...\n\n\n\n\n\n\n\nNote that each line has 32 bits, thus 4 bytes.\nIn Xilinx bistream format, each four bytes is a packet (analogous to CPU instruction).\nEach packet has certain format, it could be a special \nheader packet\n, or a normal \ndata packet\n.\nThe header packet follows a simple assembly-like instruction set to dictate the configuration process.\nThe bitstream file is a sequence of these four bytes packets. \n\n\nWhy it sounds so complicated, a sequence of instructions?!\nI think the short answer is that configuraing FPGA is not an easy task,\nand any wrong doings may permanently harm the chip.\nNatually, the designer would have a on-chip state machine to control the configuration process,\nnot only to control the whole process but also to ensure safety.\n\n\nEach Xilinx FPGA has an on-chip \nconfiguration packet processor\n.\nAll configuration methods such as JTAG, SelectMAP, ICAP merge into this final narrow bridge to carry out the configuration.\nThe configuration packet processor has many internal registers (similar to x86 RAX, CRn, MSR registers).\nThe bitstream usually interact with one of the registers at a time to do one thing.\nFor a more detailed explanation, check out \nthis blog\n,\nand UG570 chapter 9.\n\n\nTo this end, a bitstream consits of three parts:\n\n\n\n\n1) Header packets to prepare the configuration process.\n\n\n2) The actual configuration bits in a contiguous sequence of data packets.\n     AN write to the \nFDRI\n register marks the beginning of this section.\n     The length of this section is described by the packet following the FDRI header packet.\n\n\n3) Header packets to clean up the configuration process.\n\n\n\n\nThe actual configuration bits are the ones determine the FPGA functionality.\nNote that if you are using an SSI Xilinx device like VCU118, the bitstream format is a bit more complicated.\nBasically, each die has the above three parts. If an chip has N dies, it will have N above triplet.\nI have complained about this is not well documented \nhere\n\nand \nhere\n.\n\n\nI wrote a simple \nC program\n\nto parse the \n.rbt\n file and associate a human-reable syntax with each line.\nI didn\nt have a complete coverage of the header packet format.\nThe following snippt shows a parsed \n.rbt\n file with header removed.\nHere, \n0xffffffff\n has no effect, like a NOP.\n\n0x000000bb\n and \n0x11220044\n are special bus detect words.\n\n0xaa995566\n is another special work marking the synchronization status.\nThe last few lines mark the beginning of the configuration bits section.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\nParsed from base_mb_wrapper.rbt\nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \n\n000000\nbb Bus Width Sync\n\n11220044\n Bus Width Detect\nffffffff \nffffffff \naa995566  SYNC\n\n20000000\n \n\n20000000\n \n\n30022001\n Write to regs \n17\n\n\n00000000\n \n\n30020001\n Write to regs \n16\n\n\n00000000\n \n\n30008001\n Write to CMD\n\n00000000\n \n\n20000000\n \n\n30008001\n Write to CMD\n\n00000007\n \n\n20000000\n \n\n20000000\n \n\n30002001\n Write to FAR\n\n00000000\n \n\n30026001\n Write to regs \n19\n\n\n00000000\n \n\n30012001\n Write to regs \n9\n\n\n38003\nfe5 Write to regs \n1\n\n\n3001\nc001 Write to regs \n14\n\n\n00400000\n \n\n30018001\n Write to IDCODE\n\n04\nb31093 IDCODE\n=\n4\nb31093\n\n30008001\n Write to CMD\n\n00000009\n \n\n20000000\n \n\n3000\nc001 Write to regs \n6\n\n\n00000001\n \n\n3000\na001 Write to regs \n5\n\n\n00000101\n \n\n3000\nc001 Write to regs \n6\n\n\n00000000\n \n\n30030001\n Write to regs \n24\n\n\n00000000\n \n\n20000000\n \n\n20000000\n \n\n20000000\n \n\n20000000\n \n\n20000000\n \n\n20000000\n \n\n20000000\n \n\n20000000\n \n\n30002001\n Write to FAR\n\n00000000\n \n\n30008001\n Write to CMD\n\n00000001\n \n\n20000000\n \n\n30004000\n Write to FDRI\n\n5065\neadc            \n-\n The length of configuration bits\n,\n follows a certain \nformat\n\n\n00000000\n            \n-\n The first \n4\n bytes of the configuration bits\n!\n\n\n\n\n\n\n\nHope you have learned something.\n\n\nReferences\n\n\n\n\nXilinx UG570\n\n\nXilinx bitstream files\n\n\nAnother blog on Xilinx Bitstream Internals\n \n\n\nSource code to annotate bitstream", 
            "title": "Bitstream Explained"
        }, 
        {
            "location": "/fpga/bitstream/#fpga-bitstream-explained", 
            "text": "Version History Date Description Dec 20, 2019 Update Oct 24, 2019 Created", 
            "title": "FPGA Bitstream Explained"
        }, 
        {
            "location": "/fpga/bitstream/#introduction", 
            "text": "A bitstream can configure an FPGA.\nA bitstream includes the descriptions of the hardware logic, routing, and initial\nvalues of registers and on-chip memory.\nThe common impression is that a bitstream has vendor-specific format and cannot be reversed.\nThe answer is yes and no.\nThe fact is that the bitstream file is more than the bits to configure an FPGA,\nit also has certain human-readable fields to describe the contents.\nIn addition, it has a simple assembly-like instruction set to describe the FPGA configuration process.\nThis note is trying to walk through this.  At a high-level, a bitstream file is similar to an executable program,\nin the sense that it describes everything of a desired functionality.\nAnalogous to the ELF format, a bistream has its own format to describe\nthe contents. Note that, this file format is publicly documented  1 .\nThat means you can analyze the contents of a bitstream file.\nThe undocument part is the configuration bits: FPGA vendor\ndoes not document the format of the configuration bits,\nand how they may to hardware resources.  As a normal FPGA user, you mostly do not need to understand neither of these.\nThese understandings are only required if you plan to do bitstream readback, preemption scheduling and so forth.  After reading this note, you will understand that a bitstream file is a sequence of instructions and data.\nThe FPGA has a simple state machine to parse the bitstream and then configure the chip.\nPart of the bistream file format is public, the mapping between the bitstream configuration bits\nand the actual physical resource is undocumented.", 
            "title": "Introduction"
        }, 
        {
            "location": "/fpga/bitstream/#bistream-related-files", 
            "text": "In a normal flow, Vivado only generates a simple  .bit  file.\nWhen you click  Program Device , Vivado will use this file to configure your FPGA.  In addition to generating this file, Vivado is capable of generating a bunch other files.\nYou can find a complete coverage in this  link .\nWe give a high level summary here.\nMost of the files have the same content and have similar file size.\nFor instance, the difference between a  .rbt  and a  .bit  is that the former one is in ASCII format while the latter is in binary format,\nbut they have the  same  contents. As for a  .bit  and a  .bin  file, the latter does not have some ASCII headers at the beginning of the file.  .ll , the logical link file, is very interesting.\nIt tells you the mapping between user logic and the actual bit offset in the bistream file data section.\nThis file can be used to aid preemption scheduling.\nHowever, note that, this file only documents a very small part of the mapping.\nTo the best of my knowledge, I think only the registers, on-chip memory are documented, but the routing\ninformation is missing. Thus, this file can help reserve engineer bitstream data section to some extend, but not full of it. Prjxray  is an open source project working on cracking everything on 7-series FPGA.", 
            "title": "Bistream Related Files"
        }, 
        {
            "location": "/fpga/bitstream/#details", 
            "text": "We use  .rbt  and  .bit  to demonstrate the file format.\nNote that they are essentially the same thing, except the former in human-readable ASCII format.  The target board is VCU118, the one used by many cloud vendors.  The following snippt is the first few lines of the  .rpt  file.\nThe first few lines are human-readable ASCII contents describing some general information\nabout the bitstream. Starting from line 8 is the actual bitstream file contents.\nNote that the  .bin  file starts directly from line 8, no general header info is attached.\nThe interesting part is the 1s and 0s.\nUnless otherwise noted, when we refer to bitstream format, we focus on the 1s and 0s only\nand omit any general ASICC information headers.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 Xilinx   ASCII   Bitstream  Created   by   Bitstream   2018 . 3   SW   Build   2405991   on   Thu   Dec    6   23 : 36 : 41   MST   2018  Design   name :      base_mb_wrapper ; UserID = 0XFFFFFFFF ; Version = 2018 . 3  Architecture :     virtexuplus  Part :             xcvu9p-flga2104-2L-e  Date :             Wed   Nov   20   04 : 13 : 05   2019  Bits :             641272864  11111111111111111111111111111111  11111111111111111111111111111111  11111111111111111111111111111111  ...    Note that each line has 32 bits, thus 4 bytes.\nIn Xilinx bistream format, each four bytes is a packet (analogous to CPU instruction).\nEach packet has certain format, it could be a special  header packet , or a normal  data packet .\nThe header packet follows a simple assembly-like instruction set to dictate the configuration process.\nThe bitstream file is a sequence of these four bytes packets.   Why it sounds so complicated, a sequence of instructions?!\nI think the short answer is that configuraing FPGA is not an easy task,\nand any wrong doings may permanently harm the chip.\nNatually, the designer would have a on-chip state machine to control the configuration process,\nnot only to control the whole process but also to ensure safety.  Each Xilinx FPGA has an on-chip  configuration packet processor .\nAll configuration methods such as JTAG, SelectMAP, ICAP merge into this final narrow bridge to carry out the configuration.\nThe configuration packet processor has many internal registers (similar to x86 RAX, CRn, MSR registers).\nThe bitstream usually interact with one of the registers at a time to do one thing.\nFor a more detailed explanation, check out  this blog ,\nand UG570 chapter 9.  To this end, a bitstream consits of three parts:   1) Header packets to prepare the configuration process.  2) The actual configuration bits in a contiguous sequence of data packets.\n     AN write to the  FDRI  register marks the beginning of this section.\n     The length of this section is described by the packet following the FDRI header packet.  3) Header packets to clean up the configuration process.   The actual configuration bits are the ones determine the FPGA functionality.\nNote that if you are using an SSI Xilinx device like VCU118, the bitstream format is a bit more complicated.\nBasically, each die has the above three parts. If an chip has N dies, it will have N above triplet.\nI have complained about this is not well documented  here \nand  here .  I wrote a simple  C program \nto parse the  .rbt  file and associate a human-reable syntax with each line.\nI didn t have a complete coverage of the header packet format.\nThe following snippt shows a parsed  .rbt  file with header removed.\nHere,  0xffffffff  has no effect, like a NOP. 0x000000bb  and  0x11220044  are special bus detect words. 0xaa995566  is another special work marking the synchronization status.\nThe last few lines mark the beginning of the configuration bits section.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72 Parsed from base_mb_wrapper.rbt\nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff \nffffffff  000000 bb Bus Width Sync 11220044  Bus Width Detect\nffffffff \nffffffff \naa995566  SYNC 20000000   20000000   30022001  Write to regs  17  00000000   30020001  Write to regs  16  00000000   30008001  Write to CMD 00000000   20000000   30008001  Write to CMD 00000007   20000000   20000000   30002001  Write to FAR 00000000   30026001  Write to regs  19  00000000   30012001  Write to regs  9  38003 fe5 Write to regs  1  3001 c001 Write to regs  14  00400000   30018001  Write to IDCODE 04 b31093 IDCODE = 4 b31093 30008001  Write to CMD 00000009   20000000   3000 c001 Write to regs  6  00000001   3000 a001 Write to regs  5  00000101   3000 c001 Write to regs  6  00000000   30030001  Write to regs  24  00000000   20000000   20000000   20000000   20000000   20000000   20000000   20000000   20000000   30002001  Write to FAR 00000000   30008001  Write to CMD 00000001   20000000   30004000  Write to FDRI 5065 eadc             -  The length of configuration bits ,  follows a certain  format  00000000              -  The first  4  bytes of the configuration bits !    Hope you have learned something.", 
            "title": "Details"
        }, 
        {
            "location": "/fpga/bitstream/#references", 
            "text": "Xilinx UG570  Xilinx bitstream files  Another blog on Xilinx Bitstream Internals    Source code to annotate bitstream", 
            "title": "References"
        }, 
        {
            "location": "/fpga/pr/", 
            "text": "Morphous (Dynamic-sized) Partial Reconfiguration\n\n\n\n\nVersion History\nDate\nDescription\nFeb 6, 2020\nCreated\nTraditional partital reconfiguration (PR) is limited to using fix-sized PR regions.\nWith one particular static bitstream, users are restricted to only have few pre-defined PR regions.\nIf you wish to extend the PR region size, a whole chip reprogram is needed to burn a new static bitstream.\n\n\nThis practice is suggested by FPGA vendors, and there are reasons behind it.\n\n\nHowever, during our experiment, we found that it is possible to have dynamic-sized PR regions\nwith one static design. The mechanism is quite straightforward with some simple hacks.\n\n\nI will use a MicroBlaze-based design to demonstrate the approach with a VCU118 board. Stay tuned.", 
            "title": "Morphous PR"
        }, 
        {
            "location": "/fpga/pr/#morphous-dynamic-sized-partial-reconfiguration", 
            "text": "Version History Date Description Feb 6, 2020 Created Traditional partital reconfiguration (PR) is limited to using fix-sized PR regions.\nWith one particular static bitstream, users are restricted to only have few pre-defined PR regions.\nIf you wish to extend the PR region size, a whole chip reprogram is needed to burn a new static bitstream.  This practice is suggested by FPGA vendors, and there are reasons behind it.  However, during our experiment, we found that it is possible to have dynamic-sized PR regions\nwith one static design. The mechanism is quite straightforward with some simple hacks.  I will use a MicroBlaze-based design to demonstrate the approach with a VCU118 board. Stay tuned.", 
            "title": "Morphous (Dynamic-sized) Partial Reconfiguration"
        }, 
        {
            "location": "/fpga/vivado/", 
            "text": "Vivado Practice\n\n\n\n\nVersion History\nDate\nDescription\nNov 5, 2019\nMore stuff\nNov 4, 2019\nAdd UG903\nOct 31, 2019\nHappy Halloween\nSep 20, 2019\nCreated\n\n\nCheatsheet\n\n\nPartition Pins\n\n\nThe partition pins are inserted by Vivado at the boundary of a PR region.\n\nPartPin\n is short for Partition Pins.\n\nPPLOC\n is short for Partpin LOC.\n\n\nGet the list of partition pins:\n\n1\nget_pplocs\n \n-\npins \n[\nget_pins\n \n-\nhier \n*\n]\n\n\n\n\n\n\nPartition pin (seems) map to a NODE:\n\n1\n2\n3\n4\n5\n6\n7\n%\n \nreport_property\n \n[\nget_pplocs\n \n-\npins \n[\nget_pins\n XXX\n]]\n\n\n%\n \nreport_property\n \n[\nget_pplocs\n \n-\npins \n[\nget_pins\n inst_count\n/\ncount_out\n[\n0\n]]]\n\n\n\nINFO\n:\n \n[\nVivado\n \n12\n-\n4841\n]\n Found PartPin: INT_X17Y790\n/\nNN1_E_BEG3\n\nProperty\n           Type    Read-only  Value\n\nBASE_CLOCK_REGION\n  string  true       X0Y13\n\nCLASS\n              string  true       node\n\n\n\n\n\n\n\nPblocks\n\n\nSemantic of \nEXCLUDE_PLACEMENT\n\n\nThe document describe this as: Pblock property that prevents the \nplacement\n of any logic not\nbelonging to the Pblock inside the defined Pblock range.\n\n\nDuring my own simple experiment, I found that even Vivado will not place other logics\ninto the Pblock, \nthe routes of static region\n can still go across pblock.\n\n\nSemantic of \nCONTAIN_ROUTING\n\n\nReferences: UG909 and UG905.\n\n\nThe contained routing requirement of RP Pblocks for UltraScale and UltraScale+ devices has\nbeen relaxed to allow for improved routing and timing results. Instead of routing being\nconfined strictly to the resources owned by the Pblock, the routing footprint is expanded.\n\n\nNote that this option is enabled by default. When this option is enabled,\n1) not all interface ports receive a partition pin,\n2) the RP will use routing resources outside its confined area. This is annonying in some way.\n\n\nIf this option is disabled, the implications are:\n1) each interface port (per bit) receivces a partition pin,\n2) RP will only resources confined to its pblocks,\n3) the generated PR bitstream will be smaller,\n4) \nhd_visual/\n will not be generated.\n\n\nHowever, this option does not prevent routings from the static region from crossing RPs.\n\n\nThis command is useful when you want to do some hacking about Partition Pins.\nActually, you can also do this via GUI.\n\n\n1\nset_param\n hd.routingContainmentAreaExpansion false\n\n\n\n\n\n\nBut you wouldn\nt believe that: \nStatic routing is still allowed to use resources inside of the Pblock.\n\nThe implication is also obvious: all PR bitstreams and even blank bitstream will also have the static\nrouting, if their targeted Pblocks happen to have static routing in the first place. This is also why\nwe will need the static bitstream as the base to do PR bitstream generation.\n\n\n\n\nClear RM and Lock Down Static\n\n\nThese commands clear out the Reconfigurable Module logics from the whole design\nand then lock down the static region and static routing. (Reference: UG947)\n\n\n1\n2\n3\nupdate_design\n \n-\ncell XXX \n-\nblack_box\n\n\nlock_design\n \n-\nlevel routing\n\n\n\n\n\n\n\n\nRouting\n\n\nGet the routing of a net\n\n\n1\n2\nset\n net \n[\nget_nets\n XXX\n]\n\n\nget_property\n ROUTE \n$net\n\n\n\n\n\n\n\nLock the routing of a net\n\n\nWe need to lock both the net and the connected cells. Reference is UG903.\n\n\nFollowing commands lock a route of a net. This net is already routed.\nYou could run one by one.\nAfter execution, the route will become dashed (means locked).\nReplace the net name with your interested one.\n\n1\n2\n3\n4\n5\n6\nset\n net \n[\nget_nets inst_count/count_out\n[\n0\n]]\n\nget_property ROUTE \n$net\n\nset_property FIXED_ROUTE \n[\nget_property ROUTE \n$net\n]\n \n$net\n\n\nset_property is_bel_fixed \n1\n \n[\nget_cells XXX\n]\n\nset_property is_loc_fixed \n1\n \n[\nget_cells XXX\n]\n\n\n\n\n\n\nManual routing\n\n\nA great GUI-based manual routing tutorial can be found at \nUG986 Lab 3\n. The last step of manual routing, of course is to lock down the LOC and set \nFIXED_ROUTE\n.\n\n\nBut how can we manually route an unrouted net?\nThe difficulty is that we need to manually find out all the connection nodes/tiles etc..\nThis applies to LOC placement as well.\n\n\n\n\nRead-the-docs\n\n\nBasic\n\n\n\n\nUG912 Vivado Properties Reference Guide\n\n\nExcellent resource on explaining cell, net, pin, port, and so on.\n\n\nDifferentiate \nNetlist Objects\n and \nDevice Resource Objects\n.\n\n\nNetlist Objects\n\n\npin\n: A pin is a point of logical connectivity on a primitive or\n    hierarchical cell. A pin allows the contents of a cell to be abstracted away,\n    and the logic simplified for ease-of-use. A pin is attached to a cell and can be connected to pins on other cells by a net.\n    \nget_pins\n \n-\nof\n \n[\nget_cells\n \nXXX\n]\n. \nget_pins XXX\n\n\nport\n: A port is a special type of hierarchical pin, providing an external connection point at the\n    top-level of a hierarchical design, or an internal connection point in a hierarchical cell or\n    block module to connect the internal logic to the pins on the hierarchical cell. \n\n\ncell\n: A cell is an instance of a netlist logic object, which can either be a leaf-cell or a hierarchical\n    cell. A leaf-cell is a primitive, or a primitive macro, with no further logic detail in the netlist.\n    A hierarchical cell is a module or block that contains one or more additional levels of logic,\n    and eventually concludes at leaf-cells. .. cells have PINs which are connected to NETs to define the external\n    netlist\n The CELL can be placed onto a BEL object in the case of basic logic such as flops, LUTs, and\n    MUXes; or can be placed onto a SITE object in the case of larger logic cells such as BRAMs and DSPs.\n\n\nnet\n: A net is a set of interconnected pins, ports, and wires. Every wire has a net name, which\n    identifies it. Two or more wires can have the same net name. All wires sharing a common net\n    name are part of a single NET, and all pins or ports connected to these wires are electrically connected. ..\n    In the design netlist, a NET can be connected to the PIN of a CELL, or to a PORT. ..\n    As the design is mapped onto the target Xilinx FPGA, the NET is mapped to routing\n    resources such as WIREs, NODEs, and PIPs on the device, and is connected to BELs through\n    BEL_PINs, and to SITEs through SITE_PINs. \n\n\npblock\n: A Pblock is a collection of cells, and one or more rectangular areas or regions that specify\n    the device resources contained by the Pblock. Pblocks are used during floorplanning\n    placement to group related logic and assign it to a region of the target device.\n\n\nExample\n\ncreate_pblock Pblock_usbEngine\n\nadd_cells_to_pblock [get_pblocks Pblock_usbEngine] [get_cells -quiet [listusbEngine1]]\n\nresize_pblock [get_pblocks Pblock_usbEngine] -add {SLICE_X8Y105:SLICE_X23Y149}\n\nresize_pblock [get_pblocks Pblock_usbEngine] -add {DSP48_X0Y42:DSP48_X1Y59}\n\nresize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB18_X0Y42:RAMB18_X1Y59}\n\nresize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB36_X0Y21:RAMB36_X1Y29}\n\n\n\n\n\n\n\n\n\n\nDevice Resource Objects\n\n\nBEL\n: 1) leaf-level cells from the netlist design can be mapped onto bels on the target part\n    2) Bels are grouped in sites. 3) Each bel has bel_pins that map to pins on the cells.\n    4) \nget_bels\n \n-\nof\n \n[\nget_cells\n \nXX\n]\n, \nget_bels\n \n-\nof\n \n[\nget_nets\n \nXX\n]\n, and so on.\n\n\nBEL_PIN\n: 1) a pin or connection point on a BEL object. 2) BEL_PIN is a device object,\n    associated with netlist objects such as the PIN on a CELL, which is the connection point for the NET.\n    3) \nget_bel_pins\n \n-\nof_objects\n \n[\nget_pins\n \n-\nof\n \n[\nget_cells\n \nXXX\n]]\n\n\nTILE\n\n\nSITE\n\n\nNODE\n\n\nWIRE\n\n\nPIP\n\n\n\n\n\n\n\n\n\n\nCONTAIN_ROUTING\n: The \nCONTAIN_ROUTING\n property restricts the routing of signals contained within a Pblock\nto use routing resources within the area defined by the Pblock. This prevents signals inside\nthe Pblock from being routed outside the Pblock, and increases the reusability of the design.\n\n\nThis is useful when you are trying to do advanced PR hacks.\n\n\n\n\n\n\n\n\n\n\nUG835 Vivado TCL Reference Guide\n\n\naka. Vivado TCL Man Page. Read this with the above UG912.\n\n\n\n\n\n\nUG894 Vivado Using TCL scripting\n\n\nGet you started with Vivado TCL\n\n\n\n\n\n\n\n\nUG903 Using Constraints\n\n\n\n\nAbout Xilinx XDC files. You will need to understand UG912 first.\n\n\nPhysical Constraints\n\n\nDONT_TOUCH\n. Prevent netlist optimizations. 1) prevent a net from being optimized away. 2) Prevent merging of manually replicated logic.\n\n\nPlacement constraints\n\n\nRouting constraints\n\n\n\n\n\n\n\n\n\n\n\n\nBook: Practical Programming in Tcl and Tk\n\n\n\n\n\n\nPartial Reconfiguration Related\n\n\n\n\nUG909 Partial Reconfiguration\n\n\nPartition Pins\n\n\nInterface points called partition pins are automatically created within the Pblock ranges\ndefined for the Reconfigurable Partition. These virtual I/O are established within\ninterconnect tiles as the anchor points that remain consistent from one module to the next.\n\n\nIn UltraScale or UltraScale+ designs, \nnot all interface ports receive a partition pin\n. With the\n\nrouting expansion\n feature, as explained in Expansion of \nCONTAIN_ROUTING\n Area, some\ninterface nets are completely contained within the expanded region. When this happens, no\npartition pin is inserted; the entire net, including the source and all loads, is contained\nwithin the area captured by the partial bit file. Rather than pick an unnecessary\nintermediate point for the route, the entire net is rerouted, giving the Vivado tools the\nflexibility to pick an optimal solution.\n\n\nExmaple\n\nset_property HD.PARTPIN_LOCS INT_R_X4Y153 [get_ports \n]\n\nset_property HD.PARTPIN_RANGE SLICE_X4Y153:SLICE_X5Y157 [get_ports \n]\n\nset_property HD.PARTPIN_RANGE {SLICE_Xx0Yx0:SLICE_Xx1Yy1 SLICE_XxNYyN:SLICE_XxMYyM} [get_pins \n/*]\n\n\nThese pins can be manually relocated and locked.\n\n\n\n\n\n\n\n\n\n\nUG905 Hierarchical Design\n\n\nAdd the \nCONTAIN_ROUTING\n property to all OOC Pblocks. Without this property,\n\nlock_design\n cannot lock the routing of an imported module because it cannot be\nguaranteed that there are no routing conflicts\n\n\n\n\n\n\n\n\n\n\nSome IPs\n\n\n\n\nUG947 has the sample code for the PR Controller IP\n\n\nIt does not support simulation. Thus we can not probe any ICAP related signals.\n\n\n\n\n\n\nUltrascale+ SEM does not have any useful ICAP usage signals in Simulation.\n\n\nxapp1230 has some TCL scripts to perform JTAG readback.", 
            "title": "Vivado Practice"
        }, 
        {
            "location": "/fpga/vivado/#vivado-practice", 
            "text": "Version History Date Description Nov 5, 2019 More stuff Nov 4, 2019 Add UG903 Oct 31, 2019 Happy Halloween Sep 20, 2019 Created", 
            "title": "Vivado Practice"
        }, 
        {
            "location": "/fpga/vivado/#cheatsheet", 
            "text": "", 
            "title": "Cheatsheet"
        }, 
        {
            "location": "/fpga/vivado/#partition-pins", 
            "text": "The partition pins are inserted by Vivado at the boundary of a PR region. PartPin  is short for Partition Pins. PPLOC  is short for Partpin LOC.  Get the list of partition pins: 1 get_pplocs   - pins  [ get_pins   - hier  * ]    Partition pin (seems) map to a NODE: 1\n2\n3\n4\n5\n6\n7 %   report_property   [ get_pplocs   - pins  [ get_pins  XXX ]]  %   report_property   [ get_pplocs   - pins  [ get_pins  inst_count / count_out [ 0 ]]]  INFO :   [ Vivado   12 - 4841 ]  Found PartPin: INT_X17Y790 / NN1_E_BEG3 Property            Type    Read-only  Value BASE_CLOCK_REGION   string  true       X0Y13 CLASS               string  true       node", 
            "title": "Partition Pins"
        }, 
        {
            "location": "/fpga/vivado/#pblocks", 
            "text": "", 
            "title": "Pblocks"
        }, 
        {
            "location": "/fpga/vivado/#semantic-of-exclude_placement", 
            "text": "The document describe this as: Pblock property that prevents the  placement  of any logic not\nbelonging to the Pblock inside the defined Pblock range.  During my own simple experiment, I found that even Vivado will not place other logics\ninto the Pblock,  the routes of static region  can still go across pblock.", 
            "title": "Semantic of EXCLUDE_PLACEMENT"
        }, 
        {
            "location": "/fpga/vivado/#semantic-of-contain_routing", 
            "text": "References: UG909 and UG905.  The contained routing requirement of RP Pblocks for UltraScale and UltraScale+ devices has\nbeen relaxed to allow for improved routing and timing results. Instead of routing being\nconfined strictly to the resources owned by the Pblock, the routing footprint is expanded.  Note that this option is enabled by default. When this option is enabled,\n1) not all interface ports receive a partition pin,\n2) the RP will use routing resources outside its confined area. This is annonying in some way.  If this option is disabled, the implications are:\n1) each interface port (per bit) receivces a partition pin,\n2) RP will only resources confined to its pblocks,\n3) the generated PR bitstream will be smaller,\n4)  hd_visual/  will not be generated.  However, this option does not prevent routings from the static region from crossing RPs.  This command is useful when you want to do some hacking about Partition Pins.\nActually, you can also do this via GUI.  1 set_param  hd.routingContainmentAreaExpansion false   But you wouldn t believe that:  Static routing is still allowed to use resources inside of the Pblock. \nThe implication is also obvious: all PR bitstreams and even blank bitstream will also have the static\nrouting, if their targeted Pblocks happen to have static routing in the first place. This is also why\nwe will need the static bitstream as the base to do PR bitstream generation.", 
            "title": "Semantic of CONTAIN_ROUTING"
        }, 
        {
            "location": "/fpga/vivado/#clear-rm-and-lock-down-static", 
            "text": "These commands clear out the Reconfigurable Module logics from the whole design\nand then lock down the static region and static routing. (Reference: UG947)  1\n2\n3 update_design   - cell XXX  - black_box lock_design   - level routing", 
            "title": "Clear RM and Lock Down Static"
        }, 
        {
            "location": "/fpga/vivado/#routing", 
            "text": "", 
            "title": "Routing"
        }, 
        {
            "location": "/fpga/vivado/#get-the-routing-of-a-net", 
            "text": "1\n2 set  net  [ get_nets  XXX ]  get_property  ROUTE  $net", 
            "title": "Get the routing of a net"
        }, 
        {
            "location": "/fpga/vivado/#lock-the-routing-of-a-net", 
            "text": "We need to lock both the net and the connected cells. Reference is UG903.  Following commands lock a route of a net. This net is already routed.\nYou could run one by one.\nAfter execution, the route will become dashed (means locked).\nReplace the net name with your interested one. 1\n2\n3\n4\n5\n6 set  net  [ get_nets inst_count/count_out [ 0 ]] \nget_property ROUTE  $net \nset_property FIXED_ROUTE  [ get_property ROUTE  $net ]   $net \n\nset_property is_bel_fixed  1   [ get_cells XXX ] \nset_property is_loc_fixed  1   [ get_cells XXX ]", 
            "title": "Lock the routing of a net"
        }, 
        {
            "location": "/fpga/vivado/#manual-routing", 
            "text": "A great GUI-based manual routing tutorial can be found at  UG986 Lab 3 . The last step of manual routing, of course is to lock down the LOC and set  FIXED_ROUTE .  But how can we manually route an unrouted net?\nThe difficulty is that we need to manually find out all the connection nodes/tiles etc..\nThis applies to LOC placement as well.", 
            "title": "Manual routing"
        }, 
        {
            "location": "/fpga/vivado/#read-the-docs", 
            "text": "Basic   UG912 Vivado Properties Reference Guide  Excellent resource on explaining cell, net, pin, port, and so on.  Differentiate  Netlist Objects  and  Device Resource Objects .  Netlist Objects  pin : A pin is a point of logical connectivity on a primitive or\n    hierarchical cell. A pin allows the contents of a cell to be abstracted away,\n    and the logic simplified for ease-of-use. A pin is attached to a cell and can be connected to pins on other cells by a net.\n     get_pins   - of   [ get_cells   XXX ] .  get_pins XXX  port : A port is a special type of hierarchical pin, providing an external connection point at the\n    top-level of a hierarchical design, or an internal connection point in a hierarchical cell or\n    block module to connect the internal logic to the pins on the hierarchical cell.   cell : A cell is an instance of a netlist logic object, which can either be a leaf-cell or a hierarchical\n    cell. A leaf-cell is a primitive, or a primitive macro, with no further logic detail in the netlist.\n    A hierarchical cell is a module or block that contains one or more additional levels of logic,\n    and eventually concludes at leaf-cells. .. cells have PINs which are connected to NETs to define the external\n    netlist  The CELL can be placed onto a BEL object in the case of basic logic such as flops, LUTs, and\n    MUXes; or can be placed onto a SITE object in the case of larger logic cells such as BRAMs and DSPs.  net : A net is a set of interconnected pins, ports, and wires. Every wire has a net name, which\n    identifies it. Two or more wires can have the same net name. All wires sharing a common net\n    name are part of a single NET, and all pins or ports connected to these wires are electrically connected. ..\n    In the design netlist, a NET can be connected to the PIN of a CELL, or to a PORT. ..\n    As the design is mapped onto the target Xilinx FPGA, the NET is mapped to routing\n    resources such as WIREs, NODEs, and PIPs on the device, and is connected to BELs through\n    BEL_PINs, and to SITEs through SITE_PINs.   pblock : A Pblock is a collection of cells, and one or more rectangular areas or regions that specify\n    the device resources contained by the Pblock. Pblocks are used during floorplanning\n    placement to group related logic and assign it to a region of the target device.  Example \ncreate_pblock Pblock_usbEngine \nadd_cells_to_pblock [get_pblocks Pblock_usbEngine] [get_cells -quiet [listusbEngine1]] \nresize_pblock [get_pblocks Pblock_usbEngine] -add {SLICE_X8Y105:SLICE_X23Y149} \nresize_pblock [get_pblocks Pblock_usbEngine] -add {DSP48_X0Y42:DSP48_X1Y59} \nresize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB18_X0Y42:RAMB18_X1Y59} \nresize_pblock [get_pblocks Pblock_usbEngine] -add {RAMB36_X0Y21:RAMB36_X1Y29}      Device Resource Objects  BEL : 1) leaf-level cells from the netlist design can be mapped onto bels on the target part\n    2) Bels are grouped in sites. 3) Each bel has bel_pins that map to pins on the cells.\n    4)  get_bels   - of   [ get_cells   XX ] ,  get_bels   - of   [ get_nets   XX ] , and so on.  BEL_PIN : 1) a pin or connection point on a BEL object. 2) BEL_PIN is a device object,\n    associated with netlist objects such as the PIN on a CELL, which is the connection point for the NET.\n    3)  get_bel_pins   - of_objects   [ get_pins   - of   [ get_cells   XXX ]]  TILE  SITE  NODE  WIRE  PIP      CONTAIN_ROUTING : The  CONTAIN_ROUTING  property restricts the routing of signals contained within a Pblock\nto use routing resources within the area defined by the Pblock. This prevents signals inside\nthe Pblock from being routed outside the Pblock, and increases the reusability of the design.  This is useful when you are trying to do advanced PR hacks.      UG835 Vivado TCL Reference Guide  aka. Vivado TCL Man Page. Read this with the above UG912.    UG894 Vivado Using TCL scripting  Get you started with Vivado TCL     UG903 Using Constraints   About Xilinx XDC files. You will need to understand UG912 first.  Physical Constraints  DONT_TOUCH . Prevent netlist optimizations. 1) prevent a net from being optimized away. 2) Prevent merging of manually replicated logic.  Placement constraints  Routing constraints       Book: Practical Programming in Tcl and Tk    Partial Reconfiguration Related   UG909 Partial Reconfiguration  Partition Pins  Interface points called partition pins are automatically created within the Pblock ranges\ndefined for the Reconfigurable Partition. These virtual I/O are established within\ninterconnect tiles as the anchor points that remain consistent from one module to the next.  In UltraScale or UltraScale+ designs,  not all interface ports receive a partition pin . With the routing expansion  feature, as explained in Expansion of  CONTAIN_ROUTING  Area, some\ninterface nets are completely contained within the expanded region. When this happens, no\npartition pin is inserted; the entire net, including the source and all loads, is contained\nwithin the area captured by the partial bit file. Rather than pick an unnecessary\nintermediate point for the route, the entire net is rerouted, giving the Vivado tools the\nflexibility to pick an optimal solution.  Exmaple \nset_property HD.PARTPIN_LOCS INT_R_X4Y153 [get_ports  ] \nset_property HD.PARTPIN_RANGE SLICE_X4Y153:SLICE_X5Y157 [get_ports  ] \nset_property HD.PARTPIN_RANGE {SLICE_Xx0Yx0:SLICE_Xx1Yy1 SLICE_XxNYyN:SLICE_XxMYyM} [get_pins  /*]  These pins can be manually relocated and locked.      UG905 Hierarchical Design  Add the  CONTAIN_ROUTING  property to all OOC Pblocks. Without this property, lock_design  cannot lock the routing of an imported module because it cannot be\nguaranteed that there are no routing conflicts", 
            "title": "Read-the-docs"
        }, 
        {
            "location": "/fpga/vivado/#some-ips", 
            "text": "UG947 has the sample code for the PR Controller IP  It does not support simulation. Thus we can not probe any ICAP related signals.    Ultrascale+ SEM does not have any useful ICAP usage signals in Simulation.  xapp1230 has some TCL scripts to perform JTAG readback.", 
            "title": "Some IPs"
        }, 
        {
            "location": "/fpga/hls_axi/", 
            "text": "High-performance AXI-MM in HLS\n\n\nMy personal experience: the native AXI-MM in HLS is horrible.\nIt fails to generate efficient code.\nThe best practice I found is the use an external Datamover.\nIn HLS, all memory access is made via AXI-Stream.\nUsing AXI-Stream means we can wait the result asynchronously,\nhence we can deal with long memory access in a more informed manner.\n\n\nUsually using AXI-Stream and Datamover delivers code with II=1.", 
            "title": "Efficient AXI-MM"
        }, 
        {
            "location": "/fpga/hls_axi/#high-performance-axi-mm-in-hls", 
            "text": "My personal experience: the native AXI-MM in HLS is horrible.\nIt fails to generate efficient code.\nThe best practice I found is the use an external Datamover.\nIn HLS, all memory access is made via AXI-Stream.\nUsing AXI-Stream means we can wait the result asynchronously,\nhence we can deal with long memory access in a more informed manner.  Usually using AXI-Stream and Datamover delivers code with II=1.", 
            "title": "High-performance AXI-MM in HLS"
        }, 
        {
            "location": "/lego/kernel/boot/", 
            "text": "Notes on GRUB2 and Boot Sequence\n\n\nVersion History\nDate\nDescription\nMar 31, 2020\nCopied from \nhttps://github.com/lastweek/source-grub2\n.\nAbout GRUB2\n\n\nGRUB2: \nhttps://www.gnu.org/software/grub/manual/grub/grub.html#Introduction\n\n\nSource code: \nhttps://github.com/lastweek/source-grub2\n\n\nlinux v.s. linux16\n\n\nAn interesting thing is that there are two ways to\nload an kernel image in \ngrub.cfg\n, either\n\nlinux vmlinuz-3.10.0\n or \nlinux16 vmlinuz-3.10.0\n.\nThey have different effects, but not sure what are those differences.\nI remember only the linux16 one works for me,\nbut not remembering why either. At least on CentOS 7, it\ns all linux16.\n\n\nThe \nlinux16\n and \ninitrd16\n in \ngrub-core/loader/i386/pc/linux.c\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nGRUB_MOD_INIT\n(\nlinux16\n)\n\n\n{\n\n  \ncmd_linux\n \n=\n\n    \ngrub_register_command\n \n(\nlinux16\n,\n \ngrub_cmd_linux\n,\n\n               \n0\n,\n \nN_\n(\nLoad Linux.\n));\n\n  \ncmd_initrd\n \n=\n\n    \ngrub_register_command\n \n(\ninitrd16\n,\n \ngrub_cmd_initrd\n,\n\n               \n0\n,\n \nN_\n(\nLoad initrd.\n));\n\n  \nmy_mod\n \n=\n \nmod\n;\n\n\n}\n\n\n\n\n\n\nThe \nlinux\n and \ninitrd\n in \ngrub-core/loader/i386/linux.c\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nstatic\n \ngrub_command_t\n \ncmd_linux\n,\n \ncmd_initrd\n;\n\n\n\nGRUB_MOD_INIT\n(\nlinux\n)\n\n\n{\n\n  \ncmd_linux\n \n=\n \ngrub_register_command\n \n(\nlinux\n,\n \ngrub_cmd_linux\n,\n\n                     \n0\n,\n \nN_\n(\nLoad Linux.\n));\n\n  \ncmd_initrd\n \n=\n \ngrub_register_command\n \n(\ninitrd\n,\n \ngrub_cmd_initrd\n,\n\n                      \n0\n,\n \nN_\n(\nLoad initrd.\n));\n\n  \nmy_mod\n \n=\n \nmod\n;\n\n\n}\n\n\n\n\n\n\nBoot Protocol and Sequence\n\n\nThis was written for \nhttps://github.com/lastweek/source-grub2\n. I just copied it here.\n\n\nLinux (x86) has a boot protocol, described by \nhttps://www.kernel.org/doc/html/latest/x86/boot.html\n.\nEssentially, it is a contiguous memory region, just like a big C \nstruct\n:\nsome fields are filled by kernel duing compile time (\narch/x86/boot/tools/build.c\n and some in code),\nsome fields are filled by GRUB2 during boot time to tell kernel some\nimportant addresses, e.g., kernel parameters, ramdisk locations etc.\n\n\nGRUB2 code follows the protocol, and you can partially tell from the \ngrub_cmd_linux()\n function.\n\n\nLast time I working on this was late 2016, I truly spent a lot investigating\nhow GRUB and linux boot works. I will try to document a bit, if my memory serves:\n\n\n\n\n\n\nIn the Linux kernel, file \narch/x86/boot/header.S\n is the first file got run after GRUB2.\nThis file is a bit complicated but not hard to understand!\nIt has 3 parts.\nFor the first part, it detects if it was loaded by a bootloader, if not, just by printing an error message and reboot.\nIt the kernel was loaded by a bootloader like GRUB2, the first part will never execute.\nThe bootload will directly jump to the second part. This is part of the boot protocol.\nFor the second part, it lists all the fields described by the boot protocol.\nAnd finally the third part is real-mode instructions that got run after the GRUB2 jumo.\nThe starting function is called \nstart_of_setup\n, which will do some stack checking,\nand then jump to C code in \narch/x86/boot/main.c\n.\n\n\n\n\n\n\narch/x86/boot/main.c\n runs on real-mode, it will do some setup and jump to protected-mode (32-bit).\nIt is running after BIOS but before the actual Linux kernel.\nThus this piece of code must rely on BIOS to do stuff, which makes it very unique.\nThe major task of the setup code is to prepare the \nstruct boot_params\n, which has all the boot information, some of them were extracted from the \nheader.S\n. The \nstruct boot_params\n will be passed down and used by many kernel subsystems later on.\nThe final jump happens in \narch/x86/boot/pmjump.S\n\n\n1\n2\n3\n4\n5\n        \n#\n\n        \n# Jump to protected-mode kernel, 0x100000\n\n        \n# which is the compressed/head_$(BITS).o\n\n        \n#\n\n        \njmp\n     \n*%\neax\n\n\n\n\n\n\n\n\n\n\nThen, we are in \narch/x86/boot/compressed/head_64.S\n.\nAbove pmjump jumps to \nstartup_32\n, it will enable paging, tweak GDT table etc, setup pagetable, and transition to 64-bit entry point \nstartup_64\n. \nAnd finally, we are in 64-bit. The final jump will go to \narch/x86/kernel/head_64.S\n. We are close!\n\n\n\n\n\n\nNow we are in \narch/x86/kernel/head_64.S\n. We are in 64-bit. But some further setup is needed. This part is really low-level and engaging. I would never know I how managed to understand and port all this shit. It setup a lot GDT, IDT stuff, and some pgfault handlers. It turns out those early pgfault handlers are NECESSARY and I remember they played an very interesting role! Finally, this assembly will jump to \narch/x86/kernel/head64.c\n, the C code!\n\n\n\n\nI guess an interesting part is \nsecondary_startup_64\n. This code is actually run by non-booting CPUs, or secondary CPUs.\n  After the major boot CPU is up and running (already within \nstart_kernel()\n), I believe its the \nsmp_init()\n that will send IPI wakeup interrupts to all present secondary CPUs.\n  The secondary CPUs will start from real-mode, obviously. Then they will transition from 16bit to 32bit, from 32bit to 64bit. That code is in \narch/x86/realmode/rm/trampoline.S\n!\n\n\narch/x86/realmode\n is interesting. It uses piggyback technique. All the real-mode and 32bit code are in \narch/x86/realmode/rm/*\n, a special \nlinker script\n is used to construct the code in a specific way! Think about mix 16bit, 32bit, 64bit code together, nasty!\n\n\n\n\n\n\n\n\nHooray, C world. We are in \narch/x86/kernel/head64.c\n. The starting function is \nx86_64_start_kernel\n! And the end is the \nstart_kernel\n, the one in \ninit/main.c\n.\n\n\n\n\n\n\nIn all, there are a lot jumps after GRUB2 load the kernel, and its a long road before we can reach \nstart_kernel()\n. It probably should not be this complex, but the x86 architecture really makes it worse. Happy hacking!", 
            "title": "GRUB2 and Boot"
        }, 
        {
            "location": "/lego/kernel/boot/#notes-on-grub2-and-boot-sequence", 
            "text": "Version History Date Description Mar 31, 2020 Copied from  https://github.com/lastweek/source-grub2 .", 
            "title": "Notes on GRUB2 and Boot Sequence"
        }, 
        {
            "location": "/lego/kernel/boot/#about-grub2", 
            "text": "GRUB2:  https://www.gnu.org/software/grub/manual/grub/grub.html#Introduction  Source code:  https://github.com/lastweek/source-grub2", 
            "title": "About GRUB2"
        }, 
        {
            "location": "/lego/kernel/boot/#linux-vs-linux16", 
            "text": "An interesting thing is that there are two ways to\nload an kernel image in  grub.cfg , either linux vmlinuz-3.10.0  or  linux16 vmlinuz-3.10.0 .\nThey have different effects, but not sure what are those differences.\nI remember only the linux16 one works for me,\nbut not remembering why either. At least on CentOS 7, it s all linux16.  The  linux16  and  initrd16  in  grub-core/loader/i386/pc/linux.c :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 GRUB_MOD_INIT ( linux16 )  { \n   cmd_linux   = \n     grub_register_command   ( linux16 ,   grub_cmd_linux , \n                0 ,   N_ ( Load Linux. )); \n   cmd_initrd   = \n     grub_register_command   ( initrd16 ,   grub_cmd_initrd , \n                0 ,   N_ ( Load initrd. )); \n   my_mod   =   mod ;  }    The  linux  and  initrd  in  grub-core/loader/i386/linux.c :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 static   grub_command_t   cmd_linux ,   cmd_initrd ;  GRUB_MOD_INIT ( linux )  { \n   cmd_linux   =   grub_register_command   ( linux ,   grub_cmd_linux , \n                      0 ,   N_ ( Load Linux. )); \n   cmd_initrd   =   grub_register_command   ( initrd ,   grub_cmd_initrd , \n                       0 ,   N_ ( Load initrd. )); \n   my_mod   =   mod ;  }", 
            "title": "linux v.s. linux16"
        }, 
        {
            "location": "/lego/kernel/boot/#boot-protocol-and-sequence", 
            "text": "This was written for  https://github.com/lastweek/source-grub2 . I just copied it here.  Linux (x86) has a boot protocol, described by  https://www.kernel.org/doc/html/latest/x86/boot.html .\nEssentially, it is a contiguous memory region, just like a big C  struct :\nsome fields are filled by kernel duing compile time ( arch/x86/boot/tools/build.c  and some in code),\nsome fields are filled by GRUB2 during boot time to tell kernel some\nimportant addresses, e.g., kernel parameters, ramdisk locations etc.  GRUB2 code follows the protocol, and you can partially tell from the  grub_cmd_linux()  function.  Last time I working on this was late 2016, I truly spent a lot investigating\nhow GRUB and linux boot works. I will try to document a bit, if my memory serves:    In the Linux kernel, file  arch/x86/boot/header.S  is the first file got run after GRUB2.\nThis file is a bit complicated but not hard to understand!\nIt has 3 parts.\nFor the first part, it detects if it was loaded by a bootloader, if not, just by printing an error message and reboot.\nIt the kernel was loaded by a bootloader like GRUB2, the first part will never execute.\nThe bootload will directly jump to the second part. This is part of the boot protocol.\nFor the second part, it lists all the fields described by the boot protocol.\nAnd finally the third part is real-mode instructions that got run after the GRUB2 jumo.\nThe starting function is called  start_of_setup , which will do some stack checking,\nand then jump to C code in  arch/x86/boot/main.c .    arch/x86/boot/main.c  runs on real-mode, it will do some setup and jump to protected-mode (32-bit).\nIt is running after BIOS but before the actual Linux kernel.\nThus this piece of code must rely on BIOS to do stuff, which makes it very unique.\nThe major task of the setup code is to prepare the  struct boot_params , which has all the boot information, some of them were extracted from the  header.S . The  struct boot_params  will be passed down and used by many kernel subsystems later on.\nThe final jump happens in  arch/x86/boot/pmjump.S  1\n2\n3\n4\n5          # \n         # Jump to protected-mode kernel, 0x100000 \n         # which is the compressed/head_$(BITS).o \n         # \n         jmp       *% eax      Then, we are in  arch/x86/boot/compressed/head_64.S .\nAbove pmjump jumps to  startup_32 , it will enable paging, tweak GDT table etc, setup pagetable, and transition to 64-bit entry point  startup_64 . \nAnd finally, we are in 64-bit. The final jump will go to  arch/x86/kernel/head_64.S . We are close!    Now we are in  arch/x86/kernel/head_64.S . We are in 64-bit. But some further setup is needed. This part is really low-level and engaging. I would never know I how managed to understand and port all this shit. It setup a lot GDT, IDT stuff, and some pgfault handlers. It turns out those early pgfault handlers are NECESSARY and I remember they played an very interesting role! Finally, this assembly will jump to  arch/x86/kernel/head64.c , the C code!   I guess an interesting part is  secondary_startup_64 . This code is actually run by non-booting CPUs, or secondary CPUs.\n  After the major boot CPU is up and running (already within  start_kernel() ), I believe its the  smp_init()  that will send IPI wakeup interrupts to all present secondary CPUs.\n  The secondary CPUs will start from real-mode, obviously. Then they will transition from 16bit to 32bit, from 32bit to 64bit. That code is in  arch/x86/realmode/rm/trampoline.S !  arch/x86/realmode  is interesting. It uses piggyback technique. All the real-mode and 32bit code are in  arch/x86/realmode/rm/* , a special  linker script  is used to construct the code in a specific way! Think about mix 16bit, 32bit, 64bit code together, nasty!     Hooray, C world. We are in  arch/x86/kernel/head64.c . The starting function is  x86_64_start_kernel ! And the end is the  start_kernel , the one in  init/main.c .    In all, there are a lot jumps after GRUB2 load the kernel, and its a long road before we can reach  start_kernel() . It probably should not be this complex, but the x86 architecture really makes it worse. Happy hacking!", 
            "title": "Boot Protocol and Sequence"
        }, 
        {
            "location": "/lego/kernel/kconfig/", 
            "text": "Lego Kconfig\n\n\nNetwork\n\n\n\n\nEnable \nCONFIG_INFINIBAND\n\n\nEnable \nCONFIG_FIT\n\n\nSet \nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT\n: boot time connection timeout\n\n\nSet \nCONFIG_FIT_NR_NODES\n: number of Lego nodes in this run\n\n\nSet \nCONFIG_FIT_LOCAL_ID\n: current node id\n\n\n\n\nIn \nnet/lego/fit_machine.c\n, modify the \nlego_cluster_hostnames\n array to match the machines you are using.\n\n\n\n\n\n\nSet \nCONFIG_DEFAULT_MEM_NODE\n in processor manager\n\n\n\n\nSet \nCONFIG_DEFAULT_STORAGE_NODE\n if you are running with storage component.\n\n\n\n\nNetwork configuration is crucial, please make sure all Lego nodes have consistent configurations. Otherwise the system may panic or fail to connect.\n\n\nProcessor\n\n\n\n\nEnable \nCONFIG_COMP_PROCESSOR\n\n\nopen \n.config\n\n\nremove line \n# CONFIG_COMP_PROCESSOR is not set\n\n\nclose \n.config\n\n\ndo \nmake\n, you will see \nConfigure Lego as processor component (COMP_PROCESSOR) [N/y/?] (NEW)\n, select Y\n\n\nChoose default configuration for all new config options\n\n\n\n\n\n\nEnable \nCONFIG_USE_RAMFS\n if you are not using storage components\n\n\n\n\nMemory\n\n\n\n\nEnable \nCONFIG_COMP_MEMORY\n\n\nopen \n.config\n\n\nremove line \n# CONFIG_COMP_MEMORY is not set\n\n\nclose \n.config\n\n\ndo \nmake\n, you will see \nConfigure Lego as memory component manager (COMP_MEMORY) [N/y/?] (NEW)\n, select Y\n\n\nChoose default configuration for all new config options\n\n\n\n\n\n\nEnable \nCONFIG_USE_RAMFS\n if you are not using storage components\n\n\nSet \nCONFIG_RAMFS_OBJECT_FILE\n: points to \nstatic-linked\n ELF file that you want to execute.\n\n\ntips: you can put your test code under \nusr/\n directory, and a simple \nmake\n will compile everything under.\n\n\n\n\n\n\n\n\nRun without Storage Component\n\n\nTo run Lego just with one processor component and one memory component, you need to:\n\n\n\n\nEnable \nCONFIG_USE_RAMFS\n at both sides. And in memory side, you need to set the \nCONFIG_RAMFS_OBJECT_FILE\n, which points to the ELF binary you want to test.\n\n\nmake sure \nCONFIG_DEFAULT_MEM_NODE\n at processor component is pointing to memory component\ns node id.\n\n\n\n\nA typical code snippet and configuration would be:\n\n1\n2\n3\n4\nstatic\n \nconst\n \nchar\n \n*\nlego_cluster_hostnames\n[\nCONFIG_FIT_NR_NODES\n]\n \n=\n \n{\n\n        \n[\n0\n]\n     \n=\n       \nwuklab00\n,\n\n        \n[\n1\n]\n     \n=\n       \nwuklab01\n,\n\n\n};\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nwuklab00 Processor\n\n#\n# Lego Processor Component Configurations\n#\nCONFIG_COMP_PROCESSOR=y\nCONFIG_CHECKPOINT=y\nCONFIG_MEMMAP_MEMBLOCK_RESERVED=y\n# CONFIG_PCACHE_EVICT_RANDOM is not set\n# CONFIG_PCACHE_EVICT_FIFO is not set\nCONFIG_PCACHE_EVICT_LRU=y\nCONFIG_PCACHE_EVICT_GENERIC_SWEEP=y\n# CONFIG_PCACHE_EVICTION_WRITE_PROTECT is not set\n# CONFIG_PCACHE_EVICTION_PERSET_LIST is not set\nCONFIG_PCACHE_EVICTION_VICTIM=y\nCONFIG_PCACHE_EVICTION_VICTIM_NR_ENTRIES=8\nCONFIG_PCACHE_PREFETCH=y\n\n#\n# Processor DEBUG Options\n#\n\n#\n# Lego Memory Component Configurations\n#\n# CONFIG_COMP_MEMORY is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=0\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\nwuklab01 Memory\n\n#\n# Lego Memory Component Configurations\n#\nCONFIG_COMP_MEMORY=y\n\n#\n# Memory DEBUG Options\n#\n# CONFIG_MEM_PREFETCH is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\nCONFIG_RAMFS_OBJECT_FILE=\nusr/pcache_conflict.o\n\n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=1", 
            "title": "Kconfig"
        }, 
        {
            "location": "/lego/kernel/kconfig/#lego-kconfig", 
            "text": "", 
            "title": "Lego Kconfig"
        }, 
        {
            "location": "/lego/kernel/kconfig/#network", 
            "text": "Enable  CONFIG_INFINIBAND  Enable  CONFIG_FIT  Set  CONFIG_FIT_INITIAL_SLEEP_TIMEOUT : boot time connection timeout  Set  CONFIG_FIT_NR_NODES : number of Lego nodes in this run  Set  CONFIG_FIT_LOCAL_ID : current node id   In  net/lego/fit_machine.c , modify the  lego_cluster_hostnames  array to match the machines you are using.    Set  CONFIG_DEFAULT_MEM_NODE  in processor manager   Set  CONFIG_DEFAULT_STORAGE_NODE  if you are running with storage component.   Network configuration is crucial, please make sure all Lego nodes have consistent configurations. Otherwise the system may panic or fail to connect.", 
            "title": "Network"
        }, 
        {
            "location": "/lego/kernel/kconfig/#processor", 
            "text": "Enable  CONFIG_COMP_PROCESSOR  open  .config  remove line  # CONFIG_COMP_PROCESSOR is not set  close  .config  do  make , you will see  Configure Lego as processor component (COMP_PROCESSOR) [N/y/?] (NEW) , select Y  Choose default configuration for all new config options    Enable  CONFIG_USE_RAMFS  if you are not using storage components", 
            "title": "Processor"
        }, 
        {
            "location": "/lego/kernel/kconfig/#memory", 
            "text": "Enable  CONFIG_COMP_MEMORY  open  .config  remove line  # CONFIG_COMP_MEMORY is not set  close  .config  do  make , you will see  Configure Lego as memory component manager (COMP_MEMORY) [N/y/?] (NEW) , select Y  Choose default configuration for all new config options    Enable  CONFIG_USE_RAMFS  if you are not using storage components  Set  CONFIG_RAMFS_OBJECT_FILE : points to  static-linked  ELF file that you want to execute.  tips: you can put your test code under  usr/  directory, and a simple  make  will compile everything under.", 
            "title": "Memory"
        }, 
        {
            "location": "/lego/kernel/kconfig/#run-without-storage-component", 
            "text": "To run Lego just with one processor component and one memory component, you need to:   Enable  CONFIG_USE_RAMFS  at both sides. And in memory side, you need to set the  CONFIG_RAMFS_OBJECT_FILE , which points to the ELF binary you want to test.  make sure  CONFIG_DEFAULT_MEM_NODE  at processor component is pointing to memory component s node id.   A typical code snippet and configuration would be: 1\n2\n3\n4 static   const   char   * lego_cluster_hostnames [ CONFIG_FIT_NR_NODES ]   =   { \n         [ 0 ]       =         wuklab00 , \n         [ 1 ]       =         wuklab01 ,  };     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49 wuklab00 Processor\n\n#\n# Lego Processor Component Configurations\n#\nCONFIG_COMP_PROCESSOR=y\nCONFIG_CHECKPOINT=y\nCONFIG_MEMMAP_MEMBLOCK_RESERVED=y\n# CONFIG_PCACHE_EVICT_RANDOM is not set\n# CONFIG_PCACHE_EVICT_FIFO is not set\nCONFIG_PCACHE_EVICT_LRU=y\nCONFIG_PCACHE_EVICT_GENERIC_SWEEP=y\n# CONFIG_PCACHE_EVICTION_WRITE_PROTECT is not set\n# CONFIG_PCACHE_EVICTION_PERSET_LIST is not set\nCONFIG_PCACHE_EVICTION_VICTIM=y\nCONFIG_PCACHE_EVICTION_VICTIM_NR_ENTRIES=8\nCONFIG_PCACHE_PREFETCH=y\n\n#\n# Processor DEBUG Options\n#\n\n#\n# Lego Memory Component Configurations\n#\n# CONFIG_COMP_MEMORY is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=0    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 wuklab01 Memory\n\n#\n# Lego Memory Component Configurations\n#\nCONFIG_COMP_MEMORY=y\n\n#\n# Memory DEBUG Options\n#\n# CONFIG_MEM_PREFETCH is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\nCONFIG_RAMFS_OBJECT_FILE= usr/pcache_conflict.o \n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=1", 
            "title": "Run without Storage Component"
        }, 
        {
            "location": "/lego/kernel/debug/", 
            "text": "Debug Facility in Lego\n\n\nLego provides several handy debug helpers to ease our coding pain. We category them by layers, namely \n1)\n \nCore Kernel\n, the lowest level of Lego, which is shared by all managers. \n2)\n \nProcessor Manager\n, which controls processor components. \n3)\n \nMemory Manager\n, which controls memory components.\n\n\nCore Kernel\n\n\n1\n2\nvoid\n \ndump_pte\n(\npte_t\n \n*\nptep\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_page\n(\nstruct\n \npage\n \n*\npage\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\n\n\n\nThese two helpers will dump a given pte entry or a page. Use this function if you are developing core related to physical memory allocation or pcache.\n\n\n\n\n1\nvoid\n \nptdump_walk_pgd_level\n(\npgd_t\n \n*\npgd\n);\n\n\n\n\n\nThis debug helper will dump the whole pgtable ranges. Contiguous page table entries that share the same property will be merged together and will be printed once. Use this function if you are developing code related to user page tables.\n\n\n\n\n1\n2\n3\nvoid\n \nshow_state_filter\n(\nunsigned\n \nlong\n \nstate_filter\n,\n \nbool\n \nprint_rq\n);\n\n\nvoid\n \nsched_show_task\n(\nstruct\n \ntask_struct\n \n*\np\n);\n\n\nvoid\n \nsysrq_sched_debug_show\n(\nvoid\n);\n\n\n\n\n\nThis set of functions are debug helpers for local scheduler. They will print all the tasks running in the system, and detailed information about percpu \nrunqueue\n. Use this set of functions if you are developing code related to scheduler.\n\n\nProcessor Manager\n\n\n1\n2\n3\n4\nvoid\n \ndump_pcache_meta\n(\nstruct\n \npcache_meta\n \n*\npcm\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_pcache_victim\n(\nstruct\n \npcache_victim_meta\n \n*\nvictim\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_pcache_rmap\n(\nstruct\n \npcache_rmap\n \n*\nrmap\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_pcache_line\n(\nstruct\n \npcache_meta\n \n*\npcm\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\n\n\n\nThese functions dump a given pcache line, a victim line, or a given reserve mapping. The last one will print the pcache line content, which generates a lot messages, you are warned. Use these functions if you are developing pcache or victim cache code.\n\n\nMemory Manager\n\n\n1\n2\nvoid\n \ndump_lego_mm\n(\nconst\n \nstruct\n \nlego_mm_struct\n \n*\nmm\n);\n\n\nvoid\n \ndump_vma\n(\nconst\n \nstruct\n \nvm_area_struct\n \n*\nvma\n);\n\n\n\n\n\nThese two functions are used to dump the virtual address space of a process. Use these functions if you developing process VM related things.", 
            "title": "Debug"
        }, 
        {
            "location": "/lego/kernel/debug/#debug-facility-in-lego", 
            "text": "Lego provides several handy debug helpers to ease our coding pain. We category them by layers, namely  1)   Core Kernel , the lowest level of Lego, which is shared by all managers.  2)   Processor Manager , which controls processor components.  3)   Memory Manager , which controls memory components.", 
            "title": "Debug Facility in Lego"
        }, 
        {
            "location": "/lego/kernel/debug/#core-kernel", 
            "text": "1\n2 void   dump_pte ( pte_t   * ptep ,   const   char   * reason );  void   dump_page ( struct   page   * page ,   const   char   * reason );   \nThese two helpers will dump a given pte entry or a page. Use this function if you are developing core related to physical memory allocation or pcache.   1 void   ptdump_walk_pgd_level ( pgd_t   * pgd );   \nThis debug helper will dump the whole pgtable ranges. Contiguous page table entries that share the same property will be merged together and will be printed once. Use this function if you are developing code related to user page tables.   1\n2\n3 void   show_state_filter ( unsigned   long   state_filter ,   bool   print_rq );  void   sched_show_task ( struct   task_struct   * p );  void   sysrq_sched_debug_show ( void );   \nThis set of functions are debug helpers for local scheduler. They will print all the tasks running in the system, and detailed information about percpu  runqueue . Use this set of functions if you are developing code related to scheduler.", 
            "title": "Core Kernel"
        }, 
        {
            "location": "/lego/kernel/debug/#processor-manager", 
            "text": "1\n2\n3\n4 void   dump_pcache_meta ( struct   pcache_meta   * pcm ,   const   char   * reason );  void   dump_pcache_victim ( struct   pcache_victim_meta   * victim ,   const   char   * reason );  void   dump_pcache_rmap ( struct   pcache_rmap   * rmap ,   const   char   * reason );  void   dump_pcache_line ( struct   pcache_meta   * pcm ,   const   char   * reason );   \nThese functions dump a given pcache line, a victim line, or a given reserve mapping. The last one will print the pcache line content, which generates a lot messages, you are warned. Use these functions if you are developing pcache or victim cache code.", 
            "title": "Processor Manager"
        }, 
        {
            "location": "/lego/kernel/debug/#memory-manager", 
            "text": "1\n2 void   dump_lego_mm ( const   struct   lego_mm_struct   * mm );  void   dump_vma ( const   struct   vm_area_struct   * vma );   \nThese two functions are used to dump the virtual address space of a process. Use these functions if you developing process VM related things.", 
            "title": "Memory Manager"
        }, 
        {
            "location": "/lego/kernel/grub/", 
            "text": "Use GRUB2 to boot Lego\n\n\nLast Updated: 02/02/2018\n\n\nThis document explains: \n1)\n how Lego itself is written to pretend as a Linux kernel, \n2)\n how to boot Lego kernel with GRUB2, \n3)\n GRUB2 configurations specific to Lego.\n\n\nHow Lego pretend as a Linux kernel\n\n\nasdsad\n\n\nHow to config GRUB2 for Lego\n\n\nasdsa", 
            "title": "GRUB"
        }, 
        {
            "location": "/lego/kernel/grub/#use-grub2-to-boot-lego", 
            "text": "Last Updated: 02/02/2018  This document explains:  1)  how Lego itself is written to pretend as a Linux kernel,  2)  how to boot Lego kernel with GRUB2,  3)  GRUB2 configurations specific to Lego.", 
            "title": "Use GRUB2 to boot Lego"
        }, 
        {
            "location": "/lego/kernel/grub/#how-lego-pretend-as-a-linux-kernel", 
            "text": "asdsad", 
            "title": "How Lego pretend as a Linux kernel"
        }, 
        {
            "location": "/lego/kernel/grub/#how-to-config-grub2-for-lego", 
            "text": "asdsa", 
            "title": "How to config GRUB2 for Lego"
        }, 
        {
            "location": "/lego/kernel/profile/", 
            "text": "Lego Profilers\n\n\nLego has three runtime profilers in kernel:\n\n\n\n\nstrace\n\n\nheatmap\n\n\nprofile points\n\n\n\n\nCombined together, they can provide the following information. Sweet, huh?\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n[\n \n1017.047366\n]\n \nKernel\n \nstrace\n\n\n[\n \n1017.050276\n]\n \nTask\n:\n \n20\n:\n20\n \nnr_accumulated_threads\n:\n \n46\n\n\n[\n \n1017.055837\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n1017.063213\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.071648\n]\n  \n98.16\n   \n33.839597842\n     \n1879978\n        \n18\n         \n0\n \nsys_futex\n\n\n[\n \n1017.079406\n]\n   \n0.26\n    \n0.260143997\n      \n260144\n         \n1\n         \n0\n \nsys_execve\n\n\n[\n \n1017.087260\n]\n   \n0.18\n    \n0.185456860\n        \n7133\n        \n26\n         \n0\n \nsys_write\n\n\n[\n \n1017.095017\n]\n   \n0.50\n    \n0.050189546\n         \n913\n        \n55\n         \n0\n \nsys_munmap\n\n\n[\n \n1017.102870\n]\n   \n0.25\n    \n0.025223661\n         \n255\n        \n99\n         \n0\n \nsys_mmap\n\n\n[\n \n1017.110531\n]\n   \n0.50\n    \n0.000505134\n          \n12\n        \n45\n         \n0\n \nsys_clone\n\n\n[\n \n1017.118288\n]\n   \n0.20\n    \n0.000202327\n          \n26\n         \n8\n         \n0\n \nsys_read\n\n\n[\n \n1017.125947\n]\n   \n0.14\n    \n0.000144065\n          \n17\n         \n9\n         \n0\n \nsys_open\n\n\n[\n \n1017.133608\n]\n   \n0.67\n    \n0.000067251\n           \n7\n        \n11\n         \n0\n \nsys_brk\n\n\n[\n \n1017.141171\n]\n   \n0.30\n    \n0.000030361\n           \n7\n         \n5\n         \n0\n \nsys_newfstat\n\n\n[\n \n1017.149219\n]\n   \n0.64\n    \n0.000006410\n           \n1\n         \n9\n         \n0\n \nsys_close\n\n\n[\n \n1017.156976\n]\n   \n0.48\n    \n0.000004842\n           \n1\n        \n45\n         \n0\n \nsys_madvise\n\n\n[\n \n1017.164927\n]\n   \n0.34\n    \n0.000003443\n           \n1\n        \n47\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n1017.173653\n]\n   \n0.21\n    \n0.000002137\n           \n1\n        \n52\n         \n0\n \nsys_mprotect\n\n\n[\n \n1017.181702\n]\n   \n0.71\n    \n0.000000717\n           \n1\n         \n4\n         \n0\n \nsys_gettimeofday\n\n\n[\n \n1017.190137\n]\n   \n0.60\n    \n0.000000608\n           \n1\n         \n3\n         \n0\n \nsys_time\n\n\n[\n \n1017.197797\n]\n   \n0.51\n    \n0.000000513\n           \n1\n         \n2\n         \n0\n \nsys_getrlimit\n\n\n[\n \n1017.205942\n]\n   \n0.49\n    \n0.000000498\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n1017.214572\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n4\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n1017.223008\n]\n   \n0.45\n    \n0.000000453\n           \n1\n         \n2\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n1017.231249\n]\n   \n0.27\n    \n0.000000272\n           \n1\n         \n2\n         \n0\n \nsys_newuname\n\n\n[\n \n1017.239298\n]\n   \n0.13\n    \n0.000000135\n           \n1\n         \n2\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n1017.248025\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.256460\n]\n \n100.00\n   \n34.361581541\n                   \n451\n         \n0\n \ntotal\n\n\n[\n \n1017.263830\n]\n\n\n[\n \n1017.308295\n]\n\n\n[\n \n1017.309754\n]\n \nKernel\n \nHeatmap\n \n(\ntop\n \n#\n10\n)\n\n\n[\n \n1017.313731\n]\n          \nAddress\n              \nFunction\n          \nNR\n          \n%\n\n\n[\n \n1017.321294\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.328858\n]\n \nffffffff8101a600\n              \ncpu_idle\n      \n112082\n      \n73.11\n\n\n[\n \n1017.336421\n]\n \nffffffff810666f0\n            \n__schedule\n       \n19192\n      \n12.95\n\n\n[\n \n1017.343983\n]\n \nffffffff8104f500\n       \nmlx4_ib_poll_cq\n        \n5551\n       \n3.99\n\n\n[\n \n1017.351546\n]\n \nffffffff8103bf50\n             \ndelay_tsc\n        \n5393\n       \n3.83\n\n\n[\n \n1017.359110\n]\n \nffffffff81034a10\n    \nvictim_flush_async\n        \n3766\n       \n2.72\n\n\n[\n \n1017.366673\n]\n \nffffffff8102b220\n   \nslob_alloc\n.\nconstpro\n        \n1992\n       \n1.47\n\n\n[\n \n1017.374235\n]\n \nffffffff810668d0\n              \nschedule\n        \n1519\n       \n0.15\n\n\n[\n \n1017.381800\n]\n \nffffffff810648f0\n   \nfit_send_reply_with\n         \n956\n       \n0.95\n\n\n[\n \n1017.389362\n]\n \nffffffff81062370\n   \nibapi_send_reply_ti\n         \n307\n       \n0.30\n\n\n[\n \n1017.396925\n]\n \nffffffff8105a0d0\n   \nib_mad_completion_h\n         \n232\n       \n0.23\n\n\n[\n \n1017.404487\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.412052\n]\n                                             \n151994\n     \n100.00\n\n\n[\n \n1017.419613\n]\n\n\n[\n \n1017.421267\n]\n\n\n[\n \n1017.422911\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n \n1017.426594\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n \n1017.436292\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n \n1017.445988\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000153470\n                \n55\n              \n2791\n\n\n[\n \n1017.455685\n]\n     \noff\n     \npcache_cache_miss\n      \n16.147020152\n            \n274698\n             \n58781\n\n\n[\n \n1017.465381\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------", 
            "title": "Profile"
        }, 
        {
            "location": "/lego/kernel/profile/#lego-profilers", 
            "text": "Lego has three runtime profilers in kernel:   strace  heatmap  profile points   Combined together, they can provide the following information. Sweet, huh?  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 [   1017.047366 ]   Kernel   strace  [   1017.050276 ]   Task :   20 : 20   nr_accumulated_threads :   46  [   1017.055837 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   1017.063213 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.071648 ]    98.16     33.839597842       1879978          18           0   sys_futex  [   1017.079406 ]     0.26      0.260143997        260144           1           0   sys_execve  [   1017.087260 ]     0.18      0.185456860          7133          26           0   sys_write  [   1017.095017 ]     0.50      0.050189546           913          55           0   sys_munmap  [   1017.102870 ]     0.25      0.025223661           255          99           0   sys_mmap  [   1017.110531 ]     0.50      0.000505134            12          45           0   sys_clone  [   1017.118288 ]     0.20      0.000202327            26           8           0   sys_read  [   1017.125947 ]     0.14      0.000144065            17           9           0   sys_open  [   1017.133608 ]     0.67      0.000067251             7          11           0   sys_brk  [   1017.141171 ]     0.30      0.000030361             7           5           0   sys_newfstat  [   1017.149219 ]     0.64      0.000006410             1           9           0   sys_close  [   1017.156976 ]     0.48      0.000004842             1          45           0   sys_madvise  [   1017.164927 ]     0.34      0.000003443             1          47           0   sys_set_robust_list  [   1017.173653 ]     0.21      0.000002137             1          52           0   sys_mprotect  [   1017.181702 ]     0.71      0.000000717             1           4           0   sys_gettimeofday  [   1017.190137 ]     0.60      0.000000608             1           3           0   sys_time  [   1017.197797 ]     0.51      0.000000513             1           2           0   sys_getrlimit  [   1017.205942 ]     0.49      0.000000498             1           2           0   sys_rt_sigprocmask  [   1017.214572 ]     0.46      0.000000469             1           4           0   sys_rt_sigaction  [   1017.223008 ]     0.45      0.000000453             1           2           0   sys_arch_prctl  [   1017.231249 ]     0.27      0.000000272             1           2           0   sys_newuname  [   1017.239298 ]     0.13      0.000000135             1           2           0   sys_set_tid_address  [   1017.248025 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.256460 ]   100.00     34.361581541                     451           0   total  [   1017.263830 ]  [   1017.308295 ]  [   1017.309754 ]   Kernel   Heatmap   ( top   # 10 )  [   1017.313731 ]            Address                Function            NR            %  [   1017.321294 ]   ----------------    --------------------    ----------    ---------  [   1017.328858 ]   ffffffff8101a600                cpu_idle        112082        73.11  [   1017.336421 ]   ffffffff810666f0              __schedule         19192        12.95  [   1017.343983 ]   ffffffff8104f500         mlx4_ib_poll_cq          5551         3.99  [   1017.351546 ]   ffffffff8103bf50               delay_tsc          5393         3.83  [   1017.359110 ]   ffffffff81034a10      victim_flush_async          3766         2.72  [   1017.366673 ]   ffffffff8102b220     slob_alloc . constpro          1992         1.47  [   1017.374235 ]   ffffffff810668d0                schedule          1519         0.15  [   1017.381800 ]   ffffffff810648f0     fit_send_reply_with           956         0.95  [   1017.389362 ]   ffffffff81062370     ibapi_send_reply_ti           307         0.30  [   1017.396925 ]   ffffffff8105a0d0     ib_mad_completion_h           232         0.23  [   1017.404487 ]   ----------------    --------------------    ----------    ---------  [   1017.412052 ]                                               151994       100.00  [   1017.419613 ]  [   1017.421267 ]  [   1017.422911 ]   Kernel   Profile   Points  [   1017.426594 ]    status                    name               total                  nr              avg . ns  [   1017.436292 ]   -------    --------------------    ----------------    ----------------    ----------------  [   1017.445988 ]       off        flush_tlb_others         0.000153470                  55                2791  [   1017.455685 ]       off       pcache_cache_miss        16.147020152              274698               58781  [   1017.465381 ]   -------    --------------------    ----------------    ----------------    ----------------", 
            "title": "Lego Profilers"
        }, 
        {
            "location": "/lego/kernel/profile_strace/", 
            "text": "Lego Profile strace\n\n\nLego has a built-in kernel-version syscall tracer, similar to \nstrace\n utility in the user space. Below we will just call our Lego\ns syscall tracer as strace for simplicity.\n\n\nDesign\n\n\nThere are essentially three important metrics to track for each syscall\n\n\n\n\nnumber of times invoked\n\n\nnumber of times error happened\n\n\ntotal execution, or per-call latency\n\n\n\n\nBesides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:\n\n\n\n\nPerformance: set of counters are \natomic_t\n, updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.\n\n\nSimplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.\n\n\n\n\nBelow is our simple design, where each thread has a \nstruct strace_info\n, which include a set of counters for each syscall. All \nstrace_info\n within a process are chained together by a doubly-linked list.\n\n\n\n\nWhen we want to look at the strace statistic numbers, we need to \naccumulate\n counters from all threads within a process, including those dead threads. We do the \naccumulate\n when the last thread of this process is going to exit.\n\n\nThe benefit of doubly-linked \nstrace_info\n is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if \ntask_struct\n is reaped, \nstrace_info\n is still there and linked.\n\n\nFor example, assume thread_3 has a SIGSEGV, and did a \nzap_other_threads\n. And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.\n\n\n\nDetails\n\n\nThere are essentially three hooks in core kernel:\n\n\n\n\nsyscall\n: before and after \nsys_call_table\n\n\nfork/clone\n: create \nstrace_info\n for each thread\n\n\ndo_exit()\n: when group_dead(signal-\nlive==1), accumulate\n\n\n\n\nExample Output\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n[\n \n1017.047366\n]\n \nKernel\n \nstrace\n\n\n[\n \n1017.050276\n]\n \nTask\n:\n \n20\n:\n20\n \nnr_accumulated_threads\n:\n \n46\n\n\n[\n \n1017.055837\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n1017.063213\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.071648\n]\n  \n98.16\n   \n33.839597842\n     \n1879978\n        \n18\n         \n0\n \nsys_futex\n\n\n[\n \n1017.079406\n]\n   \n0.26\n    \n0.260143997\n      \n260144\n         \n1\n         \n0\n \nsys_execve\n\n\n[\n \n1017.087260\n]\n   \n0.18\n    \n0.185456860\n        \n7133\n        \n26\n         \n0\n \nsys_write\n\n\n[\n \n1017.095017\n]\n   \n0.50\n    \n0.050189546\n         \n913\n        \n55\n         \n0\n \nsys_munmap\n\n\n[\n \n1017.102870\n]\n   \n0.25\n    \n0.025223661\n         \n255\n        \n99\n         \n0\n \nsys_mmap\n\n\n[\n \n1017.110531\n]\n   \n0.50\n    \n0.000505134\n          \n12\n        \n45\n         \n0\n \nsys_clone\n\n\n[\n \n1017.118288\n]\n   \n0.20\n    \n0.000202327\n          \n26\n         \n8\n         \n0\n \nsys_read\n\n\n[\n \n1017.125947\n]\n   \n0.14\n    \n0.000144065\n          \n17\n         \n9\n         \n0\n \nsys_open\n\n\n[\n \n1017.133608\n]\n   \n0.67\n    \n0.000067251\n           \n7\n        \n11\n         \n0\n \nsys_brk\n\n\n[\n \n1017.141171\n]\n   \n0.30\n    \n0.000030361\n           \n7\n         \n5\n         \n0\n \nsys_newfstat\n\n\n[\n \n1017.149219\n]\n   \n0.64\n    \n0.000006410\n           \n1\n         \n9\n         \n0\n \nsys_close\n\n\n[\n \n1017.156976\n]\n   \n0.48\n    \n0.000004842\n           \n1\n        \n45\n         \n0\n \nsys_madvise\n\n\n[\n \n1017.164927\n]\n   \n0.34\n    \n0.000003443\n           \n1\n        \n47\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n1017.173653\n]\n   \n0.21\n    \n0.000002137\n           \n1\n        \n52\n         \n0\n \nsys_mprotect\n\n\n[\n \n1017.181702\n]\n   \n0.71\n    \n0.000000717\n           \n1\n         \n4\n         \n0\n \nsys_gettimeofday\n\n\n[\n \n1017.190137\n]\n   \n0.60\n    \n0.000000608\n           \n1\n         \n3\n         \n0\n \nsys_time\n\n\n[\n \n1017.197797\n]\n   \n0.51\n    \n0.000000513\n           \n1\n         \n2\n         \n0\n \nsys_getrlimit\n\n\n[\n \n1017.205942\n]\n   \n0.49\n    \n0.000000498\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n1017.214572\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n4\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n1017.223008\n]\n   \n0.45\n    \n0.000000453\n           \n1\n         \n2\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n1017.231249\n]\n   \n0.27\n    \n0.000000272\n           \n1\n         \n2\n         \n0\n \nsys_newuname\n\n\n[\n \n1017.239298\n]\n   \n0.13\n    \n0.000000135\n           \n1\n         \n2\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n1017.248025\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.256460\n]\n \n100.00\n   \n34.361581541\n                   \n451\n         \n0\n \ntotal\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: April 05, 2018\n\nLast Updated: April 05, 2018", 
            "title": "Profile strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#lego-profile-strace", 
            "text": "Lego has a built-in kernel-version syscall tracer, similar to  strace  utility in the user space. Below we will just call our Lego s syscall tracer as strace for simplicity.", 
            "title": "Lego Profile strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#design", 
            "text": "There are essentially three important metrics to track for each syscall   number of times invoked  number of times error happened  total execution, or per-call latency   Besides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:   Performance: set of counters are  atomic_t , updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.  Simplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.   Below is our simple design, where each thread has a  struct strace_info , which include a set of counters for each syscall. All  strace_info  within a process are chained together by a doubly-linked list.   When we want to look at the strace statistic numbers, we need to  accumulate  counters from all threads within a process, including those dead threads. We do the  accumulate  when the last thread of this process is going to exit.  The benefit of doubly-linked  strace_info  is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if  task_struct  is reaped,  strace_info  is still there and linked.  For example, assume thread_3 has a SIGSEGV, and did a  zap_other_threads . And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.", 
            "title": "Design"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#details", 
            "text": "There are essentially three hooks in core kernel:   syscall : before and after  sys_call_table  fork/clone : create  strace_info  for each thread  do_exit() : when group_dead(signal- live==1), accumulate", 
            "title": "Details"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#example-output", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 [   1017.047366 ]   Kernel   strace  [   1017.050276 ]   Task :   20 : 20   nr_accumulated_threads :   46  [   1017.055837 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   1017.063213 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.071648 ]    98.16     33.839597842       1879978          18           0   sys_futex  [   1017.079406 ]     0.26      0.260143997        260144           1           0   sys_execve  [   1017.087260 ]     0.18      0.185456860          7133          26           0   sys_write  [   1017.095017 ]     0.50      0.050189546           913          55           0   sys_munmap  [   1017.102870 ]     0.25      0.025223661           255          99           0   sys_mmap  [   1017.110531 ]     0.50      0.000505134            12          45           0   sys_clone  [   1017.118288 ]     0.20      0.000202327            26           8           0   sys_read  [   1017.125947 ]     0.14      0.000144065            17           9           0   sys_open  [   1017.133608 ]     0.67      0.000067251             7          11           0   sys_brk  [   1017.141171 ]     0.30      0.000030361             7           5           0   sys_newfstat  [   1017.149219 ]     0.64      0.000006410             1           9           0   sys_close  [   1017.156976 ]     0.48      0.000004842             1          45           0   sys_madvise  [   1017.164927 ]     0.34      0.000003443             1          47           0   sys_set_robust_list  [   1017.173653 ]     0.21      0.000002137             1          52           0   sys_mprotect  [   1017.181702 ]     0.71      0.000000717             1           4           0   sys_gettimeofday  [   1017.190137 ]     0.60      0.000000608             1           3           0   sys_time  [   1017.197797 ]     0.51      0.000000513             1           2           0   sys_getrlimit  [   1017.205942 ]     0.49      0.000000498             1           2           0   sys_rt_sigprocmask  [   1017.214572 ]     0.46      0.000000469             1           4           0   sys_rt_sigaction  [   1017.223008 ]     0.45      0.000000453             1           2           0   sys_arch_prctl  [   1017.231249 ]     0.27      0.000000272             1           2           0   sys_newuname  [   1017.239298 ]     0.13      0.000000135             1           2           0   sys_set_tid_address  [   1017.248025 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.256460 ]   100.00     34.361581541                     451           0   total    \nYizhou Shan \nCreated: April 05, 2018 \nLast Updated: April 05, 2018", 
            "title": "Example Output"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/", 
            "text": "Lego Profile Kernel Heatmap\n\n\nTo get a sense of what is the hottest function within kernel, Lego adds a  counter based heatmap. It is the same with Linux\ns \n/proc/profile\n.\n\n\nMechanism\n\n\nGeneral idea: for each possible function/instruction byte in the kernel, we attach to a counter to it. Once we detect this function/instruction was executed, we increment its associated counter.\n\n\nHowever, fine granularity counting will need a lot extra memory, and it is not necessary to track each single instruction byte. Besides, it is hard to track down every time the function was executed. Furthermore, we only need an approximate heatmap.\n\n\nThus, kernel\ns solutions are:\n\n\n\n\nCoarse granularity\n: maintain a counter for each \n1\nprof_shift\n bytes.\n\n\nUpdate counter on timer interrupt tick\n, which is a constant stable entry.\n\n\n\n\nSupported Features\n\n\nCurrently, we only support \nCPU_PROFILING\n, which profile on each timer interrupt tick. We could also add \nSCHED_PROFILING\n, or \nSLEEP_PROFILING\n. But we are fine with current setting.\n\n\nOf course, we also have a simple dump function \nvoid print_profile_heatmap_nr(int nr)\n, which is similar to userspace tool \nreadprofile\n.\n\n\nExample Output\n\n\nWorkload is: MT-Phoenix word count, with 1GB data. (We probably want to rule out \ncpu_idle()\n)\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n[\n \n1017.309754\n]\n \nKernel\n \nHeatmap\n \n(\ntop\n \n#\n10\n)\n\n\n[\n \n1017.313731\n]\n          \nAddress\n              \nFunction\n          \nNR\n          \n%\n\n\n[\n \n1017.321294\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.328858\n]\n \nffffffff8101a600\n              \ncpu_idle\n      \n112082\n      \n73.11\n\n\n[\n \n1017.336421\n]\n \nffffffff810666f0\n            \n__schedule\n       \n19192\n      \n12.95\n\n\n[\n \n1017.343983\n]\n \nffffffff8104f500\n       \nmlx4_ib_poll_cq\n        \n5551\n       \n3.99\n\n\n[\n \n1017.351546\n]\n \nffffffff8103bf50\n             \ndelay_tsc\n        \n5393\n       \n3.83\n\n\n[\n \n1017.359110\n]\n \nffffffff81034a10\n    \nvictim_flush_async\n        \n3766\n       \n2.72\n\n\n[\n \n1017.366673\n]\n \nffffffff8102b220\n   \nslob_alloc\n.\nconstpro\n        \n1992\n       \n1.47\n\n\n[\n \n1017.374235\n]\n \nffffffff810668d0\n              \nschedule\n        \n1519\n       \n0.15\n\n\n[\n \n1017.381800\n]\n \nffffffff810648f0\n   \nfit_send_reply_with\n         \n956\n       \n0.95\n\n\n[\n \n1017.389362\n]\n \nffffffff81062370\n   \nibapi_send_reply_ti\n         \n307\n       \n0.30\n\n\n[\n \n1017.396925\n]\n \nffffffff8105a0d0\n   \nib_mad_completion_h\n         \n232\n       \n0.23\n\n\n[\n \n1017.404487\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.412052\n]\n                                             \n151994\n     \n100.00\n\n\n[\n \n1017.419613\n]\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: April 06, 2018\n\nLast Updated: April 06, 2018", 
            "title": "Profile heatmap"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#lego-profile-kernel-heatmap", 
            "text": "To get a sense of what is the hottest function within kernel, Lego adds a  counter based heatmap. It is the same with Linux s  /proc/profile .", 
            "title": "Lego Profile Kernel Heatmap"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#mechanism", 
            "text": "General idea: for each possible function/instruction byte in the kernel, we attach to a counter to it. Once we detect this function/instruction was executed, we increment its associated counter.  However, fine granularity counting will need a lot extra memory, and it is not necessary to track each single instruction byte. Besides, it is hard to track down every time the function was executed. Furthermore, we only need an approximate heatmap.  Thus, kernel s solutions are:   Coarse granularity : maintain a counter for each  1 prof_shift  bytes.  Update counter on timer interrupt tick , which is a constant stable entry.", 
            "title": "Mechanism"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#supported-features", 
            "text": "Currently, we only support  CPU_PROFILING , which profile on each timer interrupt tick. We could also add  SCHED_PROFILING , or  SLEEP_PROFILING . But we are fine with current setting.  Of course, we also have a simple dump function  void print_profile_heatmap_nr(int nr) , which is similar to userspace tool  readprofile .", 
            "title": "Supported Features"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#example-output", 
            "text": "Workload is: MT-Phoenix word count, with 1GB data. (We probably want to rule out  cpu_idle() )  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 [   1017.309754 ]   Kernel   Heatmap   ( top   # 10 )  [   1017.313731 ]            Address                Function            NR            %  [   1017.321294 ]   ----------------    --------------------    ----------    ---------  [   1017.328858 ]   ffffffff8101a600                cpu_idle        112082        73.11  [   1017.336421 ]   ffffffff810666f0              __schedule         19192        12.95  [   1017.343983 ]   ffffffff8104f500         mlx4_ib_poll_cq          5551         3.99  [   1017.351546 ]   ffffffff8103bf50               delay_tsc          5393         3.83  [   1017.359110 ]   ffffffff81034a10      victim_flush_async          3766         2.72  [   1017.366673 ]   ffffffff8102b220     slob_alloc . constpro          1992         1.47  [   1017.374235 ]   ffffffff810668d0                schedule          1519         0.15  [   1017.381800 ]   ffffffff810648f0     fit_send_reply_with           956         0.95  [   1017.389362 ]   ffffffff81062370     ibapi_send_reply_ti           307         0.30  [   1017.396925 ]   ffffffff8105a0d0     ib_mad_completion_h           232         0.23  [   1017.404487 ]   ----------------    --------------------    ----------    ---------  [   1017.412052 ]                                               151994       100.00  [   1017.419613 ]    \nYizhou Shan \nCreated: April 06, 2018 \nLast Updated: April 06, 2018", 
            "title": "Example Output"
        }, 
        {
            "location": "/lego/kernel/profile_points/", 
            "text": "Lego Profile Points\n\n\nLego profile points facility is added to trace specific functions, or even a small piece of code. It is added in the hope that it can help to find performance bottleneck. It is added in the hope that it can reduce the redundant coding chore.\n\n\nExample\n\n\nTo trace TLB shootdown cost.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nDEFINE_PROFILE_POINT\n(\nflush_tlb_others\n)\n\n\n\n\nvoid\n \nflush_tlb_others\n(\nconst\n \nstruct\n \ncpumask\n \n*\ncpumask\n,\n \nstruct\n \nmm_struct\n \n*\nmm\n,\n\n                      \nunsigned\n \nlong\n \nstart\n,\n \nunsigned\n \nlong\n \nend\n)\n\n\n{\n       \n        \nstruct\n \nflush_tlb_info\n \ninfo\n;\n\n\n        \nPROFILE_POINT_TIME\n(\nflush_tlb_others\n)\n\n\n\n        \nif\n \n(\nend\n \n==\n \n0\n)\n\n                \nend\n \n=\n \nstart\n \n+\n \nPAGE_SIZE\n;\n\n        \ninfo\n.\nflush_mm\n \n=\n \nmm\n;\n\n        \ninfo\n.\nflush_start\n \n=\n \nstart\n;\n\n        \ninfo\n.\nflush_end\n \n=\n \nend\n;\n\n\n\n        \nprofile_point_start\n(\nflush_tlb_others\n);\n\n\n        \nsmp_call_function_many\n(\ncpumask\n,\n \nflush_tlb_func\n,\n \ninfo\n,\n \n1\n);\n\n\n        \nprofile_point_leave\n(\nflush_tlb_others\n);\n\n\n}\n\n\n\n\n\n\nExplanation: \nDEFINE_PROFILE_POINT()\n will define a local structure, that contains the profile point name, number of invoked times, and total execution time. \nPROFILE_POINT_TIME()\n will define a stack local variable, to save the starting time. \nprofile_point_start()\n will save the current time in nanosecond, while \nprofile_point_leave()\n will calculate the execution of this run, and update the global counters defined by \nDEFINE_PROFILE_POINT()\n.\n\n\nSystem-wide profile points will be printed together if you invoke \nprint_profile_points()\n:\n\n1\n2\n3\n4\n5\n6\n[\n \n1017.422911\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n \n1017.426594\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n \n1017.436292\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n \n1017.445988\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000153470\n                \n55\n              \n2791\n\n\n[\n \n1017.455685\n]\n     \noff\n     \npcache_cache_miss\n      \n16.147020152\n            \n274698\n             \n58781\n\n\n[\n \n1017.465381\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\nMechanism\n\n\nOnce again, the profile points are aggregated by linker script. Each profile point will be in a special section \n.profile.point\n. The linker will merge them into one section, and export the starting and ending address of this section.\n\n\nPart I. Annotate.\n\n1\n2\n3\n4\n5\n6\n7\n#define __profile_point         __section(.profile.point)\n\n\n\n#define DEFINE_PROFILE_POINT(name)                                                      \\\n\n\n        struct profile_point _PP_NAME(name) __profile_point = {\n\n        \n...\n\n        \n...\n\n        \n};\n\n\n\n\n\n\nPart II. Link script merge.\n\n1\n2\n3\n4\n5\n6\n.\n \n=\n \nALIGN\n(\nL1_CACHE_BYTES\n);\n\n\n.\nprofile\n.\npoint\n \n:\n \nAT\n(\nADDR\n(.\nprofile\n.\npoint\n)\n \n-\n \nLOAD_OFFSET\n)\n \n{\n\n    \n__sprofilepoint\n \n=\n \n.;\n\n    \n*\n(.\nprofile\n.\npoint\n)\n\n    \n__eprofilepoint\n \n=\n \n.;\n\n\n}\n\n\n\n\n\n\nPart III. Walk through.\n\n1\n2\n3\n4\n5\n6\n7\n8\nvoid\n \nprint_profile_points\n(\nvoid\n)\n\n\n{\n\n        \nstruct\n \nprofile_point\n \n*\npp\n;\n\n\n        \nfor\n \n(\npp\n \n=\n \n__sprofilepoint\n;\n \npp\n \n \n__eprofilepoint\n;\n \npp\n++\n)\n \n{\n\n                \nprint_profile_point\n(\npp\n);\n\n        \n...\n\n    \n}\n  \n\n\n\n\n\nI really love the linker script. ;-)\n\n\n\nYizhou Shan\n\nCreated: April 06, 2018\n\nLast Updated: April 06, 2018", 
            "title": "Profile points"
        }, 
        {
            "location": "/lego/kernel/profile_points/#lego-profile-points", 
            "text": "Lego profile points facility is added to trace specific functions, or even a small piece of code. It is added in the hope that it can help to find performance bottleneck. It is added in the hope that it can reduce the redundant coding chore.", 
            "title": "Lego Profile Points"
        }, 
        {
            "location": "/lego/kernel/profile_points/#example", 
            "text": "To trace TLB shootdown cost.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 DEFINE_PROFILE_POINT ( flush_tlb_others )   void   flush_tlb_others ( const   struct   cpumask   * cpumask ,   struct   mm_struct   * mm , \n                       unsigned   long   start ,   unsigned   long   end )  {        \n         struct   flush_tlb_info   info ;           PROFILE_POINT_TIME ( flush_tlb_others )  \n         if   ( end   ==   0 ) \n                 end   =   start   +   PAGE_SIZE ; \n         info . flush_mm   =   mm ; \n         info . flush_start   =   start ; \n         info . flush_end   =   end ;           profile_point_start ( flush_tlb_others );           smp_call_function_many ( cpumask ,   flush_tlb_func ,   info ,   1 );           profile_point_leave ( flush_tlb_others );  }    Explanation:  DEFINE_PROFILE_POINT()  will define a local structure, that contains the profile point name, number of invoked times, and total execution time.  PROFILE_POINT_TIME()  will define a stack local variable, to save the starting time.  profile_point_start()  will save the current time in nanosecond, while  profile_point_leave()  will calculate the execution of this run, and update the global counters defined by  DEFINE_PROFILE_POINT() .  System-wide profile points will be printed together if you invoke  print_profile_points() : 1\n2\n3\n4\n5\n6 [   1017.422911 ]   Kernel   Profile   Points  [   1017.426594 ]    status                    name               total                  nr              avg . ns  [   1017.436292 ]   -------    --------------------    ----------------    ----------------    ----------------  [   1017.445988 ]       off        flush_tlb_others         0.000153470                  55                2791  [   1017.455685 ]       off       pcache_cache_miss        16.147020152              274698               58781  [   1017.465381 ]   -------    --------------------    ----------------    ----------------    ----------------", 
            "title": "Example"
        }, 
        {
            "location": "/lego/kernel/profile_points/#mechanism", 
            "text": "Once again, the profile points are aggregated by linker script. Each profile point will be in a special section  .profile.point . The linker will merge them into one section, and export the starting and ending address of this section.  Part I. Annotate. 1\n2\n3\n4\n5\n6\n7 #define __profile_point         __section(.profile.point)  #define DEFINE_PROFILE_POINT(name)                                                      \\          struct profile_point _PP_NAME(name) __profile_point = { \n         ... \n         ... \n         };    Part II. Link script merge. 1\n2\n3\n4\n5\n6 .   =   ALIGN ( L1_CACHE_BYTES );  . profile . point   :   AT ( ADDR (. profile . point )   -   LOAD_OFFSET )   { \n     __sprofilepoint   =   .; \n     * (. profile . point ) \n     __eprofilepoint   =   .;  }    Part III. Walk through. 1\n2\n3\n4\n5\n6\n7\n8 void   print_profile_points ( void )  { \n         struct   profile_point   * pp ; \n\n         for   ( pp   =   __sprofilepoint ;   pp     __eprofilepoint ;   pp ++ )   { \n                 print_profile_point ( pp ); \n         ... \n     }      I really love the linker script. ;-)  \nYizhou Shan \nCreated: April 06, 2018 \nLast Updated: April 06, 2018", 
            "title": "Mechanism"
        }, 
        {
            "location": "/lego/kernel/trampoline/", 
            "text": "How trampoline works in Lego\n\n\nWhat is trampoline code?\n\n\nTrampoline code is used by \nBSP\n to boot other secondary CPUs.\nAt startup, \nBSP\n wakeup secondary CPUs by sending a \nAPIC INIT\n\ncommand, which carry the \n[start_ip]\n where the secondary CPUs should\nstart to run.\n\n\nThe trampoline code is the code starting from \n[start_ip]\n. Used\nby the secondary CPU to jump from \n16-bit realmode\n to \n64-bit\n code\n(the first instruction of 64-bit code will be in \narch/x86/kernel/head_64.S\n).\n\n\nWhere is the trampoline source code?\n\n\nThe source files are all in \narch/x86/realmode/\n. There are two parts: \n1)\n \narch/x86/realmode/rm/trampoline.S\n: which is the code that will run. And it is a mix of 16-bit, 32-bit, 64-bit code (ugh..). \n2)\n \narch/x86/realmode/piggy.S\n: Since the trampoline code can not to linked\ninto kernel image directly. So we have to piggyback the trampoline.bin binary\ncode into a section, which is described by \ntrampoline_start\n and \ntrampoline_end\n. So the kernel can address the trampoline code via these two symbols.\n\n\nThe compile flow is:\n\n1\n2\n3\n4\n5\n6\n    arch/x86/realmode/rm/trmapoline.S\n    -\n CC__ arch/x86/realmode/rm/trmapoline.o\n       -\n LD arch/x86/realmode/rm/trampoline\n          -\n OBJCOPY arch/x86/realmode/rm/trampoline.bin\n             -\n This bin goes into piggy.o\n            -\n piggy.o goes into vmImage\n\n\n\n\n\nWhat happened at runtime?\n\n\nThe setup code was loaded by GRUB below 1MB. Inside \narch/x86/boot/main.c\n, we\nwill save the \ncs()\n into the \nboot_params\n and pass it to kernel. In \nsetup_arch()\n, we will copy the trampoline.bin code to the \ncs()\n address reported by \nboot_param\n. This means we will override setup code, which is okay.\n\n\nAt last, we wake up the secondary CPUs inside \nsmp_init()\n.\n\n\nCompare with Linux\n\n\nI vaguely remember how Linux implement this. The only thing I remember is that Linux use some sort of structure, which is filled by BSP and then passed, or used by secondary CPUs. The mechanism has no difference, though. Linux just has more robust debugging facilities.\n\n\n\nYizhou Shan\n\nMar 3, 2017", 
            "title": "Trampoline"
        }, 
        {
            "location": "/lego/kernel/trampoline/#how-trampoline-works-in-lego", 
            "text": "", 
            "title": "How trampoline works in Lego"
        }, 
        {
            "location": "/lego/kernel/trampoline/#what-is-trampoline-code", 
            "text": "Trampoline code is used by  BSP  to boot other secondary CPUs.\nAt startup,  BSP  wakeup secondary CPUs by sending a  APIC INIT \ncommand, which carry the  [start_ip]  where the secondary CPUs should\nstart to run.  The trampoline code is the code starting from  [start_ip] . Used\nby the secondary CPU to jump from  16-bit realmode  to  64-bit  code\n(the first instruction of 64-bit code will be in  arch/x86/kernel/head_64.S ).", 
            "title": "What is trampoline code?"
        }, 
        {
            "location": "/lego/kernel/trampoline/#where-is-the-trampoline-source-code", 
            "text": "The source files are all in  arch/x86/realmode/ . There are two parts:  1)   arch/x86/realmode/rm/trampoline.S : which is the code that will run. And it is a mix of 16-bit, 32-bit, 64-bit code (ugh..).  2)   arch/x86/realmode/piggy.S : Since the trampoline code can not to linked\ninto kernel image directly. So we have to piggyback the trampoline.bin binary\ncode into a section, which is described by  trampoline_start  and  trampoline_end . So the kernel can address the trampoline code via these two symbols.  The compile flow is: 1\n2\n3\n4\n5\n6     arch/x86/realmode/rm/trmapoline.S\n    -  CC__ arch/x86/realmode/rm/trmapoline.o\n       -  LD arch/x86/realmode/rm/trampoline\n          -  OBJCOPY arch/x86/realmode/rm/trampoline.bin\n             -  This bin goes into piggy.o\n            -  piggy.o goes into vmImage", 
            "title": "Where is the trampoline source code?"
        }, 
        {
            "location": "/lego/kernel/trampoline/#what-happened-at-runtime", 
            "text": "The setup code was loaded by GRUB below 1MB. Inside  arch/x86/boot/main.c , we\nwill save the  cs()  into the  boot_params  and pass it to kernel. In  setup_arch() , we will copy the trampoline.bin code to the  cs()  address reported by  boot_param . This means we will override setup code, which is okay.  At last, we wake up the secondary CPUs inside  smp_init() .", 
            "title": "What happened at runtime?"
        }, 
        {
            "location": "/lego/kernel/trampoline/#compare-with-linux", 
            "text": "I vaguely remember how Linux implement this. The only thing I remember is that Linux use some sort of structure, which is filled by BSP and then passed, or used by secondary CPUs. The mechanism has no difference, though. Linux just has more robust debugging facilities.  \nYizhou Shan \nMar 3, 2017", 
            "title": "Compare with Linux"
        }, 
        {
            "location": "/lego/kernel/pagefault_disable/", 
            "text": "The story of pagefault_disable/enable\n\n\npagefault_disable()\n is not really disabling the whole pgfault handling code. It is used to disable only the handling of pgfault that landed from \nuser virtual address\n. Please note the difference between \nuser virtual address\n and \nuser mode fault\n. The first means the faulting address belongs to user virtual address space, while it can come from either user mode (CPL3) or kernel mode (CPL0). The second is a fault come from user mode (CPL3).\n\n\nIf pgfault is disabled, then \ndo_page_fault()\n function will \nNOT\n try to solve the pgfault by calling into \npcache\n, instead, it will go straight to \nfixup\n code (in no_context()).\n\n\nThis function is not intended to be used standalone. Normally, we do \n1)\n \npagefault_disable()\n, \n2)\n then use some functions that have \nfixup\n code, \n3)\n then \npagefault_enable()\n. (The \nfixup\n code is another magic inside kernel. We will cover it in another document.)\n\n\nCurrently in Lego, this is only used by \nfutex\n, which needs something like \natomic_cmpxchg()\n with an user virtual address. If pgfault happens in the middle, then this will not be atomic since kernel need to do pcache operations, which further needs to through network.\n\n\nHowever, do note the difference with \nuaccess\n family functions. Most \nuaccess\n functions will not disable pgfault handling, which means pcache will be invoked. If pcache returns a \nSEGFAULT\n, pgfault code will go into \nfixup\n code. And that, my friend, is where \nuaccess\n returns \n-EFAULT\n to caller.\n\n\n\nYizhou Shan\n\nFeb 01, 2018", 
            "title": "Disable pgfault"
        }, 
        {
            "location": "/lego/kernel/pagefault_disable/#the-story-of-pagefault_disableenable", 
            "text": "pagefault_disable()  is not really disabling the whole pgfault handling code. It is used to disable only the handling of pgfault that landed from  user virtual address . Please note the difference between  user virtual address  and  user mode fault . The first means the faulting address belongs to user virtual address space, while it can come from either user mode (CPL3) or kernel mode (CPL0). The second is a fault come from user mode (CPL3).  If pgfault is disabled, then  do_page_fault()  function will  NOT  try to solve the pgfault by calling into  pcache , instead, it will go straight to  fixup  code (in no_context()).  This function is not intended to be used standalone. Normally, we do  1)   pagefault_disable() ,  2)  then use some functions that have  fixup  code,  3)  then  pagefault_enable() . (The  fixup  code is another magic inside kernel. We will cover it in another document.)  Currently in Lego, this is only used by  futex , which needs something like  atomic_cmpxchg()  with an user virtual address. If pgfault happens in the middle, then this will not be atomic since kernel need to do pcache operations, which further needs to through network.  However, do note the difference with  uaccess  family functions. Most  uaccess  functions will not disable pgfault handling, which means pcache will be invoked. If pcache returns a  SEGFAULT , pgfault code will go into  fixup  code. And that, my friend, is where  uaccess  returns  -EFAULT  to caller.  \nYizhou Shan \nFeb 01, 2018", 
            "title": "The story of pagefault_disable/enable"
        }, 
        {
            "location": "/lego/kernel/stop_machine/", 
            "text": "The highest priority thread in kernel\n\n\nThis document is about \nmigration/N\n kernel threads, \nstop_sched\n schdueling class, and the interesting source file \nkernel/stop_machine.c\n. Background on kernel scheduler design is recommended.\n\n\nScheduler uses the following code to pick the next runnable task:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nstatic\n \ninline\n \nstruct\n \ntask_struct\n \n*\n\n\npick_next_task\n(\nstruct\n \nrq\n \n*\nrq\n,\n \nstruct\n \ntask_struct\n \n*\nprev\n)\n\n\n{\n\n        \nstruct\n \ntask_struct\n \n*\np\n;\n\n        \nconst\n \nstruct\n \nsched_class\n \n*\nclass\n;\n\n\n\nagain\n:\n\n        \nfor_each_class\n(\nclass\n)\n \n{\n\n                \np\n \n=\n \nclass\n-\npick_next_task\n(\nrq\n,\n \nprev\n);\n\n                \nif\n \n(\np\n)\n \n{\n\n                        \nif\n \n(\nunlikely\n(\np\n \n==\n \nRETRY_TASK\n))\n\n                                \ngoto\n \nagain\n;\n\n                        \nreturn\n \np\n;\n\n                \n}\n    \n        \n}\n\n        \nBUG\n();\n\n\n}\n\n\n\n\n\n\nwhile the class is linked together as:\n\n1\n2\n3\n#define sched_class_highest     (\nstop_sched_class)                                                       \n\n\n#define for_each_class(class) \\                                                                           \n\n   \nfor\n \n(\nclass\n \n=\n \nsched_class_highest\n;\n \nclass\n;\n \nclass\n \n=\n \nclass\n-\nnext\n)\n\n\n\n\n\n\nClearly, the highest priority class is \nstop_sched_class\n. Whenever this scheduling has class runnable threads, scheduler will always run them first. So what kernel threads are using this scheduling class? Well, you must have seen something like \nmigration/0\n when you do \nps aux\n in Linux. And yes, these kernel threads are the only users.\n\n\nThese threads are sleeping most of their lifetime, they will be invoked to do some very urgent stuff. For example, when a user thread that is currently running on CPU0 calls \nsched_setaffinity()\n to bind to CPU1, kernel is not able to do this because this user thread is currently running (runqueue can not move a \nrunning\n task out, it can only move queued task out). Then, scheduler has to ask \nmigration/0\n for help. Once there is a job enqueued, \nmigration/0\n will be invoked. Since it has the highest-priority, it will start execution immediately. Thus the migration from CPU0 to CPU1 is performed safely and fast.\n\n\nmigration\n code is defined in \nkernel/stop_machine.c\n. They are created during early boot. They use the \nsmpboot_register_percpu_thread\n to create threads. They are written in this way because Linux supports cpu hotplug. To simplify we can also create them manually through \nkthread_create\n. Since Lego does not support cpu hotplug, and this \ncpu_stop_init\n is called after SMP is initialized, so Lego has slight different initialiaztion:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nvoid\n \n__init\n \ncpu_stop_init\n(\nvoid\n)\n\n\n{\n\n        \nunsigned\n \nint\n \ncpu\n;\n\n\n        \nfor_each_possible_cpu\n(\ncpu\n)\n \n{\n\n                \nstruct\n \ncpu_stopper\n \n*\nstopper\n \n=\n \nper_cpu\n(\ncpu_stopper\n,\n \ncpu\n);\n\n\n                \nspin_lock_init\n(\nstopper\n-\nlock\n);\n\n                \nINIT_LIST_HEAD\n(\nstopper\n-\nworks\n);\n\n        \n}\n\n\n        \nBUG_ON\n(\nsmpboot_register_percpu_thread\n(\ncpu_stop_threads\n));\n\n\n        \n/*\n\n\n         * smpboot_create_threads use kthread_create_on_cpu() to\n\n\n         * create new threads. And they are parked, too.\n\n\n         * Since we call this function after smp_init(), all CPUs\n\n\n         * are already online, thus we need to unpark them manually.\n\n\n         */\n\n        \nfor_each_online_cpu\n(\ncpu\n)\n\n                \nstop_machine_unpark\n(\ncpu\n);\n\n\n\n\n\n\nInternally, it also use a list to keep enqueued jobs. Once the thread is waken up, it tries to lookup this list and dequeue jobs (similar to kthread creation, kworker etc.):\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nstatic\n \nvoid\n \ncpu_stopper_thread\n(\nunsigned\n \nint\n \ncpu\n)\n\n\n{\n\n        \nstruct\n \ncpu_stopper\n \n*\nstopper\n \n=\n \nper_cpu\n(\ncpu_stopper\n,\n \ncpu\n);\n\n        \nstruct\n \ncpu_stop_work\n \n*\nwork\n;\n\n\n\nrepeat\n:\n\n        \nwork\n \n=\n \nNULL\n;\n\n        \nspin_lock_irq\n(\nstopper\n-\nlock\n);\n\n        \nif\n \n(\n!\nlist_empty\n(\nstopper\n-\nworks\n))\n \n{\n\n                \nwork\n \n=\n \nlist_first_entry\n(\nstopper\n-\nworks\n,\n\n                                        \nstruct\n \ncpu_stop_work\n,\n \nlist\n);\n\n                \nlist_del_init\n(\nwork\n-\nlist\n);\n\n        \n}\n   \n        \nspin_unlock_irq\n(\nstopper\n-\nlock\n);\n\n\n        \nif\n \n(\nwork\n)\n \n{\n\n                \n...\n\n                \nret\n \n=\n \nfn\n(\narg\n);\n\n                \n...\n\n                \ngoto\n \nrepeat\n;\n\n        \n}\n   \n\n}\n\n\n\n\n\n\nIt has several interesting public APIs that are quite similar to \nsmp_call_functions\n, but the difference is: this set of APIs provide a guaranteed time-to-execute waiting time, because it will simply preempt anything running on CPU.\n\n\n1\n2\n3\nint\n \nstop_one_cpu\n(\nunsigned\n \nint\n \ncpu\n,\n \ncpu_stop_fn_t\n \nfn\n,\n \nvoid\n \n*\narg\n);\n\n\nint\n \nstop_cpus\n(\nconst\n \nstruct\n \ncpumask\n \n*\ncpumask\n,\n \ncpu_stop_fn_t\n \nfn\n,\n \nvoid\n \n*\narg\n);\n\n\nint\n \ntry_stop_cpus\n(\nconst\n \nstruct\n \ncpumask\n \n*\ncpumask\n,\n \ncpu_stop_fn_t\n \nfn\n,\n \nvoid\n \n*\narg\n);\n\n\n\n\n\n\n\nThey are used only when there are some very urgent things to do. So, please use with caution.\n\n\n\nYizhou Shan\n\nCreated: Feb 12, 2018\n\nLast Updated: Feb 12, 2018", 
            "title": "Stop machine"
        }, 
        {
            "location": "/lego/kernel/stop_machine/#the-highest-priority-thread-in-kernel", 
            "text": "This document is about  migration/N  kernel threads,  stop_sched  schdueling class, and the interesting source file  kernel/stop_machine.c . Background on kernel scheduler design is recommended.  Scheduler uses the following code to pick the next runnable task:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 static   inline   struct   task_struct   *  pick_next_task ( struct   rq   * rq ,   struct   task_struct   * prev )  { \n         struct   task_struct   * p ; \n         const   struct   sched_class   * class ;  again : \n         for_each_class ( class )   { \n                 p   =   class - pick_next_task ( rq ,   prev ); \n                 if   ( p )   { \n                         if   ( unlikely ( p   ==   RETRY_TASK )) \n                                 goto   again ; \n                         return   p ; \n                 }     \n         } \n         BUG ();  }    while the class is linked together as: 1\n2\n3 #define sched_class_highest     ( stop_sched_class)                                                         #define for_each_class(class) \\                                                                            \n    for   ( class   =   sched_class_highest ;   class ;   class   =   class - next )    Clearly, the highest priority class is  stop_sched_class . Whenever this scheduling has class runnable threads, scheduler will always run them first. So what kernel threads are using this scheduling class? Well, you must have seen something like  migration/0  when you do  ps aux  in Linux. And yes, these kernel threads are the only users.  These threads are sleeping most of their lifetime, they will be invoked to do some very urgent stuff. For example, when a user thread that is currently running on CPU0 calls  sched_setaffinity()  to bind to CPU1, kernel is not able to do this because this user thread is currently running (runqueue can not move a  running  task out, it can only move queued task out). Then, scheduler has to ask  migration/0  for help. Once there is a job enqueued,  migration/0  will be invoked. Since it has the highest-priority, it will start execution immediately. Thus the migration from CPU0 to CPU1 is performed safely and fast.  migration  code is defined in  kernel/stop_machine.c . They are created during early boot. They use the  smpboot_register_percpu_thread  to create threads. They are written in this way because Linux supports cpu hotplug. To simplify we can also create them manually through  kthread_create . Since Lego does not support cpu hotplug, and this  cpu_stop_init  is called after SMP is initialized, so Lego has slight different initialiaztion:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 void   __init   cpu_stop_init ( void )  { \n         unsigned   int   cpu ; \n\n         for_each_possible_cpu ( cpu )   { \n                 struct   cpu_stopper   * stopper   =   per_cpu ( cpu_stopper ,   cpu ); \n\n                 spin_lock_init ( stopper - lock ); \n                 INIT_LIST_HEAD ( stopper - works ); \n         } \n\n         BUG_ON ( smpboot_register_percpu_thread ( cpu_stop_threads )); \n\n         /*           * smpboot_create_threads use kthread_create_on_cpu() to           * create new threads. And they are parked, too.           * Since we call this function after smp_init(), all CPUs           * are already online, thus we need to unpark them manually.           */ \n         for_each_online_cpu ( cpu ) \n                 stop_machine_unpark ( cpu );    Internally, it also use a list to keep enqueued jobs. Once the thread is waken up, it tries to lookup this list and dequeue jobs (similar to kthread creation, kworker etc.):  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 static   void   cpu_stopper_thread ( unsigned   int   cpu )  { \n         struct   cpu_stopper   * stopper   =   per_cpu ( cpu_stopper ,   cpu ); \n         struct   cpu_stop_work   * work ;  repeat : \n         work   =   NULL ; \n         spin_lock_irq ( stopper - lock ); \n         if   ( ! list_empty ( stopper - works ))   { \n                 work   =   list_first_entry ( stopper - works , \n                                         struct   cpu_stop_work ,   list ); \n                 list_del_init ( work - list ); \n         }    \n         spin_unlock_irq ( stopper - lock ); \n\n         if   ( work )   { \n                 ... \n                 ret   =   fn ( arg ); \n                 ... \n                 goto   repeat ; \n         }     }    It has several interesting public APIs that are quite similar to  smp_call_functions , but the difference is: this set of APIs provide a guaranteed time-to-execute waiting time, because it will simply preempt anything running on CPU.  1\n2\n3 int   stop_one_cpu ( unsigned   int   cpu ,   cpu_stop_fn_t   fn ,   void   * arg );  int   stop_cpus ( const   struct   cpumask   * cpumask ,   cpu_stop_fn_t   fn ,   void   * arg );  int   try_stop_cpus ( const   struct   cpumask   * cpumask ,   cpu_stop_fn_t   fn ,   void   * arg );    They are used only when there are some very urgent things to do. So, please use with caution.  \nYizhou Shan \nCreated: Feb 12, 2018 \nLast Updated: Feb 12, 2018", 
            "title": "The highest priority thread in kernel"
        }, 
        {
            "location": "/lego/kernel/loader/", 
            "text": "Lego Program Loader\n\n\nThis document explains the high-level workflow of Lego\ns program loader, and how we change the normal loader to fit the disaggregated operating system model. Background on linking and loading is recommended.\n\n\nStatus\n\n\n\n\n\n\n\n\nFormats\n\n\nSupported\n\n\n\n\n\n\n\n\n\n\nELF (static-linked)\n\n\n\n\n\n\n\n\nELF (dynamic-linked)\n\n\n\n\n\n\n\n\n\n\nOverall\n\n\nIn order to support different executable formats, Lego has a \nvirtual loader layer\n above all specific formats, which is quite similar to \nvirtual file system\n. In Lego, \nexecve()\n is divided into two parts: \n1)\n syscall hook at processor side, \n2)\n real loader at memory side. Combined together, they provide the same semantic of \nexecve()\n as described in Linux man page. Also for the code, we divide the Linux implementation into parts. But our emulation model introduces several interesting workarounds.\n\n\nLego\ns Loader\n\n\nLego basically divide the Linux loader into two parts, one in memory manager and other in processor manager. Most dirty work is done by memory manager. Processor manager only needs to make sure the new execution has a fresh environment to start.\n\n\nEntry Point\n\n\nSo the normal entry point is \ndo_execve()\n. Above that, it can be invoked by syscall from user space, or from kernel space by calling \ndo_execve()\n directly. There are not too many places that will call \ndo_execve\n within kernel. One notable case is how kernel starts the \npid 1\n user program. This happens after kernel finished all initialization. The code is:\n\n1\n2\n3\n4\n5\nstatic\n \nint\n \nrun_init_process\n(\nconst\n \nchar\n \n*\ninit_filename\n)\n                                                    \n\n{\n\n        \nargv_init\n[\n0\n]\n \n=\n \ninit_filename\n;\n\n        \nreturn\n \ndo_execve\n(\ninit_filename\n,\n \nargv_init\n,\n \nenvp_init\n);\n\n\n}\n\n\n\n\n\n\nMemory Manager\ns Job\n\n\nMemory manager side will do most of the dirty loading work. It will parse the ELF image, create new VMAs based on ELF information. After that, it only pass \nstart_ip\n and \nstart_stack\n back to processor manager. Once processor manager starts running this new execution, pages will be fetched from memory component on demand.\n\n\nLoad ld-linux\n\n\nFor dynamically-linked images, kernel ELF loader needs to load the \nld-linux.so\n as well. It will first try to map the \nld-linux.so\n into this process\ns virtual address space. Furthermore, the first user instruction that will run is no longer \n__libc_main_start\n, kernel will transfer the kernel to \nld-linux.so\n instead. Thus, for a normal user program, \nld-linux.so\n will load all the shared libraries before running glibc.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n\n        \n...\n\n        \n/* Dynamically-linked */\n\n        \nif\n \n(\nelf_interpreter\n)\n \n{\n\n                \nunsigned\n \nlong\n \ninterp_map_addr\n \n=\n \n0\n;\n\n\n\n                \nelf_entry\n \n=\n \nload_elf_interp\n(\ntsk\n,\n \nloc\n-\ninterp_elf_ex\n,\n\n\n                                            \ninterpreter\n,\n\n                                            \ninterp_map_addr\n,\n\n                                            \nload_bias\n,\n \ninterp_elf_phdata\n);\n\n                \nif\n \n(\n!\nIS_ERR\n((\nvoid\n \n*\n)\nelf_entry\n))\n \n{\n\n                        \n/*\n\n\n                         * load_elf_interp() returns relocation\n\n\n                         * adjustment\n\n\n                         */\n\n                        \ninterp_load_addr\n \n=\n \nelf_entry\n;\n\n                        \nelf_entry\n \n+=\n \nloc\n-\ninterp_elf_ex\n.\ne_entry\n;\n\n                \n}\n\n                \nif\n \n(\nBAD_ADDR\n(\nelf_entry\n))\n \n{\n\n                        \nretval\n \n=\n \nIS_ERR\n((\nvoid\n \n*\n)\nelf_entry\n)\n \n?\n\n                                        \n(\nint\n)\nelf_entry\n \n:\n \n-\nEINVAL\n;\n\n                        \ngoto\n \nout_free_dentry\n;\n\n                \n}\n\n                \nreloc_func_desc\n \n=\n \ninterp_load_addr\n;\n\n\n                \nput_lego_file\n(\ninterpreter\n);\n\n                \nkfree\n(\nelf_interpreter\n);\n\n        \n}\n \nelse\n \n{\n\n        \n/* Statically-linked */\n\n                \n/*\n\n\n                 * e_entry is the VA to which the system first transfers control\n\n\n                 * Not the start_code! Normally, it is the \n_start\n function.\n\n\n                 */\n\n\n                \nelf_entry\n \n=\n \nloc\n-\nelf_ex\n.\ne_entry\n;\n\n\n                \nif\n \n(\nBAD_ADDR\n(\nelf_entry\n))\n \n{\n\n                        \nretval\n \n=\n \n-\nEINVAL\n;\n\n                        \ngoto\n \nout_free_dentry\n;\n\n                \n}\n\n        \n}\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nProcessor Manager\ns Job\n\n\nIt needs to flush old execution environment, and setup the new execution environment, such as signal, FPU. Notably, processor manager need to run \nflush_old_exec()\n, and \nsetup_new_exec()\n.\n\n\nDestroy old context: flush_old_exec()\n\n\nZap other threads\n\n\nde_thread\n is used to kill other threads within the same thread group, thus make sure this process has its own signal table. Furthermore, A \nexec\n starts a new thread group with the same TGID of the previous thread group, so we probably also need to switch PID if calling thread is not a leader.\n\n\nSwitch to new address space\n\n\nWe also need to release the old mm, and allocate a new mm. The new mm only has the high address kernel mapping established. Do note that in Lego, pgtable is used to emulate the processor cache:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nstatic\n \nint\n \nexec_mmap\n(\nvoid\n)\n\n\n{\n\n        \nstruct\n \nmm_struct\n \n*\nnew_mm\n;\n\n        \nstruct\n \nmm_struct\n \n*\nold_mm\n;\n\n        \nstruct\n \ntask_struct\n \n*\ntsk\n;\n\n\n        \nnew_mm\n \n=\n \nmm_alloc\n();\n\n        \nif\n \n(\n!\nnew_mm\n)\n\n                \nreturn\n \n-\nENOMEM\n;\n\n\n        \ntsk\n \n=\n \ncurrent\n;\n\n        \nold_mm\n \n=\n \ncurrent\n-\nmm\n;\n\n        \nmm_release\n(\ntsk\n,\n \nold_mm\n);\n\n\n        \ntask_lock\n(\ntsk\n);\n\n        \ntsk\n-\nmm\n \n=\n \nnew_mm\n;\n\n        \ntsk\n-\nactive_mm\n \n=\n \nnew_mm\n;\n\n        \nactivate_mm\n(\nold_mm\n,\n \nnew_mm\n);\n\n        \ntask_unlock\n(\ntsk\n);\n\n\n        \nif\n \n(\nold_mm\n)\n\n                \nmmput\n(\nold_mm\n);\n\n        \nreturn\n \n0\n;\n\n\n}\n\n\n\n\n\n\nClear Architecture-Specific state\n\n\nThis is performed by \nflush_thread()\n, which is an architecture-specific callback. In x86, we need to clear FPU state, and reset TLS array:\n\n1\n2\n3\n4\n5\n6\n7\nvoid\n \nflush_thread\n(\nvoid\n)\n\n\n{\n\n        \nstruct\n \ntask_struct\n \n*\ntsk\n \n=\n \ncurrent\n;\n\n        \nmemset\n(\ntsk\n-\nthread\n.\ntls_array\n,\n \n0\n,\n \nsizeof\n(\ntsk\n-\nthread\n.\ntls_array\n));\n\n\n        \nfpu__clear\n(\ntsk\n-\nthread\n.\nfpu\n);\n\n\n}\n\n\n\n\n\n\nSetup new context: setup_new_exec()\n\n\nLego\ns \nsetup_new_exec()\n is quite different from Linux\ns default implementation. Lego moves several functions to memory component, like the \narch_pick_mmap_layout\n stuff. Thus, Lego only flush the signal handlers and reset the signal stack stuff:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nstatic\n \nvoid\n \nsetup_new_exec\n(\nconst\n \nchar\n \n*\nfilename\n)\n\n\n{\n\n        \n/* This is the point of no return */\n\n        \ncurrent\n-\nsas_ss_sp\n \n=\n \ncurrent\n-\nsas_ss_size\n \n=\n \n0\n;\n\n\n        \nset_task_comm\n(\ncurrent\n,\n \nkbasename\n(\nfilename\n));\n\n\n        \nflush_signal_handlers\n(\ncurrent\n,\n \n0\n);\n\n\n}\n\n\n\n\n\n\nChange return frame in stack\n\n\nWe do not return to user mode here, we simply replace the return IP of the regs frame. While the kernel thread returns, it will simply merge to syscall return path (check ret_from_fork() in entry.S for detail).\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n/**\n\n\n * start_thread - Starting a new user thread\n\n\n * @regs: pointer to pt_regs\n\n\n * @new_ip: the first instruction IP of user thread\n\n\n * @new_sp: the new stack pointer of user thread\n\n\n */\n\n\nvoid\n \nstart_thread\n(\nstruct\n \npt_regs\n \n*\nregs\n,\n \nunsigned\n \nlong\n \nnew_ip\n,\n\n                  \nunsigned\n \nlong\n \nnew_sp\n)\n\n\n{\n\n        \nloadsegment\n(\nfs\n,\n \n0\n);\n\n        \nloadsegment\n(\nes\n,\n \n0\n);\n\n        \nloadsegment\n(\nds\n,\n \n0\n);\n\n        \nload_gs_index\n(\n0\n);\n\n        \nregs\n-\nip\n                \n=\n \nnew_ip\n;\n\n        \nregs\n-\nsp\n                \n=\n \nnew_sp\n;\n\n        \nregs\n-\ncs\n                \n=\n \n__USER_CS\n;\n\n        \nregs\n-\nss\n                \n=\n \n__USER_DS\n;\n\n        \nregs\n-\nflags\n             \n=\n \nX86_EFLAGS_IF\n;\n\n\n}\n\n\n\n\n\n\nIf calling \nexecve()\n from userspace, the return frame is saved in the stack, we can simply do \nstart_thread\n above, and merge to syscall return path. However, if calling \nexecve()\n from a kernel thread, things changed. As you can see, all forked threads will run from \nret_from_fork\n when it wakes for the first time. If it is a kernel thread, it jumps to \nline 23\n, to execute the kernel function. Normally, the function should not return. If it does return, it normally has called an \nexecve()\n, and return frame has been changed by \nstart_thread()\n. So we jump to \nline 16\n to let it merge to syscall return path.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n/*\n\n \n*\n \nA\n \nnewly\n \nforked\n \nprocess\n \ndirectly\n \ncontext\n \nswitches\n \ninto\n \nthis\n \naddress.\n\n \n*\n\n \n*\n \nrax:\n \nprev\n \ntask\n \nwe\n \nswitched\n \nfrom\n\n \n*\n \nrbx:\n \nkernel\n \nthread\n \nfunc\n \n(\nNULL\n \nfor\n \nuser\n \nthread\n)\n\n \n*\n \nr12:\n \nkernel\n \nthread\n \narg\n\n \n*/\n\n\nENTRY\n(\nret_from_fork\n)\n\n        \nmovq\n    \n%rax\n,\n \n%rdi\n\n        \ncall\n    \nschedule_tail\n           \n/\n*\n \nrdi\n:\n \nprev\n \ntask\n \nparameter\n \n*\n/\n\n\n        \ntestq\n   \n%rbx\n,\n \n%rbx\n              \n/\n*\n \nfrom\n \nkernel_thread\n?\n \n*\n/\n\n        \njnz\n     \n1\nf\n                      \n/\n*\n \nkernel\n \nthreads\n \nare\n \nuncommon\n \n*\n/\n\n\n\n2:\n\n\n        \nmovq\n    \n%rsp\n,\n \n%rdi\n\n\n        \ncall\n    \nsyscall_return_slowpath\n \n/\n*\n \nreturn\n \nwith\n \nIRQs\n \ndisabled\n \n*\n/\n\n        \nSWAPGS\n                          \n/\n*\n \nswitch\n \nto\n \nuser\n \ngs.base\n \n*\n/\n\n        \njmp\n     \nrestore_regs_and_iret\n\n\n\n1:\n\n        \n/*\n \nkernel\n \nthread\n \n*\n/\n\n\n        \nmovq\n    \n%r12\n,\n \n%rdi\n\n\n        \ncall\n    \n*\n%rbx\n\n        \n/*\n  \n         \n*\n \nA\n \nkernel\n \nthread\n \nis\n \nallowed\n \nto\n \nreturn\n \nhere\n \nafter\n \nsuccessfully\n\n         \n*\n \ncalling\n \ndo_execve\n().\n  \nExit\n \nto\n \nuserspace\n \nto\n \ncomplete\n \nthe\n \nexecve\n()\n\n         \n*\n \nsyscall:\n\n         \n*/\n\n        \nmovq\n    \n$0\n,\n \nRAX\n(\n%rsp\n)\n\n        \njmp\n     \n2\nb\n  \n\nEND\n(\nret_from_fork\n)\n\n\n\n\n\n\n\nThis is such a typical control flow hijacking. :-)\n\n\nFeatures\n\n\nThis section lists various features, or behaviors and Lego\ns program loader.\n\n\n\n\nVirtual Address Space Range\n\n\nUser\ns virtual address falls into this range:\n\n1\n[sysctl_mmap_min_addr, TASK_SIZE)\n\n\n\n\n\nBy default,\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nunsigned\n \nlong\n \nsysctl_mmap_min_addr\n \n=\n \nPAGE_SIZE\n;\n\n\n\n/*\n\n\n * User space process size. 47bits minus one guard page.  The guard\n\n\n * page is necessary on Intel CPUs: if a SYSCALL instruction is at\n\n\n * the highest possible canonical userspace address, then that\n\n\n * syscall will enter the kernel with a non-canonical return\n\n\n * address, and SYSRET will explode dangerously.  We avoid this\n\n\n * particular problem by preventing anything from being mapped\n\n\n * at the maximum canonical address.\n\n\n */\n                                                                                                       \n\n#define TASK_SIZE       ((1UL \n 47) - PAGE_SIZE)\n\n\n\n\n\n\nEssentially:\n\n1\n[0x1000, 0x7ffffffff000)\n\n\n\n\n\n\n\nPre-Populated \n.bss\n and \n.brk\n\n\nThe heap vma created at loading time is a combination of \n.bss\n and \n.brk\n segments. Since brk usage is 0 (will it be non-zero?) at this moment, so the heap vma is essentially just \n.bss\n pages. Normally, Linux kernel does not populate pages for this vma during loading, but Lego does. It can save several page allocation cost for heap pcache miss. It is controlled by \nvm_brk()\n.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nint\n \nvm_brk\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n\n           \nunsigned\n \nlong\n \nstart\n,\n \nunsigned\n \nlong\n \nlen\n)\n\n\n{\n\n        \nint\n \nret\n;\n\n        \nstruct\n \nlego_mm_struct\n \n*\nmm\n \n=\n \ntsk\n-\nmm\n;\n\n\n        \nif\n \n(\ndown_write_killable\n(\nmm\n-\nmmap_sem\n))\n\n                \nreturn\n \n-\nEINTR\n;\n\n\n        \nret\n \n=\n \ndo_brk\n(\ntsk\n,\n \nstart\n,\n \nlen\n);\n\n        \nup_write\n(\nmm\n-\nmmap_sem\n);\n\n\n        \n/* Prepopulate brk pages */\n\n        \nif\n \n(\n!\nret\n)\n\n                \nlego_mm_populate\n(\nmm\n,\n \nstart\n,\n \nlen\n);\n\n\n        \nreturn\n \nret\n;\n\n\n}\n\n\n\n\n\n\n\n\nUn-Populated stack\n\n\nStack vma is manually expanded to \n32 pages + pages for argv info\n by loader to accommodate future usage. Only pages for argv are populated by default, the extra 32 pages are not. A typical program may need 1 page for saving argv info, plus the 32 extra, the layout will be:\n\n1\n7ffffffde000-7ffffffff000 rw-p 00000000 [stack]\n\n\n\n\n\nThe code to expand stack is done when ELF loader tries to finalize the stack vma, by calling \nsetup_arg_pages()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nint\n \nsetup_arg_pages\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                    \nunsigned\n \nlong\n \nstack_top\n,\n \nint\n \nexecutable_stack\n)\n\n\n{\n\n        \n...\n\n        \n/*\n\n\n         * 32*4k (or 2*64k) pages\n\n\n         */\n\n        \nstack_expand\n \n=\n \n131072UL\n;\n\n        \nstack_size\n \n=\n \nvma\n-\nvm_end\n \n-\n \nvma\n-\nvm_start\n;\n\n        \nstack_base\n \n=\n \nvma\n-\nvm_start\n \n-\n \nstack_expand\n;\n\n\n        \nmm\n-\nstart_stack\n \n=\n \nbprm\n-\np\n;\n\n        \nret\n \n=\n \nexpand_stack\n(\nvma\n,\n \nstack_base\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\n\nUn-Populated \n.text\n and \n.data\n\n\nIn essence, all PT_LOAD segments of ELF image are not pre-populated. They will be fetched from storage on demand. This is the traditional on-demand paging way. If we want to reduce the overhead of code and data\ns on-demand paging, we can prefault them in the future.\n\n\n\n\nDisabled Randomized Top of Stack\n\n\nLego currently does not randomize the stack top. The stack vma is allocated by \nbprm_mm_init()\n at early execve time. There is no randomization at the allocation time, and this applies to all exectuable formats. The end of vma is just \nTASK_SIZE\n:\n\n1\n2\n3\n4\n5\n6\n7\nstatic\n \nint\n \n__bprm_mm_init\n(\nstruct\n \nlego_binprm\n \n*\nbprm\n)\n\n\n{\n\n        \n...\n\n        \nvma\n-\nvm_end\n \n=\n \nTASK_SIZE\n;\n\n        \n...\n\n\n}\n\n\n(\nmanagers\n/\nmemory\n/\nloader\n/\nelf\n.\nc\n)\n\n\n\n\n\n\nTop of stack randomization happens within each specific format loader. They do this by calling back to virtual loader layer\ns \nsetup_arg_pages()\n function, which is used to finalize the top of stack:\n\n1\n2\nint\n \nsetup_arg_pages\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                    \nunsigned\n \nlong\n \nstack_top\n,\n \nint\n \nexecutable_stack\n);\n\n\n\n\n\n\nSo, to actually randomize the top of stack, you can simply do the following:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nstatic\n \nunsigned\n \nlong\n \nrandomize_stack_top\n(\nunsigned\n \nlong\n \nstack_top\n)\n\n\n{\n                                \n        \nunsigned\n \nlong\n \nrandom_variable\n \n=\n \n0\n;\n\n\n        \nif\n \n((\ncurrent\n-\nflags\n \n \nPF_RANDOMIZE\n)\n \n\n                \n!\n(\ncurrent\n-\npersonality\n \n \nADDR_NO_RANDOMIZE\n))\n \n{\n\n                \nrandom_variable\n \n=\n \nget_random_long\n();\n\n                \nrandom_variable\n \n=\n \nSTACK_RND_MASK\n;\n\n                \nrandom_variable\n \n=\n \nPAGE_SHIFT\n;\n\n        \n}\n\n\n#ifdef CONFIG_STACK_GROWSUP\n\n        \nreturn\n \nPAGE_ALIGN\n(\nstack_top\n)\n \n+\n \nrandom_variable\n;\n\n\n#else           \n\n        \nreturn\n \nPAGE_ALIGN\n(\nstack_top\n)\n \n-\n \nrandom_variable\n;\n\n\n#endif\n\n\n}\n\n\n\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n        \nretval\n \n=\n \nsetup_arg_pages\n(\nbprm\n,\n \nrandomize_stack_top\n(\nTASK_SIZE\n),\n\n                                 \nexecutable_stack\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\nHowever, current Lego disables randomization by passing \nTASK_SIZE\n:\n\n1\n2\n3\n4\n5\n6\n7\n8\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n        \nretval\n \n=\n \nsetup_arg_pages\n(\ntsk\n,\n \nbprm\n,\n \nTASK_SIZE\n,\n \nexecutable_stack\n);\n\n        \n...\n\n\n}\n\n\n(\nmanagers\n/\nmemory\n/\nloader\n/\nelf\n.\nc\n)\n\n\n\n\n\n\n\n\nNo vDSO\n\n\nCurrently, Lego does not have \nvDSO\n support. There are not too many syscalls mapped in the vDSO, for \nx86-64\n:\n\n\n\n\nclock_gettime\n\n\ngetcpu\n\n\ngettimeofday\n\n\ntime\n\n\n\n\nThe reason to add it back is simple: if those syscalls are used \na lot\n and hurt overall performance. Do note that when we add it back, it will be different from the common design: vDSO \nmust\n be mapped at processor side, mapped in our emulated pgtable.\n\n\nBelow is the original part where loader maps vDSO:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n\n#ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES\n\n        \n/*\n\n\n         * TODO: vdso\n\n\n         * x86 can map vdso vma here\n\n\n         */\n\n\n#endif\n\n        \n...\n\n\n}\n\n\nmanagers\n/\nmemory\n/\nloader\n/\nelf\n.\nc\n\n\n\n\n\n\nFor lego, we should move it to processor right before \nstart_thread()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nint\n \ndo_execve\n(\nconst\n \nchar\n \n*\nfilename\n,\n\n              \nconst\n \nchar\n \n*\n \nconst\n \n*\nargv\n,\n\n              \nconst\n \nchar\n \n*\n \nconst\n \n*\nenvp\n)\n\n\n{\n\n        \n...\n\n        \n/* Should be here */\n\n\n        \nstart_thread\n(\nregs\n,\n \nnew_ip\n,\n \nnew_sp\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\nBesides, don\nt forget to report the \nvDSO\n address in the aux vector:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nstatic\n \nint\n \ncreate_elf_tables\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                \nstruct\n \nelfhdr\n \n*\nexec\n,\n \nunsigned\n \nlong\n \nload_addr\n,\n \nunsigned\n \nlong\n \ninterp_load_addr\n,\n\n                \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n\n#ifdef ARCH_DLINFO\n\n        \n/*\n\n\n         * ARCH_DLINFO must come first so PPC can do its special alignment of\n\n\n         * AUXV.\n\n\n         * update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in\n\n\n         * ARCH_DLINFO changes\n\n\n         */\n\n        \nARCH_DLINFO\n;\n\n\n#endif\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 16, 2018\n\nLast Updated: Feb 27, 2018", 
            "title": "Program Loader"
        }, 
        {
            "location": "/lego/kernel/loader/#lego-program-loader", 
            "text": "This document explains the high-level workflow of Lego s program loader, and how we change the normal loader to fit the disaggregated operating system model. Background on linking and loading is recommended.", 
            "title": "Lego Program Loader"
        }, 
        {
            "location": "/lego/kernel/loader/#status", 
            "text": "Formats  Supported      ELF (static-linked)     ELF (dynamic-linked)", 
            "title": "Status"
        }, 
        {
            "location": "/lego/kernel/loader/#overall", 
            "text": "In order to support different executable formats, Lego has a  virtual loader layer  above all specific formats, which is quite similar to  virtual file system . In Lego,  execve()  is divided into two parts:  1)  syscall hook at processor side,  2)  real loader at memory side. Combined together, they provide the same semantic of  execve()  as described in Linux man page. Also for the code, we divide the Linux implementation into parts. But our emulation model introduces several interesting workarounds.", 
            "title": "Overall"
        }, 
        {
            "location": "/lego/kernel/loader/#legos-loader", 
            "text": "Lego basically divide the Linux loader into two parts, one in memory manager and other in processor manager. Most dirty work is done by memory manager. Processor manager only needs to make sure the new execution has a fresh environment to start.", 
            "title": "Lego's Loader"
        }, 
        {
            "location": "/lego/kernel/loader/#entry-point", 
            "text": "So the normal entry point is  do_execve() . Above that, it can be invoked by syscall from user space, or from kernel space by calling  do_execve()  directly. There are not too many places that will call  do_execve  within kernel. One notable case is how kernel starts the  pid 1  user program. This happens after kernel finished all initialization. The code is: 1\n2\n3\n4\n5 static   int   run_init_process ( const   char   * init_filename )                                                      { \n         argv_init [ 0 ]   =   init_filename ; \n         return   do_execve ( init_filename ,   argv_init ,   envp_init );  }", 
            "title": "Entry Point"
        }, 
        {
            "location": "/lego/kernel/loader/#memory-managers-job", 
            "text": "Memory manager side will do most of the dirty loading work. It will parse the ELF image, create new VMAs based on ELF information. After that, it only pass  start_ip  and  start_stack  back to processor manager. Once processor manager starts running this new execution, pages will be fetched from memory component on demand.", 
            "title": "Memory Manager's Job"
        }, 
        {
            "location": "/lego/kernel/loader/#load-ld-linux", 
            "text": "For dynamically-linked images, kernel ELF loader needs to load the  ld-linux.so  as well. It will first try to map the  ld-linux.so  into this process s virtual address space. Furthermore, the first user instruction that will run is no longer  __libc_main_start , kernel will transfer the kernel to  ld-linux.so  instead. Thus, for a normal user program,  ld-linux.so  will load all the shared libraries before running glibc.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n\n         ... \n         /* Dynamically-linked */ \n         if   ( elf_interpreter )   { \n                 unsigned   long   interp_map_addr   =   0 ;                   elf_entry   =   load_elf_interp ( tsk ,   loc - interp_elf_ex ,                                               interpreter , \n                                             interp_map_addr , \n                                             load_bias ,   interp_elf_phdata ); \n                 if   ( ! IS_ERR (( void   * ) elf_entry ))   { \n                         /*                           * load_elf_interp() returns relocation                           * adjustment                           */ \n                         interp_load_addr   =   elf_entry ; \n                         elf_entry   +=   loc - interp_elf_ex . e_entry ; \n                 } \n                 if   ( BAD_ADDR ( elf_entry ))   { \n                         retval   =   IS_ERR (( void   * ) elf_entry )   ? \n                                         ( int ) elf_entry   :   - EINVAL ; \n                         goto   out_free_dentry ; \n                 } \n                 reloc_func_desc   =   interp_load_addr ; \n\n                 put_lego_file ( interpreter ); \n                 kfree ( elf_interpreter ); \n         }   else   { \n         /* Statically-linked */ \n                 /*                   * e_entry is the VA to which the system first transfers control                   * Not the start_code! Normally, it is the  _start  function.                   */                   elf_entry   =   loc - elf_ex . e_entry ;                   if   ( BAD_ADDR ( elf_entry ))   { \n                         retval   =   - EINVAL ; \n                         goto   out_free_dentry ; \n                 } \n         } \n         ...  }", 
            "title": "Load ld-linux"
        }, 
        {
            "location": "/lego/kernel/loader/#processor-managers-job", 
            "text": "It needs to flush old execution environment, and setup the new execution environment, such as signal, FPU. Notably, processor manager need to run  flush_old_exec() , and  setup_new_exec() .", 
            "title": "Processor Manager's Job"
        }, 
        {
            "location": "/lego/kernel/loader/#destroy-old-context-flush_old_exec", 
            "text": "", 
            "title": "Destroy old context: flush_old_exec()"
        }, 
        {
            "location": "/lego/kernel/loader/#zap-other-threads", 
            "text": "de_thread  is used to kill other threads within the same thread group, thus make sure this process has its own signal table. Furthermore, A  exec  starts a new thread group with the same TGID of the previous thread group, so we probably also need to switch PID if calling thread is not a leader.", 
            "title": "Zap other threads"
        }, 
        {
            "location": "/lego/kernel/loader/#switch-to-new-address-space", 
            "text": "We also need to release the old mm, and allocate a new mm. The new mm only has the high address kernel mapping established. Do note that in Lego, pgtable is used to emulate the processor cache:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 static   int   exec_mmap ( void )  { \n         struct   mm_struct   * new_mm ; \n         struct   mm_struct   * old_mm ; \n         struct   task_struct   * tsk ; \n\n         new_mm   =   mm_alloc (); \n         if   ( ! new_mm ) \n                 return   - ENOMEM ; \n\n         tsk   =   current ; \n         old_mm   =   current - mm ; \n         mm_release ( tsk ,   old_mm ); \n\n         task_lock ( tsk ); \n         tsk - mm   =   new_mm ; \n         tsk - active_mm   =   new_mm ; \n         activate_mm ( old_mm ,   new_mm ); \n         task_unlock ( tsk ); \n\n         if   ( old_mm ) \n                 mmput ( old_mm ); \n         return   0 ;  }", 
            "title": "Switch to new address space"
        }, 
        {
            "location": "/lego/kernel/loader/#clear-architecture-specific-state", 
            "text": "This is performed by  flush_thread() , which is an architecture-specific callback. In x86, we need to clear FPU state, and reset TLS array: 1\n2\n3\n4\n5\n6\n7 void   flush_thread ( void )  { \n         struct   task_struct   * tsk   =   current ; \n         memset ( tsk - thread . tls_array ,   0 ,   sizeof ( tsk - thread . tls_array )); \n\n         fpu__clear ( tsk - thread . fpu );  }", 
            "title": "Clear Architecture-Specific state"
        }, 
        {
            "location": "/lego/kernel/loader/#setup-new-context-setup_new_exec", 
            "text": "Lego s  setup_new_exec()  is quite different from Linux s default implementation. Lego moves several functions to memory component, like the  arch_pick_mmap_layout  stuff. Thus, Lego only flush the signal handlers and reset the signal stack stuff: 1\n2\n3\n4\n5\n6\n7\n8\n9 static   void   setup_new_exec ( const   char   * filename )  { \n         /* This is the point of no return */ \n         current - sas_ss_sp   =   current - sas_ss_size   =   0 ; \n\n         set_task_comm ( current ,   kbasename ( filename )); \n\n         flush_signal_handlers ( current ,   0 );  }", 
            "title": "Setup new context: setup_new_exec()"
        }, 
        {
            "location": "/lego/kernel/loader/#change-return-frame-in-stack", 
            "text": "We do not return to user mode here, we simply replace the return IP of the regs frame. While the kernel thread returns, it will simply merge to syscall return path (check ret_from_fork() in entry.S for detail).  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 /**   * start_thread - Starting a new user thread   * @regs: pointer to pt_regs   * @new_ip: the first instruction IP of user thread   * @new_sp: the new stack pointer of user thread   */  void   start_thread ( struct   pt_regs   * regs ,   unsigned   long   new_ip , \n                   unsigned   long   new_sp )  { \n         loadsegment ( fs ,   0 ); \n         loadsegment ( es ,   0 ); \n         loadsegment ( ds ,   0 ); \n         load_gs_index ( 0 ); \n         regs - ip                  =   new_ip ; \n         regs - sp                  =   new_sp ; \n         regs - cs                  =   __USER_CS ; \n         regs - ss                  =   __USER_DS ; \n         regs - flags               =   X86_EFLAGS_IF ;  }    If calling  execve()  from userspace, the return frame is saved in the stack, we can simply do  start_thread  above, and merge to syscall return path. However, if calling  execve()  from a kernel thread, things changed. As you can see, all forked threads will run from  ret_from_fork  when it wakes for the first time. If it is a kernel thread, it jumps to  line 23 , to execute the kernel function. Normally, the function should not return. If it does return, it normally has called an  execve() , and return frame has been changed by  start_thread() . So we jump to  line 16  to let it merge to syscall return path.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 /* \n  *   A   newly   forked   process   directly   context   switches   into   this   address. \n  * \n  *   rax:   prev   task   we   switched   from \n  *   rbx:   kernel   thread   func   ( NULL   for   user   thread ) \n  *   r12:   kernel   thread   arg \n  */  ENTRY ( ret_from_fork ) \n         movq      %rax ,   %rdi \n         call      schedule_tail             / *   rdi :   prev   task   parameter   * / \n\n         testq     %rbx ,   %rbx                / *   from   kernel_thread ?   * / \n         jnz       1 f                        / *   kernel   threads   are   uncommon   * /  2:           movq      %rsp ,   %rdi           call      syscall_return_slowpath   / *   return   with   IRQs   disabled   * / \n         SWAPGS                            / *   switch   to   user   gs.base   * / \n         jmp       restore_regs_and_iret  1: \n         /*   kernel   thread   * /           movq      %r12 ,   %rdi           call      * %rbx \n         /*   \n          *   A   kernel   thread   is   allowed   to   return   here   after   successfully \n          *   calling   do_execve ().    Exit   to   userspace   to   complete   the   execve () \n          *   syscall: \n          */ \n         movq      $0 ,   RAX ( %rsp ) \n         jmp       2 b    END ( ret_from_fork )    This is such a typical control flow hijacking. :-)", 
            "title": "Change return frame in stack"
        }, 
        {
            "location": "/lego/kernel/loader/#features", 
            "text": "This section lists various features, or behaviors and Lego s program loader.", 
            "title": "Features"
        }, 
        {
            "location": "/lego/kernel/loader/#virtual-address-space-range", 
            "text": "User s virtual address falls into this range: 1 [sysctl_mmap_min_addr, TASK_SIZE)   By default,  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 unsigned   long   sysctl_mmap_min_addr   =   PAGE_SIZE ;  /*   * User space process size. 47bits minus one guard page.  The guard   * page is necessary on Intel CPUs: if a SYSCALL instruction is at   * the highest possible canonical userspace address, then that   * syscall will enter the kernel with a non-canonical return   * address, and SYSRET will explode dangerously.  We avoid this   * particular problem by preventing anything from being mapped   * at the maximum canonical address.   */                                                                                                         #define TASK_SIZE       ((1UL   47) - PAGE_SIZE)    Essentially: 1 [0x1000, 0x7ffffffff000)", 
            "title": "Virtual Address Space Range"
        }, 
        {
            "location": "/lego/kernel/loader/#pre-populated-bss-and-brk", 
            "text": "The heap vma created at loading time is a combination of  .bss  and  .brk  segments. Since brk usage is 0 (will it be non-zero?) at this moment, so the heap vma is essentially just  .bss  pages. Normally, Linux kernel does not populate pages for this vma during loading, but Lego does. It can save several page allocation cost for heap pcache miss. It is controlled by  vm_brk() .  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 int   vm_brk ( struct   lego_task_struct   * tsk , \n            unsigned   long   start ,   unsigned   long   len )  { \n         int   ret ; \n         struct   lego_mm_struct   * mm   =   tsk - mm ; \n\n         if   ( down_write_killable ( mm - mmap_sem )) \n                 return   - EINTR ; \n\n         ret   =   do_brk ( tsk ,   start ,   len ); \n         up_write ( mm - mmap_sem ); \n\n         /* Prepopulate brk pages */ \n         if   ( ! ret ) \n                 lego_mm_populate ( mm ,   start ,   len ); \n\n         return   ret ;  }", 
            "title": "Pre-Populated .bss and .brk"
        }, 
        {
            "location": "/lego/kernel/loader/#un-populated-stack", 
            "text": "Stack vma is manually expanded to  32 pages + pages for argv info  by loader to accommodate future usage. Only pages for argv are populated by default, the extra 32 pages are not. A typical program may need 1 page for saving argv info, plus the 32 extra, the layout will be: 1 7ffffffde000-7ffffffff000 rw-p 00000000 [stack]   The code to expand stack is done when ELF loader tries to finalize the stack vma, by calling  setup_arg_pages() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 int   setup_arg_pages ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                     unsigned   long   stack_top ,   int   executable_stack )  { \n         ... \n         /*           * 32*4k (or 2*64k) pages           */ \n         stack_expand   =   131072UL ; \n         stack_size   =   vma - vm_end   -   vma - vm_start ; \n         stack_base   =   vma - vm_start   -   stack_expand ; \n\n         mm - start_stack   =   bprm - p ; \n         ret   =   expand_stack ( vma ,   stack_base ); \n         ...  }", 
            "title": "Un-Populated stack"
        }, 
        {
            "location": "/lego/kernel/loader/#un-populated-text-and-data", 
            "text": "In essence, all PT_LOAD segments of ELF image are not pre-populated. They will be fetched from storage on demand. This is the traditional on-demand paging way. If we want to reduce the overhead of code and data s on-demand paging, we can prefault them in the future.", 
            "title": "Un-Populated .text and .data"
        }, 
        {
            "location": "/lego/kernel/loader/#disabled-randomized-top-of-stack", 
            "text": "Lego currently does not randomize the stack top. The stack vma is allocated by  bprm_mm_init()  at early execve time. There is no randomization at the allocation time, and this applies to all exectuable formats. The end of vma is just  TASK_SIZE : 1\n2\n3\n4\n5\n6\n7 static   int   __bprm_mm_init ( struct   lego_binprm   * bprm )  { \n         ... \n         vma - vm_end   =   TASK_SIZE ; \n         ...  }  ( managers / memory / loader / elf . c )    Top of stack randomization happens within each specific format loader. They do this by calling back to virtual loader layer s  setup_arg_pages()  function, which is used to finalize the top of stack: 1\n2 int   setup_arg_pages ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                     unsigned   long   stack_top ,   int   executable_stack );    So, to actually randomize the top of stack, you can simply do the following:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 static   unsigned   long   randomize_stack_top ( unsigned   long   stack_top )  {                                 \n         unsigned   long   random_variable   =   0 ; \n\n         if   (( current - flags     PF_RANDOMIZE )   \n                 ! ( current - personality     ADDR_NO_RANDOMIZE ))   { \n                 random_variable   =   get_random_long (); \n                 random_variable   =   STACK_RND_MASK ; \n                 random_variable   =   PAGE_SHIFT ; \n         }  #ifdef CONFIG_STACK_GROWSUP \n         return   PAGE_ALIGN ( stack_top )   +   random_variable ;  #else            \n         return   PAGE_ALIGN ( stack_top )   -   random_variable ;  #endif  }  static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ... \n         retval   =   setup_arg_pages ( bprm ,   randomize_stack_top ( TASK_SIZE ), \n                                  executable_stack ); \n         ...  }    However, current Lego disables randomization by passing  TASK_SIZE : 1\n2\n3\n4\n5\n6\n7\n8 static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ... \n         retval   =   setup_arg_pages ( tsk ,   bprm ,   TASK_SIZE ,   executable_stack ); \n         ...  }  ( managers / memory / loader / elf . c )", 
            "title": "Disabled Randomized Top of Stack"
        }, 
        {
            "location": "/lego/kernel/loader/#no-vdso", 
            "text": "Currently, Lego does not have  vDSO  support. There are not too many syscalls mapped in the vDSO, for  x86-64 :   clock_gettime  getcpu  gettimeofday  time   The reason to add it back is simple: if those syscalls are used  a lot  and hurt overall performance. Do note that when we add it back, it will be different from the common design: vDSO  must  be mapped at processor side, mapped in our emulated pgtable.  Below is the original part where loader maps vDSO:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ...  #ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES \n         /*           * TODO: vdso           * x86 can map vdso vma here           */  #endif \n         ...  }  managers / memory / loader / elf . c    For lego, we should move it to processor right before  start_thread() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 int   do_execve ( const   char   * filename , \n               const   char   *   const   * argv , \n               const   char   *   const   * envp )  { \n         ... \n         /* Should be here */ \n\n         start_thread ( regs ,   new_ip ,   new_sp ); \n         ...  }    Besides, don t forget to report the  vDSO  address in the aux vector:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 static   int   create_elf_tables ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                 struct   elfhdr   * exec ,   unsigned   long   load_addr ,   unsigned   long   interp_load_addr , \n                 unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ...  #ifdef ARCH_DLINFO \n         /*           * ARCH_DLINFO must come first so PPC can do its special alignment of           * AUXV.           * update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in           * ARCH_DLINFO changes           */ \n         ARCH_DLINFO ;  #endif \n         ...  }    \nYizhou Shan \nCreated: Feb 16, 2018 \nLast Updated: Feb 27, 2018", 
            "title": "No vDSO"
        }, 
        {
            "location": "/lego/kernel/vDSO/", 
            "text": "vDSO and vsyscall\n\n\nWe have choice but port vDSO and vsyscall for Lego, because some dynamic-linked ELF images will use these features.\n\n\nReferences:\n\n\n\n\nhttps://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html\n\n\n\n\n\nYizhou Shan\n\nCreated: March 01, 2018\n\nLast Updated: March 01, 2018", 
            "title": "vDSO"
        }, 
        {
            "location": "/lego/kernel/vDSO/#vdso-and-vsyscall", 
            "text": "We have choice but port vDSO and vsyscall for Lego, because some dynamic-linked ELF images will use these features.  References:   https://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html   \nYizhou Shan \nCreated: March 01, 2018 \nLast Updated: March 01, 2018", 
            "title": "vDSO and vsyscall"
        }, 
        {
            "location": "/lego/kernel/vfs/", 
            "text": "Processor Manager\ns Virtual File System\n\n\nLego processor manager has an virtual file system layer to accommodate the famous legacy \nEverything is a file\n philosophy. But we implement this in a very dirty way.\n\n\nCover later.\n\n\n\nYizhou Shan\n\nCreated: Feb 20, 2018\n\nLast Updated: Feb 20, 2018", 
            "title": "Virtual File System"
        }, 
        {
            "location": "/lego/kernel/vfs/#processor-managers-virtual-file-system", 
            "text": "Lego processor manager has an virtual file system layer to accommodate the famous legacy  Everything is a file  philosophy. But we implement this in a very dirty way.  Cover later.  \nYizhou Shan \nCreated: Feb 20, 2018 \nLast Updated: Feb 20, 2018", 
            "title": "Processor Manager's Virtual File System"
        }, 
        {
            "location": "/lego/kernel/vm/", 
            "text": "Process Virtual Memory\n\n\nLimits\n\n\nMax Number of VMAs\n\n\nBy default, the maximum number of VMAs is: \n65530\n. It is defined by the following variable:\n\n1\n2\n3\n4\n#define MAPCOUNT_ELF_CORE_MARGIN        (5)\n\n\n#define DEFAULT_MAX_MAP_COUNT   (USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\n\n\n\nint\n \nsysctl_max_map_count\n \n__read_mostly\n \n=\n \nDEFAULT_MAX_MAP_COUNT\n;\n\n\n\n\n\n\nFacts\n\n\nmunmap\n can split vma\n\n\nmunmap\n can create a hole with an existing vma, thus divide one existing vma to two new vmas. Do note that, \nmunmap\n can create hole for both anonymous vma \nand\n file-backed vma.\n\n\nmsync()\n is not atomic\n\n\nDuring \nmsync()\n, pages are being written back to disk one by one (or batched). Consider the case where few pages have been flushed back, while some other few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.\u000b\u000b\n\n\nmsync()\n need concurrency control\n\n\nWith a multi-threaded application, does msync() provide the synchronization semantic? The answer is NO. Other threads within the same process are able to write to pages currently under \nmsync()\n. This implies that application need to handle concurrency by themselves, e.g., rwlocks.\n\n\n\nYizhou Shan\n\nCreated: Feb 19, 2018\n\nLast Updated: Feb 19, 2018", 
            "title": "Process Virtual Memory"
        }, 
        {
            "location": "/lego/kernel/vm/#process-virtual-memory", 
            "text": "", 
            "title": "Process Virtual Memory"
        }, 
        {
            "location": "/lego/kernel/vm/#limits", 
            "text": "", 
            "title": "Limits"
        }, 
        {
            "location": "/lego/kernel/vm/#max-number-of-vmas", 
            "text": "By default, the maximum number of VMAs is:  65530 . It is defined by the following variable: 1\n2\n3\n4 #define MAPCOUNT_ELF_CORE_MARGIN        (5)  #define DEFAULT_MAX_MAP_COUNT   (USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)  int   sysctl_max_map_count   __read_mostly   =   DEFAULT_MAX_MAP_COUNT ;", 
            "title": "Max Number of VMAs"
        }, 
        {
            "location": "/lego/kernel/vm/#facts", 
            "text": "", 
            "title": "Facts"
        }, 
        {
            "location": "/lego/kernel/vm/#munmap-can-split-vma", 
            "text": "munmap  can create a hole with an existing vma, thus divide one existing vma to two new vmas. Do note that,  munmap  can create hole for both anonymous vma  and  file-backed vma.", 
            "title": "munmap can split vma"
        }, 
        {
            "location": "/lego/kernel/vm/#msync-is-not-atomic", 
            "text": "During  msync() , pages are being written back to disk one by one (or batched). Consider the case where few pages have been flushed back, while some other few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.", 
            "title": "msync() is not atomic"
        }, 
        {
            "location": "/lego/kernel/vm/#msync-need-concurrency-control", 
            "text": "With a multi-threaded application, does msync() provide the synchronization semantic? The answer is NO. Other threads within the same process are able to write to pages currently under  msync() . This implies that application need to handle concurrency by themselves, e.g., rwlocks.  \nYizhou Shan \nCreated: Feb 19, 2018 \nLast Updated: Feb 19, 2018", 
            "title": "msync() need concurrency control"
        }, 
        {
            "location": "/lego/kernel/fpu/", 
            "text": "x86 Floating Point Unit\n\n\nThis is not a document about the FPU technology, this is just a simple note on FPU code and my debugging lesson.\n\n\nFPU is heavily used by user level code. You may not use it directly, but glibc library is using it a lot, e.g. the \nstrcmp\n function. x86 FPU is really another complex thing designed by Intel. Of course its performance is good and widely used, but the legacy compatible feature? Hmm.\n\n\nI would say, without Ingo Molnar\ns \nx86 FPU code rewrite\n, there is no way for me to easily understand it. The current x86 FPU code is well-written. Even though I don\nt quite understand what and why the code is, but I enjoy reading it. The naming convention, the code organization, the file organization, the header files, it is a nice piece of art.\n\n\nAnyway, Lego ported this low-level FPU code from Linux without any change. The porting is painful because it requires a lot other related features. And it also deals with compatible syscalls a little bit. Below I will just briefly list other subsystems that are using FPU, and talk about my thoughts.\n\n\nBoot\n\n\nFPU detection and init happen during early boot. You should know the \nstruct fpu\n is a dynamically-sized structure. The size of it depends on what features the underlying CPU support. Since \nstruct fpu\n is part of \ntask_struct\n, that implies \ntask_struct\n is dynamically-sized too. Apparently, \ncpu_init()\n will also callback to init its local FPU.\n\n\nContext Switch\n\n\nFPU consists a lot registers, and each thread has its own FPU context. However, CPU will not save the FPU registers for us, it is software\ns duty to save and restore FPU context properly. FPU context is saved in \nstruct fpu\n.\n\n\nThus whenever we switch task, we also need to switch FPU context:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n__visible\n \nstruct\n \ntask_struct\n \n*\n\n\n__switch_to\n(\nstruct\n \ntask_struct\n \n*\nprev_p\n,\n \nstruct\n \ntask_struct\n \n*\nnext_p\n)\n\n\n{\n\n        \n..\n\n        \nfpu_switch\n \n=\n \nswitch_fpu_prepare\n(\nprev_fpu\n,\n \nnext_fpu\n,\n \ncpu\n);\n\n        \n..\n\n        \nswitch_fpu_finish\n(\nnext_fpu\n,\n \nfpu_switch\n);\n\n        \n..\n\n\n}\n\n\n\n\n\n\nSYSCALL\n\n\n\n\n\n\nfork() and clone(): When a new thread or process is created, the FPU context is copied from the calling thread.\n\n\n\n\n\n\nexecve(): When \nexecve()\n is called, the FPU context will be cleared.\n\n\n\n\n\n\nexit(): When a thread exit,, FPU will do cleanup based on if \neagerfpu\n or \nlazyfpu\n is used.\n\n\n\n\n\n\nExceptions\n\n\nLike the \ndevice not available\n exception, which may be triggered if lazyfpu is used. Also, \ndo_simd_exception\n and \ndo_coprocessor_error\n, which are some math related exceptions.\n\n\nSignal\n\n\nKernel needs to setup a \nsigframe\n for user level signal handlers. \nsigframe\n is a contiguous stack memory consists the general purpose registers and FPU registers. So signal handling part will also call back to FPU to setup and copy the FPU registers to \nsigframe\n in stack.\n\n\nThoughts\n\n\nI\nve been debugging this FPU introduced bugs for over a month. And during this month, I\nm always not sure if it is FPU\ns bug, or some other code that corrupts memory. So I\nm lazy to re-port FPU again. But after rule out every other possibilities, I turned back to FPU. At first I did not port all FPU code, cause I don\nt think I need all of it.\n\n\nOne stupid thing is I forgot to turn on DEBUG_FPU, which should help me in the first place. I kind of lost myself in various engineering work during this debugging. I really need some big context switch in the middle to fresh my mind. Anyway, glad it is all done today (Feb 23), and I\nm able to move to next stage.\n\n\nCompatibility is a heavy thing to carry. But it is also a nice thing for marketing. No one can deny the success of Intel on its backward compatibility. Bad for programmers.\n\n\n\nYizhou Shan\n\nCreated: Feb 22, 2018\n\nLast Updated: Feb 23, 2018", 
            "title": "x86 FPU"
        }, 
        {
            "location": "/lego/kernel/fpu/#x86-floating-point-unit", 
            "text": "This is not a document about the FPU technology, this is just a simple note on FPU code and my debugging lesson.  FPU is heavily used by user level code. You may not use it directly, but glibc library is using it a lot, e.g. the  strcmp  function. x86 FPU is really another complex thing designed by Intel. Of course its performance is good and widely used, but the legacy compatible feature? Hmm.  I would say, without Ingo Molnar s  x86 FPU code rewrite , there is no way for me to easily understand it. The current x86 FPU code is well-written. Even though I don t quite understand what and why the code is, but I enjoy reading it. The naming convention, the code organization, the file organization, the header files, it is a nice piece of art.  Anyway, Lego ported this low-level FPU code from Linux without any change. The porting is painful because it requires a lot other related features. And it also deals with compatible syscalls a little bit. Below I will just briefly list other subsystems that are using FPU, and talk about my thoughts.", 
            "title": "x86 Floating Point Unit"
        }, 
        {
            "location": "/lego/kernel/fpu/#boot", 
            "text": "FPU detection and init happen during early boot. You should know the  struct fpu  is a dynamically-sized structure. The size of it depends on what features the underlying CPU support. Since  struct fpu  is part of  task_struct , that implies  task_struct  is dynamically-sized too. Apparently,  cpu_init()  will also callback to init its local FPU.", 
            "title": "Boot"
        }, 
        {
            "location": "/lego/kernel/fpu/#context-switch", 
            "text": "FPU consists a lot registers, and each thread has its own FPU context. However, CPU will not save the FPU registers for us, it is software s duty to save and restore FPU context properly. FPU context is saved in  struct fpu .  Thus whenever we switch task, we also need to switch FPU context: 1\n2\n3\n4\n5\n6\n7\n8\n9 __visible   struct   task_struct   *  __switch_to ( struct   task_struct   * prev_p ,   struct   task_struct   * next_p )  { \n         .. \n         fpu_switch   =   switch_fpu_prepare ( prev_fpu ,   next_fpu ,   cpu ); \n         .. \n         switch_fpu_finish ( next_fpu ,   fpu_switch ); \n         ..  }", 
            "title": "Context Switch"
        }, 
        {
            "location": "/lego/kernel/fpu/#syscall", 
            "text": "fork() and clone(): When a new thread or process is created, the FPU context is copied from the calling thread.    execve(): When  execve()  is called, the FPU context will be cleared.    exit(): When a thread exit,, FPU will do cleanup based on if  eagerfpu  or  lazyfpu  is used.", 
            "title": "SYSCALL"
        }, 
        {
            "location": "/lego/kernel/fpu/#exceptions", 
            "text": "Like the  device not available  exception, which may be triggered if lazyfpu is used. Also,  do_simd_exception  and  do_coprocessor_error , which are some math related exceptions.", 
            "title": "Exceptions"
        }, 
        {
            "location": "/lego/kernel/fpu/#signal", 
            "text": "Kernel needs to setup a  sigframe  for user level signal handlers.  sigframe  is a contiguous stack memory consists the general purpose registers and FPU registers. So signal handling part will also call back to FPU to setup and copy the FPU registers to  sigframe  in stack.", 
            "title": "Signal"
        }, 
        {
            "location": "/lego/kernel/fpu/#thoughts", 
            "text": "I ve been debugging this FPU introduced bugs for over a month. And during this month, I m always not sure if it is FPU s bug, or some other code that corrupts memory. So I m lazy to re-port FPU again. But after rule out every other possibilities, I turned back to FPU. At first I did not port all FPU code, cause I don t think I need all of it.  One stupid thing is I forgot to turn on DEBUG_FPU, which should help me in the first place. I kind of lost myself in various engineering work during this debugging. I really need some big context switch in the middle to fresh my mind. Anyway, glad it is all done today (Feb 23), and I m able to move to next stage.  Compatibility is a heavy thing to carry. But it is also a nice thing for marketing. No one can deny the success of Intel on its backward compatibility. Bad for programmers.  \nYizhou Shan \nCreated: Feb 22, 2018 \nLast Updated: Feb 23, 2018", 
            "title": "Thoughts"
        }, 
        {
            "location": "/lego/kernel/irq/", 
            "text": "IRQ\n\n\nIRQ is majorly ported based on \nlinux-4.4\n. The decision of porting of whole IRQ stack from linux was made at early stage of Lego, when I\nm not so familiar with this stuff. This technique decision has pros and cons.\n\n\nThe whole thing is made complicated by having IRQ domain. IRQ domain is introduced to address the multiple interrupt controller issue. And in x86, we kind of have mutiple as well: IO-APIC, REMAP, LAPIC. Although we are not supporting IRQ remap now.\n\n\nInit\n\n\n\n\nThe first part of initialization is \ntrap_init()\n at early \nsetup_arch()\n.\n\n\nThe second major entry point is \nirq_init()\n at \nstart_kernel()\n. This \nirq_init()\n is actually a combination of linux\ns:\n\n\nearly_irq_init()\n: 1) setup \nirq_desc[]\n array, and then call \narch_early_irq_init()\n, which will register two IRQ domains (x86_vector_domain, msi_domain).\n\n\ninit_IRQ()\n: is actually a callback to low-level x86 interrupt setup. It mainly setup the desc\ns data/chip etc, and register all different handlers.\n\n\nIn Lego, you will be able to find all the functionalitis are moved into \narch_irq_init()\n. And, to this point, we have a complete setup.\n\n\n\n\n\n\nThe third (and last) entry point is \nsmp_prepare_cpus()\n:\n\n1\n2\n3\n4\n5\nsmp_prepare_cpus()\n-\n apic_bsp_setup()\n   -\n setup_local_APIC()\n   -\n setup_IO_APIC()\n   -\n x86_init.timers.setup_percpu_clockev()\n\n\n\n\n\n\n\nIRQ Domain\n\n\nWe should have at least 2 or 3 IRQ domains:\n\n\n\n\nx86_vector\n\n\nx86_msi\n\n\nx86_ioapic-N (each ioapic has one)\n\n\n\n\nThe first two guys are created during \narch_irq_init()\n. While the latter ioapic ones are created during \nsetup_IO_APIC()\n. All of them are allocated eventually by \n__irq_domain_add()\n, and linked at \nLIST_HEAD(irq_domain_list)\n.\n\n\nSo....  Lego or Linux maintains its own IRQ numbers, starting from 0 to NR_IRQs.\nHowever, this IRQ number MAY not have a identical mapping to hardware\ns own IRQ number (let us call it hwirq). Given this, we want to know the mapping between IRQ and hwirq. That\ns the purpose of having \nlinear_revmap\n and \nrevmap_tree\n within each domain, it is used to translate hwirq to IRQ.\n\n\nWhy two different data structures? \nlinear_revmap\n is fairly simple, an array, which is indexed by hwirq. However, the hwirq maybe very large, we don\nt want to waste memory, that\ns how we want to use trees.\n\n\nThese two can be used together. If we fail to insert into \nlinear_revmap\n, we insert into tree. During search time, we need to look up both.\n\n\nBy default, \nx86_vector\n and \nx86_msi\n use radix tree only. \nx86_ioapic-N\n uses a mix of linear and radix tree.\n\n\nTo dump all IRQ domains, call \ndump_irq_domain_list()\n, which give you something like this:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n[\n  \n118.308544\n]\n  \nname\n              \nmapped\n  \nlinear\n-\nmax\n  \ndirect\n-\nmax\n  \ndevtree\n-\nnode\n\n\n[\n  \n118.316114\n]\n  \nx86_ioapic\n-\n2\n          \n24\n          \n24\n           \n0\n    \n\n[\n  \n118.322707\n]\n  \nx86_ioapic\n-\n1\n          \n24\n          \n24\n           \n0\n    \n\n[\n  \n118.329299\n]\n  \nx86_ioapic\n-\n0\n          \n24\n          \n24\n           \n0\n    \n\n[\n  \n118.335893\n]\n  \nx86_msi\n               \n25\n           \n0\n           \n0\n    \n\n[\n  \n118.342486\n]\n \n*\nx86_vector\n            \n40\n           \n0\n           \n0\n    \n\n[\n  \n118.349078\n]\n \nirq\n    \nhwirq\n    \nchip\n \nname\n        \nchip\n \ndata\n           \nactive\n  \ntype\n            \ndomain\n\n\n[\n  \n118.358775\n]\n     \n1\n  \n0x00001\n  \nIO\n-\nAPIC\n          \n0xffff88107fcae000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.368858\n]\n     \n3\n  \n0x00003\n  \nIO\n-\nAPIC\n          \n0xffff88107fc8f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.378940\n]\n     \n4\n  \n0x00004\n  \nIO\n-\nAPIC\n          \n0xffff88107fc6e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.389025\n]\n     \n5\n  \n0x00005\n  \nIO\n-\nAPIC\n          \n0xffff88107fc6f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.399109\n]\n     \n6\n  \n0x00006\n  \nIO\n-\nAPIC\n          \n0xffff88107fc4e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.409192\n]\n     \n7\n  \n0x00007\n  \nIO\n-\nAPIC\n          \n0xffff88107fc4f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.419276\n]\n     \n8\n  \n0x00008\n  \nIO\n-\nAPIC\n          \n0xffff88107fc2e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.429358\n]\n     \n9\n  \n0x00009\n  \nIO\n-\nAPIC\n          \n0xffff88107fc2f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.439442\n]\n    \n10\n  \n0x0000a\n  \nIO\n-\nAPIC\n          \n0xffff88107fc0e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.449525\n]\n    \n11\n  \n0x0000b\n  \nIO\n-\nAPIC\n          \n0xffff88107fc0f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.459609\n]\n    \n12\n  \n0x0000c\n  \nIO\n-\nAPIC\n          \n0xffff88107fff0000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.469692\n]\n    \n13\n  \n0x0000d\n  \nIO\n-\nAPIC\n          \n0xffff88107fff1000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.479776\n]\n    \n14\n  \n0x0000e\n  \nIO\n-\nAPIC\n          \n0xffff88107fff2000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.489860\n]\n    \n15\n  \n0x0000f\n  \nIO\n-\nAPIC\n          \n0xffff88107fff3000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.499943\n]\n    \n24\n  \n0x300000\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.509833\n]\n    \n25\n  \n0x300001\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.519722\n]\n    \n26\n  \n0x300002\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.529612\n]\n    \n27\n  \n0x300003\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.539501\n]\n    \n28\n  \n0x300004\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n           \nRADIX\n          \nx86_msi\n\n\n\n\n\n\nAug 20, 2018\n\n\nWell, I\nve ported the IRQ stuff at early days of Lego. At that time, I mainly ported the low-level APIC, IO-APIC, and ACPI stuff, along with the upper layer irqchip, irqdesc stuff.\n\n\nThese days, I was verifying our IB code and tried to add back mlx4en\ns interrupt handler, somehow, there is no interrupt after \nrequest_irq()\n.\n\n\nTwo possible reasons: 1) I missed something during PCI setup, 2) underlying APIC and IO-APIC need more work.\n\n\n\nLast Updated: Aug 28, 2018", 
            "title": "IRQ"
        }, 
        {
            "location": "/lego/kernel/irq/#irq", 
            "text": "IRQ is majorly ported based on  linux-4.4 . The decision of porting of whole IRQ stack from linux was made at early stage of Lego, when I m not so familiar with this stuff. This technique decision has pros and cons.  The whole thing is made complicated by having IRQ domain. IRQ domain is introduced to address the multiple interrupt controller issue. And in x86, we kind of have mutiple as well: IO-APIC, REMAP, LAPIC. Although we are not supporting IRQ remap now.", 
            "title": "IRQ"
        }, 
        {
            "location": "/lego/kernel/irq/#init", 
            "text": "The first part of initialization is  trap_init()  at early  setup_arch() .  The second major entry point is  irq_init()  at  start_kernel() . This  irq_init()  is actually a combination of linux s:  early_irq_init() : 1) setup  irq_desc[]  array, and then call  arch_early_irq_init() , which will register two IRQ domains (x86_vector_domain, msi_domain).  init_IRQ() : is actually a callback to low-level x86 interrupt setup. It mainly setup the desc s data/chip etc, and register all different handlers.  In Lego, you will be able to find all the functionalitis are moved into  arch_irq_init() . And, to this point, we have a complete setup.    The third (and last) entry point is  smp_prepare_cpus() : 1\n2\n3\n4\n5 smp_prepare_cpus()\n-  apic_bsp_setup()\n   -  setup_local_APIC()\n   -  setup_IO_APIC()\n   -  x86_init.timers.setup_percpu_clockev()", 
            "title": "Init"
        }, 
        {
            "location": "/lego/kernel/irq/#irq-domain", 
            "text": "We should have at least 2 or 3 IRQ domains:   x86_vector  x86_msi  x86_ioapic-N (each ioapic has one)   The first two guys are created during  arch_irq_init() . While the latter ioapic ones are created during  setup_IO_APIC() . All of them are allocated eventually by  __irq_domain_add() , and linked at  LIST_HEAD(irq_domain_list) .  So....  Lego or Linux maintains its own IRQ numbers, starting from 0 to NR_IRQs.\nHowever, this IRQ number MAY not have a identical mapping to hardware s own IRQ number (let us call it hwirq). Given this, we want to know the mapping between IRQ and hwirq. That s the purpose of having  linear_revmap  and  revmap_tree  within each domain, it is used to translate hwirq to IRQ.  Why two different data structures?  linear_revmap  is fairly simple, an array, which is indexed by hwirq. However, the hwirq maybe very large, we don t want to waste memory, that s how we want to use trees.  These two can be used together. If we fail to insert into  linear_revmap , we insert into tree. During search time, we need to look up both.  By default,  x86_vector  and  x86_msi  use radix tree only.  x86_ioapic-N  uses a mix of linear and radix tree.  To dump all IRQ domains, call  dump_irq_domain_list() , which give you something like this:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 [    118.308544 ]    name                mapped    linear - max    direct - max    devtree - node  [    118.316114 ]    x86_ioapic - 2            24            24             0      [    118.322707 ]    x86_ioapic - 1            24            24             0      [    118.329299 ]    x86_ioapic - 0            24            24             0      [    118.335893 ]    x86_msi                 25             0             0      [    118.342486 ]   * x86_vector              40             0             0      [    118.349078 ]   irq      hwirq      chip   name          chip   data             active    type              domain  [    118.358775 ]       1    0x00001    IO - APIC            0xffff88107fcae000          LINEAR            x86_ioapic - 0  [    118.368858 ]       3    0x00003    IO - APIC            0xffff88107fc8f000          LINEAR            x86_ioapic - 0  [    118.378940 ]       4    0x00004    IO - APIC            0xffff88107fc6e000          LINEAR            x86_ioapic - 0  [    118.389025 ]       5    0x00005    IO - APIC            0xffff88107fc6f000          LINEAR            x86_ioapic - 0  [    118.399109 ]       6    0x00006    IO - APIC            0xffff88107fc4e000          LINEAR            x86_ioapic - 0  [    118.409192 ]       7    0x00007    IO - APIC            0xffff88107fc4f000          LINEAR            x86_ioapic - 0  [    118.419276 ]       8    0x00008    IO - APIC            0xffff88107fc2e000          LINEAR            x86_ioapic - 0  [    118.429358 ]       9    0x00009    IO - APIC            0xffff88107fc2f000          LINEAR            x86_ioapic - 0  [    118.439442 ]      10    0x0000a    IO - APIC            0xffff88107fc0e000          LINEAR            x86_ioapic - 0  [    118.449525 ]      11    0x0000b    IO - APIC            0xffff88107fc0f000          LINEAR            x86_ioapic - 0  [    118.459609 ]      12    0x0000c    IO - APIC            0xffff88107fff0000          LINEAR            x86_ioapic - 0  [    118.469692 ]      13    0x0000d    IO - APIC            0xffff88107fff1000          LINEAR            x86_ioapic - 0  [    118.479776 ]      14    0x0000e    IO - APIC            0xffff88107fff2000          LINEAR            x86_ioapic - 0  [    118.489860 ]      15    0x0000f    IO - APIC            0xffff88107fff3000          LINEAR            x86_ioapic - 0  [    118.499943 ]      24    0x300000    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.509833 ]      25    0x300001    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.519722 ]      26    0x300002    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.529612 ]      27    0x300003    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.539501 ]      28    0x300004    PCI - MSI                        ( null )             RADIX            x86_msi", 
            "title": "IRQ Domain"
        }, 
        {
            "location": "/lego/kernel/irq/#aug-20-2018", 
            "text": "Well, I ve ported the IRQ stuff at early days of Lego. At that time, I mainly ported the low-level APIC, IO-APIC, and ACPI stuff, along with the upper layer irqchip, irqdesc stuff.  These days, I was verifying our IB code and tried to add back mlx4en s interrupt handler, somehow, there is no interrupt after  request_irq() .  Two possible reasons: 1) I missed something during PCI setup, 2) underlying APIC and IO-APIC need more work.  \nLast Updated: Aug 28, 2018", 
            "title": "Aug 20, 2018"
        }, 
        {
            "location": "/lego/kernel/net_thpool/", 
            "text": "Thread Pool Model for Handling Network Requests\n\n\n\n\nPassive\n: whenever a network request comes in, callback to thpool.\n\n\nActive\n: thpool keep polling if there is new network requests queued.\n\n\n\n\nPreviously, our memory side use the Active mode to handle requests, which has very bad latency. Several days ago we changed to the Passive mode, which has a very good latency! One \nib_send_reply\n RRT drops from \n~20us\n to a normal \n~6us\n for a TensorFlow run.\n\n\nNever thought this could make such a big difference (~3x slowdown)! Dark network!\n\n\n\nYizhou Shan\n\nCreated: April 29, 2018\n\nLast Updated: April 29, 2018", 
            "title": "Network Thpool"
        }, 
        {
            "location": "/lego/kernel/net_thpool/#thread-pool-model-for-handling-network-requests", 
            "text": "Passive : whenever a network request comes in, callback to thpool.  Active : thpool keep polling if there is new network requests queued.   Previously, our memory side use the Active mode to handle requests, which has very bad latency. Several days ago we changed to the Passive mode, which has a very good latency! One  ib_send_reply  RRT drops from  ~20us  to a normal  ~6us  for a TensorFlow run.  Never thought this could make such a big difference (~3x slowdown)! Dark network!  \nYizhou Shan \nCreated: April 29, 2018 \nLast Updated: April 29, 2018", 
            "title": "Thread Pool Model for Handling Network Requests"
        }, 
        {
            "location": "/lego/syscall/facts/", 
            "text": "Lego SYSCALL Facts\n\n\nThis document is about the general concepts of Lego syscall implementation. If you are developing syscall, please read this document first.\n\n\nInterrupts Enabled\n\n\nEach syscall is invoked with interrupts enabled. Also, it must return with interrupts enabled as well. Any buggy syscall implementation will be catched by \nsyscall_return_slowpath()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nvoid\n \nsyscall_return_slowpath\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \nif\n \n(\nWARN\n(\nirqs_disabled\n(),\n \nsyscall %ld left IRQs disabled\n,\n \nregs\n-\norig_ax\n))\n\n                \nlocal_irq_enable\n();\n\n\n        \nlocal_irq_disable\n();\n\n        \nprepare_exit_to_usermode\n(\nregs\n);\n\n\n}\n\n\n\nvoid\n \ndo_syscall_64\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \n..\n\n        \nlocal_irq_enable\n();\n\n\n        \nif\n \n(\nlikely\n(\nnr\n \n \nNR_syscalls\n))\n \n{\n\n                \nregs\n-\nax\n \n=\n \nsys_call_table\n[\nnr\n](\n\n                        \nregs\n-\ndi\n,\n \nregs\n-\nsi\n,\n \nregs\n-\ndx\n,\n\n                        \nregs\n-\nr10\n,\n \nregs\n-\nr8\n,\n \nregs\n-\nr9\n);\n\n        \n}\n   \n\n        \nsyscall_return_slowpath\n(\nregs\n);\n\n        \n..\n\n\n}\n\n\n\n\n\n\nGet User Entry pt_regs\n\n\nThe macro \ntask_pt_regs()\n always return the \npt_regs\n, that saves the user context when it issued the syscall, no matter how many levels interrupts are nested when you call \ntask_pt_regs()\n. This is based on the fact that kernel stack is empty at syscall entry, thus this user \npt_regs\n was saved at the \ntop\n of kernel stack:\n\n1\n#define task_pt_regs(tsk)       ((struct pt_regs *)(tsk)-\nthread.sp0 - 1)\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\nENTRY\n(\nentry_SYSCALL_64\n)\n\n        \nSWAPGS\n\n\n        \n/*\n\n         \n*\n \nSYSCALL\n \ndoes\n \nnot\n \nchange\n \nrsp\n \nfor\n \nus\n!\n\n         \n*\n \nSave\n \nthe\n \nprevious\n \nrsp\n \nand\n \nload\n \nthe\n \ntop\n \nof\n \nkernel\n \nstack.\n\n         \n*\n \nIt\n \nmust\n \nbe\n \nthe\n \ntop\n \nof\n \nkernel\n \nstack\n,\n \nsince\n \nwe\n \ncame\n \nhere\n\n         \n*\n \nfrom\n \n*\nuserspace\n*.\n\n         \n*/\n\n        \nmovq\n    \n%rsp\n,\n \nPER_CPU_VAR\n(\nrsp_scratch\n)\n\n        \nmovq\n    \nPER_CPU_VAR\n(\ncpu_current_top_of_stack\n),\n \n%rsp\n\n\n        \n/*\n\n         \n*\n \nConstruct\n \nstruct\n \npt_regs\n \non\n \nstack\n\n         \n*\n\n         \n*\n \nIn\n \nany\n \nsyscall\n \nhandler\n,\n \nyou\n \ncan\n \nuse\n\n         \n*\n      \ncurrent_pt_regs\n()\n\n         \n*\n \nto\n \nget\n \nthese\n \nregisters.\n\n         \n*/\n\n        \npushq\n   \n$__USER_DS\n                      \n/\n*\n \npt_regs-\nss\n \n*\n/\n\n        \npushq\n   \nPER_CPU_VAR\n(\nrsp_scratch\n)\n        \n/\n*\n \npt_regs-\nsp\n \n*\n/\n\n        \npushq\n   \n%r11\n                            \n/\n*\n \npt_regs-\nflags\n \n*\n/\n\n        \npushq\n   \n$__USER_CS\n                      \n/\n*\n \npt_regs-\ncs\n \n*\n/\n\n        \npushq\n   \n%rcx\n                            \n/\n*\n \npt_regs-\nip\n \n*\n/\n\n        \npushq\n   \n%rax\n                            \n/\n*\n \npt_regs-\norig_ax\n \n*\n/\n\n        \npushq\n   \n%rdi\n                            \n/\n*\n \npt_regs-\ndi\n \n*\n/\n\n        \npushq\n   \n%rsi\n                            \n/\n*\n \npt_regs-\nsi\n \n*\n/\n\n        \npushq\n   \n%rdx\n                            \n/\n*\n \npt_regs-\ndx\n \n*\n/\n\n        \npushq\n   \n%rcx\n                            \n/\n*\n \npt_regs-\ncx\n \n*\n/\n\n        \npushq\n   \n$-ENOSYS\n                        \n/\n*\n \npt_regs-\nax\n \n*\n/\n\n        \npushq\n   \n%r8\n                             \n/\n*\n \npt_regs-\nr8\n \n*\n/\n\n        \npushq\n   \n%r9\n                             \n/\n*\n \npt_regs-\nr9\n \n*\n/\n\n        \npushq\n   \n%r10\n                            \n/\n*\n \npt_regs-\nr10\n \n*\n/\n\n        \npushq\n   \n%r11\n                            \n/\n*\n \npt_regs-\nr11\n \n*\n/\n\n        \nsub\n     \n$\n(\n6\n*\n8\n),\n \n%rsp\n                    \n/\n*\n \npt_regs-\nbp\n,\n \nbx\n,\n \nr12-15\n \n*\n/\n\n        \n....\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 22, 2018\n\nLast Updated: Feb 22, 2018", 
            "title": "Facts"
        }, 
        {
            "location": "/lego/syscall/facts/#lego-syscall-facts", 
            "text": "This document is about the general concepts of Lego syscall implementation. If you are developing syscall, please read this document first.", 
            "title": "Lego SYSCALL Facts"
        }, 
        {
            "location": "/lego/syscall/facts/#interrupts-enabled", 
            "text": "Each syscall is invoked with interrupts enabled. Also, it must return with interrupts enabled as well. Any buggy syscall implementation will be catched by  syscall_return_slowpath() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 void   syscall_return_slowpath ( struct   pt_regs   * regs )  { \n         if   ( WARN ( irqs_disabled (),   syscall %ld left IRQs disabled ,   regs - orig_ax )) \n                 local_irq_enable (); \n\n         local_irq_disable (); \n         prepare_exit_to_usermode ( regs );  }  void   do_syscall_64 ( struct   pt_regs   * regs )  { \n         .. \n         local_irq_enable (); \n\n         if   ( likely ( nr     NR_syscalls ))   { \n                 regs - ax   =   sys_call_table [ nr ]( \n                         regs - di ,   regs - si ,   regs - dx , \n                         regs - r10 ,   regs - r8 ,   regs - r9 ); \n         }    \n\n         syscall_return_slowpath ( regs ); \n         ..  }", 
            "title": "Interrupts Enabled"
        }, 
        {
            "location": "/lego/syscall/facts/#get-user-entry-pt_regs", 
            "text": "The macro  task_pt_regs()  always return the  pt_regs , that saves the user context when it issued the syscall, no matter how many levels interrupts are nested when you call  task_pt_regs() . This is based on the fact that kernel stack is empty at syscall entry, thus this user  pt_regs  was saved at the  top  of kernel stack: 1 #define task_pt_regs(tsk)       ((struct pt_regs *)(tsk)- thread.sp0 - 1)     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 ENTRY ( entry_SYSCALL_64 ) \n         SWAPGS \n\n         /* \n          *   SYSCALL   does   not   change   rsp   for   us ! \n          *   Save   the   previous   rsp   and   load   the   top   of   kernel   stack. \n          *   It   must   be   the   top   of   kernel   stack ,   since   we   came   here \n          *   from   * userspace *. \n          */ \n         movq      %rsp ,   PER_CPU_VAR ( rsp_scratch ) \n         movq      PER_CPU_VAR ( cpu_current_top_of_stack ),   %rsp \n\n         /* \n          *   Construct   struct   pt_regs   on   stack \n          * \n          *   In   any   syscall   handler ,   you   can   use \n          *        current_pt_regs () \n          *   to   get   these   registers. \n          */ \n         pushq     $__USER_DS                        / *   pt_regs- ss   * / \n         pushq     PER_CPU_VAR ( rsp_scratch )          / *   pt_regs- sp   * / \n         pushq     %r11                              / *   pt_regs- flags   * / \n         pushq     $__USER_CS                        / *   pt_regs- cs   * / \n         pushq     %rcx                              / *   pt_regs- ip   * / \n         pushq     %rax                              / *   pt_regs- orig_ax   * / \n         pushq     %rdi                              / *   pt_regs- di   * / \n         pushq     %rsi                              / *   pt_regs- si   * / \n         pushq     %rdx                              / *   pt_regs- dx   * / \n         pushq     %rcx                              / *   pt_regs- cx   * / \n         pushq     $-ENOSYS                          / *   pt_regs- ax   * / \n         pushq     %r8                               / *   pt_regs- r8   * / \n         pushq     %r9                               / *   pt_regs- r9   * / \n         pushq     %r10                              / *   pt_regs- r10   * / \n         pushq     %r11                              / *   pt_regs- r11   * / \n         sub       $ ( 6 * 8 ),   %rsp                      / *   pt_regs- bp ,   bx ,   r12-15   * / \n         ....    \nYizhou Shan \nCreated: Feb 22, 2018 \nLast Updated: Feb 22, 2018", 
            "title": "Get User Entry pt_regs"
        }, 
        {
            "location": "/lego/kernel/profile_strace/", 
            "text": "Lego Profile strace\n\n\nLego has a built-in kernel-version syscall tracer, similar to \nstrace\n utility in the user space. Below we will just call our Lego\ns syscall tracer as strace for simplicity.\n\n\nDesign\n\n\nThere are essentially three important metrics to track for each syscall\n\n\n\n\nnumber of times invoked\n\n\nnumber of times error happened\n\n\ntotal execution, or per-call latency\n\n\n\n\nBesides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:\n\n\n\n\nPerformance: set of counters are \natomic_t\n, updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.\n\n\nSimplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.\n\n\n\n\nBelow is our simple design, where each thread has a \nstruct strace_info\n, which include a set of counters for each syscall. All \nstrace_info\n within a process are chained together by a doubly-linked list.\n\n\n\n\nWhen we want to look at the strace statistic numbers, we need to \naccumulate\n counters from all threads within a process, including those dead threads. We do the \naccumulate\n when the last thread of this process is going to exit.\n\n\nThe benefit of doubly-linked \nstrace_info\n is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if \ntask_struct\n is reaped, \nstrace_info\n is still there and linked.\n\n\nFor example, assume thread_3 has a SIGSEGV, and did a \nzap_other_threads\n. And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.\n\n\n\nDetails\n\n\nThere are essentially three hooks in core kernel:\n\n\n\n\nsyscall\n: before and after \nsys_call_table\n\n\nfork/clone\n: create \nstrace_info\n for each thread\n\n\ndo_exit()\n: when group_dead(signal-\nlive==1), accumulate\n\n\n\n\nExample Output\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n[\n \n1017.047366\n]\n \nKernel\n \nstrace\n\n\n[\n \n1017.050276\n]\n \nTask\n:\n \n20\n:\n20\n \nnr_accumulated_threads\n:\n \n46\n\n\n[\n \n1017.055837\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n1017.063213\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.071648\n]\n  \n98.16\n   \n33.839597842\n     \n1879978\n        \n18\n         \n0\n \nsys_futex\n\n\n[\n \n1017.079406\n]\n   \n0.26\n    \n0.260143997\n      \n260144\n         \n1\n         \n0\n \nsys_execve\n\n\n[\n \n1017.087260\n]\n   \n0.18\n    \n0.185456860\n        \n7133\n        \n26\n         \n0\n \nsys_write\n\n\n[\n \n1017.095017\n]\n   \n0.50\n    \n0.050189546\n         \n913\n        \n55\n         \n0\n \nsys_munmap\n\n\n[\n \n1017.102870\n]\n   \n0.25\n    \n0.025223661\n         \n255\n        \n99\n         \n0\n \nsys_mmap\n\n\n[\n \n1017.110531\n]\n   \n0.50\n    \n0.000505134\n          \n12\n        \n45\n         \n0\n \nsys_clone\n\n\n[\n \n1017.118288\n]\n   \n0.20\n    \n0.000202327\n          \n26\n         \n8\n         \n0\n \nsys_read\n\n\n[\n \n1017.125947\n]\n   \n0.14\n    \n0.000144065\n          \n17\n         \n9\n         \n0\n \nsys_open\n\n\n[\n \n1017.133608\n]\n   \n0.67\n    \n0.000067251\n           \n7\n        \n11\n         \n0\n \nsys_brk\n\n\n[\n \n1017.141171\n]\n   \n0.30\n    \n0.000030361\n           \n7\n         \n5\n         \n0\n \nsys_newfstat\n\n\n[\n \n1017.149219\n]\n   \n0.64\n    \n0.000006410\n           \n1\n         \n9\n         \n0\n \nsys_close\n\n\n[\n \n1017.156976\n]\n   \n0.48\n    \n0.000004842\n           \n1\n        \n45\n         \n0\n \nsys_madvise\n\n\n[\n \n1017.164927\n]\n   \n0.34\n    \n0.000003443\n           \n1\n        \n47\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n1017.173653\n]\n   \n0.21\n    \n0.000002137\n           \n1\n        \n52\n         \n0\n \nsys_mprotect\n\n\n[\n \n1017.181702\n]\n   \n0.71\n    \n0.000000717\n           \n1\n         \n4\n         \n0\n \nsys_gettimeofday\n\n\n[\n \n1017.190137\n]\n   \n0.60\n    \n0.000000608\n           \n1\n         \n3\n         \n0\n \nsys_time\n\n\n[\n \n1017.197797\n]\n   \n0.51\n    \n0.000000513\n           \n1\n         \n2\n         \n0\n \nsys_getrlimit\n\n\n[\n \n1017.205942\n]\n   \n0.49\n    \n0.000000498\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n1017.214572\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n4\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n1017.223008\n]\n   \n0.45\n    \n0.000000453\n           \n1\n         \n2\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n1017.231249\n]\n   \n0.27\n    \n0.000000272\n           \n1\n         \n2\n         \n0\n \nsys_newuname\n\n\n[\n \n1017.239298\n]\n   \n0.13\n    \n0.000000135\n           \n1\n         \n2\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n1017.248025\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.256460\n]\n \n100.00\n   \n34.361581541\n                   \n451\n         \n0\n \ntotal\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: April 05, 2018\n\nLast Updated: April 05, 2018", 
            "title": "strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#lego-profile-strace", 
            "text": "Lego has a built-in kernel-version syscall tracer, similar to  strace  utility in the user space. Below we will just call our Lego s syscall tracer as strace for simplicity.", 
            "title": "Lego Profile strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#design", 
            "text": "There are essentially three important metrics to track for each syscall   number of times invoked  number of times error happened  total execution, or per-call latency   Besides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:   Performance: set of counters are  atomic_t , updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.  Simplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.   Below is our simple design, where each thread has a  struct strace_info , which include a set of counters for each syscall. All  strace_info  within a process are chained together by a doubly-linked list.   When we want to look at the strace statistic numbers, we need to  accumulate  counters from all threads within a process, including those dead threads. We do the  accumulate  when the last thread of this process is going to exit.  The benefit of doubly-linked  strace_info  is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if  task_struct  is reaped,  strace_info  is still there and linked.  For example, assume thread_3 has a SIGSEGV, and did a  zap_other_threads . And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.", 
            "title": "Design"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#details", 
            "text": "There are essentially three hooks in core kernel:   syscall : before and after  sys_call_table  fork/clone : create  strace_info  for each thread  do_exit() : when group_dead(signal- live==1), accumulate", 
            "title": "Details"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#example-output", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 [   1017.047366 ]   Kernel   strace  [   1017.050276 ]   Task :   20 : 20   nr_accumulated_threads :   46  [   1017.055837 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   1017.063213 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.071648 ]    98.16     33.839597842       1879978          18           0   sys_futex  [   1017.079406 ]     0.26      0.260143997        260144           1           0   sys_execve  [   1017.087260 ]     0.18      0.185456860          7133          26           0   sys_write  [   1017.095017 ]     0.50      0.050189546           913          55           0   sys_munmap  [   1017.102870 ]     0.25      0.025223661           255          99           0   sys_mmap  [   1017.110531 ]     0.50      0.000505134            12          45           0   sys_clone  [   1017.118288 ]     0.20      0.000202327            26           8           0   sys_read  [   1017.125947 ]     0.14      0.000144065            17           9           0   sys_open  [   1017.133608 ]     0.67      0.000067251             7          11           0   sys_brk  [   1017.141171 ]     0.30      0.000030361             7           5           0   sys_newfstat  [   1017.149219 ]     0.64      0.000006410             1           9           0   sys_close  [   1017.156976 ]     0.48      0.000004842             1          45           0   sys_madvise  [   1017.164927 ]     0.34      0.000003443             1          47           0   sys_set_robust_list  [   1017.173653 ]     0.21      0.000002137             1          52           0   sys_mprotect  [   1017.181702 ]     0.71      0.000000717             1           4           0   sys_gettimeofday  [   1017.190137 ]     0.60      0.000000608             1           3           0   sys_time  [   1017.197797 ]     0.51      0.000000513             1           2           0   sys_getrlimit  [   1017.205942 ]     0.49      0.000000498             1           2           0   sys_rt_sigprocmask  [   1017.214572 ]     0.46      0.000000469             1           4           0   sys_rt_sigaction  [   1017.223008 ]     0.45      0.000000453             1           2           0   sys_arch_prctl  [   1017.231249 ]     0.27      0.000000272             1           2           0   sys_newuname  [   1017.239298 ]     0.13      0.000000135             1           2           0   sys_set_tid_address  [   1017.248025 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.256460 ]   100.00     34.361581541                     451           0   total    \nYizhou Shan \nCreated: April 05, 2018 \nLast Updated: April 05, 2018", 
            "title": "Example Output"
        }, 
        {
            "location": "/lego/syscall/compat/", 
            "text": "Compat SYSCALL in Lego\n\n\nLego does \nnot\n support compatible syscalls, where one is able to run 32-bit image on 64-bit OS. However, the ugly FPU code and signal part in Linux is heavily hacked with the assumption that compat syscall is supported. We are no expert in this FPU thing, just to make sure we don\nt break this FPU evil, Lego adds the fake compat syscall support. Fake means whenever a 32-bit syscall is issued, Lego will just panic.\n\n\nKconfig\n\n\nIf one compiles a x86_64 Linux kernel, compat syscalls are supported by default. Everything related to compat syscalls are controlled by the following two Kconfig options. Lego may want to support compat syscalls in the future, thus we add these two Kconfigs to avoid future mess:\n\n\n\n\nCONFIG_COMPAT\n\n\nCONFIG_IA32_EMULATION\n\n\n\n\nInternal\n\n\nEntry Points\n\n\nThe assembly entry points are defined in \nentry/entry_64_compat.S\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nENTRY\n(\nentry_SYSENTER_compat\n)\n\n        \n...\n\n        \ncall\n    \ndo_fast_syscall_32\n\n\nGLOBAL\n(\n__end_entry_SYSENTER_compat\n)\n\n\nENDPROC\n(\nentry_SYSENTER_compat\n)\n\n\n\nENTRY\n(\nentry_SYSCALL_compat\n)\n\n        \n...\n\n        \ncall\n    \ndo_fast_syscall_32\n\n\nEND\n(\nentry_SYSCALL_compat\n)\n\n\n\nENTRY\n(\nentry_INT80_compat\n)\n\n        \n...\n\n        \ncall\n    \ndo_int80_syscall_32\n\n\nEND\n(\nentry_INT80_compat\n)\n\n\n\n\n\n\nEntry Points Setup\n\n\nThe assembly entry points are filled to system registers and IDT table. So users can \nactually\n issue those calls, Lego is able to catch them:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\nstatic\n \nvoid\n \nsyscall_init\n(\nvoid\n)\n\n\n{\n\n        \nwrmsr\n(\nMSR_STAR\n,\n \n0\n,\n \n(\n__USER32_CS\n \n \n16\n)\n \n|\n \n__KERNEL_CS\n);\n\n        \nwrmsrl\n(\nMSR_LSTAR\n,\n \n(\nunsigned\n \nlong\n)\nentry_SYSCALL_64\n);\n\n\n\n#ifdef CONFIG_IA32_EMULATION\n\n        \nwrmsrl\n(\nMSR_CSTAR\n,\n \n(\nunsigned\n \nlong\n)\nentry_SYSCALL_compat\n);\n\n        \n/*  \n\n\n         * This only works on Intel CPUs.\n\n\n         * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.\n\n\n         * This does not cause SYSENTER to jump to the wrong location, because\n\n\n         * AMD doesn\nt allow SYSENTER in long mode (either 32- or 64-bit).\n\n\n         */\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_CS\n,\n \n(\nu64\n)\n__KERNEL_CS\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_ESP\n,\n \n0ULL\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_EIP\n,\n \n(\nu64\n)\nentry_SYSENTER_compat\n);\n\n\n#else\n\n        \nwrmsrl\n(\nMSR_CSTAR\n,\n \n(\nunsigned\n \nlong\n)\nignore_sysret\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_CS\n,\n \n(\nu64\n)\nGDT_ENTRY_INVALID_SEG\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_ESP\n,\n \n0ULL\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_EIP\n,\n \n0ULL\n);\n\n\n#endif\n\n\n\n        \n/* Flags to clear on syscall */\n\n        \nwrmsrl\n(\nMSR_SYSCALL_MASK\n,\n\n               \nX86_EFLAGS_TF\n|\nX86_EFLAGS_DF\n|\nX86_EFLAGS_IF\n|\n\n               \nX86_EFLAGS_IOPL\n|\nX86_EFLAGS_AC\n|\nX86_EFLAGS_NT\n);\n\n\n}\n\n\narch\n/\nx86\n/\nkernel\n/\ncpu\n/\ncommon\n.\nc\n\n\n\nvoid\n \n__init\n \ntrap_init\n(\nvoid\n)\n\n\n{\n\n        \n...\n\n\n#ifdef CONFIG_IA32_EMULATION\n\n        \nset_system_intr_gate\n(\nIA32_SYSCALL_VECTOR\n,\n \nentry_INT80_compat\n);\n\n        \nset_bit\n(\nIA32_SYSCALL_VECTOR\n,\n \nused_vectors\n);\n\n\n#endif\n\n        \n...\n\n\n}\n\n\narch\n/\nx86\n/\nkernel\n/\ntraps\n.\nc\n\n\n\n\n\n\nC code\n\n\nThe actual C code is in \nentry/common.c\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)\n\n\nstatic\n \n__always_inline\n \nvoid\n \ndo_syscall_32_irqs_on\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n\n#ifdef CONFIG_IA32_EMULATION\n\n        \ncurrent\n-\nthread\n.\nstatus\n \n|=\n \nTS_COMPAT\n;\n\n\n#endif\n\n\n        \nBUG\n();\n\n\n}\n\n\n\n/* Handles int $0x80 */\n\n\n__visible\n \nvoid\n \ndo_int80_syscall_32\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \nBUG\n();\n\n\n}\n\n\n\n/* Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */\n\n\n__visible\n \nlong\n \ndo_fast_syscall_32\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \nBUG\n();\n\n\n}\n\n\n#endif\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 22, 2018\n\nLast Updated: Feb 22, 2018", 
            "title": "compat"
        }, 
        {
            "location": "/lego/syscall/compat/#compat-syscall-in-lego", 
            "text": "Lego does  not  support compatible syscalls, where one is able to run 32-bit image on 64-bit OS. However, the ugly FPU code and signal part in Linux is heavily hacked with the assumption that compat syscall is supported. We are no expert in this FPU thing, just to make sure we don t break this FPU evil, Lego adds the fake compat syscall support. Fake means whenever a 32-bit syscall is issued, Lego will just panic.", 
            "title": "Compat SYSCALL in Lego"
        }, 
        {
            "location": "/lego/syscall/compat/#kconfig", 
            "text": "If one compiles a x86_64 Linux kernel, compat syscalls are supported by default. Everything related to compat syscalls are controlled by the following two Kconfig options. Lego may want to support compat syscalls in the future, thus we add these two Kconfigs to avoid future mess:   CONFIG_COMPAT  CONFIG_IA32_EMULATION", 
            "title": "Kconfig"
        }, 
        {
            "location": "/lego/syscall/compat/#internal", 
            "text": "", 
            "title": "Internal"
        }, 
        {
            "location": "/lego/syscall/compat/#entry-points", 
            "text": "The assembly entry points are defined in  entry/entry_64_compat.S :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 ENTRY ( entry_SYSENTER_compat ) \n         ... \n         call      do_fast_syscall_32  GLOBAL ( __end_entry_SYSENTER_compat )  ENDPROC ( entry_SYSENTER_compat )  ENTRY ( entry_SYSCALL_compat ) \n         ... \n         call      do_fast_syscall_32  END ( entry_SYSCALL_compat )  ENTRY ( entry_INT80_compat ) \n         ... \n         call      do_int80_syscall_32  END ( entry_INT80_compat )", 
            "title": "Entry Points"
        }, 
        {
            "location": "/lego/syscall/compat/#entry-points-setup", 
            "text": "The assembly entry points are filled to system registers and IDT table. So users can  actually  issue those calls, Lego is able to catch them:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 static   void   syscall_init ( void )  { \n         wrmsr ( MSR_STAR ,   0 ,   ( __USER32_CS     16 )   |   __KERNEL_CS ); \n         wrmsrl ( MSR_LSTAR ,   ( unsigned   long ) entry_SYSCALL_64 );  #ifdef CONFIG_IA32_EMULATION \n         wrmsrl ( MSR_CSTAR ,   ( unsigned   long ) entry_SYSCALL_compat ); \n         /*             * This only works on Intel CPUs.           * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.           * This does not cause SYSENTER to jump to the wrong location, because           * AMD doesn t allow SYSENTER in long mode (either 32- or 64-bit).           */ \n         wrmsrl_safe ( MSR_IA32_SYSENTER_CS ,   ( u64 ) __KERNEL_CS ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_ESP ,   0ULL ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_EIP ,   ( u64 ) entry_SYSENTER_compat );  #else \n         wrmsrl ( MSR_CSTAR ,   ( unsigned   long ) ignore_sysret ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_CS ,   ( u64 ) GDT_ENTRY_INVALID_SEG ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_ESP ,   0ULL ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_EIP ,   0ULL );  #endif \n\n\n         /* Flags to clear on syscall */ \n         wrmsrl ( MSR_SYSCALL_MASK , \n                X86_EFLAGS_TF | X86_EFLAGS_DF | X86_EFLAGS_IF | \n                X86_EFLAGS_IOPL | X86_EFLAGS_AC | X86_EFLAGS_NT );  }  arch / x86 / kernel / cpu / common . c  void   __init   trap_init ( void )  { \n         ...  #ifdef CONFIG_IA32_EMULATION \n         set_system_intr_gate ( IA32_SYSCALL_VECTOR ,   entry_INT80_compat ); \n         set_bit ( IA32_SYSCALL_VECTOR ,   used_vectors );  #endif \n         ...  }  arch / x86 / kernel / traps . c", 
            "title": "Entry Points Setup"
        }, 
        {
            "location": "/lego/syscall/compat/#c-code", 
            "text": "The actual C code is in  entry/common.c :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 #if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)  static   __always_inline   void   do_syscall_32_irqs_on ( struct   pt_regs   * regs )  {  #ifdef CONFIG_IA32_EMULATION \n         current - thread . status   |=   TS_COMPAT ;  #endif \n\n         BUG ();  }  /* Handles int $0x80 */  __visible   void   do_int80_syscall_32 ( struct   pt_regs   * regs )  { \n         BUG ();  }  /* Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */  __visible   long   do_fast_syscall_32 ( struct   pt_regs   * regs )  { \n         BUG ();  }  #endif    \nYizhou Shan \nCreated: Feb 22, 2018 \nLast Updated: Feb 22, 2018", 
            "title": "C code"
        }, 
        {
            "location": "/lego/syscall/msync/", 
            "text": "msync()\n\n\nThe document is a summary I wrote after reading \nFailure-atomic msync()\n paper, which help me understand several questions related to \nmsync()\n.\n\n\n\n\n\n\nmsync() is not atomic.\n During msync(), pages are being written back to disk one by one (or batched): few pages have been flushed back, but few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.\u000b\u000b\n\n\n\n\n\n\nmsync() need concurrency control\n. This actually is the issue I asked before. With a multi-threaded application, does msync() provide the synchronization semantic? The answer is no. Other threads within the same process are able to write to pages under msync(). This implies, application need to handle concurrency by themselves, e.g., rwlocks. \u000b\u000bAt the very beginning, I thought msync() provide this semantic. The only way to implement this should be: kernel make all pages\n PTE read-only, and then perform flush back. If any other threads does a write during flush, they will have a page fault. And in the pgfault function, we hold the threads until the pages are written back.\n\n\n\n\n\n\nProbably some nice reading. \nfsync, fdatasync\n1\n.\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 01, 2018\n\nLast Updated: Mar 23, 2018\n\n\n\n\n\n\n\n\n\n\nRFLUSH: Rethink the Flush", 
            "title": "msync()"
        }, 
        {
            "location": "/lego/syscall/msync/#msync", 
            "text": "The document is a summary I wrote after reading  Failure-atomic msync()  paper, which help me understand several questions related to  msync() .    msync() is not atomic.  During msync(), pages are being written back to disk one by one (or batched): few pages have been flushed back, but few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.\u000b\u000b    msync() need concurrency control . This actually is the issue I asked before. With a multi-threaded application, does msync() provide the synchronization semantic? The answer is no. Other threads within the same process are able to write to pages under msync(). This implies, application need to handle concurrency by themselves, e.g., rwlocks. \u000b\u000bAt the very beginning, I thought msync() provide this semantic. The only way to implement this should be: kernel make all pages  PTE read-only, and then perform flush back. If any other threads does a write during flush, they will have a page fault. And in the pgfault function, we hold the threads until the pages are written back.    Probably some nice reading.  fsync, fdatasync 1 .    \nYizhou Shan \nCreated: Feb 01, 2018 \nLast Updated: Mar 23, 2018      RFLUSH: Rethink the Flush", 
            "title": "msync()"
        }, 
        {
            "location": "/lego/syscall/mremap/", 
            "text": "mremap()", 
            "title": "mremap()"
        }, 
        {
            "location": "/lego/syscall/mremap/#mremap", 
            "text": "", 
            "title": "mremap()"
        }, 
        {
            "location": "/lego/syscall/fork/", 
            "text": "fork()\n\n\nMemory Manager\n\n\nWe need to duplicate the address space in the memory manager side. Follow the traditional \nfork()\n semantic, both the existing and newly created address space will be write-protected.\n\n\nSince we have the flexibility to implement any VM organization, we should be careful while duplicating the address space. Currently, we are using page-based VM, thus the duplicating is basically creating a new \npgd\n and copy existing pgtables, and further downgrade permission to read-only. This is now performed by \nlego_copy_page_range()\n.\n\n\nThe final write-protect is performed by \nlego_copy_one_pte()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nstatic\n \ninline\n \nint\n \nlego_copy_one_pte\n(..)\n\n\n{\n\n    \n..\n\n    \n/*\n\n\n     * If it\ns a COW mapping, write protect it both\n\n\n     * in the parent and the child\n\n\n     */\n\n    \nif\n \n(\nis_cow_mapping\n(\nvm_flags\n))\n \n{\n\n        \nptep_set_wrprotect\n(\nsrc_pte\n);\n   \n        \npte\n \n=\n \npte_wrprotect\n(\npte\n);\n      \n    \n}\n\n    \n...\n\n\n}\n\n\n\n\n\n\nDuplicate VM Free Pool\n\n\nTODO\n Yutong\n\n\nProcessor Manager\n\n\nBoring implementation details in the processor manager side.\n\n\nEntry Points\n\n\n\n\nfork()\n\n\nvfork()\n\n\nclone()\n\n\nkernel_thread()\n\n\n\n\nAll of them land on \ndo_fork()\n, which is Lego\ns main fork function.\n\n\ndo_fork()\n\n\nThere are mainly three parts within \ndo_fork()\n: \n1)\n \ncopy_process()\n, which duplicates a new task based on \ncurrent\n, including allocate new kernel stack, new task_struct, increase mm reference counter, etc. \n2)\n If we are creating a new process, then tell global monitor or memory manager to let them update bookkeeping and create corresponding data structures. \n3)\n \nwake_up_new_task()\n, which gives away the newly created task to local scheduler.\n\n\ncopy_process()\n\n\nThe routine is kind of boring. It do a lot dirty work to copy information from calling thread to new thread. The most important data structures of course are \ntask_struct\n, \nmm_sturct\n, \nsighand\n, and so on. This section only talks about few of them, and leave others to readers who are interested.\n\n\nSanity Checking\n\n\nMainly check if \nclone_flags\n are passed properly. For example, if user is creating a new thread, that implies certain data structures are shared, cause new thread belongs to the same process with the calling thread. If \nCLONE_THREAD\n is passed, then \nCLONE_SIGHAND\n, \nCLONE_VM\n, and so on must be set as well.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n    \n/*\n\n\n     * Thread groups must share signals as well, and detached threads\n\n\n     * can only be started up within the thread group.\n\n\n     */\n\n    \nif\n \n((\nclone_flags\n \n \nCLONE_THREAD\n)\n \n \n!\n(\nclone_flags\n \n \nCLONE_SIGHAND\n))\n\n        \nreturn\n \nERR_PTR\n(\n-\nEINVAL\n);\n\n\n    \n/*\n\n\n     * Shared signal handlers imply shared VM. By way of the above,\n\n\n     * thread groups also imply shared VM. Blocking this case allows\n\n\n     * for various simplifications in other code.\n\n\n     */\n\n    \nif\n \n((\nclone_flags\n \n \nCLONE_SIGHAND\n)\n \n \n!\n(\nclone_flags\n \n \nCLONE_VM\n))\n\n        \nreturn\n \nERR_PTR\n(\n-\nEINVAL\n);\n\n\n\n\n\n\ndup_task_struct()\n\n\nTwo main things: 1) duplicate a new \ntask_struct\n, 2) duplicate a new kernel stack. x86 is just a weird architecture, the size of \ntask_struct\n depends on the size of fpu. So the allocation and duplication need to callback to x86-specific code to duplicate the task_struct and fpu info.\n\n1\n2\n3\n4\n5\n6\nint\n \narch_dup_task_struct\n(\nstruct\n \ntask_struct\n \n*\ndst\n,\n \nstruct\n \ntask_struct\n \n*\nsrc\n)\n\n\n{\n\n    \nmemcpy\n(\ndst\n,\n \nsrc\n,\n \narch_task_struct_size\n);\n\n\n    \nreturn\n \nfpu__copy\n(\ndst\n-\nthread\n.\nfpu\n,\n \nsrc\n-\nthread\n.\nfpu\n);\n\n\n}\n\n\n\n\n\nThe stack duplication is fairly simple, just copy everything from the old stack to new stack. Of course, it needs to setup the \nthread_info\n to points to this new thread, so the \ncurrent\n macro will work.\n\n1\n2\n3\n4\n5\n6\n7\n8\nstatic\n \nvoid\n \nsetup_thread_stack\n(\nstruct\n \ntask_struct\n \n*\np\n,\n \nstruct\n \ntask_struct\n \n*\norg\n)\n\n\n{\n\n        \n/* Duplicate whole stack! */\n\n        \n*\ntask_thread_info\n(\np\n)\n \n=\n \n*\ntask_thread_info\n(\norg\n);\n\n\n        \n/* Make the `current\n macro work */\n\n        \ntask_thread_info\n(\np\n)\n-\ntask\n \n=\n \np\n;\n\n\n}\n\n\n\n\n\n\ncopy_mm()\n\n\nThis is where threads within a process will share the virtual address space happens. If we are creating a new process, then this function will create a new \nmm_struct\n, and also a new \npgd\n:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n/*\n\n\n * pgd_alloc() will duplicate the identity kernel mapping\n\n\n * but leaves other entries empty:\n\n\n */\n\n\nmm\n-\npgd\n \n=\n \npgd_alloc\n(\nmm\n);\n\n\nif\n \n(\nunlikely\n(\n!\nmm\n-\npgd\n))\n \n{\n\n        \nkfree\n(\nmm\n);\n\n        \nreturn\n \nNULL\n;\n\n\n}\n\n\n\n\n\n\nDuplicate pcache data\n\n\nTODO\n\n\nTODO: hook with pcache\nWe need to duplicate the pcache vm_range array, once Yutong finished the code.\nsetup_sched_fork()\n\n\nCallback to scheduler to setup this new task. It may reset all scheduler related information. Here we also have a chance to change this task\ns scheduler class:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\nint\n \nsetup_sched_fork\n(\nunsigned\n \nlong\n \nclone_flags\n,\n \nstruct\n \ntask_struct\n \n*\np\n)\n\n\n{\n\n        \nint\n \ncpu\n \n=\n \nget_cpu\n();\n\n\n        \n__sched_fork\n(\nclone_flags\n,\n \np\n);\n\n\n        \np\n-\nstate\n \n=\n \nTASK_NEW\n;\n\n        \n...\n\n        \nif\n \n(\nunlikely\n(\np\n-\nsched_reset_on_fork\n))\n \n{\n\n                \nif\n \n(\ntask_has_rt_policy\n(\np\n))\n \n{\n\n                        \np\n-\npolicy\n \n=\n \nSCHED_NORMAL\n;\n\n                        \np\n-\nstatic_prio\n \n=\n \nNICE_TO_PRIO\n(\n0\n);\n\n                        \np\n-\nrt_priority\n \n=\n \n0\n;\n\n                \n}\n \nelse\n \nif\n \n(\nPRIO_TO_NICE\n(\np\n-\nstatic_prio\n)\n \n \n0\n)\n\n                        \np\n-\nstatic_prio\n \n=\n \nNICE_TO_PRIO\n(\n0\n);\n\n\n                \np\n-\nprio\n \n=\n \np\n-\nnormal_prio\n \n=\n \n__normal_prio\n(\np\n);\n\n                \nset_load_weight\n(\np\n);\n\n                \n...\n\n        \n}\n    \n\n        \nif\n \n(\nrt_prio\n(\np\n-\nprio\n))\n\n                \np\n-\nsched_class\n \n=\n \nrt_sched_class\n;\n\n        \nelse\n \n{\n\n                \np\n-\nsched_class\n \n=\n \nfair_sched_class\n;\n\n                \nset_load_weight\n(\np\n);\n\n        \n}\n    \n\n        \n__set_task_cpu\n(\np\n,\n \ncpu\n);\n\n        \nif\n \n(\np\n-\nsched_class\n-\ntask_fork\n)\n\n                \np\n-\nsched_class\n-\ntask_fork\n(\np\n);\n\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nAllocate new pid\n\n\nIn both Lego and Linux, we don\nt allocate new pid for a new thread, if that thread is an \nidle thread\n. So callers of \ndo_fork\n needs to pass something to let \ndo_fork\n know. In Linux, they use \nstruct pid, init_struct_pid\n to check. In Lego, we introduce an new clone_flag \nCLONE_IDLE_THREAD\n. If that flag is set, \ndo_fork()\n will try to allocate a new pid for the new thread. Otherwise, it will be 0:\n\n1\n2\n3\n4\n5\n6\n/* clone idle thread, whose pid is 0 */\n\n\nif\n \n(\n!\n(\nclone_flags\n \n \nCLONE_IDLE_THREAD\n))\n \n{\n\n        \npid\n \n=\n \nalloc_pid\n(\np\n);\n\n        \nif\n \n(\n!\npid\n)\n\n                \ngoto\n \nout_cleanup_thread\n;\n\n\n}\n\n\n\n\n\n\nSo, only the \ninit_idle()\n function can pass this \nCLONE_IDLE_THREAD\n down. All other usages are wrong and should be reported.\n\n\nIn order to avoid conflict with Linux clone_flag, we define it as:\n\n1\n#define CLONE_IDLE_THREAD       0x100000000\n\n\n\n\n\n\nSETTID/CLEARTID\n\n\nThese are some futex related stuff. I will cover these stuff in futex document:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\np\n-\nset_child_tid\n \n=\n \n(\nclone_flags\n \n \nCLONE_CHILD_SETTID\n)\n \n?\n \nchild_tidptr\n \n:\n \nNULL\n;\n\n\n/*  \n\n\n * Clear TID on mm_release()?\n\n\n */\n\n\np\n-\nclear_child_tid\n \n=\n \n(\nclone_flags\n \n \nCLONE_CHILD_CLEARTID\n)\n \n?\n \nchild_tidptr\n \n:\n \nNULL\n;\n\n\n\n#ifdef CONFIG_FUTEX\n\n\np\n-\nrobust_list\n \n=\n \nNULL\n;\n\n\n#endif\n\n\n\n\n\n\ncopy_thread_tls()\n\n\nThis is the most interesting function. Cover later.\n\n\np2m_fork()\n\n\nIn order to track user activities, we need to know when user are going to create new process. Fork is the best time and the only time we kernel know. So, Lego adds this special hook to tell remote global monitor or memory manager that there is a new process going to be created. Upon receiving this message, remote monitor will update its bookkeeping for this specific user/vNode.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n/* Tell remote memory component */\n\n\n#ifdef CONFIG_COMP_PROCESSOR\n\n\nif\n \n(\nclone_flags\n \n \nCLONE_GLOBAL_THREAD\n)\n \n{\n\n        \n...\n\n        \np2m_fork\n(\np\n,\n \nclone_flags\n);\n\n        \n...\n\n\n}\n   \n\n#endif\n\n\n\n\n\n\n\nThe \nCLONE_GLOBAL_THREAD\n should only be set, if the following cases happen:\n\n\n\n\nfork()\n\n\nvfork()\n\n\nclone(), without \nCLONE_THREAD\n being set\n\n\n\n\nIn order to avoid conflict with Linux clone_flag, we define it as:\n\n1\n#define CLONE_GLOBAL_THREAD     0x200000000\n\n\n\n\n\n\nwake_up_new_task()\n\n\nThe last step of \ndo_fork\n is waking up the new thread or process, which is performed by \nwake_up_new_task()\n function. The first question this function will ask is: \nwhich cpu to land?\n The answer comes from \nselect_task_rq()\n:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nstatic\n \ninline\n\n\nint\n \nselect_task_rq\n(\nstruct\n \ntask_struct\n \n*\np\n,\n \nint\n \ncpu\n,\n \nint\n \nsd_flags\n,\n \nint\n \nwake_flags\n)\n\n\n{\n\n        \nif\n \n(\np\n-\nnr_cpus_allowed\n \n \n1\n)\n\n                \ncpu\n \n=\n \np\n-\nsched_class\n-\nselect_task_rq\n(\np\n,\n \ncpu\n,\n \nsd_flags\n,\n \nwake_flags\n);\n\n        \nelse\n\n                \ncpu\n \n=\n \ncpumask_any\n(\np\n-\ncpus_allowed\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nClearly, this is determined by \ncpus_allowed\n, which is the same with its parent at this point. That being said, if the parent is only able to run on one specific CPU, then all its children will end up running on the same CPU when they wake up (they could change their affinity later). This is also the default on Linux: \nA child created via fork(2) inherits its parent\ns CPU affinity mask. The affinity mask is preserved across an execve(2).\n\n\nAfter landing CPU is selected, following operation is simple: just enqueue this task into landing CPU\ns runqueue, and we are done:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nvoid\n \nwake_up_new_task\n(\nstruct\n \ntask_struct\n \n*\np\n)\n\n\n{\n\n        \n...\n\n\n/* Select a CPU for new thread to run */\n\n\n#ifdef CONFIG_SMP\n\n        \n/*   \n\n\n         * Fork balancing, do it here and not earlier because:\n\n\n         *  - cpus_allowed can change in the fork path\n\n\n         *  - any previously selected cpu might disappear through hotplug\n\n\n         */\n\n        \nset_task_cpu\n(\np\n,\n \nselect_task_rq\n(\np\n,\n \ntask_cpu\n(\np\n),\n \nSD_BALANCE_FORK\n,\n \n0\n));\n\n\n#endif\n\n\n        \nrq\n \n=\n \n__task_rq_lock\n(\np\n);\n\n        \nactivate_task\n(\nrq\n,\n \np\n,\n \n0\n);\n\n        \np\n-\non_rq\n \n=\n \nTASK_ON_RQ_QUEUED\n;\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 11, 2018\n\nLast Updated: Feb 27, 2018", 
            "title": "fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#fork", 
            "text": "", 
            "title": "fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#memory-manager", 
            "text": "We need to duplicate the address space in the memory manager side. Follow the traditional  fork()  semantic, both the existing and newly created address space will be write-protected.  Since we have the flexibility to implement any VM organization, we should be careful while duplicating the address space. Currently, we are using page-based VM, thus the duplicating is basically creating a new  pgd  and copy existing pgtables, and further downgrade permission to read-only. This is now performed by  lego_copy_page_range() .  The final write-protect is performed by  lego_copy_one_pte() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 static   inline   int   lego_copy_one_pte (..)  { \n     .. \n     /*       * If it s a COW mapping, write protect it both       * in the parent and the child       */ \n     if   ( is_cow_mapping ( vm_flags ))   { \n         ptep_set_wrprotect ( src_pte );    \n         pte   =   pte_wrprotect ( pte );       \n     } \n     ...  }", 
            "title": "Memory Manager"
        }, 
        {
            "location": "/lego/syscall/fork/#duplicate-vm-free-pool", 
            "text": "TODO  Yutong", 
            "title": "Duplicate VM Free Pool"
        }, 
        {
            "location": "/lego/syscall/fork/#processor-manager", 
            "text": "Boring implementation details in the processor manager side.", 
            "title": "Processor Manager"
        }, 
        {
            "location": "/lego/syscall/fork/#entry-points", 
            "text": "fork()  vfork()  clone()  kernel_thread()   All of them land on  do_fork() , which is Lego s main fork function.", 
            "title": "Entry Points"
        }, 
        {
            "location": "/lego/syscall/fork/#do_fork", 
            "text": "There are mainly three parts within  do_fork() :  1)   copy_process() , which duplicates a new task based on  current , including allocate new kernel stack, new task_struct, increase mm reference counter, etc.  2)  If we are creating a new process, then tell global monitor or memory manager to let them update bookkeeping and create corresponding data structures.  3)   wake_up_new_task() , which gives away the newly created task to local scheduler.", 
            "title": "do_fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#copy_process", 
            "text": "The routine is kind of boring. It do a lot dirty work to copy information from calling thread to new thread. The most important data structures of course are  task_struct ,  mm_sturct ,  sighand , and so on. This section only talks about few of them, and leave others to readers who are interested.", 
            "title": "copy_process()"
        }, 
        {
            "location": "/lego/syscall/fork/#sanity-checking", 
            "text": "Mainly check if  clone_flags  are passed properly. For example, if user is creating a new thread, that implies certain data structures are shared, cause new thread belongs to the same process with the calling thread. If  CLONE_THREAD  is passed, then  CLONE_SIGHAND ,  CLONE_VM , and so on must be set as well.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14      /*       * Thread groups must share signals as well, and detached threads       * can only be started up within the thread group.       */ \n     if   (( clone_flags     CLONE_THREAD )     ! ( clone_flags     CLONE_SIGHAND )) \n         return   ERR_PTR ( - EINVAL ); \n\n     /*       * Shared signal handlers imply shared VM. By way of the above,       * thread groups also imply shared VM. Blocking this case allows       * for various simplifications in other code.       */ \n     if   (( clone_flags     CLONE_SIGHAND )     ! ( clone_flags     CLONE_VM )) \n         return   ERR_PTR ( - EINVAL );", 
            "title": "Sanity Checking"
        }, 
        {
            "location": "/lego/syscall/fork/#dup_task_struct", 
            "text": "Two main things: 1) duplicate a new  task_struct , 2) duplicate a new kernel stack. x86 is just a weird architecture, the size of  task_struct  depends on the size of fpu. So the allocation and duplication need to callback to x86-specific code to duplicate the task_struct and fpu info. 1\n2\n3\n4\n5\n6 int   arch_dup_task_struct ( struct   task_struct   * dst ,   struct   task_struct   * src )  { \n     memcpy ( dst ,   src ,   arch_task_struct_size ); \n\n     return   fpu__copy ( dst - thread . fpu ,   src - thread . fpu );  }   \nThe stack duplication is fairly simple, just copy everything from the old stack to new stack. Of course, it needs to setup the  thread_info  to points to this new thread, so the  current  macro will work. 1\n2\n3\n4\n5\n6\n7\n8 static   void   setup_thread_stack ( struct   task_struct   * p ,   struct   task_struct   * org )  { \n         /* Duplicate whole stack! */ \n         * task_thread_info ( p )   =   * task_thread_info ( org ); \n\n         /* Make the `current  macro work */ \n         task_thread_info ( p ) - task   =   p ;  }", 
            "title": "dup_task_struct()"
        }, 
        {
            "location": "/lego/syscall/fork/#copy_mm", 
            "text": "This is where threads within a process will share the virtual address space happens. If we are creating a new process, then this function will create a new  mm_struct , and also a new  pgd : 1\n2\n3\n4\n5\n6\n7\n8\n9 /*   * pgd_alloc() will duplicate the identity kernel mapping   * but leaves other entries empty:   */  mm - pgd   =   pgd_alloc ( mm );  if   ( unlikely ( ! mm - pgd ))   { \n         kfree ( mm ); \n         return   NULL ;  }", 
            "title": "copy_mm()"
        }, 
        {
            "location": "/lego/syscall/fork/#duplicate-pcache-data", 
            "text": "TODO  TODO: hook with pcache We need to duplicate the pcache vm_range array, once Yutong finished the code.", 
            "title": "Duplicate pcache data"
        }, 
        {
            "location": "/lego/syscall/fork/#setup_sched_fork", 
            "text": "Callback to scheduler to setup this new task. It may reset all scheduler related information. Here we also have a chance to change this task s scheduler class:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 int   setup_sched_fork ( unsigned   long   clone_flags ,   struct   task_struct   * p )  { \n         int   cpu   =   get_cpu (); \n\n         __sched_fork ( clone_flags ,   p ); \n\n         p - state   =   TASK_NEW ; \n         ... \n         if   ( unlikely ( p - sched_reset_on_fork ))   { \n                 if   ( task_has_rt_policy ( p ))   { \n                         p - policy   =   SCHED_NORMAL ; \n                         p - static_prio   =   NICE_TO_PRIO ( 0 ); \n                         p - rt_priority   =   0 ; \n                 }   else   if   ( PRIO_TO_NICE ( p - static_prio )     0 ) \n                         p - static_prio   =   NICE_TO_PRIO ( 0 ); \n\n                 p - prio   =   p - normal_prio   =   __normal_prio ( p ); \n                 set_load_weight ( p ); \n                 ... \n         }     \n\n         if   ( rt_prio ( p - prio )) \n                 p - sched_class   =   rt_sched_class ; \n         else   { \n                 p - sched_class   =   fair_sched_class ; \n                 set_load_weight ( p ); \n         }     \n\n         __set_task_cpu ( p ,   cpu ); \n         if   ( p - sched_class - task_fork ) \n                 p - sched_class - task_fork ( p ); \n\n         ...  }", 
            "title": "setup_sched_fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#allocate-new-pid", 
            "text": "In both Lego and Linux, we don t allocate new pid for a new thread, if that thread is an  idle thread . So callers of  do_fork  needs to pass something to let  do_fork  know. In Linux, they use  struct pid, init_struct_pid  to check. In Lego, we introduce an new clone_flag  CLONE_IDLE_THREAD . If that flag is set,  do_fork()  will try to allocate a new pid for the new thread. Otherwise, it will be 0: 1\n2\n3\n4\n5\n6 /* clone idle thread, whose pid is 0 */  if   ( ! ( clone_flags     CLONE_IDLE_THREAD ))   { \n         pid   =   alloc_pid ( p ); \n         if   ( ! pid ) \n                 goto   out_cleanup_thread ;  }    So, only the  init_idle()  function can pass this  CLONE_IDLE_THREAD  down. All other usages are wrong and should be reported.  In order to avoid conflict with Linux clone_flag, we define it as: 1 #define CLONE_IDLE_THREAD       0x100000000", 
            "title": "Allocate new pid"
        }, 
        {
            "location": "/lego/syscall/fork/#settidcleartid", 
            "text": "These are some futex related stuff. I will cover these stuff in futex document: 1\n2\n3\n4\n5\n6\n7\n8\n9 p - set_child_tid   =   ( clone_flags     CLONE_CHILD_SETTID )   ?   child_tidptr   :   NULL ;  /*     * Clear TID on mm_release()?   */  p - clear_child_tid   =   ( clone_flags     CLONE_CHILD_CLEARTID )   ?   child_tidptr   :   NULL ;  #ifdef CONFIG_FUTEX  p - robust_list   =   NULL ;  #endif", 
            "title": "SETTID/CLEARTID"
        }, 
        {
            "location": "/lego/syscall/fork/#copy_thread_tls", 
            "text": "This is the most interesting function. Cover later.", 
            "title": "copy_thread_tls()"
        }, 
        {
            "location": "/lego/syscall/fork/#p2m_fork", 
            "text": "In order to track user activities, we need to know when user are going to create new process. Fork is the best time and the only time we kernel know. So, Lego adds this special hook to tell remote global monitor or memory manager that there is a new process going to be created. Upon receiving this message, remote monitor will update its bookkeeping for this specific user/vNode.  1\n2\n3\n4\n5\n6\n7\n8 /* Tell remote memory component */  #ifdef CONFIG_COMP_PROCESSOR  if   ( clone_flags     CLONE_GLOBAL_THREAD )   { \n         ... \n         p2m_fork ( p ,   clone_flags ); \n         ...  }     #endif    The  CLONE_GLOBAL_THREAD  should only be set, if the following cases happen:   fork()  vfork()  clone(), without  CLONE_THREAD  being set   In order to avoid conflict with Linux clone_flag, we define it as: 1 #define CLONE_GLOBAL_THREAD     0x200000000", 
            "title": "p2m_fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#wake_up_new_task", 
            "text": "The last step of  do_fork  is waking up the new thread or process, which is performed by  wake_up_new_task()  function. The first question this function will ask is:  which cpu to land?  The answer comes from  select_task_rq() :  1\n2\n3\n4\n5\n6\n7\n8\n9 static   inline  int   select_task_rq ( struct   task_struct   * p ,   int   cpu ,   int   sd_flags ,   int   wake_flags )  { \n         if   ( p - nr_cpus_allowed     1 ) \n                 cpu   =   p - sched_class - select_task_rq ( p ,   cpu ,   sd_flags ,   wake_flags ); \n         else \n                 cpu   =   cpumask_any ( p - cpus_allowed ); \n         ...  }    Clearly, this is determined by  cpus_allowed , which is the same with its parent at this point. That being said, if the parent is only able to run on one specific CPU, then all its children will end up running on the same CPU when they wake up (they could change their affinity later). This is also the default on Linux:  A child created via fork(2) inherits its parent s CPU affinity mask. The affinity mask is preserved across an execve(2).  After landing CPU is selected, following operation is simple: just enqueue this task into landing CPU s runqueue, and we are done:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 void   wake_up_new_task ( struct   task_struct   * p )  { \n         ...  /* Select a CPU for new thread to run */  #ifdef CONFIG_SMP \n         /*              * Fork balancing, do it here and not earlier because:           *  - cpus_allowed can change in the fork path           *  - any previously selected cpu might disappear through hotplug           */ \n         set_task_cpu ( p ,   select_task_rq ( p ,   task_cpu ( p ),   SD_BALANCE_FORK ,   0 ));  #endif \n\n         rq   =   __task_rq_lock ( p ); \n         activate_task ( rq ,   p ,   0 ); \n         p - on_rq   =   TASK_ON_RQ_QUEUED ; \n         ...  }    \nYizhou Shan \nCreated: Feb 11, 2018 \nLast Updated: Feb 27, 2018", 
            "title": "wake_up_new_task()"
        }, 
        {
            "location": "/lego/syscall/getrusage/", 
            "text": "getrusage\n\n\nThe syscall \ngetrusage\n is used to get user program resource usage. It is a nice syscall. But only nice if kernel has all the nice bookkeeping. It is a luxury for us to have all the counting.\n\n\nThe syscall is added recently due to \nwait\n family syscalls, which use and bookkeep some of \nrusage\n.\n\n\nAs on the last updated date (Mar 7), the syscall in Lego only reports number of context switches and a few others.\n\n\n\nYizhou Shan\n\nCreated: Mar 7, 2018\n\nLast Updated: Mar 7, 2018", 
            "title": "getrusage()"
        }, 
        {
            "location": "/lego/syscall/getrusage/#getrusage", 
            "text": "The syscall  getrusage  is used to get user program resource usage. It is a nice syscall. But only nice if kernel has all the nice bookkeeping. It is a luxury for us to have all the counting.  The syscall is added recently due to  wait  family syscalls, which use and bookkeep some of  rusage .  As on the last updated date (Mar 7), the syscall in Lego only reports number of context switches and a few others.  \nYizhou Shan \nCreated: Mar 7, 2018 \nLast Updated: Mar 7, 2018", 
            "title": "getrusage"
        }, 
        {
            "location": "/lego/syscall/wait_and_exit/", 
            "text": "wait4(), waitid(), and exit()\n\n\nLego supports \nwait4()\n and \nwaitid()\n syscalls, and they are compatible with Linux programs. These two syscalls rely on \nexit_notify()\n function when a thread \nexit()\n. Basically, when a thread exit, it will notify its parent, and reparent\n3\n its children if necessary.\n\n\nFacts in Lego:\n\n\n\n\nLego does not have \nprocess group and session\n2\n concept. Each process is within its own process group and session.\n\n\nThis implies Lego will not have \nOrphaned Process Group\n1\n when a process exit.\n\n\nOrphan process\n3\n is adopted by init process (pid 1) if its father is a single-thread process, otherwise it will be adopted by other thread within its father\ns process. This is performed by function \nforget_original_parent()\n.\n\n\nwait, signal, exec, fork are close related.\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 8, 2018\n\nLast Updated: Mar 10, 2018\n\n\n\n\n\n\n\n\n\n\nOrphaned Process Groups\n\n\n\n\n\n\nProcess Group\n\n\n\n\n\n\nOrphan Process", 
            "title": "wait4 and exit()"
        }, 
        {
            "location": "/lego/syscall/wait_and_exit/#wait4-waitid-and-exit", 
            "text": "Lego supports  wait4()  and  waitid()  syscalls, and they are compatible with Linux programs. These two syscalls rely on  exit_notify()  function when a thread  exit() . Basically, when a thread exit, it will notify its parent, and reparent 3  its children if necessary.  Facts in Lego:   Lego does not have  process group and session 2  concept. Each process is within its own process group and session.  This implies Lego will not have  Orphaned Process Group 1  when a process exit.  Orphan process 3  is adopted by init process (pid 1) if its father is a single-thread process, otherwise it will be adopted by other thread within its father s process. This is performed by function  forget_original_parent() .  wait, signal, exec, fork are close related.   \nYizhou Shan \nCreated: Mar 8, 2018 \nLast Updated: Mar 10, 2018      Orphaned Process Groups    Process Group    Orphan Process", 
            "title": "wait4(), waitid(), and exit()"
        }, 
        {
            "location": "/lego/pcache/config/", 
            "text": "Pcache Configuration\n\n\nThis doc explains what configuration options pcache has, and how to config them properly. Pcache is only enabled in Lego\ns processor manager and currently it uses DRAM to emulate the last-level cache (or, L4).\n\n\nKconfig\n\n\nCONFIG_MEMMAP_MEMBLOCK_RESERVED\n\n\nDEFAULT: Y\n\n\nBy default, boot command line option \nmemmap $\n will reserve a range of physical memory.\nThis reserved memory will be marked reserved in e820 table, which\nmeans this range will not be registered into \nmemblock\n. Only memory that has been\nregistered into \nmemblock\n will be assigned \nstruct page\n with it (both \nmemblock.memory\n and \nmemblock.reserve\n will have). And do note that this part of reserved memory can be mapped as 1GB page at boot time.\n\n\nIn other words, by default (the linux semantic), users need to \nioremap\n\nthe \nmemmap $\n reserved physical memory, and use the returned kernel virtual address afterwards.\nAnd do note that the \nioremap()\n only support 4KB mapping.\n\n\nIn Lego, if this option is enabled, the memory marked by \nmemmap $\n will \nNOT\n be marked\nreserved into e820 table, instead, it will be pushed into \nmemblock\n, which means\nit is mapped into kernel direct mapping and has \nstruct page\n.\n\n\nFor those who have done DAX, or NVM related stuff, you must have struggled with\n\nmemmap $\n, and complained why it does not have \nstruct page\n, I guess? So here is\nthe simple code to do so:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nif\n \n(\n*\np\n \n==\n \n@\n)\n \n{\n\n        \nstart_at\n \n=\n \nmemparse\n(\np\n+\n1\n,\n \np\n);\n\n        \ne820_add_region\n(\nstart_at\n,\n \nmem_size\n,\n \nE820_RAM\n);\n\n\n}\n \nelse\n \nif\n \n(\n*\np\n \n==\n \n#\n)\n \n{\n\n        \nstart_at\n \n=\n \nmemparse\n(\np\n+\n1\n,\n \np\n);\n\n        \ne820_add_region\n(\nstart_at\n,\n \nmem_size\n,\n \nE820_ACPI\n);\n\n\n}\n \nelse\n \nif\n \n(\n*\np\n \n==\n \n$\n)\n \n{\n\n        \nstart_at\n \n=\n \nmemparse\n(\np\n+\n1\n,\n \np\n);\n\n\n\n#ifdef CONFIG_MEMMAP_MEMBLOCK_RESERVED\n\n        \nmemblock_reserve\n(\nstart_at\n,\n \nmem_size\n);\n\n\n#else\n\n        \ne820_add_region\n(\nstart_at\n,\n \nmem_size\n,\n \nE820_RESERVED\n);\n\n\n#endif\n\n\n\n\n\n\nBut why we are having this? Because I think the \ndirect 1GB mapping\n may have\nbetter performance: huge page mapping can truly save us a lot TLB misses. However, the real performance number is unknown.\n\n\nIf unsure, say \nY\n.\n\n\n\nYizhou Shan \n\nCreated: Feb 01, 2018\n\nLast Updated: Feb 01, 2018", 
            "title": "Config"
        }, 
        {
            "location": "/lego/pcache/config/#pcache-configuration", 
            "text": "This doc explains what configuration options pcache has, and how to config them properly. Pcache is only enabled in Lego s processor manager and currently it uses DRAM to emulate the last-level cache (or, L4).", 
            "title": "Pcache Configuration"
        }, 
        {
            "location": "/lego/pcache/config/#kconfig", 
            "text": "", 
            "title": "Kconfig"
        }, 
        {
            "location": "/lego/pcache/config/#config_memmap_memblock_reserved", 
            "text": "DEFAULT: Y  By default, boot command line option  memmap $  will reserve a range of physical memory.\nThis reserved memory will be marked reserved in e820 table, which\nmeans this range will not be registered into  memblock . Only memory that has been\nregistered into  memblock  will be assigned  struct page  with it (both  memblock.memory  and  memblock.reserve  will have). And do note that this part of reserved memory can be mapped as 1GB page at boot time.  In other words, by default (the linux semantic), users need to  ioremap \nthe  memmap $  reserved physical memory, and use the returned kernel virtual address afterwards.\nAnd do note that the  ioremap()  only support 4KB mapping.  In Lego, if this option is enabled, the memory marked by  memmap $  will  NOT  be marked\nreserved into e820 table, instead, it will be pushed into  memblock , which means\nit is mapped into kernel direct mapping and has  struct page .  For those who have done DAX, or NVM related stuff, you must have struggled with memmap $ , and complained why it does not have  struct page , I guess? So here is\nthe simple code to do so:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 if   ( * p   ==   @ )   { \n         start_at   =   memparse ( p + 1 ,   p ); \n         e820_add_region ( start_at ,   mem_size ,   E820_RAM );  }   else   if   ( * p   ==   # )   { \n         start_at   =   memparse ( p + 1 ,   p ); \n         e820_add_region ( start_at ,   mem_size ,   E820_ACPI );  }   else   if   ( * p   ==   $ )   { \n         start_at   =   memparse ( p + 1 ,   p );  #ifdef CONFIG_MEMMAP_MEMBLOCK_RESERVED \n         memblock_reserve ( start_at ,   mem_size );  #else \n         e820_add_region ( start_at ,   mem_size ,   E820_RESERVED );  #endif    But why we are having this? Because I think the  direct 1GB mapping  may have\nbetter performance: huge page mapping can truly save us a lot TLB misses. However, the real performance number is unknown.  If unsure, say  Y .  \nYizhou Shan  \nCreated: Feb 01, 2018 \nLast Updated: Feb 01, 2018", 
            "title": "CONFIG_MEMMAP_MEMBLOCK_RESERVED"
        }, 
        {
            "location": "/lego/pcache/sweep/", 
            "text": "Pcache Sweep\n\n\nSome notes while coding pcache sweep thread. The sweep thread wants to detect the hotness of pages, and then adjust LRU list accordingly.\n\n\nData Worth a Billion\n\n\nPcache-reclaim, or any other object reclaim, need some \ndata\n to algorithm about. So specific algorithm can select the \nbest\n candidate to reclaim. In reality, algorithms are designed quite well, but \nhow to get the data\n part becomes extremely hard. I think this applies to many different systems.\n\n\nFor example, to select the hot pages in x86 is notorious hard because x86 hardware only provides a \nReferenced\n bit for system software to reason about. To make it worse, \nReferenced\n bit is cached in TLB, which means CPU will \nNOT\n  set the \nReferenced\n bit even you reset in PTE, because CPU think the bit is already set. In order to get an \naccurate\n hot pages tracking, you probably need a TLB flush after reset \nReferenced\n bit.\u00a0But, are you kidding me, a TLB flush after each reset? We have to say NO here. The Linux code explains it well:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nstatic\n \ninline\n \nint\n \nptep_clear_flush_young\n(\npte_t\n \n*\nptep\n)\n\n\n{\n\n        \n/*\n\n\n         * On x86 CPUs, clearing the accessed bit without a TLB flush\n\n\n         * doesn\nt cause data corruption. [ It could cause incorrect\n\n\n         * page aging and the (mistaken) reclaim of hot pages, but the\n\n\n         * chance of that should be relatively low. ]\n\n\n         *\n\n\n         * So as a performance optimization don\nt flush the TLB when\n\n\n         * clearing the accessed bit, it will eventually be flushed by\n\n\n         * a context switch or a VM operation anyway. [ In the rare\n\n\n         * event of it not getting flushed for a long time the delay\n\n\n         * shouldn\nt really matter because there\ns no real memory\n\n\n         * pressure for swapout to react to. ]\n\n\n         */\n\n        \nreturn\n \nptep_test_and_clear_young\n(\nptep\n);\n\n\n}\n\n\n\n\n\n\nAggressiveness\n\n\nAn aggressive sweep algorithm will disturb the normal operations a lot. In Lego, there 4 main factors that define the aggressiveness:\n\n\n\n\nTime interval between each run\n\n\nNumber of sets to look at during each run\n\n\nSkip if it is not full\n\n\nSkip if it is under eviction\n\n\n\n\n\n\nNumber of lines to look at for each set\n\n\nSmaller or equal to associativity\n\n\n\n\n\n\nNumber of lines to adjust for each set\n\n\nSmaller or equal to lines to look at\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 18, 2018\n\nLast Updated: Mar 18, 2018", 
            "title": "Sweep"
        }, 
        {
            "location": "/lego/pcache/sweep/#pcache-sweep", 
            "text": "Some notes while coding pcache sweep thread. The sweep thread wants to detect the hotness of pages, and then adjust LRU list accordingly.", 
            "title": "Pcache Sweep"
        }, 
        {
            "location": "/lego/pcache/sweep/#data-worth-a-billion", 
            "text": "Pcache-reclaim, or any other object reclaim, need some  data  to algorithm about. So specific algorithm can select the  best  candidate to reclaim. In reality, algorithms are designed quite well, but  how to get the data  part becomes extremely hard. I think this applies to many different systems.  For example, to select the hot pages in x86 is notorious hard because x86 hardware only provides a  Referenced  bit for system software to reason about. To make it worse,  Referenced  bit is cached in TLB, which means CPU will  NOT   set the  Referenced  bit even you reset in PTE, because CPU think the bit is already set. In order to get an  accurate  hot pages tracking, you probably need a TLB flush after reset  Referenced  bit.\u00a0But, are you kidding me, a TLB flush after each reset? We have to say NO here. The Linux code explains it well:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 static   inline   int   ptep_clear_flush_young ( pte_t   * ptep )  { \n         /*           * On x86 CPUs, clearing the accessed bit without a TLB flush           * doesn t cause data corruption. [ It could cause incorrect           * page aging and the (mistaken) reclaim of hot pages, but the           * chance of that should be relatively low. ]           *           * So as a performance optimization don t flush the TLB when           * clearing the accessed bit, it will eventually be flushed by           * a context switch or a VM operation anyway. [ In the rare           * event of it not getting flushed for a long time the delay           * shouldn t really matter because there s no real memory           * pressure for swapout to react to. ]           */ \n         return   ptep_test_and_clear_young ( ptep );  }", 
            "title": "Data Worth a Billion"
        }, 
        {
            "location": "/lego/pcache/sweep/#aggressiveness", 
            "text": "An aggressive sweep algorithm will disturb the normal operations a lot. In Lego, there 4 main factors that define the aggressiveness:   Time interval between each run  Number of sets to look at during each run  Skip if it is not full  Skip if it is under eviction    Number of lines to look at for each set  Smaller or equal to associativity    Number of lines to adjust for each set  Smaller or equal to lines to look at     \nYizhou Shan \nCreated: Mar 18, 2018 \nLast Updated: Mar 18, 2018", 
            "title": "Aggressiveness"
        }, 
        {
            "location": "/lego/pcache/tlb/", 
            "text": "TLB Coherence\n\n\nx86 does not keep TLB coherent across cores, nor with in-memory page table. And that is why we need explicit TLB flush when some PTE modifications happen (e.g. downgrade RW to RO, clear PTE, etc.). Besides, TLB flush is very important and affect application correctness. I\nve had some really awful debugging experience which was eventually introduced by missed TLB flush. Below is a list of operations that should have TLB flush followed:\n\n\n\n\nmunmap\n (optional, can be optimized by holding the old VA range)\n\n\nmremap\n (required)\n\n\nfork (RW-\nRO)\n (required)\n\n\nCoW (RO-\nRW)\n (required)\n\n\nmprotect\n (required)\n\n\nmigration\n (required)\n\n\n\n\nUnfortunately, TLB flush is costly, especially if we need to shootdown TLB entries on remote core. TLB shootdown\n1\n2\n3\n is performed by sending IPI to remote core, and remote core will flush local TLB entries within its handler. Linux optimize this by batching TLB flush until context switch happens. Lego currently does not have this nice feature, we flush TLB one by one for each PTE change (listed as \nTODO\n).\n\n\n\nYizhou Shan\n\nCreated: Mar 19, 2018\n\nLast Updated: Mar 19, 2018\n\n\n\n\n\n\n\n\n\n\nOptimizing the TLB Shootdown Algorithm with Page Access Tracking, ATC\n18\n\n\n\n\n\n\nLATR: Lazy Translation Coherence, ASPLOS\n18\n\n\n\n\n\n\nHardware Translation Coherence for Virtualized Systems, ISCA\n17", 
            "title": "TLB Coherence"
        }, 
        {
            "location": "/lego/pcache/tlb/#tlb-coherence", 
            "text": "x86 does not keep TLB coherent across cores, nor with in-memory page table. And that is why we need explicit TLB flush when some PTE modifications happen (e.g. downgrade RW to RO, clear PTE, etc.). Besides, TLB flush is very important and affect application correctness. I ve had some really awful debugging experience which was eventually introduced by missed TLB flush. Below is a list of operations that should have TLB flush followed:   munmap  (optional, can be optimized by holding the old VA range)  mremap  (required)  fork (RW- RO)  (required)  CoW (RO- RW)  (required)  mprotect  (required)  migration  (required)   Unfortunately, TLB flush is costly, especially if we need to shootdown TLB entries on remote core. TLB shootdown 1 2 3  is performed by sending IPI to remote core, and remote core will flush local TLB entries within its handler. Linux optimize this by batching TLB flush until context switch happens. Lego currently does not have this nice feature, we flush TLB one by one for each PTE change (listed as  TODO ).  \nYizhou Shan \nCreated: Mar 19, 2018 \nLast Updated: Mar 19, 2018      Optimizing the TLB Shootdown Algorithm with Page Access Tracking, ATC 18    LATR: Lazy Translation Coherence, ASPLOS 18    Hardware Translation Coherence for Virtualized Systems, ISCA 17", 
            "title": "TLB Coherence"
        }, 
        {
            "location": "/lego/pcache/pgtable-lock/", 
            "text": "Fine-grain Page Table Lock\n\n\nIn old Linux or previous Lego, user page table operations, such as set, clear, are protected by \nmm-\npage_table_lock\n. This one single lock prohibits a lot parallelisms on big SMP machines. An ideal solution is to have finer-granularity locks, so that faults on different parts of the user address space can be handled with less contention.\n\n\nBut finer-granularity locks means you need more memory for the locks. This is a simple trade-off. Lego currently mimic the Linux x86 default setting\n1\n, where each PMD \nand\n PTE page table pages has their own lock. The lock is a spinlock embedded in the \nstruct page\n. As illustrated below:\n\n\n\n\nBoth Processor and Memory managers are using the same mechanism to increase parallelism. And it is something that can improve performance \na lot\n.\n\n\n\nYizhou Shan\n\nCreated: Mar 22, 2018\n\nLast Updated: April 13, 2018\n\n\n\n\n\n\n\n\n\n\nSplit page table locks", 
            "title": "PageTable Lock"
        }, 
        {
            "location": "/lego/pcache/pgtable-lock/#fine-grain-page-table-lock", 
            "text": "In old Linux or previous Lego, user page table operations, such as set, clear, are protected by  mm- page_table_lock . This one single lock prohibits a lot parallelisms on big SMP machines. An ideal solution is to have finer-granularity locks, so that faults on different parts of the user address space can be handled with less contention.  But finer-granularity locks means you need more memory for the locks. This is a simple trade-off. Lego currently mimic the Linux x86 default setting 1 , where each PMD  and  PTE page table pages has their own lock. The lock is a spinlock embedded in the  struct page . As illustrated below:   Both Processor and Memory managers are using the same mechanism to increase parallelism. And it is something that can improve performance  a lot .  \nYizhou Shan \nCreated: Mar 22, 2018 \nLast Updated: April 13, 2018      Split page table locks", 
            "title": "Fine-grain Page Table Lock"
        }, 
        {
            "location": "/lego/pcache/victim/", 
            "text": "Victim Cache\n\n\n\nYizhou Shan\n\nCreated: Mar 12, 2018\n\nLast Updated: Mar 12, 2018", 
            "title": "Victim Cache"
        }, 
        {
            "location": "/lego/pcache/victim/#victim-cache", 
            "text": "Yizhou Shan \nCreated: Mar 12, 2018 \nLast Updated: Mar 12, 2018", 
            "title": "Victim Cache"
        }, 
        {
            "location": "/lego/pcache/virtual_cache/", 
            "text": "Virtual Cache\n\n\n\n\nSynonymous\n\n\nimpact Cache coherence\n\n\nimpact TLB shootdown\n\n\nThe good thing is, synonymous actually happen very rare in really workload. But when OS is invoked, it actually creates a lot synonymous. Because the physical page is mapped both low user virtual address and high kernel virtual address.\n\n\nWhat is this?\n\n\nWhat is bad about this?\n\n\nWhen does this happen? (All cases: kernel, shared vma mapping)\n\n\nHow to solve this?\n\n\nSoftware solution: OS level detection, global virtual address, identical virtual address etc.\n\n\nHardware solution: detect and manage at runtime. Back pointer, Dynamic synonymous remapping, reverse mapping. Similar ideas.\n\n\nA nice summary can be found on \nA new perspective for efficient virtual-cache coherence, ISCA\n13\n.\n\n\n\n\n\n\n\n\n\n\n\n\nLet me share my reading list. I think I\nve collected most of the important virtual cache papers:\n\n\n\n\n\n\n\nTLB and cache line lifetime:\n\n\n\n\nEnigma, ISC\n10\n: For each case where valid data exists in the cache hierarchy without a corresponding valid translation entry, systems with physically-tagged caches have to resolve the translation miss. Only after the page table has been \u201cwalked\u201d and a valid translation entry installed can the already cache-resident data be provided to the processing core. Especially in the faster levels of cache, the additional page table walk can add significant latency to what otherwise would have been a low-latency cache hit.\n\n\n\nGPU virtual cache, ASPLOS\n18\n: We notice that the per-CU TLB miss ratio is high; however, many TLB misses hit in the GPU caches. Only 34% of references that miss in the 32-entry per-CU L1 TLB are also L2 cache misses and access main memory (blue bars). An average of 31% of total per-CU TLB misses find the corresponding data in private L1 caches (black bars), and an additional 35% of the total misses hit in a shared L2 virtual cache (red bars). These hits occur because blocks in the cache hierarchy are likely to reside longer than the lifetime of the corresponding per-CU TLB entries\n\n\n\n\n\n\n\n\n\nReading the \nGPU virtual cache ASPLOS\n18\n paper today. I mostly interested in how they handle synonymous and mremap issue.\n\n\n\n\nSynonymous:\n\n\nTheir solution for synonymous is quite simple (not sure if practical or effective): use a \nleading\n virtual address, which is the first VA that has the virtual cache miss. Subsequent misses that from \ndifferent\n VA will not have the their cache lines filled, instead, they will make subsequent VA forever miss, and fetch the content from the leading VA cache line (they call it replay). In all, synonymous is solved by only having one cache line, and does not fill other VA cache lines.\n\n\nmremap:\n\n\nThey did not mention mremap. But I guess they do not need to care this. When remap happens, the original PTE is invalidated first, and TLB shootdown follows, all they need to do is to invalidate the virtual cache line (need to be flushed back to memory if dirty). When the new VA mapping established and accessed, it will be a normal virtual cache miss\n\n\nOVC also does not need to care about this because they are doing a similar way (I guess).\n\n\nLego need to handle mremap differently. Because we don\nt want to flush the dirty line back to memory, to save 1) one clflush, 2) another pcache miss. This means Lego wants to keep the content in Pcache. So the set_index of new VA and old VA matters in our case.\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 28, 2018\n\nLast Updated: Mar 29, 2018", 
            "title": "Virtual Cache"
        }, 
        {
            "location": "/lego/pcache/virtual_cache/#virtual-cache", 
            "text": "Synonymous  impact Cache coherence  impact TLB shootdown  The good thing is, synonymous actually happen very rare in really workload. But when OS is invoked, it actually creates a lot synonymous. Because the physical page is mapped both low user virtual address and high kernel virtual address.  What is this?  What is bad about this?  When does this happen? (All cases: kernel, shared vma mapping)  How to solve this?  Software solution: OS level detection, global virtual address, identical virtual address etc.  Hardware solution: detect and manage at runtime. Back pointer, Dynamic synonymous remapping, reverse mapping. Similar ideas.  A nice summary can be found on  A new perspective for efficient virtual-cache coherence, ISCA 13 .       Let me share my reading list. I think I ve collected most of the important virtual cache papers:    TLB and cache line lifetime:   Enigma, ISC 10 : For each case where valid data exists in the cache hierarchy without a corresponding valid translation entry, systems with physically-tagged caches have to resolve the translation miss. Only after the page table has been \u201cwalked\u201d and a valid translation entry installed can the already cache-resident data be provided to the processing core. Especially in the faster levels of cache, the additional page table walk can add significant latency to what otherwise would have been a low-latency cache hit.  GPU virtual cache, ASPLOS 18 : We notice that the per-CU TLB miss ratio is high; however, many TLB misses hit in the GPU caches. Only 34% of references that miss in the 32-entry per-CU L1 TLB are also L2 cache misses and access main memory (blue bars). An average of 31% of total per-CU TLB misses find the corresponding data in private L1 caches (black bars), and an additional 35% of the total misses hit in a shared L2 virtual cache (red bars). These hits occur because blocks in the cache hierarchy are likely to reside longer than the lifetime of the corresponding per-CU TLB entries     Reading the  GPU virtual cache ASPLOS 18  paper today. I mostly interested in how they handle synonymous and mremap issue.   Synonymous:  Their solution for synonymous is quite simple (not sure if practical or effective): use a  leading  virtual address, which is the first VA that has the virtual cache miss. Subsequent misses that from  different  VA will not have the their cache lines filled, instead, they will make subsequent VA forever miss, and fetch the content from the leading VA cache line (they call it replay). In all, synonymous is solved by only having one cache line, and does not fill other VA cache lines.  mremap:  They did not mention mremap. But I guess they do not need to care this. When remap happens, the original PTE is invalidated first, and TLB shootdown follows, all they need to do is to invalidate the virtual cache line (need to be flushed back to memory if dirty). When the new VA mapping established and accessed, it will be a normal virtual cache miss  OVC also does not need to care about this because they are doing a similar way (I guess).  Lego need to handle mremap differently. Because we don t want to flush the dirty line back to memory, to save 1) one clflush, 2) another pcache miss. This means Lego wants to keep the content in Pcache. So the set_index of new VA and old VA matters in our case.     \nYizhou Shan \nCreated: Mar 28, 2018 \nLast Updated: Mar 29, 2018", 
            "title": "Virtual Cache"
        }, 
        {
            "location": "/lego/pcache/rmap/", 
            "text": "Reverse Mapping of Pcache\n\n\nThis document explains Lego\ns reverse mapping design for pcache. We also present Lego internal functions that eventually manipulate rmap data structures.\nFor readers who are not familiar with reverse mapping, I recommend you search \nwhat is rmap in Linux\n first.\n\n\nDesign\n\n\nThe reverse mapping, or rmap, of our pcache is implemented in a very basic and\nstraightforward way: pointing back to all page table entries (ptes) directly.\nShared pcache lines will have a list of ptes that point to this pcache line.\nWe also did this way in Hotpot.\n\n\nrmap is used by \n1)\n a bunch of syscalls, such as \nfork()\n, \nexecv()\n, \nmmap()\n,\n\nmunmap()\n, \nmremap()\n, \nbrk()\n. \n2)\n page reclaim, which needs to unmap all ptes for a\ngiven swapped page. Other than \nfork()\n and \nexecv()\n, other vm related syscalls\nare invoked very frequently for a typical datacenter application. Moreover, page\nreclaim and swap also run concurrently to gain exclusive access to rmap.\n\n\nSo, rmap operations have to be fast. Directly pointing to pte seems the best\nsolution here. However, this fine-granularity design will consume a lot memory\nfor the per-pte list.\nFurthermore, vma creation, deletion, split and merge happen frequently, the overhead\nto manage rmap is quite high. No wonder Linux choses another object-based way to do so,\nwhich leverages vma itself to take a longer path towards pte.\n\n\nThe important question is: \ndoes this naive solution fit \ncurrent\n Lego?\n\n\nYes, it fits, for several reasons. \n1)\n Current Lego run static-linked ELF binary only,\nthus there will not be any shared hot library pages, which implies rmap list maintenance\nis simplified. \n2)\n Our targeted applications\nmostly are single process. Even for multiple process ones, the number of processes\nstay stable and \nfork()\n happen at early init time. \n3)\n major users of rmap such\nas \nmremap()\n and \nmunmap()\n  perform rmap operation explicitly, \nmmap()\n perform\nrmap implicitly via pgfault (or pcache miss), \npcache reclaim\n perform sweep async.\nAll of them, combined with 1) and 2), most of the time will perform rmap operation\non a single pte.\n\n\nInternal\n\n\nThe following table describes different contexts that manipulate rmap data structures. Currently, rmap only has four possible operations. The context field describes the large context that trigger such rmap operation. The related functions and pcache callback field lists functions that actually did the dirty work.\n\n\n\n\n\n\n\n\nrmap operation\n\n\nContext\n\n\nRelated functions and pcache callback\n\n\n\n\n\n\n\n\n\n\nAdd\n\n\nfork()\n \npgfault\n\n\ncopy_pte_range()\n -\n \npcache_copy_pte()\n \n \npcache_add_rmap()\n\n\n\n\n\n\nRemove\n\n\nmunmap()\n \n \nexit_mmap()\n \n \nwrite_protected\n\n\nzap_pte_range()\n -\n \npcache_zap_pte()\n \n \npcache_do_wp\n -\n \npcache_remove_rmap\n\n\n\n\n\n\nUpdate\n\n\nmremap()\n\n\nmove_ptes()\n -\n \npcache_move_pte()\n\n\n\n\n\n\nLookup\n\n\npcache eviction sweep, etc.\n\n\npcache_referenced()\n, \npcache_wrprotect()\n \n \npcache_try_to_unmap()\n\n\n\n\n\n\n\n\nEach rmap holds one refcount of pcache. The refcount is increased after \npcache_add_rmap\n, and must be decreased after removing pcache rmap, can from \npcache_remove_rmap\n, \npcache_zap_pte\n or \npcache_move_pte_slowpath\n.\n\n\nThought\n\n\nOne function I personally love the most is \nrmap_walk()\n, whose name pretty much tells the story. To use \nrmap_walk()\n, caller passes a \nstruct rmap_walk_control\n, which including caller specific callback for each rmap. This function also isolates the specific data structures used by rmap from various callers. In Lego, a lot pcache functions are built upon \nrmap_walk()\n.\n\n\nstruct rmap_walk_control\n, or \nstruct scan_control\n, or \nstruct something_control\n are used a lot by Linux kernel. Personally I do love this way of doing data structure walk, or reuse functions. However, even this way can greatly reduce duplicated code size, it will make the code unnecessary complex. As a system developer, no more expects to see a function longer than 100 lines. People love saying: \nDo one thing and do it better\n, while it not always works that perfectly. Coding is nothing different life, it is all about trade-off.\n\n\n\nYizhou Shan \n\nCreated: Feb 02, 2018\n\nLast Updated: Mar 10, 2018", 
            "title": "Reverse Mapping"
        }, 
        {
            "location": "/lego/pcache/rmap/#reverse-mapping-of-pcache", 
            "text": "This document explains Lego s reverse mapping design for pcache. We also present Lego internal functions that eventually manipulate rmap data structures.\nFor readers who are not familiar with reverse mapping, I recommend you search  what is rmap in Linux  first.", 
            "title": "Reverse Mapping of Pcache"
        }, 
        {
            "location": "/lego/pcache/rmap/#design", 
            "text": "The reverse mapping, or rmap, of our pcache is implemented in a very basic and\nstraightforward way: pointing back to all page table entries (ptes) directly.\nShared pcache lines will have a list of ptes that point to this pcache line.\nWe also did this way in Hotpot.  rmap is used by  1)  a bunch of syscalls, such as  fork() ,  execv() ,  mmap() , munmap() ,  mremap() ,  brk() .  2)  page reclaim, which needs to unmap all ptes for a\ngiven swapped page. Other than  fork()  and  execv() , other vm related syscalls\nare invoked very frequently for a typical datacenter application. Moreover, page\nreclaim and swap also run concurrently to gain exclusive access to rmap.  So, rmap operations have to be fast. Directly pointing to pte seems the best\nsolution here. However, this fine-granularity design will consume a lot memory\nfor the per-pte list.\nFurthermore, vma creation, deletion, split and merge happen frequently, the overhead\nto manage rmap is quite high. No wonder Linux choses another object-based way to do so,\nwhich leverages vma itself to take a longer path towards pte.  The important question is:  does this naive solution fit  current  Lego?  Yes, it fits, for several reasons.  1)  Current Lego run static-linked ELF binary only,\nthus there will not be any shared hot library pages, which implies rmap list maintenance\nis simplified.  2)  Our targeted applications\nmostly are single process. Even for multiple process ones, the number of processes\nstay stable and  fork()  happen at early init time.  3)  major users of rmap such\nas  mremap()  and  munmap()   perform rmap operation explicitly,  mmap()  perform\nrmap implicitly via pgfault (or pcache miss),  pcache reclaim  perform sweep async.\nAll of them, combined with 1) and 2), most of the time will perform rmap operation\non a single pte.", 
            "title": "Design"
        }, 
        {
            "location": "/lego/pcache/rmap/#internal", 
            "text": "The following table describes different contexts that manipulate rmap data structures. Currently, rmap only has four possible operations. The context field describes the large context that trigger such rmap operation. The related functions and pcache callback field lists functions that actually did the dirty work.     rmap operation  Context  Related functions and pcache callback      Add  fork()   pgfault  copy_pte_range()  -   pcache_copy_pte()     pcache_add_rmap()    Remove  munmap()     exit_mmap()     write_protected  zap_pte_range()  -   pcache_zap_pte()     pcache_do_wp  -   pcache_remove_rmap    Update  mremap()  move_ptes()  -   pcache_move_pte()    Lookup  pcache eviction sweep, etc.  pcache_referenced() ,  pcache_wrprotect()     pcache_try_to_unmap()     Each rmap holds one refcount of pcache. The refcount is increased after  pcache_add_rmap , and must be decreased after removing pcache rmap, can from  pcache_remove_rmap ,  pcache_zap_pte  or  pcache_move_pte_slowpath .", 
            "title": "Internal"
        }, 
        {
            "location": "/lego/pcache/rmap/#thought", 
            "text": "One function I personally love the most is  rmap_walk() , whose name pretty much tells the story. To use  rmap_walk() , caller passes a  struct rmap_walk_control , which including caller specific callback for each rmap. This function also isolates the specific data structures used by rmap from various callers. In Lego, a lot pcache functions are built upon  rmap_walk() .  struct rmap_walk_control , or  struct scan_control , or  struct something_control  are used a lot by Linux kernel. Personally I do love this way of doing data structure walk, or reuse functions. However, even this way can greatly reduce duplicated code size, it will make the code unnecessary complex. As a system developer, no more expects to see a function longer than 100 lines. People love saying:  Do one thing and do it better , while it not always works that perfectly. Coding is nothing different life, it is all about trade-off.  \nYizhou Shan  \nCreated: Feb 02, 2018 \nLast Updated: Mar 10, 2018", 
            "title": "Thought"
        }, 
        {
            "location": "/lego/pcache/evict_and_ref/", 
            "text": "Mumble pcache eviction and refcount\n\n\nThis is about how Lego is doing eviction against live references of pcache. Unlike the \ngarbage collection\n where it only reclaims object that has no references, pcache eviction may try to evict a pcache that is currently being used by another thread. Both parties need to be very careful. A tricky business.\n\n\nTo describe the issue in a high-level, let us consider this case: the system now has two threads running on two different cores. The first thread try to evict a pcache line, and it truly find a candidate and prepare to evict. Meanwhile, the other thread is currently using this pcache line to do some operations such as \nzap_rmap()\n. If the first thread evict the pcache line without synchronization with the second thread, oops, the second thread is playing with a wrong pcache.\n\n\nThe textbook idea is adding refcount. However, this is not enough in C. Because:\n\n\n\n\nThere is no way to prevent the second thread from getting the \npointer\n to that pcm.\n\n\nA simple \ninc_refcount()\n from the second thread can happen anytime in the middle of first thread\ns eviction.\n\n\n\n\nSolutions:\n\n\n\n\nTo actually prevent the second thread from getting the pointer, we should think about \nhow\n it get the pointer? Luckily, in Lego, there is only one entry point, which is from \npte to pcm\n (aka. pcache_meta). So to synchronize pte change becomes very important. Luckily, we are doing pte_lock before getting the pcm. So this simple pte lock ensures the second thread a safe, will-not-be-evicted pcm (of course, with some other checkings). This idea can also be generalized to any data structures that need pointer references: \nprotect your pointer\n!\n\n\nRefcount checking is also necessary. In the eviction routine, we need to use  \natomic_xchg\n to reset the refcount. If this fails, it means someone else is using it. Do note, this \natomic_xchg\n is carried out with pcm locked. Thus the ordering of locking, get/put matters in the code.\n\n\n\n\nThe code itself tells a much more complete story, I strongly recommend you read the code if you are interested. Here I will list the most interesting part. For the other users except eviction, they need to do this:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\npcm\n \n=\n \npte_to_pcache_meta\n(\nptent\n);\n\n\n/*   \n\n\n * We have a strict lock ordering everyone should obey:\n\n\n *      lock pcache\n\n\n *      lock pte\n\n\n * The caller already locked pte, thus we should avoid deadlock here\n\n\n * by droping pte lock first and then acquire both of them in order.\n\n\n */\n\n\nif\n \n(\nunlikely\n(\n!\ntrylock_pcache\n(\npcm\n)))\n \n{\n\n    \n/* in case it got evicted and @pcm becomes invalid */\n\n    \nget_pcache\n(\npcm\n);\n\n\n    \n/*\n\n\n     * Once we release the pte lock, this pcm may be\n\n\n     * unmapped by another thread who is doing eviction.\n\n\n     * Since we have grabbed one extra ref above, so even\n\n\n     * it is unmapped, eviction thread will not fail to free it.\n\n\n     */\n\n    \nspin_unlock\n(\nptl\n);\n\n\n    \nlock_pcache\n(\npcm\n);\n\n    \nspin_lock\n(\nptl\n);\n\n\n    \n/*   \n\n\n     * Since we dropped the lock, the pcache line might\n\n\n     * be got evicted in the middle.\n\n\n     */\n\n\n    \nif\n \n(\n!\npte_same\n(\n*\npte\n,\n \nptent\n))\n \n{\n\n\n        \nunlock_pcache\n(\npcm\n);\n\n        \n/*   \n\n\n         * This put maybe decreases the ref to 0\n\n\n         * and eventually free the pcache line.\n\n\n         * This happens if the @pcm was selected\n\n\n         * to be evicted at the same time.\n\n\n         */\n\n        \nput_pcache\n(\npcm\n);\n\n        \nreturn\n \n-\nEAGAIN\n;\n\n    \n}\n    \n    \nput_pcache\n(\npcm\n);\n\n\n}\n\n\n\n\n\n\nAs for the eviction thread, it needs to make sure it is the last user using this pcm:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n/*  \n\n\n * Each rmap counts one refcount, plus the one grabbed\n\n\n * during evict_find_line(), we should have (nr_mapped + 1)\n\n\n * here if there are no any other users.\n\n\n *\n\n\n * Furthurmore, others can not go from munmap/mremap/wp to\n\n\n * put_pcache() within pcache_zap_pte(), pcache_move_pte()\n\n\n * or pcache_do_wp_page(). Thus the refcount must larger or\n\n\n * equal to (nr_mapped + 1).\n\n\n *\n\n\n * But if there truly other users (refcount \n nr_mapped + 1),\n\n\n * then we should manually sub the refcount. The other users\n\n\n * which are currently holding the ref, will free the pcache\n\n\n * once it call put_pcache.\n\n\n */\n\n\nPCACHE_BUG_ON_PCM\n(\npcache_ref_count\n(\npcm\n)\n \n \nnr_mapped\n \n+\n \n1\n,\n \npcm\n);\n\n\nif\n \n(\nunlikely\n(\n!\npcache_ref_freeze\n(\npcm\n,\n \nnr_mapped\n \n+\n \n1\n)))\n \n{\n\n    \nif\n \n(\nunlikely\n(\npcache_ref_sub_and_test\n(\npcm\n,\n \nnr_mapped\n \n+\n \n1\n)))\n \n{\n\n        \npr_info\n(\nBUG: pcm refcount, nr_mapped: %d\n\\n\n,\n \nnr_mapped\n);\n\n        \ndump_pcache_meta\n(\npcm\n,\n \nref error\n);\n\n        \nBUG\n();\n\n    \n}\n   \n\n    \nClearPcacheReclaim\n(\npcm\n);\n\n    \nadd_to_lru_list\n(\npcm\n,\n \npset\n);\n\n    \nunlock_pcache\n(\npcm\n);\n\n\n    \ninc_pcache_event\n(\nPCACHE_EVICTION_EAGAIN_CONCURRENT\n);\n\n    \nreturn\n \nPCACHE_EVICT_EAGAIN_CONCURRENT\n;\n\n\n}\n\n\n\n\n\n\n\n\nMy personal thought: live eviction against live objects/references is very hard. You first need to use refcount to ensure a correct ordering. You also need to have a way to prevent others from using the going-to-be-evicted pointer, or have a way to detect a under-use pointer.  In this Lego pcache case, we use the combination of pte lock, pcache lock, and pcache refcount, to ensure everyone is safe. And all these is quite similar to Linux page operations. I learned a lot from its code. But I still not fully understand how it ensures the page is not used by others, it has way more parties than lego that can use the page at the same time of eviction. Magic kernel folks.\n\n\n\nYizhou Shan \n\nCreated: Mar 15, 2018\n\nLast Updated: Mar 16, 2018", 
            "title": "Evict and Refcount"
        }, 
        {
            "location": "/lego/pcache/evict_and_ref/#mumble-pcache-eviction-and-refcount", 
            "text": "This is about how Lego is doing eviction against live references of pcache. Unlike the  garbage collection  where it only reclaims object that has no references, pcache eviction may try to evict a pcache that is currently being used by another thread. Both parties need to be very careful. A tricky business.  To describe the issue in a high-level, let us consider this case: the system now has two threads running on two different cores. The first thread try to evict a pcache line, and it truly find a candidate and prepare to evict. Meanwhile, the other thread is currently using this pcache line to do some operations such as  zap_rmap() . If the first thread evict the pcache line without synchronization with the second thread, oops, the second thread is playing with a wrong pcache.  The textbook idea is adding refcount. However, this is not enough in C. Because:   There is no way to prevent the second thread from getting the  pointer  to that pcm.  A simple  inc_refcount()  from the second thread can happen anytime in the middle of first thread s eviction.   Solutions:   To actually prevent the second thread from getting the pointer, we should think about  how  it get the pointer? Luckily, in Lego, there is only one entry point, which is from  pte to pcm  (aka. pcache_meta). So to synchronize pte change becomes very important. Luckily, we are doing pte_lock before getting the pcm. So this simple pte lock ensures the second thread a safe, will-not-be-evicted pcm (of course, with some other checkings). This idea can also be generalized to any data structures that need pointer references:  protect your pointer !  Refcount checking is also necessary. In the eviction routine, we need to use   atomic_xchg  to reset the refcount. If this fails, it means someone else is using it. Do note, this  atomic_xchg  is carried out with pcm locked. Thus the ordering of locking, get/put matters in the code.   The code itself tells a much more complete story, I strongly recommend you read the code if you are interested. Here I will list the most interesting part. For the other users except eviction, they need to do this:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 pcm   =   pte_to_pcache_meta ( ptent );  /*      * We have a strict lock ordering everyone should obey:   *      lock pcache   *      lock pte   * The caller already locked pte, thus we should avoid deadlock here   * by droping pte lock first and then acquire both of them in order.   */  if   ( unlikely ( ! trylock_pcache ( pcm )))   { \n     /* in case it got evicted and @pcm becomes invalid */ \n     get_pcache ( pcm ); \n\n     /*       * Once we release the pte lock, this pcm may be       * unmapped by another thread who is doing eviction.       * Since we have grabbed one extra ref above, so even       * it is unmapped, eviction thread will not fail to free it.       */ \n     spin_unlock ( ptl ); \n\n     lock_pcache ( pcm ); \n     spin_lock ( ptl ); \n\n     /*          * Since we dropped the lock, the pcache line might       * be got evicted in the middle.       */       if   ( ! pte_same ( * pte ,   ptent ))   {           unlock_pcache ( pcm ); \n         /*              * This put maybe decreases the ref to 0           * and eventually free the pcache line.           * This happens if the @pcm was selected           * to be evicted at the same time.           */ \n         put_pcache ( pcm ); \n         return   - EAGAIN ; \n     }     \n     put_pcache ( pcm );  }    As for the eviction thread, it needs to make sure it is the last user using this pcm:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 /*     * Each rmap counts one refcount, plus the one grabbed   * during evict_find_line(), we should have (nr_mapped + 1)   * here if there are no any other users.   *   * Furthurmore, others can not go from munmap/mremap/wp to   * put_pcache() within pcache_zap_pte(), pcache_move_pte()   * or pcache_do_wp_page(). Thus the refcount must larger or   * equal to (nr_mapped + 1).   *   * But if there truly other users (refcount   nr_mapped + 1),   * then we should manually sub the refcount. The other users   * which are currently holding the ref, will free the pcache   * once it call put_pcache.   */  PCACHE_BUG_ON_PCM ( pcache_ref_count ( pcm )     nr_mapped   +   1 ,   pcm );  if   ( unlikely ( ! pcache_ref_freeze ( pcm ,   nr_mapped   +   1 )))   { \n     if   ( unlikely ( pcache_ref_sub_and_test ( pcm ,   nr_mapped   +   1 )))   { \n         pr_info ( BUG: pcm refcount, nr_mapped: %d \\n ,   nr_mapped ); \n         dump_pcache_meta ( pcm ,   ref error ); \n         BUG (); \n     }    \n\n     ClearPcacheReclaim ( pcm ); \n     add_to_lru_list ( pcm ,   pset ); \n     unlock_pcache ( pcm ); \n\n     inc_pcache_event ( PCACHE_EVICTION_EAGAIN_CONCURRENT ); \n     return   PCACHE_EVICT_EAGAIN_CONCURRENT ;  }     My personal thought: live eviction against live objects/references is very hard. You first need to use refcount to ensure a correct ordering. You also need to have a way to prevent others from using the going-to-be-evicted pointer, or have a way to detect a under-use pointer.  In this Lego pcache case, we use the combination of pte lock, pcache lock, and pcache refcount, to ensure everyone is safe. And all these is quite similar to Linux page operations. I learned a lot from its code. But I still not fully understand how it ensures the page is not used by others, it has way more parties than lego that can use the page at the same time of eviction. Magic kernel folks.  \nYizhou Shan  \nCreated: Mar 15, 2018 \nLast Updated: Mar 16, 2018", 
            "title": "Mumble pcache eviction and refcount"
        }, 
        {
            "location": "/lego/pcache/smp_design/", 
            "text": "SMP Design Thought\n\n\nCoding pcache is nothing different from coding mm code. It is the same with your familiar mixed pgfault, LRU, page cache and writeback code. Each pcache line can be involved with multiple activities at the same time. We have to use different states to synchronize among them. If you have ever read linux mm code, you will know that sometimes, comment is literally more than code. SMP pain in ass.\n\n\nI don\nt think this document is well written. It is just some random thoughts I wrote down while coding. Some of them might be wrong. But it is still worth looking back.\n\n\nPcache and Victim Cache Organization\n\n\nOur pcache and victim cache are allocated and arranged as a big array. As for\npcache we look at it in a \ncache set view\n, which means consecutive pcache lines\nare not relevant in natual. As for victim cache, we simply treat it as a big array\nand walk through it one by one.\n\n\nAllocation/Eviction SMP Consideration\n\n\nThe alloc/free of both pcache and victim cache are simple: each pcache line or\nvictim cache line has a \nAllocated\n bit to indicate if this line is free or not.\nThe \nAllocated\n bit is manipulated by atomic bit operations, thus SMP safe. This\nfurther implies that we do not need another spinlock to guard allocation.\n\n\nHowever, other activities such as explict eviction, background sweep may walk\nthrough the cache lines at the same time of cache allocation, a single \nAllocated\n\nbit is not enough. Because an allocated cache line will need some initial setup,\nsuch as reset refcount, clear flags (prep_new_pcache),\nthus there is a small time gap between Allocated bit being set and the cache line\nbeing truly safe to use. Other activities must wait the cache line to be usable,\nand then they can do further operations on this cache line.\n\n\nTo solve this race condition, there two possible solutions:\n1) Add another bit: \nUsable\n, which is set once initial setup is done.\n   In this case, functions excluding alloction code should always check if the \nUsable\n\n   bit is set or not. a) If it is set, this means the cache line is safe for further operations\n   b) If not, and \nAllocated\n bit is set, this means the cache line is under setup in another core,\n   We should skip it.\n   c) If not, and \nAllocated\n bit is not set, this means this cache line is simply free.\n   We should skip it.\n\n\n2) Add allocated cache lines to a list (such as LRU list), and functions excluding allocation\n   code will only look into cache lines within this list. In other words, others will only\n   look into surely usable cache lines.\n\n\nBoth solutions try to avoid others looking into \nun-mature\n cache lines in SMP envorinment.\nThe rule is simple: function should \nNOT\n look into data that is not supposed to be seen.\nThe cache line that has Allocated bit set but under setup is a typical case.\n\n\nAs an example, the physical page allocator, page reclaim, page cache in Linux are implemented with\nthe second solution. Pages freshly allocated will be added a LRU list or page cache own list.\nAnd page reclaim code will only look into pages within the LRU list, it will not go through all\nphysical pages to do so. The reason for Linux to do so is simple: kernel can not scan the whole\nphysical pages to find out pages to operate.\n\n\nPcache:\n When it comes to pcache, we use both.\nIn our envision, pcache will have high-associativity such as 64 or 128.\nIt will have very bad performance if our eviction algorithm or sweep thread need to go through every\ncache lines within a set to find out candidates, while there might be only 1 or 2 allocated lines.\nHowever, additional \nUsable\n bit is added for debug purpose.\n\n\nVictim Cache:\n When it comes to victim cache, the first solution seems a better choice.\nBecause victim cache only a few cache lines, e.g., 8 or 16. This means a whole victim cache line\nwalk is fast. While the list deletion and addition seem may introduce some unnecessary overhead.\nIt is all about trade-off.\n\n\nThese choices affect the usage of pcache and victim cache, mostly the eviction code.\n\n\nMore on above two solutions\n\n\nThe first solution is used if evict_random is configured. The second solution is used when\nevict_lru is configured.\n\n\nI do not have any doubt about second solution, it works, though with a lot SMP pain in ass.\nBut I do have more to say about the first solution, which is adding another usable bit.\nThe \nUsable\n bit \nonly\n ensures other threads will not use unmature pcache, but it can not\nprevent other threads seeing a going-to-be-freed pcache.\n\n\nWhat is this going-to-be-freed asshole? Let us consider this case: CPU0 is doing eviction\nand checked the \nUsable\n bit, which is set. Then CPU0 thought this cache line is all set,\nready to be torqued. Before doing all the dirty work, CPU0 will \nget_pcache_unless_zero()\n\nfirst to make sure the pcache will not go away in the middle. However, meanwhile, CPU1 did\na \nput_pcache()\n \nand\n a consecutive \npcache_alloc()\n right before CPU0 did called\n\nget_pcache_unless_zero()\n. Bang! CPU0 may use an mature pcache line, cause CPU1\ns \npcache_init_ref_count()\n\nmay come before CPU1\ns \nget_pcache_unless_zero()\n! How to solve this? CPU0 need to add\nadditional checking after \nget_pcache_unless_zero()\n.\n\n\nFor more details, please check the code in \npcache/evcit_random.c\n, which has more pretty explanation.\n\n\n\nYizhou Shan\n\nJan 31, 2018", 
            "title": "SMP Design"
        }, 
        {
            "location": "/lego/pcache/smp_design/#smp-design-thought", 
            "text": "Coding pcache is nothing different from coding mm code. It is the same with your familiar mixed pgfault, LRU, page cache and writeback code. Each pcache line can be involved with multiple activities at the same time. We have to use different states to synchronize among them. If you have ever read linux mm code, you will know that sometimes, comment is literally more than code. SMP pain in ass.  I don t think this document is well written. It is just some random thoughts I wrote down while coding. Some of them might be wrong. But it is still worth looking back.", 
            "title": "SMP Design Thought"
        }, 
        {
            "location": "/lego/pcache/smp_design/#pcache-and-victim-cache-organization", 
            "text": "Our pcache and victim cache are allocated and arranged as a big array. As for\npcache we look at it in a  cache set view , which means consecutive pcache lines\nare not relevant in natual. As for victim cache, we simply treat it as a big array\nand walk through it one by one.", 
            "title": "Pcache and Victim Cache Organization"
        }, 
        {
            "location": "/lego/pcache/smp_design/#allocationeviction-smp-consideration", 
            "text": "The alloc/free of both pcache and victim cache are simple: each pcache line or\nvictim cache line has a  Allocated  bit to indicate if this line is free or not.\nThe  Allocated  bit is manipulated by atomic bit operations, thus SMP safe. This\nfurther implies that we do not need another spinlock to guard allocation.  However, other activities such as explict eviction, background sweep may walk\nthrough the cache lines at the same time of cache allocation, a single  Allocated \nbit is not enough. Because an allocated cache line will need some initial setup,\nsuch as reset refcount, clear flags (prep_new_pcache),\nthus there is a small time gap between Allocated bit being set and the cache line\nbeing truly safe to use. Other activities must wait the cache line to be usable,\nand then they can do further operations on this cache line.  To solve this race condition, there two possible solutions:\n1) Add another bit:  Usable , which is set once initial setup is done.\n   In this case, functions excluding alloction code should always check if the  Usable \n   bit is set or not. a) If it is set, this means the cache line is safe for further operations\n   b) If not, and  Allocated  bit is set, this means the cache line is under setup in another core,\n   We should skip it.\n   c) If not, and  Allocated  bit is not set, this means this cache line is simply free.\n   We should skip it.  2) Add allocated cache lines to a list (such as LRU list), and functions excluding allocation\n   code will only look into cache lines within this list. In other words, others will only\n   look into surely usable cache lines.  Both solutions try to avoid others looking into  un-mature  cache lines in SMP envorinment.\nThe rule is simple: function should  NOT  look into data that is not supposed to be seen.\nThe cache line that has Allocated bit set but under setup is a typical case.  As an example, the physical page allocator, page reclaim, page cache in Linux are implemented with\nthe second solution. Pages freshly allocated will be added a LRU list or page cache own list.\nAnd page reclaim code will only look into pages within the LRU list, it will not go through all\nphysical pages to do so. The reason for Linux to do so is simple: kernel can not scan the whole\nphysical pages to find out pages to operate.  Pcache:  When it comes to pcache, we use both.\nIn our envision, pcache will have high-associativity such as 64 or 128.\nIt will have very bad performance if our eviction algorithm or sweep thread need to go through every\ncache lines within a set to find out candidates, while there might be only 1 or 2 allocated lines.\nHowever, additional  Usable  bit is added for debug purpose.  Victim Cache:  When it comes to victim cache, the first solution seems a better choice.\nBecause victim cache only a few cache lines, e.g., 8 or 16. This means a whole victim cache line\nwalk is fast. While the list deletion and addition seem may introduce some unnecessary overhead.\nIt is all about trade-off.  These choices affect the usage of pcache and victim cache, mostly the eviction code.", 
            "title": "Allocation/Eviction SMP Consideration"
        }, 
        {
            "location": "/lego/pcache/smp_design/#more-on-above-two-solutions", 
            "text": "The first solution is used if evict_random is configured. The second solution is used when\nevict_lru is configured.  I do not have any doubt about second solution, it works, though with a lot SMP pain in ass.\nBut I do have more to say about the first solution, which is adding another usable bit.\nThe  Usable  bit  only  ensures other threads will not use unmature pcache, but it can not\nprevent other threads seeing a going-to-be-freed pcache.  What is this going-to-be-freed asshole? Let us consider this case: CPU0 is doing eviction\nand checked the  Usable  bit, which is set. Then CPU0 thought this cache line is all set,\nready to be torqued. Before doing all the dirty work, CPU0 will  get_pcache_unless_zero() \nfirst to make sure the pcache will not go away in the middle. However, meanwhile, CPU1 did\na  put_pcache()   and  a consecutive  pcache_alloc()  right before CPU0 did called get_pcache_unless_zero() . Bang! CPU0 may use an mature pcache line, cause CPU1 s  pcache_init_ref_count() \nmay come before CPU1 s  get_pcache_unless_zero() ! How to solve this? CPU0 need to add\nadditional checking after  get_pcache_unless_zero() .  For more details, please check the code in  pcache/evcit_random.c , which has more pretty explanation.  \nYizhou Shan \nJan 31, 2018", 
            "title": "More on above two solutions"
        }, 
        {
            "location": "/lego/pcache/replication/", 
            "text": "Memory Replication\n\n\n\n\n\n\nKeep a single copy of each page in DRAM, with redundant copies on secondary storage such as disk or flash. This makes replication nearly free in terms of cost, and energy usage. But we should consider the extra network cost.\n\n\n\n\n\n\nRAMCloud has two components running on a single machine: \nmaster\n, and \nbackup\n. In lego, \nmaster\n is the handler running on \nMemory\n, \nbackup\n is the handler running on \nStorage\n.\n\n\n\n\n\n\nBecause of \ndual-Memory solution\n, we don\nt need a hash table from \npid, user_vaddr\n to objects in log: M1 has its own \nVA-PA\n mapping table, and it will not be updated on replication. M2 does not need to look up.\n\n\n\n\n\n\nRAMCloud use 8MB segment. Logs are first appended within each segment. Each log has different size, depends on the objects being written. Lego is different. Replication is triggered by pcache/victim flush, which means the data is always the size of a pcache line (4KB now). This make things somehow simpler. But other general rules still apply.\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 31, 2018\n\nLast Updated: Mar 31, 2018", 
            "title": "Replication"
        }, 
        {
            "location": "/lego/pcache/replication/#memory-replication", 
            "text": "Keep a single copy of each page in DRAM, with redundant copies on secondary storage such as disk or flash. This makes replication nearly free in terms of cost, and energy usage. But we should consider the extra network cost.    RAMCloud has two components running on a single machine:  master , and  backup . In lego,  master  is the handler running on  Memory ,  backup  is the handler running on  Storage .    Because of  dual-Memory solution , we don t need a hash table from  pid, user_vaddr  to objects in log: M1 has its own  VA-PA  mapping table, and it will not be updated on replication. M2 does not need to look up.    RAMCloud use 8MB segment. Logs are first appended within each segment. Each log has different size, depends on the objects being written. Lego is different. Replication is triggered by pcache/victim flush, which means the data is always the size of a pcache line (4KB now). This make things somehow simpler. But other general rules still apply.    \nYizhou Shan \nCreated: Mar 31, 2018 \nLast Updated: Mar 31, 2018", 
            "title": "Memory Replication"
        }, 
        {
            "location": "/lego/driver/pci/", 
            "text": "PCI Subsystem\n\n\nWhat we have ported so far\n\n\n\n\nPCI data structures such as \npci_dev\n, \npci_bus\n, and so on.\n\n\nMechanism to scan bus and build data structures during boot. Performed by \npci_scan_root_bus()\n, and most code is in \ndriver/pci/probe.c\n\n\n\n\nUnfinished business\n\n\n\n\nWays to go through all PCI device.\n\n\npci_init_capabilities()\n: for each PCI device\n\n\npci_fixup_device()\n: a lot quicks, maybe not useful\n\n\npcie_aspm_init_link_state()\n: PCIe link state\n\n\npci_iov_bus_range\n: all SR-IOV support\n\n\n\n\n\nYizhou Shan\n\nCreated: July 5, 2018\n\nLast Updated: July 5, 2018", 
            "title": "PCI"
        }, 
        {
            "location": "/lego/driver/pci/#pci-subsystem", 
            "text": "", 
            "title": "PCI Subsystem"
        }, 
        {
            "location": "/lego/driver/pci/#what-we-have-ported-so-far", 
            "text": "PCI data structures such as  pci_dev ,  pci_bus , and so on.  Mechanism to scan bus and build data structures during boot. Performed by  pci_scan_root_bus() , and most code is in  driver/pci/probe.c", 
            "title": "What we have ported so far"
        }, 
        {
            "location": "/lego/driver/pci/#unfinished-business", 
            "text": "Ways to go through all PCI device.  pci_init_capabilities() : for each PCI device  pci_fixup_device() : a lot quicks, maybe not useful  pcie_aspm_init_link_state() : PCIe link state  pci_iov_bus_range : all SR-IOV support   \nYizhou Shan \nCreated: July 5, 2018 \nLast Updated: July 5, 2018", 
            "title": "Unfinished business"
        }, 
        {
            "location": "/lego/driver/ib/", 
            "text": "Infiniband Subsystem\n\n\nCurrent Status\n\n\nLego\ns IB stack is ported based on \nlinux-3.11.1\n. We ported:\n\n\n\n\nib_core\n\n\nmlx4_ib\n\n\nmlx4_core\n\n\n\n\nLego does not support uverbs. At the time of writing, Lego IB stack has only been tested on \nMellanox Technologies MT27500 Family [ConnectX-3]\n.\n\n\nRandom summary\n\n\nThe stack is SUPER complex, a lot data structures and pointers fly all over. Good thing is the whole stack is layered clearly.\n\n\nTop down\n\n\nib_core\n\n\n\n\nIB core code is in \ndriver/infiniband/core\n, which exposes the major IB API to both user and kernel applications. Inside, it has two parts. The first part is function callback, that call back to underlying device-specific functions. The second part is the management stack, including communication manager (cm), management datagram (mad), and so on.\n\n\nIn IB, each port\ns QP0 and QP1 are reserved for management purpose. They will receive/send MAD from/to subnet manager, who typically runs on switch. All the IB management stuff is carried out by exchanging MAD.\n\n\nThere are several key data structures: ib_client, ib_device, and mad_agent. MAD, CM, and some others are ib_client, which means they use IB device, and will be called back whenever a device has been added. mad_agent is something that will be called back whenever a device received a MAD message from switch (see \nib_mad_completion_handler()\n). A lot layers, huh?\n\n\nib_mad_completion_handler()\n: we changed the behavior of it. we use busy polling instead of interrupt. Originally, it will be invoked by mlx4_core/eq.c\n\n\n\n\nmlx4_ib and mlx4_core\n\n\n\n\n\n\nmlx4_core is actually the Ethernet driver for Mellanox NIC device (drivers/net/ethernet/mellanox/hw/mlx4), which do the actual dirty work of talking with device. On the other hand, mlx4_ib is the glue code between ib_core and mlx4_core, who do the translation.\n\n\n\n\n\n\nA lot IB verbs are ultimately translated into \nfw.c __mlx4_cmd()\n, which actually send commands to device and get the result. There are two ways of getting result: 1) polling: after writing to device memory the command, the same thread keep polling. 2) sleep and wait for interrupt. By default, the interrupt way is used (obviously). But, at the time of writing (Aug 20, 2018), we don\nt really have a working IRQ subsystem, so we use polling instead. \nI\nm still a little concerned that without interrupt handler, we might lose some events and the NIC may behavave incorrectly if interrupts are not handled.\n\n\n\n\n\n\nInit Sequence\n\n\n\n\nInit PCI subsystem, build data structures\n\n\nCore IB layer register \nib_client\n\n\nmlx4_init()\n: register PCI driver, provide a callback\n\n\n__mlx4_init_one()\n: initialize the hardware itself, register interrupt handler.\n\n\nmlx4_ib_init()\n: allocate a ib_device, and register, which will callback through all \nib_client\n registered at step 1.\n\n\n\n\n\nYizhou Shan\n\nCreated: Aug 20, 2018\n\nLast Updated: Aug 20, 2018", 
            "title": "Infiniband"
        }, 
        {
            "location": "/lego/driver/ib/#infiniband-subsystem", 
            "text": "", 
            "title": "Infiniband Subsystem"
        }, 
        {
            "location": "/lego/driver/ib/#current-status", 
            "text": "Lego s IB stack is ported based on  linux-3.11.1 . We ported:   ib_core  mlx4_ib  mlx4_core   Lego does not support uverbs. At the time of writing, Lego IB stack has only been tested on  Mellanox Technologies MT27500 Family [ConnectX-3] .", 
            "title": "Current Status"
        }, 
        {
            "location": "/lego/driver/ib/#random-summary", 
            "text": "The stack is SUPER complex, a lot data structures and pointers fly all over. Good thing is the whole stack is layered clearly.  Top down", 
            "title": "Random summary"
        }, 
        {
            "location": "/lego/driver/ib/#ib_core", 
            "text": "IB core code is in  driver/infiniband/core , which exposes the major IB API to both user and kernel applications. Inside, it has two parts. The first part is function callback, that call back to underlying device-specific functions. The second part is the management stack, including communication manager (cm), management datagram (mad), and so on.  In IB, each port s QP0 and QP1 are reserved for management purpose. They will receive/send MAD from/to subnet manager, who typically runs on switch. All the IB management stuff is carried out by exchanging MAD.  There are several key data structures: ib_client, ib_device, and mad_agent. MAD, CM, and some others are ib_client, which means they use IB device, and will be called back whenever a device has been added. mad_agent is something that will be called back whenever a device received a MAD message from switch (see  ib_mad_completion_handler() ). A lot layers, huh?  ib_mad_completion_handler() : we changed the behavior of it. we use busy polling instead of interrupt. Originally, it will be invoked by mlx4_core/eq.c", 
            "title": "ib_core"
        }, 
        {
            "location": "/lego/driver/ib/#mlx4_ib-and-mlx4_core", 
            "text": "mlx4_core is actually the Ethernet driver for Mellanox NIC device (drivers/net/ethernet/mellanox/hw/mlx4), which do the actual dirty work of talking with device. On the other hand, mlx4_ib is the glue code between ib_core and mlx4_core, who do the translation.    A lot IB verbs are ultimately translated into  fw.c __mlx4_cmd() , which actually send commands to device and get the result. There are two ways of getting result: 1) polling: after writing to device memory the command, the same thread keep polling. 2) sleep and wait for interrupt. By default, the interrupt way is used (obviously). But, at the time of writing (Aug 20, 2018), we don t really have a working IRQ subsystem, so we use polling instead.  I m still a little concerned that without interrupt handler, we might lose some events and the NIC may behavave incorrectly if interrupts are not handled.", 
            "title": "mlx4_ib and mlx4_core"
        }, 
        {
            "location": "/lego/driver/ib/#init-sequence", 
            "text": "Init PCI subsystem, build data structures  Core IB layer register  ib_client  mlx4_init() : register PCI driver, provide a callback  __mlx4_init_one() : initialize the hardware itself, register interrupt handler.  mlx4_ib_init() : allocate a ib_device, and register, which will callback through all  ib_client  registered at step 1.   \nYizhou Shan \nCreated: Aug 20, 2018 \nLast Updated: Aug 20, 2018", 
            "title": "Init Sequence"
        }, 
        {
            "location": "/lego/paper/nmp/", 
            "text": "Near Memory Processing\n\n\n\n\nNMP: Near Memory Processing\n\n\n\n\nNDC: Near Data Computing\n\n\n\n\n\n\nPRIME\n:\n \nA\n \nNovel\n \nProcessing\n-\nin\n-\nmemory\n \nArchitecture\n \nfor\n \nNeural\n \nNetwork\nComputation\n \nin\n \nReRAM\n-\nbased\n \nMain\n \nMemory\n,\n \nISCA\n16\n\n\n\n\nHigh-performance\nacceleration of NN requires high memory bandwidth since\nthe \nPUs are hungry for fetching the synaptic weights [17]\n. To\naddress this challenge, recent special-purpose chip designs\nhave adopted large on-chip memory to store the synaptic\nweights. For example, DaDianNao [18] employed a large\non-chip eDRAM for both high bandwidth and data locality;\nTrueNorth utilized an SRAM crossbar memory for synapses\nin each core [19].\n\n\n\n\n\n\nDianNao\n and \nDaDianNao\n\n\n \nmemory bandwidth requirements\n of two important\nlayer types: convolutional layers with private kernels\n(used in DNNs) and classifier layers used in both CNNs and\nDNNs. For these types of layers, the total number of required\nsynapses can be massive, in the millions of parameters, or\neven tens or hundreds thereof.\n\n\nproviding sufficient eDRAM capacity to hold\nall \nsynapse\n on the combined eDRAM of all chips will\nsave on \noff-chip DRAM accesses\n, which are particularly\ncostly energy-wise\n\n\nSynapses\n. In a perceptron layer, all synapses are usually\nunique, and thus there is no reuse within the layer. On the\nother hand, the synapses are reused across network invocations,\ni.e., for each new input data (also called \u201cinput row\u201d)\npresented to the neural network. So a sufficiently large L2\ncould store all network synapses and take advantage of that\nlocality. For DNNs with private kernels, this is not possible\nas the total number of synapses are in the tens or hundreds\nof millions (the largest network to date has a billion\nsynapses [26]). However, for both CNNs and DNNs with\nshared kernels, the total number of synapses range in the\nmillions, which is within the reach of an L2 cache. In Figure\n6, see CLASS1 - Tiled+L2, we emulate the case where reuse\nacross network invocations is possible by considering only\nthe perceptron layer; as a result, the total bandwidth requirements\nare now drastically reduced.\n\n\nSo, ML workloads do need large memory bandwidth, and need a lot memory. But how about \ntemporary working set size\n? It\ns the best if it has a reasonable working set size that can fit the cache.\n\n\n\n\n\n\nTPU\n\n\nEach model needs between 5M and 100M weights (9\nth\n\ncolumn of Table 1), which can take a lot of time and energy to\naccess. To amortize the access costs, \nthe same weights are reused\nacross a batch of independent examples during inference or\ntraining\n, which improves performance.\n\n\nThe weights for the matrix unit are staged through an onchip\n\nWeight FIFO\n that reads from an \noff-chip 8 GiB DRAM\ncalled Weight Memory\n (for inference, weights are read-only; 8\nGiB supports many simultaneously active models). The weight\nFIFO is four tiles deep. The intermediate results are held in the \n24\nMiB on-chip Unified Buffer\n, which can serve as inputs to the Matrix Unit.\n\n\nIn virtual cache model, we actually can assign those weights to some designated sets, thus avoid conflicting with other data, which means we can sustain those weights in cache!\n\n\n\n\n\n\n\n\nTo conclude:\n\n\na)\n ML needs to use weight/synapses during computation, and those data will be reused repeatly across different stages. Besides, output from last stage serves the input of next stage, so buffering the \nintermediate data\n is important. Most ML accelerators use some kind of \non-chip memory\n (\nWeighted FIFO, Unified Cache in TPU\n) to buffer those data. This fits the \nHBM+Disaggregated Memory\n model: HBM is the on-chip memory, while disaggregated memory is the off-chip memory. \nb)\n Combined with virtual cache, we could assign special virtual addresses to weight data, so they stay in some designated cache sets. Kernel can avoid allocating conflict virtual addresses later. Thus we can retain these weight data in virtual cache easily.", 
            "title": "NMP"
        }, 
        {
            "location": "/lego/paper/nmp/#near-memory-processing", 
            "text": "NMP: Near Memory Processing   NDC: Near Data Computing    PRIME :   A   Novel   Processing - in - memory   Architecture   for   Neural   Network Computation   in   ReRAM - based   Main   Memory ,   ISCA 16   High-performance\nacceleration of NN requires high memory bandwidth since\nthe  PUs are hungry for fetching the synaptic weights [17] . To\naddress this challenge, recent special-purpose chip designs\nhave adopted large on-chip memory to store the synaptic\nweights. For example, DaDianNao [18] employed a large\non-chip eDRAM for both high bandwidth and data locality;\nTrueNorth utilized an SRAM crossbar memory for synapses\nin each core [19].    DianNao  and  DaDianNao    memory bandwidth requirements  of two important\nlayer types: convolutional layers with private kernels\n(used in DNNs) and classifier layers used in both CNNs and\nDNNs. For these types of layers, the total number of required\nsynapses can be massive, in the millions of parameters, or\neven tens or hundreds thereof.  providing sufficient eDRAM capacity to hold\nall  synapse  on the combined eDRAM of all chips will\nsave on  off-chip DRAM accesses , which are particularly\ncostly energy-wise  Synapses . In a perceptron layer, all synapses are usually\nunique, and thus there is no reuse within the layer. On the\nother hand, the synapses are reused across network invocations,\ni.e., for each new input data (also called \u201cinput row\u201d)\npresented to the neural network. So a sufficiently large L2\ncould store all network synapses and take advantage of that\nlocality. For DNNs with private kernels, this is not possible\nas the total number of synapses are in the tens or hundreds\nof millions (the largest network to date has a billion\nsynapses [26]). However, for both CNNs and DNNs with\nshared kernels, the total number of synapses range in the\nmillions, which is within the reach of an L2 cache. In Figure\n6, see CLASS1 - Tiled+L2, we emulate the case where reuse\nacross network invocations is possible by considering only\nthe perceptron layer; as a result, the total bandwidth requirements\nare now drastically reduced.  So, ML workloads do need large memory bandwidth, and need a lot memory. But how about  temporary working set size ? It s the best if it has a reasonable working set size that can fit the cache.    TPU  Each model needs between 5M and 100M weights (9 th \ncolumn of Table 1), which can take a lot of time and energy to\naccess. To amortize the access costs,  the same weights are reused\nacross a batch of independent examples during inference or\ntraining , which improves performance.  The weights for the matrix unit are staged through an onchip Weight FIFO  that reads from an  off-chip 8 GiB DRAM\ncalled Weight Memory  (for inference, weights are read-only; 8\nGiB supports many simultaneously active models). The weight\nFIFO is four tiles deep. The intermediate results are held in the  24\nMiB on-chip Unified Buffer , which can serve as inputs to the Matrix Unit.  In virtual cache model, we actually can assign those weights to some designated sets, thus avoid conflicting with other data, which means we can sustain those weights in cache!     To conclude:  a)  ML needs to use weight/synapses during computation, and those data will be reused repeatly across different stages. Besides, output from last stage serves the input of next stage, so buffering the  intermediate data  is important. Most ML accelerators use some kind of  on-chip memory  ( Weighted FIFO, Unified Cache in TPU ) to buffer those data. This fits the  HBM+Disaggregated Memory  model: HBM is the on-chip memory, while disaggregated memory is the off-chip memory.  b)  Combined with virtual cache, we could assign special virtual addresses to weight data, so they stay in some designated cache sets. Kernel can avoid allocating conflict virtual addresses later. Thus we can retain these weight data in virtual cache easily.", 
            "title": "Near Memory Processing"
        }, 
        {
            "location": "/lego/paper/processor_oom/", 
            "text": "Process/Memory Kernel Memory\n\n\nThis document is based on discussion with Yiying, about how to deal with processor or memory component\ns out-of-kernel-memory situation. It mainly bothers processor component, which has a small kernel memory while needs to support all running user threads.\n\n\nProcess\ns local kernel memory is limited by design. There are several major users:\n\n\n\n\n1) pcache\ns rmap, which is propotional to pcache size.\n\n\n2) IB, which depends on concurrent outgoing messages.\n\n\n3) running threads. For each thread at processor, Lego needs to allocate some kernel memory for it, e.g, \nkernel stack\n, \ntask_strcut\n, and so on.\n\n\n\n\nBoth 1) and 2) are fine, they can be easily controlled. However we can not limit how many threads user can create, thus 3) becomes the critical criminal of oom.\n\n\nWhen processor is running out of kernel memory, Lego needs to deal with it. Currently, we propose three different solutions:\n\n\n\n\ns1) \nSwap\n kernel memory to remote memory component\n\n\ns2) \nKill\n some threads to have some usable memory (OOM killer)\n\n\ns3) \nMigrate\n, or \ncheckpoint\n, threads to processors that have usable kernel memory\n\n\n\n\nFor solution 3), there is a case where \nall\n processors are running out of memory. Then we have to use solution 1) or 2).\n\n\n\nYizhou Shan\n\nFeb 17, 2018", 
            "title": "Processor OOM"
        }, 
        {
            "location": "/lego/paper/processor_oom/#processmemory-kernel-memory", 
            "text": "This document is based on discussion with Yiying, about how to deal with processor or memory component s out-of-kernel-memory situation. It mainly bothers processor component, which has a small kernel memory while needs to support all running user threads.  Process s local kernel memory is limited by design. There are several major users:   1) pcache s rmap, which is propotional to pcache size.  2) IB, which depends on concurrent outgoing messages.  3) running threads. For each thread at processor, Lego needs to allocate some kernel memory for it, e.g,  kernel stack ,  task_strcut , and so on.   Both 1) and 2) are fine, they can be easily controlled. However we can not limit how many threads user can create, thus 3) becomes the critical criminal of oom.  When processor is running out of kernel memory, Lego needs to deal with it. Currently, we propose three different solutions:   s1)  Swap  kernel memory to remote memory component  s2)  Kill  some threads to have some usable memory (OOM killer)  s3)  Migrate , or  checkpoint , threads to processors that have usable kernel memory   For solution 3), there is a case where  all  processors are running out of memory. Then we have to use solution 1) or 2).  \nYizhou Shan \nFeb 17, 2018", 
            "title": "Process/Memory Kernel Memory"
        }, 
        {
            "location": "/lego/paper/genz/", 
            "text": "Interconnect Technology Comparison\n\n\n\n\n\n\n\n\nInterconnect Technology\n\n\nProducts or Vendor\n\n\nPhysical Domain\n\n\nCache Coherent\n\n\nAccess Semantic\n\n\nMaximum Bandwidth\n\n\nMedium Latency\n\n\n\n\n\n\n\n\n\n\nGen-Z\n7\n8\n\n\nN/A\n\n\nCross components\n\n\n\n\nMemory\n\n\n32 GBps ~ 400+ GBps \n \nUnidirectional\n\n\n100ns\n\n\n\n\n\n\nOpenCAPI\n7\n\n\nIBM Power9\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n50 GBps per lane \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nCCIX\n7\n\n\nN/A\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n32/40/50 GBps/lane \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nOmniPath\n9\n10\n\n\nIntel KnightsLanding\n\n\nCross networrk\n\n\n\n\nNetwork\n\n\n25 GBps/port \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nPCIe 3.0\n\n\nA Lot\n\n\nMotherboard\n\n\n\n\nPCIe\n\n\n~1GBps/lane\n12\n\n\n4B Read ~756ns\n11\n\n\n\n\n\n\nPCIe 4.0\n\n\nSoon\n\n\nMotherboard\n\n\n\n\nPCIe\n\n\n~2GBps/lane\n\n\n?\n\n\n\n\n\n\nIB EDR\n\n\nMellanox ConnectX4,X5\n\n\nCross network\n\n\n\n\nNetwork\n\n\n100Gbps\n\n\n0.5us\n\n\n\n\n\n\nIB HDR\n\n\nMellanox ConnectX6\n\n\nCross network\n\n\n\n\nNetwork\n\n\n200Gbps\n\n\n0.5us\n\n\n\n\n\n\nHyperTransport\n4\n\n\nAMD\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n51.2 GBps per link \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nNVLink\n2\n\n\nNVIDIA V100 \n IBM Power9\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n50GBps per link \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nQPI\n5\n6\n\n\nIntel\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n?\n\n\n?\n\n\n\n\n\n\nIntel Main Memory Bus\n\n\nIntel\n\n\nProcessor\n\n\n\n\nMemory\n\n\nE7-8894 v4 \n85 GB/s\n \n E5-2620 v3 \n59 GB/s\n\n\n?\n\n\n\n\n\n\nEthernet\n3\n\n\nA Lot\n\n\nMotherboard\n\n\n\n\nNetwork\n\n\nMellanox \n200Gbps\n \n Cisco ASR \n100 Gbps\n1\n\n\n?\n\n\n\n\n\n\n\n\n\n\nPOWER9, NVLink 2.0, 300GB/s\n\n\n\n\n\nCreated: Feb 28, 2018\n\nLast Updated: March 01, 2018\n\n\n\n\n\n\n\n\n\n\nEthernet Cisco ASR 9000 Series 4-Port 100-Gigabit Ethernet\n\n\n\n\n\n\nTerabit Ethernet\n\n \nhttps://en.wikipedia.org/wiki/NVLink\n\n\n\n\n\n\nNVLink\n\n\n\n\n\n\nHyperTransport\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect\n\n\n\n\n\n\nhttps://communities.intel.com/thread/21872\n\n\n\n\n\n\nhttps://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf\n\n\n\n\n\n\nGen-Z Overview\n\n\n\n\n\n\nhttp://www.hoti.org/hoti23/slides/rimmer.pdf\n\n\n\n\n\n\nhttps://www.intel.com/content/www/us/en/products/network-io/high-performance-fabrics/omni-path-edge-switch-100-series.html\n\n\n\n\n\n\nhttps://forum.stanford.edu/events/posterslides/LowLatencyNetworkInterfaces.pdf\n\n\n\n\n\n\nhttps://www.xilinx.com/support/documentation/white_papers/wp350.pdf", 
            "title": "Gen-Z"
        }, 
        {
            "location": "/lego/paper/genz/#interconnect-technology-comparison", 
            "text": "Interconnect Technology  Products or Vendor  Physical Domain  Cache Coherent  Access Semantic  Maximum Bandwidth  Medium Latency      Gen-Z 7 8  N/A  Cross components   Memory  32 GBps ~ 400+ GBps    Unidirectional  100ns    OpenCAPI 7  IBM Power9  Motherboard   Memory  50 GBps per lane    Bidirectional  ?    CCIX 7  N/A  Motherboard   Memory  32/40/50 GBps/lane    Bidirectional  ?    OmniPath 9 10  Intel KnightsLanding  Cross networrk   Network  25 GBps/port    Bidirectional  ?    PCIe 3.0  A Lot  Motherboard   PCIe  ~1GBps/lane 12  4B Read ~756ns 11    PCIe 4.0  Soon  Motherboard   PCIe  ~2GBps/lane  ?    IB EDR  Mellanox ConnectX4,X5  Cross network   Network  100Gbps  0.5us    IB HDR  Mellanox ConnectX6  Cross network   Network  200Gbps  0.5us    HyperTransport 4  AMD  Motherboard   Memory  51.2 GBps per link    Bidirectional  ?    NVLink 2  NVIDIA V100   IBM Power9  Motherboard   Memory  50GBps per link    Bidirectional  ?    QPI 5 6  Intel  Motherboard   Memory  ?  ?    Intel Main Memory Bus  Intel  Processor   Memory  E7-8894 v4  85 GB/s    E5-2620 v3  59 GB/s  ?    Ethernet 3  A Lot  Motherboard   Network  Mellanox  200Gbps    Cisco ASR  100 Gbps 1  ?      POWER9, NVLink 2.0, 300GB/s   \nCreated: Feb 28, 2018 \nLast Updated: March 01, 2018      Ethernet Cisco ASR 9000 Series 4-Port 100-Gigabit Ethernet    Terabit Ethernet \n  https://en.wikipedia.org/wiki/NVLink    NVLink    HyperTransport    https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect    https://communities.intel.com/thread/21872    https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf    Gen-Z Overview    http://www.hoti.org/hoti23/slides/rimmer.pdf    https://www.intel.com/content/www/us/en/products/network-io/high-performance-fabrics/omni-path-edge-switch-100-series.html    https://forum.stanford.edu/events/posterslides/LowLatencyNetworkInterfaces.pdf    https://www.xilinx.com/support/documentation/white_papers/wp350.pdf", 
            "title": "Interconnect Technology Comparison"
        }, 
        {
            "location": "/lego/paper/replication/", 
            "text": "Replication, Checkpoint, Logging, and Recovery\n\n\nDiscussion\n\n\n\n\n\n\n03/25/18:\n\n\n\n\nRevisit RAMCloud, which has a very similar goal with Lego. It keeps a full copy of data in DRAM, use disk to ensure crash consistency. The key assumption of RAMCloud is the battery-backed DRAM or PM on its disk side.\n\n\nWe don\nt need to provide a 100% recoverable model. Our goal here is to reduce the failure probabilities introduced by more components. Let us say Lego do the persist in a batching fashion, instead of per-page. We are not able to recover if and only if failure happen \nwhile\n we do the batch persist. But we are safe if failure happen between batched persist.\n\n\nThat actually also means we need to checkpoint process state in Processor side. We have to save all the process context along with the persisted memory log! Otherwise, the memory content is useless, we don\nt know the exact IP and other things.\n\n\nI\nm wrong. :-)\n\n\n\n\n\n\n\n\n03/20/18: when memory is enough, use pessimistic replication, when demand is high, use optimistic to save memory components.\n\n\n\n\n\n\nReplication\n\n\nBefore started, I spent some time recap, and found Wiki pages\n1\n2\n3\n are actually very good.\n\n\nTwo main approaches:\n\n\n\n\nOptimistic (Lazy, Passive) Replication\n \n4\n, in which replicas are allowed to diverge\n\n\nEventual consistency\n5\n6\n7\n, meaning that replicas are guaranteed to converge only when the system has been quiesced for a period of time\n\n\n\n\n\n\nPessimistic (Active, Multi-master\n8\n) Replication\n, tries to guarantee from the beginning that all of the replicas are identical to each other, as if there was only a single copy of the data all along.\n\n\n\n\nLego is more towards memory replication, not storage replication. We may want to conduct some ideas from DSM replication (MRSW, MRMW), or in-memory DB such as RAMCloud, VoltDB?\n\n\nCheckpointing\n\n\nSome nice reading\n9\n.\n\n\nApplication types:\n\n\n\n\nLong-running v.s. Short-lived\n\n\nBuilt-in checkpoint/journaling v.s. no built-in checkpoint/journaling\n\n\n\n\nTwo main approaches:\n\n\n\n\nCoordinated\n\n\n2PC\n\n\n\n\n\n\nUn-coordinated\n\n\nDomino effect\n\n\n\n\n\n\n\n\nWe should favor \n[Long-running \n no built-in checkpoint/journaling]\n applications. Normally they are not distributed systems, right? Even it is, it might be running as a single-node version. Based on this, I think we should favor coordinated checkpointing.\n\n\nHPC community\n10\n11\n12\n has a lot publications on checkpoint/recovery (e.g., Lawrence National Laboratory).\n\n\nMISC\n\n\nSome other interesting topics:\n\n\n\n\nErasure Coding\n\n\nLess space overhead\n\n\nParity Calculation is CPU-intensive\n\n\nIncreased latency\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 19, 2018\n\nLast Updated: Mar 19, 2018\n\n\n\n\n\n\n\n\n\n\nWiki: Replication\n\n\n\n\n\n\nWiki: High-availability_cluster\n\n\n\n\n\n\nWiki: Virtual synchrony\n\n\n\n\n\n\nWiki: Optimistic Replication\n\n\n\n\n\n\nWiki: Quiesce\n\n\n\n\n\n\nWiki: Eventual Consistency\n\n\n\n\n\n\nWiki: CAP Theorem\n\n\n\n\n\n\nWiki: Multi-master replication\n\n\n\n\n\n\nWiki: Application Checkpointing\n\n\n\n\n\n\nPaper: A Survey of Checkpoint/Restart Implementations \n\n\n\n\n\n\nPaper: The Design and Implementation of Berkeley Lab\u2019s Linux\nCheckpoint/Restart\n\n\n\n\n\n\nBerkeley Lab Checkpoint/Restart (BLCR) for LINUX", 
            "title": "Replication"
        }, 
        {
            "location": "/lego/paper/replication/#replication-checkpoint-logging-and-recovery", 
            "text": "", 
            "title": "Replication, Checkpoint, Logging, and Recovery"
        }, 
        {
            "location": "/lego/paper/replication/#discussion", 
            "text": "03/25/18:   Revisit RAMCloud, which has a very similar goal with Lego. It keeps a full copy of data in DRAM, use disk to ensure crash consistency. The key assumption of RAMCloud is the battery-backed DRAM or PM on its disk side.  We don t need to provide a 100% recoverable model. Our goal here is to reduce the failure probabilities introduced by more components. Let us say Lego do the persist in a batching fashion, instead of per-page. We are not able to recover if and only if failure happen  while  we do the batch persist. But we are safe if failure happen between batched persist.  That actually also means we need to checkpoint process state in Processor side. We have to save all the process context along with the persisted memory log! Otherwise, the memory content is useless, we don t know the exact IP and other things.  I m wrong. :-)     03/20/18: when memory is enough, use pessimistic replication, when demand is high, use optimistic to save memory components.", 
            "title": "Discussion"
        }, 
        {
            "location": "/lego/paper/replication/#replication", 
            "text": "Before started, I spent some time recap, and found Wiki pages 1 2 3  are actually very good.  Two main approaches:   Optimistic (Lazy, Passive) Replication   4 , in which replicas are allowed to diverge  Eventual consistency 5 6 7 , meaning that replicas are guaranteed to converge only when the system has been quiesced for a period of time    Pessimistic (Active, Multi-master 8 ) Replication , tries to guarantee from the beginning that all of the replicas are identical to each other, as if there was only a single copy of the data all along.   Lego is more towards memory replication, not storage replication. We may want to conduct some ideas from DSM replication (MRSW, MRMW), or in-memory DB such as RAMCloud, VoltDB?", 
            "title": "Replication"
        }, 
        {
            "location": "/lego/paper/replication/#checkpointing", 
            "text": "Some nice reading 9 .  Application types:   Long-running v.s. Short-lived  Built-in checkpoint/journaling v.s. no built-in checkpoint/journaling   Two main approaches:   Coordinated  2PC    Un-coordinated  Domino effect     We should favor  [Long-running   no built-in checkpoint/journaling]  applications. Normally they are not distributed systems, right? Even it is, it might be running as a single-node version. Based on this, I think we should favor coordinated checkpointing.  HPC community 10 11 12  has a lot publications on checkpoint/recovery (e.g., Lawrence National Laboratory).", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/lego/paper/replication/#misc", 
            "text": "Some other interesting topics:   Erasure Coding  Less space overhead  Parity Calculation is CPU-intensive  Increased latency     \nYizhou Shan \nCreated: Mar 19, 2018 \nLast Updated: Mar 19, 2018      Wiki: Replication    Wiki: High-availability_cluster    Wiki: Virtual synchrony    Wiki: Optimistic Replication    Wiki: Quiesce    Wiki: Eventual Consistency    Wiki: CAP Theorem    Wiki: Multi-master replication    Wiki: Application Checkpointing    Paper: A Survey of Checkpoint/Restart Implementations     Paper: The Design and Implementation of Berkeley Lab\u2019s Linux\nCheckpoint/Restart    Berkeley Lab Checkpoint/Restart (BLCR) for LINUX", 
            "title": "MISC"
        }, 
        {
            "location": "/lego/paper/related/", 
            "text": "dRedBox\n\n\n\n\nnews\n\n\nIBM Advancing cloud with memory disaggregation\n\n\n[Slides: Open Source Cloud Ecosystem for Next-Gen Disaggregated Datacenters] (\nhttps://schd.ws/hosted_files/osseu17/60/dReDBox.CloudOpen2017.talk.pdf\n)\n\n\nSlides: Demo", 
            "title": "Related"
        }, 
        {
            "location": "/lego/paper/related/#dredbox", 
            "text": "news  IBM Advancing cloud with memory disaggregation  [Slides: Open Source Cloud Ecosystem for Next-Gen Disaggregated Datacenters] ( https://schd.ws/hosted_files/osseu17/60/dReDBox.CloudOpen2017.talk.pdf )  Slides: Demo", 
            "title": "dRedBox"
        }
    ]
}