{
    "docs": [
        {
            "location": "/", 
            "text": "I\nm \nYizhou Shan\n \n, a third-year Ph.D. student at \nPurdue ECE\n,\nadvised by Prof. \nYiying Zhang\n. I work at \nWuklab.io\n. Yes, the \nMonkey King\n Lab. \nCV\n.\n\n\nSocial\n\n\n\n\nGithub\n\n\nTwitter\n\n\nLinkedIn\n\n\n\n\nWhat\ns up\n\n\n\n\n[Oct 2018] Let\ns talk about FPGA\n\n\n[Jul 2018] Our paper \nLegoOS: Disaggregated Operating System\n was accepted to \nOSDI\n18\n\n\n[May 2018] Intern at \nVMware Research\n, with \nStanko Novakovic\n.\n\n\n\n\nResearch\n\n\nMy research interests span Operating System, Distributed Systems,\nand Non-Volatile Memory. I like hardcore hacking.\n\n\nContact\n\n\n465 Northwestern Ave\nPurdue University\nWest Lafayette, IN 47907\nOffice: EE 345\n\n\nEmail: \nys@purdue.edu\n\n\nConferences\n\n\n\n\n\n\nLegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation\n\n\n \nYizhou Shan\n, Yutong Huang, Yilun Chen, Yiying Zhang\n\n \n13\nth\n USENIX Symposium on Operating Systems Design and Implementation (\nOSDI\n18\n)\n \n(Best Paper Award)\n\n\n \n[Source Code]\n\n\n\n\n\n\nDistributed Shared Persistent Memory\n\n\n \nYizhou Shan\n, Shin-Yeh Tsai, Yiying Zhang\n\n \nProceedings of the ACM Symposium on Cloud Computing 2017 (\nSoCC\n17\n)\n\n\n \n[Source Code]\n\n\n\n\n\n\nWorkshops\n\n\n\n\n\n\nChallenges in Building and Deploying Disaggregated Persistent Memory\n\n\n \nYizhou Shan\n, Yutong Huang, Yiying Zhang\n\n \n10\nth\n Annual Non-Volatile Memories Workshop (\nNVMW\n19\n)\n\n\n\n\n\n\nDisaggregating Memory with Software-Managed Virtual Cache\n\n\n \nYizhou Shan\n, Yiying Zhang\n\n \n2018 Workshop on Warehouse-scale Memory Systems (\nWAMS\n18\n) (co-located with ASPLOS \n18)\n\n\n\n\n\n\nDistributed Shared Persistent Memory\n\n\n \nYizhou Shan\n, Shin-Yeh Tsai, Yiying Zhang\n\n \n9\nth\n Annual Non-Volatile Memories Workshop (\nNVMW\n18\n)\n\n\n\n\n\n\nDisaggregated Operating System\n\n\n Yiying Zhang, \nYizhou Shan\n, Sumukh Hallymysore\n\n \n17\nth\n International Workshop on High Performance Transaction Systems (\nHPTS\n17\n)\n\n\n\n\n\n\nPosters\n\n\n\n\n\n\nLego: A Distributed, Decomposed OS for Resource Disaggregation\n\n\n \nYizhou Shan\n, Yilun Chen, Yutong Huang, Sumukh Hallymysore, Yiying Zhang\n\n \nPoster at the 26\nth\n ACM Symposium on Operating Systems Principles (\nSOSP \n17\n)\n\n\n\n\n\n\nDisaggregated Operating System\n\n\n \nYizhou Shan\n, Sumukh Hallymysore, Yutong Huang, Yilun Chen, Yiying Zhang\n\n \nPoster at the ACM Symposium on Cloud Computing 2017 (\nSoCC \n17\n)", 
            "title": "Home"
        }, 
        {
            "location": "/#social", 
            "text": "Github  Twitter  LinkedIn", 
            "title": "Social"
        }, 
        {
            "location": "/#whats-up", 
            "text": "[Oct 2018] Let s talk about FPGA  [Jul 2018] Our paper  LegoOS: Disaggregated Operating System  was accepted to  OSDI 18  [May 2018] Intern at  VMware Research , with  Stanko Novakovic .", 
            "title": "What's up"
        }, 
        {
            "location": "/#research", 
            "text": "My research interests span Operating System, Distributed Systems,\nand Non-Volatile Memory. I like hardcore hacking.", 
            "title": "Research"
        }, 
        {
            "location": "/#contact", 
            "text": "465 Northwestern Ave\nPurdue University\nWest Lafayette, IN 47907\nOffice: EE 345  Email:  ys@purdue.edu", 
            "title": "Contact"
        }, 
        {
            "location": "/#conferences", 
            "text": "LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation    Yizhou Shan , Yutong Huang, Yilun Chen, Yiying Zhang   13 th  USENIX Symposium on Operating Systems Design and Implementation ( OSDI 18 )   (Best Paper Award)    [Source Code]    Distributed Shared Persistent Memory    Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang   Proceedings of the ACM Symposium on Cloud Computing 2017 ( SoCC 17 )    [Source Code]", 
            "title": "Conferences"
        }, 
        {
            "location": "/#workshops", 
            "text": "Challenges in Building and Deploying Disaggregated Persistent Memory    Yizhou Shan , Yutong Huang, Yiying Zhang   10 th  Annual Non-Volatile Memories Workshop ( NVMW 19 )    Disaggregating Memory with Software-Managed Virtual Cache    Yizhou Shan , Yiying Zhang   2018 Workshop on Warehouse-scale Memory Systems ( WAMS 18 ) (co-located with ASPLOS  18)    Distributed Shared Persistent Memory    Yizhou Shan , Shin-Yeh Tsai, Yiying Zhang   9 th  Annual Non-Volatile Memories Workshop ( NVMW 18 )    Disaggregated Operating System   Yiying Zhang,  Yizhou Shan , Sumukh Hallymysore   17 th  International Workshop on High Performance Transaction Systems ( HPTS 17 )", 
            "title": "Workshops"
        }, 
        {
            "location": "/#posters", 
            "text": "Lego: A Distributed, Decomposed OS for Resource Disaggregation    Yizhou Shan , Yilun Chen, Yutong Huang, Sumukh Hallymysore, Yiying Zhang   Poster at the 26 th  ACM Symposium on Operating Systems Principles ( SOSP  17 )    Disaggregated Operating System    Yizhou Shan , Sumukh Hallymysore, Yutong Huang, Yilun Chen, Yiying Zhang   Poster at the ACM Symposium on Cloud Computing 2017 ( SoCC  17 )", 
            "title": "Posters"
        }, 
        {
            "location": "/misc/essential/", 
            "text": "System Developing Essentials\n\n\nTools\n\n\n\n\nStack and Register Dumper\n\n\nNMI and software Watchdog\n\n\nTracepoint and Ring Buffer\n\n\nProfilers\n\n\nCounters\n\n\nWhiskey and Luck\n\n\n\n\nKeep in mind\n\n\n\n\nStress your system\n\n\nEvery single critical subsystem\n\n\nConfident with your base subsystem\n\n\nFix bug/Improve perf at early stage\n\n\n\n\n\n\nPlan ahead\n\n\nSingle thread, or thread pool?\n\n\nHow to avoid using \nlock\n?\n\n\nWhat lock to use?\n\n\nHow to reduce \nlock contention\n?\n\n\nDoes this data structure need \nreference counter\n?\n\n\nShould I use per-cpu data structures?\n\n\nShould I pad this lock $-line aligned to avoid pingpong?\n\n\n\n\n\n\n\n\nDecent Cleanup\n\n\n\n\nI\nm fucking hate a crap kernel module just kill my machine, either stuck or bug.\n\n\nFree buffer/structure\n\n\nRemove the \npointer\n from friends\n list/tree. If you forgot to do so, mostly you will have some silent memory corruption. So be kind, cleanup what you have done during intilization.\n\n\nReport error. Do not be SILENT.\n\n\n\n\n\n\n\n\nClever Buffer Management\n\n\n\n\nkmem_cache?\n\n\nstatic pre-allocated array?\n\n\nRing buffer?\n\n\nOther than kmem_cache, I used other two solutions to optimize various dynamic allocation in LegoOS. The motivation is very simple: some data structures will be allocated/free very very frequently at runtime. So we want to speed it up!\n\n\n\n\n\n\n\n\nSystem Building Advice\n\n\n\n\nJohn Ousterhout\n\n\nIf you don\nt know what the problem was, you haven\nt fixed it\n\n\nIf it hasn\nt been used, it doesn\nt work", 
            "title": "Essential"
        }, 
        {
            "location": "/misc/essential/#system-developing-essentials", 
            "text": "", 
            "title": "System Developing Essentials"
        }, 
        {
            "location": "/misc/essential/#tools", 
            "text": "Stack and Register Dumper  NMI and software Watchdog  Tracepoint and Ring Buffer  Profilers  Counters  Whiskey and Luck", 
            "title": "Tools"
        }, 
        {
            "location": "/misc/essential/#keep-in-mind", 
            "text": "Stress your system  Every single critical subsystem  Confident with your base subsystem  Fix bug/Improve perf at early stage    Plan ahead  Single thread, or thread pool?  How to avoid using  lock ?  What lock to use?  How to reduce  lock contention ?  Does this data structure need  reference counter ?  Should I use per-cpu data structures?  Should I pad this lock $-line aligned to avoid pingpong?     Decent Cleanup   I m fucking hate a crap kernel module just kill my machine, either stuck or bug.  Free buffer/structure  Remove the  pointer  from friends  list/tree. If you forgot to do so, mostly you will have some silent memory corruption. So be kind, cleanup what you have done during intilization.  Report error. Do not be SILENT.     Clever Buffer Management   kmem_cache?  static pre-allocated array?  Ring buffer?  Other than kmem_cache, I used other two solutions to optimize various dynamic allocation in LegoOS. The motivation is very simple: some data structures will be allocated/free very very frequently at runtime. So we want to speed it up!", 
            "title": "Keep in mind"
        }, 
        {
            "location": "/misc/essential/#system-building-advice", 
            "text": "John Ousterhout  If you don t know what the problem was, you haven t fixed it  If it hasn t been used, it doesn t work", 
            "title": "System Building Advice"
        }, 
        {
            "location": "/misc/cheatsheet/", 
            "text": "cheatsheet\n\n\nvirsh\n\n\n\n\nPass commands to QEMU in the virsh bash:\n\n1\n# qemu-monitor-command guest_os_id --hmp \ninfo cpus\n\n\n\n\n\n\n\n\nMarkdown\n\n\n\n\nEmoji cheatsheet\n\n\n\n\ntmux\n\n\n\n\nInstall \ntmux-plugins\n, it makes your terminal bling bling.\n\n\n\n\nbash\n\n\n\n\nShow current git branch in PS1:\n\n1\n2\n3\n4\n5\nparse_git_branch\n()\n \n{\n\n     git branch \n2\n /dev/null \n|\n sed -e \n/^[^*]/d\n -e \ns/* \\(.*\\)/ git:(\\1)/\n\n\n}\n\n\n\nPS1\n=\n\\[\\e[32m\\][\\u@\\h: \\W\\e[33m\\]\\$(parse_git_branch)\\[\\033[32m\\]]\\[\\e[00m\\] \n$\n \n\n\n\n\n\n\n\n\nQEMU\n\n\n\n\nRun standalone kernel:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n# Create a new directory to store the serial output from printk().\n\n\nOUTPUT_DIR\n=\ntest-output\n\n\nif\n \n[\n -e \n$OUTPUT_DIR\n \n]\n;\n \nthen\n\n        \nif\n \n[\n -f \n$OUTPUT_DIR\n \n]\n;\n \nthen\n\n                \necho\n \nERROR: \n$OUTPUT_DIR\n is not a directly\n\n                \nexit\n \n1\n\n        \nfi\n\n\nelse\n\n        mkdir -p \n$OUTPUT_DIR\n\n\nfi\n\n\n\nKERNEL\n=\narch/x86_64/boot/bzImage\n\n\nKERNEL_PARAM\n=\nconsole=ttyS0 earlyprintk=serial,ttyS0,115200\n\n\nSERIAL\n=\n-serial file:\n$OUTPUT_DIR\n/ttyS0 -serial file:\n$OUTPUT_DIR\n/ttyS1\n\n\n\n# -cpu Haswell,+tsc,+sse,+xsave,+aes,+avx,+erms,+pdpe1gb,+pge \\\n\n\n# Above -cpu option may not work with some kernels.\n\nqemu-system-x86_64 -s  \n\\\n\n        -nographic \n\\\n\n        -kernel \n$KERNEL\n -append \n$KERNEL_PARAM\n \n\\\n\n        -no-reboot \n\\\n\n        -d int,cpu_reset -D \n$OUTPUT_DIR\n/qemu.log \n\\\n\n        \n$SERIAL\n \n\\\n\n        -m 16G \n\\\n\n        -monitor stdio \n\\\n\n        -smp \ncpus\n=\n24\n,cores\n=\n12\n,threads\n=\n2\n,sockets\n=\n2\n \n\\\n\n        -numa node,cpus\n=\n0\n-11,mem\n=\n8G,nodeid\n=\n0\n \n\\\n\n        -numa node,cpus\n=\n12\n-23,mem\n=\n8G,nodeid\n=\n1\n\n\n\n\n\n\n\n\nInstall CentOS on Dell PowerEdge\n\n\n\n\nEnable \nSR-IOV\n for future usage\n\n\nPress \nF11 Boot Manager\n during boot\n\n\nFind \nIntegrated Devices\n\n\nEnable \nSR-IOV Global Enable\n\n\n\n\n\n\nPartition\n\n\n/boot\n: e.g, 50GB\n\n\nswap\n: e.g, 4G\n\n\n/\n: all left\n\n\n\n\n\n\nDon\nt forget to enable Network during installation.\n\n\nChange SSH port\n\n\nDisable \nfirewalld\n\n\nsystemctl stop firewalld\n\n\nsystemctl disable firewalld\n\n\n\n\n\n\nIf SELinux is enabled\n\n\nyum install policycoreutils-python\n\n\nsemanage port -a -t ssh_port_t -p tcp #PORTNUMBER\n\n\n\n\n\n\nChange \n/etc/ssh/sshd_config\n\n\nsystemctl restart sshd\n\n\n\n\n\n\n\n\nAvoid Typing SSH Password\n\n\n\n\nGenerate keys: \nssh-keygen -t rsa\n\n\nCopy to remote: \nssh-copy-id -i ~/.ssh/id_rsa.pub username@remotehost -p 22\n\n\n\n\nGRUB2 on Ubuntu\n\n\n\n\nNothing like grubby?! Shame on you.\n\n\nStep I: \ncat /boot/grub/grub.cfg | grep menuentry\n\n\n1\n2\nmenuentry \nUbuntu, with Linux 4.16.0\n --class ubuntu  ...\nmenuentry \nUbuntu, with Linux 4.9.92\n --class ubuntu  ...\n\n\n\n\n\nStep II: Open \n/etc/default/grub\n, change\n\n\nGRUB_DEFAULT=\nAdvanced options for Ubuntu\nUbuntu, with Linux 4.16.0\n\n\nGRUB_DEFAULT=\nAdvanced options for Ubuntu\nUbuntu, with Linux 4.9.92\n\n\n\n\n\n\nStep III: \nsudo update-grub", 
            "title": "Cheatsheet"
        }, 
        {
            "location": "/misc/cheatsheet/#cheatsheet", 
            "text": "", 
            "title": "cheatsheet"
        }, 
        {
            "location": "/misc/cheatsheet/#virsh", 
            "text": "Pass commands to QEMU in the virsh bash: 1 # qemu-monitor-command guest_os_id --hmp  info cpus", 
            "title": "virsh"
        }, 
        {
            "location": "/misc/cheatsheet/#markdown", 
            "text": "Emoji cheatsheet", 
            "title": "Markdown"
        }, 
        {
            "location": "/misc/cheatsheet/#tmux", 
            "text": "Install  tmux-plugins , it makes your terminal bling bling.", 
            "title": "tmux"
        }, 
        {
            "location": "/misc/cheatsheet/#bash", 
            "text": "Show current git branch in PS1: 1\n2\n3\n4\n5 parse_git_branch ()   { \n     git branch  2  /dev/null  |  sed -e  /^[^*]/d  -e  s/* \\(.*\\)/ git:(\\1)/  }  PS1 = \\[\\e[32m\\][\\u@\\h: \\W\\e[33m\\]\\$(parse_git_branch)\\[\\033[32m\\]]\\[\\e[00m\\]  $", 
            "title": "bash"
        }, 
        {
            "location": "/misc/cheatsheet/#qemu", 
            "text": "Run standalone kernel:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 # Create a new directory to store the serial output from printk().  OUTPUT_DIR = test-output  if   [  -e  $OUTPUT_DIR   ] ;   then \n         if   [  -f  $OUTPUT_DIR   ] ;   then \n                 echo   ERROR:  $OUTPUT_DIR  is not a directly \n                 exit   1 \n         fi  else \n        mkdir -p  $OUTPUT_DIR  fi  KERNEL = arch/x86_64/boot/bzImage  KERNEL_PARAM = console=ttyS0 earlyprintk=serial,ttyS0,115200  SERIAL = -serial file: $OUTPUT_DIR /ttyS0 -serial file: $OUTPUT_DIR /ttyS1  # -cpu Haswell,+tsc,+sse,+xsave,+aes,+avx,+erms,+pdpe1gb,+pge \\  # Above -cpu option may not work with some kernels. \nqemu-system-x86_64 -s   \\ \n        -nographic  \\ \n        -kernel  $KERNEL  -append  $KERNEL_PARAM   \\ \n        -no-reboot  \\ \n        -d int,cpu_reset -D  $OUTPUT_DIR /qemu.log  \\ \n         $SERIAL   \\ \n        -m 16G  \\ \n        -monitor stdio  \\ \n        -smp  cpus = 24 ,cores = 12 ,threads = 2 ,sockets = 2   \\ \n        -numa node,cpus = 0 -11,mem = 8G,nodeid = 0   \\ \n        -numa node,cpus = 12 -23,mem = 8G,nodeid = 1", 
            "title": "QEMU"
        }, 
        {
            "location": "/misc/cheatsheet/#install-centos-on-dell-poweredge", 
            "text": "Enable  SR-IOV  for future usage  Press  F11 Boot Manager  during boot  Find  Integrated Devices  Enable  SR-IOV Global Enable    Partition  /boot : e.g, 50GB  swap : e.g, 4G  / : all left    Don t forget to enable Network during installation.  Change SSH port  Disable  firewalld  systemctl stop firewalld  systemctl disable firewalld    If SELinux is enabled  yum install policycoreutils-python  semanage port -a -t ssh_port_t -p tcp #PORTNUMBER    Change  /etc/ssh/sshd_config  systemctl restart sshd", 
            "title": "Install CentOS on Dell PowerEdge"
        }, 
        {
            "location": "/misc/cheatsheet/#avoid-typing-ssh-password", 
            "text": "Generate keys:  ssh-keygen -t rsa  Copy to remote:  ssh-copy-id -i ~/.ssh/id_rsa.pub username@remotehost -p 22", 
            "title": "Avoid Typing SSH Password"
        }, 
        {
            "location": "/misc/cheatsheet/#grub2-on-ubuntu", 
            "text": "Nothing like grubby?! Shame on you.  Step I:  cat /boot/grub/grub.cfg | grep menuentry  1\n2 menuentry  Ubuntu, with Linux 4.16.0  --class ubuntu  ...\nmenuentry  Ubuntu, with Linux 4.9.92  --class ubuntu  ...   Step II: Open  /etc/default/grub , change  GRUB_DEFAULT= Advanced options for Ubuntu Ubuntu, with Linux 4.16.0  GRUB_DEFAULT= Advanced options for Ubuntu Ubuntu, with Linux 4.9.92    Step III:  sudo update-grub", 
            "title": "GRUB2 on Ubuntu"
        }, 
        {
            "location": "/notes/cgroup-swap/", 
            "text": "swap with cgroup\n\n\nNotes on how cgroup mm triggers swap on a user-defined \nlimit_in_bytes\n.\nThis notes assume you have adequate knowledge on overall linux mm code.\n\n\nThere are several cgroup callbacks at \nmm/memory.c\n. Those functions are called to check if cgroup can honor this page allocation.\n\n\n\n\nmem_cgroup_try_charge()\n\n\nmem_cgroup_commit_charge()\n\n\nmem_cgroup_cancel_charge()\n\n\n\n\nOkay, now at \nmm/memcontrol.c\n, the checking and swap code path:\n\n\n\n\nmem_cgroup_try_charge()\n\n\ntry_charge()\n\n\npage_counter_try_charge()\n: check if we hit \nlimit_in_bytes\n counter\n\n\ntry_to_free_mem_cgroup_pages()\n: callback to \nmm/vmscan.c\n to shrink the list (Bingo!)\n\n\n\n\n\n\n\n\n\n\n\n\nOf course there are still tons of LRU related code at \nmm/memcontrol.c\n that I don\nt understand yet. But I think they are mostly hooks/helpers for vmscan.c. Be careful about shirnk node code now, it has many hooks for cgroup, and I hope you can understand why they are there by now. It\ns super complex. Although I\nve implemented swap twice, there are still many tricks I don\nt get yet.\n\n\n\nYizhou Shan\n\nCreated: Dec 3, 2018\n\nLast Updated: Dec 4, 2018", 
            "title": "cgroup swap"
        }, 
        {
            "location": "/notes/cgroup-swap/#swap-with-cgroup", 
            "text": "Notes on how cgroup mm triggers swap on a user-defined  limit_in_bytes .\nThis notes assume you have adequate knowledge on overall linux mm code.  There are several cgroup callbacks at  mm/memory.c . Those functions are called to check if cgroup can honor this page allocation.   mem_cgroup_try_charge()  mem_cgroup_commit_charge()  mem_cgroup_cancel_charge()   Okay, now at  mm/memcontrol.c , the checking and swap code path:   mem_cgroup_try_charge()  try_charge()  page_counter_try_charge() : check if we hit  limit_in_bytes  counter  try_to_free_mem_cgroup_pages() : callback to  mm/vmscan.c  to shrink the list (Bingo!)       Of course there are still tons of LRU related code at  mm/memcontrol.c  that I don t understand yet. But I think they are mostly hooks/helpers for vmscan.c. Be careful about shirnk node code now, it has many hooks for cgroup, and I hope you can understand why they are there by now. It s super complex. Although I ve implemented swap twice, there are still many tricks I don t get yet.  \nYizhou Shan \nCreated: Dec 3, 2018 \nLast Updated: Dec 4, 2018", 
            "title": "swap with cgroup"
        }, 
        {
            "location": "/lego/log/TODO/", 
            "text": "TODO\n\n\nLast Updated: July 18, 2018\n\n\nPlanned\n\n\n\n\nTry \nfully-associative\n pcache, to see how many conflict misses can be removed (got the idea from HPCA18 google search paper)\n\n\nkmem_cache\n\n\nTSC deadline mode (one-shot tick)\n. What is the performance comparison with periodic mode?\n\n\nbatched TLB flush\n\n\n__unhash_process()\n: in exit, release pid etc.\n\n\n\n\nde_thread()\n: in exec, change pid etc.\n\n\n\n\n\n\nposix timers\n: used by exit(), wait() and others. Functions like \nposix_cpu_timers_exit_group\n.\n\n\n\n\n\n\nvDSO\n: if later we find applications are using \ngettimeofday\n, \ntime\n, and \ngetcpu\n a lot, and it truly hurt performance, then we should consider adding this in the processor side. (Check Processor Loader document for code that needs to be patched). (02/27/18)\n\n\n\n\n\n\nVA randomization\n: our loader does not add any randomization. For security reasons, we probably want to add this.\n\n\n\n\n\n\nVM Organization\n: multiple vm choice at M side, on a per-vma basis.\n\n\n\n\n\n\nfork\n:\n \ndup\n \nfree\n \npool\n: duplicate the free VA pool at both P and M.\n\n\n\n\n\n\npcache\n: send each page\ns type back. something like PcacheAnon, PcacheFile. So the pcache_evict/do_exit routine can be optimized.\n\n\n\n\n\n\nmm alloc\n: don\nt use the kmalloc to get a new mm_struct. This is a hot data structure, use get_free_page instead maybe. Like task_struct.\n\n\n\n\n\n\nfork_dup_pcache\n: have real \nvm_flags\n to guide write-protect. Get vm ranges from memory to optimize the duplication. Currently, all pages will be downgraded to read-only.\n\n\n\n\n\n\nP side mm sem\n: check if we need the sem in P side. pgfault need read, fork and others need W. Even though M side also serialize this, but  out ops are divided.\n\n\n\n\n\n\nmprotect\n: it is empty now. We assume applications are well-written. But does any of them rely on this COW feature?\n\n\n\n\n\n\nCPU_NO_HZ\n: disable timer for some cores, to reduce the overhead of timer interrupts. This is named \nCPU_NO_HZ\n and some similar Kconfigs.\n\n\n\n\n\n\nSYSCALL\n: compared with linux, we are always using the slow path, which pass all arguments. We should consider optimize this. OS-intensive applications may hurt.\n\n\n\n\n\n\nIB\n: reply is a sg list. Esp benefit pcache.\n\n\n\n\n\n\nFinished\n\n\n\n\n-\nvsyscall\n: mostly emulation-", 
            "title": "TODO"
        }, 
        {
            "location": "/lego/log/TODO/#todo", 
            "text": "Last Updated: July 18, 2018", 
            "title": "TODO"
        }, 
        {
            "location": "/lego/log/TODO/#planned", 
            "text": "Try  fully-associative  pcache, to see how many conflict misses can be removed (got the idea from HPCA18 google search paper)  kmem_cache  TSC deadline mode (one-shot tick) . What is the performance comparison with periodic mode?  batched TLB flush  __unhash_process() : in exit, release pid etc.   de_thread() : in exec, change pid etc.    posix timers : used by exit(), wait() and others. Functions like  posix_cpu_timers_exit_group .    vDSO : if later we find applications are using  gettimeofday ,  time , and  getcpu  a lot, and it truly hurt performance, then we should consider adding this in the processor side. (Check Processor Loader document for code that needs to be patched). (02/27/18)    VA randomization : our loader does not add any randomization. For security reasons, we probably want to add this.    VM Organization : multiple vm choice at M side, on a per-vma basis.    fork :   dup   free   pool : duplicate the free VA pool at both P and M.    pcache : send each page s type back. something like PcacheAnon, PcacheFile. So the pcache_evict/do_exit routine can be optimized.    mm alloc : don t use the kmalloc to get a new mm_struct. This is a hot data structure, use get_free_page instead maybe. Like task_struct.    fork_dup_pcache : have real  vm_flags  to guide write-protect. Get vm ranges from memory to optimize the duplication. Currently, all pages will be downgraded to read-only.    P side mm sem : check if we need the sem in P side. pgfault need read, fork and others need W. Even though M side also serialize this, but  out ops are divided.    mprotect : it is empty now. We assume applications are well-written. But does any of them rely on this COW feature?    CPU_NO_HZ : disable timer for some cores, to reduce the overhead of timer interrupts. This is named  CPU_NO_HZ  and some similar Kconfigs.    SYSCALL : compared with linux, we are always using the slow path, which pass all arguments. We should consider optimize this. OS-intensive applications may hurt.    IB : reply is a sg list. Esp benefit pcache.", 
            "title": "Planned"
        }, 
        {
            "location": "/lego/log/TODO/#finished", 
            "text": "- vsyscall : mostly emulation-", 
            "title": "Finished"
        }, 
        {
            "location": "/lego/log/misc/", 
            "text": "MISC\n\n\n\n\n\n\n/etc/ld.so.preload\n: GLIBC uses \naccess()\n to check if this file exist (normally it does not exist)\n1\n. This is something related to \nLD_PRELOAD\n: If both \nLD_PRELOAD\n and \n/etc/ld.so.preload\n are employed, the libraries specified by \nLD_PRELOAD\n are preloaded first. /\netc/ld.so.preload\n has a system-wide effect, causing the specified libraries to be preloaded for all programs that are executed on the system\n2\n.\n\n\n\n\n\n\nI was reading a FAST18 paper (Fail-Slow Datacenter). I found it quite interesting and some suggestions are very useful for all system designers. Especially:\n\n\n\n\nMake implicit error-masking explicit. DO NOT FAIL SILENTLY\n. Since this is not a fail-stop (\nbinary\n) issue, normally system designers will not raise exceptions. System designers should be aware of uncommon situations, raise explicit exceptions to convert a fail-slow (\nnon-binary\n) case to a fail-stop (\nbinary\n) case .Actually, this also reminds the email by Linus Torvards on BUG_ON usage\n3\n.\n\n\nExposing performance statistic information for all-level (device, firmware, system software, application)\n. However, based on my own experience, do not generate too much useless logs, it will just help to hide the root cause.\n\n\n\n\n\n\n\n\nTesting of applications is often done on a testing environment, smaller in size (perhaps only a single server) and less loaded than the \nlive\n environment. The replication behavior of such an installation may differ from a live environment in ways that mean that replication lag is unlikely to be observed in testing - masking replication-sensitive bugs.\n\n\n\n\n\n\nmmap \nPROT_NONE\n is really used by applications, or library. They have their special usage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\netc/ld.so.preload\n\n\n\n\n\n\nld.so.8.html\n\n\n\n\n\n\nLKML:BUG_ON", 
            "title": "MISC"
        }, 
        {
            "location": "/lego/log/misc/#misc", 
            "text": "/etc/ld.so.preload : GLIBC uses  access()  to check if this file exist (normally it does not exist) 1 . This is something related to  LD_PRELOAD : If both  LD_PRELOAD  and  /etc/ld.so.preload  are employed, the libraries specified by  LD_PRELOAD  are preloaded first. / etc/ld.so.preload  has a system-wide effect, causing the specified libraries to be preloaded for all programs that are executed on the system 2 .    I was reading a FAST18 paper (Fail-Slow Datacenter). I found it quite interesting and some suggestions are very useful for all system designers. Especially:   Make implicit error-masking explicit. DO NOT FAIL SILENTLY . Since this is not a fail-stop ( binary ) issue, normally system designers will not raise exceptions. System designers should be aware of uncommon situations, raise explicit exceptions to convert a fail-slow ( non-binary ) case to a fail-stop ( binary ) case .Actually, this also reminds the email by Linus Torvards on BUG_ON usage 3 .  Exposing performance statistic information for all-level (device, firmware, system software, application) . However, based on my own experience, do not generate too much useless logs, it will just help to hide the root cause.     Testing of applications is often done on a testing environment, smaller in size (perhaps only a single server) and less loaded than the  live  environment. The replication behavior of such an installation may differ from a live environment in ways that mean that replication lag is unlikely to be observed in testing - masking replication-sensitive bugs.    mmap  PROT_NONE  is really used by applications, or library. They have their special usage.        etc/ld.so.preload    ld.so.8.html    LKML:BUG_ON", 
            "title": "MISC"
        }, 
        {
            "location": "/lego/log/test-note/", 
            "text": "Scripts\n\n\nScripts used to run OSDI\n18 LegoOS experiments.\n\n\nCPU Freq\n\n\nFor fair comparision, we disable cpu freq tuning (because lego does not have it. shame!):\n\n\nAdd this to boot kernel command parameter:\n\n1\nintel_pstate=disable\n\n\n\n\n\nswap-to-ssd\n\n\nPlease remember to clear the page cache!\n\n1\n2\n3\necho 3 \n /proc/sys/vm/drop_caches\nrm -rf /tmp/mnist_model/\nlxc-execute -n test -s lxc.cgroup.memory.limit_in_bytes=128M -- python mnist.py\n\n\n\n\n\nswap-to-ramdisk\n\n\nPlease note we are using BLK_DEV_RAM, a block device based on RAM. We are NOT using tmpfs or ramfs. The difference is:.\n\n1\n2\n3\n4\n5\nmodprobe brd rd_size=16777216\ndd if=/dev/zero of=/dev/ram0 bs=4K\nmkswap /dev/ram0\nswapon /dev/ram0\nswapoff others\n\n\n\n\n\nAccelio and nbdX\n\n\nFollow \nthis\n, and \nthis\n.\n\n\nTested with\n\n\n\n\nCentOS 7.2\n\n\nkernel 3.13.1\n\n\nwuklab14, wuklab18\n\n\n\n\nSide notes\n\n\n\n\nServer side, the block device created for client, can not be raw disk/SSD. I created a file from SSD\n\n\nStick with 3.13 at both client and server. Client with 3.19 will crash\n\n\n\n\nServer:\n\n1\n2\n3\n4\ntouch /mnt/ssd/swap\ntruncate -s +4G /mnt/ssd/swap\n\nraio_server -a 10.0.0.X -p 5555 -t rdma -f 0\n\n\n\n\n\nClient:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nmodprobe xio_rdma; modprobe xio_tcp\nmodprobe nbdx\n\nnbdxadm -o create_host -i 0 -p \n10.0.0.X:5555\n\nnbdxadm -o create_device -i 0 -d 0 -f \n/mnt/ssd/swap\n\n\nnbdxadm -o show_all_devices\n\nmkswap /dev/nbdx0\nswapon /dev/nbdx0\nswapoff others\n\n\n\n\n\nInfiniswap\n\n\nTested with\n\n\n\n\nCentOS 7.2\n\n\nMLNX_OFED_LINUX-3.3-1.0.4.0-rhel7.2-x86_64\n\n\nkernel 3.13.1\n\n\n\n\nNote\n\n\n1.\nAt server side, use server ib0\ns IP address:\n\n\n1\n2\n3\n4\n5\n# ifconfig\n\n\nib0\n:\n \nflags\n=\n4163\nUP\n,\nBROADCAST\n,\nRUNNING\n,\nMULTICAST\n  \nmtu\n \n2044\n\n        \ninet\n \n10.0.0.67\n  \nnetmask\n \n255.255.255.0\n  \nbroadcast\n \n10.0.0.255\n\n\n\n.\n/\ninfiniswap\n-\ndaemon\n \n10.0.0.67\n \n9400\n\n\n\n\n\n\n\nAt client side, use server ib0\ns IP in portal.list:\n\n1\n2\n1\n10.0.0.67:9400\n\n\n\n\n\n2.\nAt client side, change the \nBACKUP_DISK\n to an unused disk, and use a CORRECT one! Otherwise, wait for kernel panic, ugh.\n\n1\n2\nUse HDD such as /dev/sdb\nA SSD will kill Infiniswap.\n\n\n\n\n\n3.\nAlso, looks like we need to remove \nmemmap\n from kernel parameter.", 
            "title": "Test Script"
        }, 
        {
            "location": "/lego/log/test-note/#scripts", 
            "text": "Scripts used to run OSDI 18 LegoOS experiments.", 
            "title": "Scripts"
        }, 
        {
            "location": "/lego/log/test-note/#cpu-freq", 
            "text": "For fair comparision, we disable cpu freq tuning (because lego does not have it. shame!):  Add this to boot kernel command parameter: 1 intel_pstate=disable", 
            "title": "CPU Freq"
        }, 
        {
            "location": "/lego/log/test-note/#swap-to-ssd", 
            "text": "Please remember to clear the page cache! 1\n2\n3 echo 3   /proc/sys/vm/drop_caches\nrm -rf /tmp/mnist_model/\nlxc-execute -n test -s lxc.cgroup.memory.limit_in_bytes=128M -- python mnist.py", 
            "title": "swap-to-ssd"
        }, 
        {
            "location": "/lego/log/test-note/#swap-to-ramdisk", 
            "text": "Please note we are using BLK_DEV_RAM, a block device based on RAM. We are NOT using tmpfs or ramfs. The difference is:. 1\n2\n3\n4\n5 modprobe brd rd_size=16777216\ndd if=/dev/zero of=/dev/ram0 bs=4K\nmkswap /dev/ram0\nswapon /dev/ram0\nswapoff others", 
            "title": "swap-to-ramdisk"
        }, 
        {
            "location": "/lego/log/test-note/#accelio-and-nbdx", 
            "text": "Follow  this , and  this .  Tested with   CentOS 7.2  kernel 3.13.1  wuklab14, wuklab18   Side notes   Server side, the block device created for client, can not be raw disk/SSD. I created a file from SSD  Stick with 3.13 at both client and server. Client with 3.19 will crash   Server: 1\n2\n3\n4 touch /mnt/ssd/swap\ntruncate -s +4G /mnt/ssd/swap\n\nraio_server -a 10.0.0.X -p 5555 -t rdma -f 0   Client:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 modprobe xio_rdma; modprobe xio_tcp\nmodprobe nbdx\n\nnbdxadm -o create_host -i 0 -p  10.0.0.X:5555 \nnbdxadm -o create_device -i 0 -d 0 -f  /mnt/ssd/swap \n\nnbdxadm -o show_all_devices\n\nmkswap /dev/nbdx0\nswapon /dev/nbdx0\nswapoff others", 
            "title": "Accelio and nbdX"
        }, 
        {
            "location": "/lego/log/test-note/#infiniswap", 
            "text": "Tested with   CentOS 7.2  MLNX_OFED_LINUX-3.3-1.0.4.0-rhel7.2-x86_64  kernel 3.13.1   Note  1.\nAt server side, use server ib0 s IP address:  1\n2\n3\n4\n5 # ifconfig  ib0 :   flags = 4163 UP , BROADCAST , RUNNING , MULTICAST    mtu   2044 \n         inet   10.0.0.67    netmask   255.255.255.0    broadcast   10.0.0.255  . / infiniswap - daemon   10.0.0.67   9400    At client side, use server ib0 s IP in portal.list: 1\n2 1\n10.0.0.67:9400   2.\nAt client side, change the  BACKUP_DISK  to an unused disk, and use a CORRECT one! Otherwise, wait for kernel panic, ugh. 1\n2 Use HDD such as /dev/sdb\nA SSD will kill Infiniswap.   3.\nAlso, looks like we need to remove  memmap  from kernel parameter.", 
            "title": "Infiniswap"
        }, 
        {
            "location": "/lego/log/log-09-2018/", 
            "text": "Sep 2018\n\n\nSep 20\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n[\n   \n54.602054\n]\n \nnr_pcache_pee_free\n:\n \n0\n\n\n[\n   \n54.602537\n]\n \nnr_pcache_pee_free_kmalloc\n:\n \n0\n\n\n[\n \n1468.765410\n]\n \nmlx4_msi_x_interrupt\n()\n:\n \nIRQ\n:\n \n27\n \nCPU\n:\n \n1\n\n\n[\n \n1468.766956\n]\n \nevent\n \nPORT_MNG_CHG\n \narrived\n\n\n[\n \n1468.768193\n]\n \nmlx4_ib\n \nhandle_port_mgmt_change_event\n:\n \nrereg\n  \n\n[\n \n1468.813660\n]\n \nib_cache\n:\n \nib_cache_update\n()\n:\n \nUpdated\n \nport\n \n1\n \nof\n \ndev\n \n0000\n:\n00\n:\n08.0\n\n\n[\n \n1468.815097\n]\n \nib_sa_event\n()\n:\n \nTODO\n\n\n[\n \n1479.178651\n]\n \nmlx4_msi_x_interrupt\n()\n:\n \nIRQ\n:\n \n27\n \nCPU\n:\n \n1\n\n\n[\n \n1479.180201\n]\n \nevent\n \nPORT_MNG_CHG\n \narrived\n\n\n[\n \n1479.181430\n]\n \nmlx4_ib\n \nhandle_port_mgmt_change_event\n:\n \nrereg\n\n\n[\n \n1479.190813\n]\n \nbad\n:\n \nscheduling\n \nfrom\n \nthe\n \nidle\n \nthread\n!\n\n\n[\n \n1479.192158\n]\n \nCPU\n:\n \n1\n \nPID\n:\n \n0\n \nComm\n:\n \nswapper\n/\n1\n \n4.0.0\n-\nlego\n+\n \n#\n146\n\n\n[\n \n1479.193622\n]\n \nStack\n:\n\n\n[\n \n1479.194408\n]\n \nffff88083fddf980\n \nffffffff8101eefc\n \nffff88083fc45d80\n \nffff88083fc45d80\n\n\n[\n \n1479.196826\n]\n \nffff88083fddf9a8\n \nffffffff8101ace4\n \n00000001\n810067\nd4\n \nffff88083fe43000\n\n\n[\n \n1479.199226\n]\n \nffffffffffff0000\n \nffff88083fddf9e0\n \nffffffff81078bf6\n \nffffffff8100e8ea\n\n\n[\n \n1479.203615\n]\n \nffffffffffff0000\n \n0000000000000000\n \nffff88083fe43000\n \nffff88083fe43000\n\n\n[\n \n1479.206532\n]\n \nffff88083fddf9f8\n \nffffffff81078ca3\n \n7ff\nfffffffffffff\n \nffff88083fddfa68\n\n\n[\n \n1479.208791\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1479.209606\n]\n \nTSK\n\n\n[\n \n1479.210322\n]\n \n[\nffffffff8101ef08\n]\n \ndequeue_task_idle\n+\n0x48\n/\n0x60\n\n\n[\n \n1479.211726\n]\n \n[\nffffffff8101ace4\n]\n \ndeactivate_task\n+\n0x44\n/\n0x50\n\n\n[\n \n1479.213092\n]\n \n[\nffffffff81078bf6\n]\n \n__schedule\n+\n0x146\n/\n0x1e0\n\n\n[\n \n1479.214410\n]\n \n[\nffffffff8100e8ea\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n1479.215960\n]\n \n[\nffffffff81078ca3\n]\n \nschedule\n+\n0x13\n/\n0x30\n\n\n[\n \n1479.217211\n]\n \n[\nffffffff810789da\n]\n \nschedule_timeout\n+\n0x12a\n/\n0x1a0\n\n\n[\n \n1479.218625\n]\n \n[\nffffffff81079e54\n]\n \n__down_common\n+\n0xaa\n/\n0x103\n\n\n[\n \n1479.219904\n]\n \n[\nffffffff81079ec5\n]\n \n__down\n+\n0x18\n/\n0x1a\n\n\n[\n \n1479.221046\n]\n \n[\nffffffff8101f24c\n]\n \ndown\n+\n0x3c\n/\n0x40\n\n\n[\n \n1479.222163\n]\n \n[\nffffffff8104dba7\n]\n \n__mlx4_cmd\n+\n0x1d7\n/\n0x3c0\n\n\n[\n \n1479.223397\n]\n \n[\nffffffff810619de\n]\n \nmlx4_MAD_IFC\n+\n0x22e\n/\n0x490\n\n\n[\n \n1479.224666\n]\n \n[\nffffffff8105d321\n]\n \n__mlx4_ib_query_pkey\n+\n0x181\n/\n0x240\n\n\n[\n \n1479.226045\n]\n \n[\nffffffff8105d3f3\n]\n \nmlx4_ib_query_pkey\n+\n0x13\n/\n0x20\n\n\n[\n \n1479.227365\n]\n \n[\nffffffff81064cb4\n]\n \nib_query_pkey\n+\n0x14\n/\n0x20\n\n\n[\n \n1479.228617\n]\n \n[\nffffffff810651a7\n]\n \nib_cache_update\n+\n0x237\n/\n0x480\n\n\n[\n \n1479.229862\n]\n \n[\nffffffff810657f8\n]\n \nib_cache_event\n+\n0x28\n/\n0x30\n\n\n[\n \n1479.231026\n]\n \n[\nffffffff81064bf0\n]\n \nib_dispatch_event\n+\n0x40\n/\n0x70\n\n\n[\n \n1479.232222\n]\n \n[\nffffffff810627c8\n]\n \nhandle_port_mgmt_change_event\n+\n0x158\n/\n0x1c0\n\n\n[\n \n1479.233602\n]\n \n[\nffffffff8105b5ac\n]\n \nmlx4_ib_event\n+\n0x7c\n/\n0xa0\n\n\n[\n \n1479.234744\n]\n \n[\nffffffff8104ee55\n]\n \nmlx4_dispatch_event\n+\n0x65\n/\n0x90\n\n\n[\n \n1479.235968\n]\n \n[\nffffffff8104f2c3\n]\n \nmlx4_eq_int\n+\n0x273\n/\n0x4f0\n\n\n[\n \n1479.237113\n]\n \n[\nffffffff8104f616\n]\n \nmlx4_msi_x_interrupt\n+\n0x36\n/\n0x40\n\n\n[\n \n1479.238352\n]\n \n[\nffffffff81017894\n]\n \nhandle_irq_event_percpu\n+\n0x24\n/\n0xa0\n\n\n[\n \n1479.239584\n]\n \n[\nffffffff81017938\n]\n \nhandle_irq_event\n+\n0x28\n/\n0x50\n\n\n[\n \n1479.240696\n]\n \n[\nffffffff810180fe\n]\n \nhandle_edge_irq\n+\n0x5e\n/\n0xc0\n\n\n[\n \n1479.241794\n]\n \n[\nffffffff810054c3\n]\n \ndo_IRQ\n+\n0x43\n/\n0xd0\n\n\n[\n \n1479.242779\n]\n \n[\nffffffff810067d4\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n1479.243971\n]\n \n[\nffffffff8100e0aa\n]\n \ncommon_interrupt\n+\n0x6a\n/\n0x6a\n\n\n[\n \n1479.245084\n]\n \n[\nffffffff8101c6b0\n]\n \n?\n \ncpu_idle\n+\n0x10\n/\n0x30\n\n\n[\n \n1479.246123\n]\n \n[\nffffffff81003425\n]\n \nstart_secondary_cpu\n+\n0x55\n/\n0x60\n\n\n[\n \n1479.247278\n]\n \nEOT\n\n\n\n\n\n\n\nSep 17\n\n\nCan not believe I\nm wasting time on this crap X again.\n\n\nSep 16\n\n\nTests done today:\n\n\n\n\n\n\n\n\nSetting\n\n\nLog\n\n\nnr_workers\n\n\nTracing (strace/counter/profiling)\n\n\nRuntime (s)\n\n\npcache_flush_net (us)\n\n\n\n\n\n\n\n\n\n\nTF-MNIST, Linux\n\n\n\n\n\n\n\n\n13.2s\n\n\n\n\n\n\n\n\nTF4-MNIST, 128MB\n\n\n0916-w14-1\n\n\n1\n\n\nON\n\n\navg 48.5s\n\n\n9891\n\n\n\n\n\n\nTF4-MNIST, 128MB\n\n\n0916-w14-2\n\n\n1\n\n\nOFF\n\n\n(46.1+44.6+45.5+45.7+44)/5 = 45.2s\n\n\nN/A\n\n\n\n\n\n\nTF4-MNIST, 128MB\n\n\n0916-w14-4\n\n\n4\n\n\nON\n\n\n(43.4+44+43.9+42.6+42.1)/5=43.2\n\n\n8351\n\n\n\n\n\n\nTF4-MNIST, 128MB\n\n\n0916-w14-3\n\n\n4\n\n\nOFF\n\n\n(40.1+42.1+42.0+41.7+42.1)/5 = 41.6\n\n\nN/A\n\n\n\n\n\n\nTF4-Cifar, Linux\n\n\n\n\n\n\n\n\n235.5s\n\n\n\n\n\n\n\n\nTF4-Cifar, 128MB\n\n\n0916-w14-5\n\n\n4\n\n\nOFF\n\n\n(636.2+635.0+636.8+637.2+634.1)/5=635.8\n\n\nN/A\n\n\n\n\n\n\nTF4-Cifar, 128MB\n\n\n0916-w14-6\n\n\n1\n\n\nOFF\n\n\n(660.2+662.2+662.8+663.8+661+5)/5=663s\n\n\nN/A\n\n\n\n\n\n\nTF4-Cifar, 256MB\n\n\n0916-w14-7\n\n\n1\n\n\nOFF\n\n\n486s\n\n\nN/A\n\n\n\n\n\n\n\n\nSep 15\n\n\nDAMN.\n\n\nLet us summarize today. Okay. Fixed the double-post-cqe issue. Hehe. The post part is the only fucking left code that I did not look into at fit_poll_recv_cq. And, ironically, there is no error checking for ib_post_recv(), which won\nt generate any error/warning.\n\n\nerror checking error checking\n\n\nAnyway fuck it.\n\n\nToday I created a new tag v0.0.9, hope we have a stable net. The RPC profile code is very stressing, and fit survived.\n\n\nThe following wanring is fixed by post rx_depth/2.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n[\n \n1812.017204\n]\n \nfit\n:\n \nTo\n \nalign\n \nfirst\n \nQPN\n,\n \nwe\n \nskipped\n:\n \n#\n72\n \n#\n72\n \n#\n73\n \n#\n74\n \n#\n75\n \n#\n76\n \n#\n77\n \n#\n78\n \n#\n79\n\n\n[\n \n1812.157570\n]\n \nfit\n:\n \nfit_post_receives_message\n()\n-\n628\n \nCPU\n \n2\n \nFail\n \nto\n \npost\n \nrecv\n \nconn_id\n:\n \n12\n\n\n[\n \n1812.166013\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1812.171152\n]\n \nWARNING\n:\n \nCPU\n:\n \n2\n \nPID\n:\n \n16\n \nat\n \nnet\n/\nlego\n/\nfit_internal\n.\nc\n:\n629\n \nfit_post_receives_message\n.\nisra\n.7\n+\n0xce\n/\n0x100\n\n\n[\n \n1812.182302\n]\n \nCPU\n:\n \n2\n \nPID\n:\n \n16\n \nComm\n:\n \nib\n-\ninitd\n \n4.0.0\n-\nlego\n+\n \n#\n95\n\n\n[\n \n1812.188314\n]\n \nStack\n:\n\n\n[\n \n1812.190544\n]\n \nffff880ff98bfd50\n \nffffffff8101299b\n \n0000000000000\ncff\n \n0000000000000060\n\n\n[\n \n1812.198689\n]\n \n0000000000000\nd00\n \n0000000000000100\n \nffff880ff98dc030\n \nffff880ff98bfd60\n\n\n[\n \n1812.206834\n]\n \nffffffff81012a8f\n \nffff880ff98bfdc8\n \nffffffff810743de\n \nfffffff4fffffff4\n\n\n[\n \n1812.214978\n]\n \nffff880ff98bfd80\n \n0000000000000000\n \n0000000000000\ncff\n \n0000000000000000\n\n\n[\n \n1812.223124\n]\n \n0000000000000000\n \nffff880ff98dc000\n \n0000000000000000\n \n000000000000000\nc\n\n\n[\n \n1812.231269\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1812.233984\n]\n \nTSK\n\n\n[\n \n1812.236116\n]\n \n[\nffffffff810129a7\n]\n \n__warn\n.\nconstprop\n.0\n+\n0xa7\n/\n0x100\n\n\n[\n \n1812.242613\n]\n \n[\nffffffff81012a8f\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n \n1812.248915\n]\n \n[\nffffffff810743de\n]\n \nfit_post_receives_message\n.\nisra\n.7\n+\n0xce\n/\n0x100\n\n\n[\n \n1812.256770\n]\n \n[\nffffffff81076a1a\n]\n \nfit_add_newnode\n+\n0xca\n/\n0x170\n\n\n[\n \n1812.262974\n]\n \n[\nffffffff81079d10\n]\n \nfit_establish_conn\n+\n0x7b0\n/\n0xaa0\n\n\n[\n \n1812.269568\n]\n \n[\nffffffff81073ce8\n]\n \n?\n \nibv_add_one\n+\n0x98\n/\n0x120\n\n\n[\n \n1812.275580\n]\n \n[\nffffffff810741f0\n]\n \n?\n \nibapi_get_node_id\n+\n0x20\n/\n0x20\n\n\n[\n \n1812.282076\n]\n \n[\nffffffff81074258\n]\n \nlego_ib_init\n+\n0x68\n/\n0xf0\n\n\n[\n \n1812.287893\n]\n \n[\nffffffff81023261\n]\n \nkthread\n+\n0x111\n/\n0x130\n\n\n[\n \n1812.293421\n]\n \n[\nffffffff81023150\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n \n1812.299820\n]\n \n[\nffffffff8100eaf2\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n[\n \n1812.305735\n]\n \nEOT\n\n\n[\n \n1812.307868\n]\n \n---\n[\n \nend\n \ntrace\n \n0000000000000000\n \n]\n---\n\n\n\n\n\n\nSep 11\n\n\nGot this log, 5 machine, p2s_open, S side has this issue. Damn.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n[\n \n1672.962279\n]\n                                                                                                                                                                                                                                \n\n*****\n                                                                                                                                                                                                                                         \n\n*****\n \nFail\n \nto\n \nto\n \nget\n \nthe\n \nCQE\n \nfrom\n \nsend_cq\n \nafter\n \n20\n \nseconds\n!\n                                                                                                                                                                                   \n\n*****\n \nThis\n \nmeans\n \nthe\n \npacket\n \nwas\n \nlost\n \nand\n \nsomething\n \nwent\n \nwrong\n                                                                                                                                                                                 \n\n*****\n \nwith\n \nyour\n \nNIC\n...\n\n\n*****\n \nconnection_id\n:\n \n11\n \ndest\n \nnode\n:\n \n0\n\n\n*****\n\n\n[\n \n1673.061668\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1673.074937\n]\n \nWARNING\n:\n \nCPU\n:\n \n10\n \nPID\n:\n \n4624\n \nat\n \n/\nroot\n/\nys\n/\nLegoOS_2M\n/\nlinux\n-\nmodules\n/\nfit\n/\nfit_internal\n.\nc\n:\n956\n \nfit_internal_poll_sendcq\n+\n0xda\n/\n0x130\n \n[\nfit\n]()\n\n\n[\n \n1673.101557\n]\n \nModules\n \nlinked\n \nin\n:\n \nstorage\n(\nOF\n)\n \nfit\n(\nOF\n)\n \nxt_CHECKSUM\n \niptable_mangle\n \nipt_MASQUERADE\n \niptable_nat\n \nnf_nat_ipv4\n \nnf_nat\n \nnf_conntrack_ipv4\n \nnf_defrag_ipv4\n \nxt_conntrack\n \nnf_conntrack\n \nipt_REJECT\n \ntun\n \nbridge\n \nstp\n \nllc\n \nebtable_filter\n \nebtable\n\n\ns\n \nip6table_filter\n \nip6_tables\n \niptable_filter\n \nxprtrdma\n \nsunrpc\n \nib_isert\n \niscsi_target_mod\n \nib_iser\n \nlibiscsi\n \nscsi_transport_iscsi\n \nib_srpt\n \ntarget_core_mod\n \nib_srp\n \nscsi_transport_srp\n \nscsi_tgt\n \nib_ipoib\n \nrdma_ucm\n \nib_ucm\n \nib_uverbs\n \nib_umad\n \nrdma_cm\n \nib_c\n\n\nm\n \niw_cm\n \nib_addr\n \nx86_pkg_temp_thermal\n \ncoretemp\n \nkvm_intel\n \nkvm\n \ncrc32_pclmul\n \nghash_clmulni_intel\n \naesni_intel\n \nlrw\n \ngf128mul\n \nglue_helper\n \nipmi_devintf\n \nablk_helper\n \ncryptd\n \nipmi_si\n \niTCO_wdt\n \nipmi_msghandler\n \niTCO_vendor_support\n \ndcdbas\n \nsg\n \npcspkr\n \nshpchp\n\n \nacpi_power_meter\n \nlpc_ich\n \nmfd_core\n \nwmi\n \nmperf\n \nuinput\n \nbinfmt_misc\n \nip_tables\n \next4\n \nmbcache\n \njbd2\n \nmlx4_ib\n\n\n[\n \n1673.182609\n]\n  \nib_sa\n \nib_mad\n \nib_core\n \nmlx4_en\n \nsd_mod\n \ncrc_t10dif\n \nmgag200\n \nsyscopyarea\n \nsysfillrect\n \nsysimgblt\n \ni2c_algo_bit\n \ndrm_kms_helper\n \nttm\n \ndrm\n \nahci\n \ncrc32c_intel\n \nlibahci\n \nmlx4_core\n \nlibata\n \ntg3\n \nnvme\n \nmegaraid_sas\n \nptp\n \ni2c_core\n \npps_core\n \ndm_mirror\n\n\ndm_region_hash\n \ndm_log\n \ndm_mod\n\n\n[\n \n1673.222604\n]\n \nCPU\n:\n \n10\n \nPID\n:\n \n4624\n \nComm\n:\n \nlego\n-\nstoraged\n \nTainted\n:\n \nGF\n       \nW\n  \nO\n \n3.11.1\n-\nvanilla\n \n#\n1\n\n\n[\n \n1673.235825\n]\n \nHardware\n \nname\n:\n \nDell\n \nInc\n.\n \nPowerEdge\n \nR730\n/\n05\n99\nV5\n,\n \nBIOS\n \n1.5.4\n \n10\n/\n002\n/\n2015\n\n\n[\n \n1673.248883\n]\n  \n000000000000000\n9\n \nffff88102186b9f8\n \nffffffff8159a5a4\n \n0000000000000000\n\n\n[\n \n1673.261795\n]\n  \nffff88102186ba30\n \nffffffff810641bd\n \nffff882027180400\n \n00000004\na817c800\n\n\n[\n \n1673.274499\n]\n  \n000001\n80\ndc3abde5\n \n0000000000000000\n \n0000000000000000\n \nffff88102186ba40\n\n\n[\n \n1673.287034\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1673.299259\n]\n  \n[\nffffffff8159a5a4\n]\n \ndump_stack\n+\n0x45\n/\n0x56\n\n\n[\n \n1673.311371\n]\n  \n[\nffffffff810641bd\n]\n \nwarn_slowpath_common\n+\n0x7d\n/\n0xa0\n\n\n[\n \n1673.323268\n]\n  \n[\nffffffff8106429a\n]\n \nwarn_slowpath_null\n+\n0x1a\n/\n0x20\n\n\n[\n \n1673.334892\n]\n  \n[\nffffffffa063669a\n]\n \nfit_internal_poll_sendcq\n+\n0xda\n/\n0x130\n \n[\nfit\n]\n\n\n[\n \n1673.346348\n]\n  \n[\nffffffff81093e25\n]\n \n?\n \ncheck_preempt_curr\n+\n0x85\n/\n0xa0\n\n\n[\n \n1673.357575\n]\n  \n[\nffffffffa06367f7\n]\n \nfit_send_message_with_rdma_write_with_imm_request\n+\n0x107\n/\n0x3f0\n \n[\nfit\n]\n\n\n[\n \n1673.368777\n]\n  \n[\nffffffff8107bde4\n]\n \n?\n \nwake_up_worker\n+\n0x24\n/\n0x30\n\n\n[\n \n1673.379741\n]\n  \n[\nffffffffa0636ee9\n]\n \nfit_reply_message\n+\n0x89\n/\n0xa0\n \n[\nfit\n]\n\n\n[\n \n1673.390497\n]\n  \n[\nffffffffa063507b\n]\n \nibapi_reply_message\n+\n0x1b\n/\n0x20\n \n[\nfit\n]\n\n\n[\n \n1673.401039\n]\n  \n[\nffffffffa0646785\n]\n \nhandle_open_request\n+\n0xa5\n/\n0xe0\n \n[\nstorage\n]\n\n\n[\n \n1673.411367\n]\n  \n[\nffffffffa0646106\n]\n \nstorage_manager\n+\n0x106\n/\n0x300\n \n[\nstorage\n]\n\n\n[\n \n1673.421470\n]\n  \n[\nffffffffa0646000\n]\n \n?\n \n0xffffffffa0645fff\n\n\n[\n \n1673.431297\n]\n  \n[\nffffffffa0646000\n]\n \n?\n \n0xffffffffa0645fff\n\n\n[\n \n1673.440797\n]\n  \n[\nffffffff81085ec0\n]\n \nkthread\n+\n0xc0\n/\n0xd0\n\n\n[\n \n1673.450034\n]\n  \n[\nffffffff81085e00\n]\n \n?\n \ninsert_kthread_work\n+\n0x40\n/\n0x40\n\n\n[\n \n1673.459063\n]\n  \n[\nffffffff815a94ac\n]\n \nret_from_fork\n+\n0x7c\n/\n0xb0\n\n\n[\n \n1673.467837\n]\n  \n[\nffffffff81085e00\n]\n \n?\n \ninsert_kthread_work\n+\n0x40\n/\n0x40\n\n\n[\n \n1673.476400\n]\n \n---\n[\n \nend\n \ntrace\n \nf9b19a31d409f910\n \n]\n---\n\n\n[\n \n1695.867276\n]\n \nstorage_self_monitor\n()\n:\n \nin_handler\n=\n1\n\n\n[\n \n1695.875906\n]\n \nhandle_replica_flush\n:\n \n0\n\n\n[\n \n1695.884613\n]\n \nhandle_replica_vma\n:\n \n0\n\n\n[\n \n1695.893265\n]\n \nhandle_replica_read\n:\n \n12740\n\n\n[\n \n1695.901920\n]\n \nhandle_replica_write\n:\n \n0\n\n\n[\n \n1713.012565\n]\n \nINFO\n:\n \nrcu_sched\n \nself\n-\ndetected\n \nstall\n \non\n \nCPU\n \n{\n \n10\n}\n  \n(\nt\n=\n60001\n \njiffies\n \ng\n=\n7646\n \nc\n=\n7645\n \nq\n=\n0\n)\n\n\n[\n \n1713.013339\n]\n \nsending\n \nNMI\n \nto\n \nall\n \nCPUs\n:\n\n\n[\n \n1713.013573\n]\n \nINFO\n:\n \nrcu_sched\n \ndetected\n \nstalls\n \non\n \nCPUs\n/\ntasks\n:\n \n{\n \n10\n}\n \n(\ndetected\n \nby\n \n15\n,\n \nt\n=\n60002\n \njiffies\n,\n \ng\n=\n7646\n,\n \nc\n=\n7645\n,\n \nq\n=\n0\n)\n\n\n[\n \n1713.014807\n]\n \nNMI\n \nbacktrace\n \nfor\n \ncpu\n \n0\n\n\n[\n \n1713.015685\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n4591\n \nComm\n:\n \nwq_handler\n \nTainted\n:\n \nGF\n       \nW\n  \nO\n \n3.11.1\n-\nvanilla\n \n#\n1\n\n\n[\n \n1713.016624\n]\n \nHardware\n \nname\n:\n \nDell\n \nInc\n.\n \nPowerEdge\n \nR730\n/\n05\n99\nV5\n,\n \nBIOS\n \n1.5.4\n \n10\n/\n002\n/\n2015\n\n\n[\n \n1713.017575\n]\n \ntask\n:\n \nffff88201f193b40\n \nti\n:\n \nffff88101a34a000\n \ntask\n.\nti\n:\n \nffff88101a34a000\n\n\n[\n \n1713.018530\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffffa0636b55\n]\n  \n[\nffffffffa0636b55\n]\n \nwaiting_queue_handler\n+\n0x75\n/\n0x140\n \n[\nfit\n]\n\n\n[\n \n1713.018530\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffffa0636b55\n]\n  \n[\nffffffffa0636b55\n]\n \nwaiting_queue_handler\n+\n0x75\n/\n0x140\n \n[\nfit\n]\n\n\n[\n \n1713.019512\n]\n \nRSP\n:\n \n001\n8\n:\nffff88101a34be78\n  \nEFLAGS\n:\n \n000002\n96\n\n\n[\n \n1713.020444\n]\n \nRAX\n:\n \n00000000000\n80080\n \nRBX\n:\n \nffff8820200253f0\n \nRCX\n:\n \nffff88201f193b40\n\n\n[\n \n1713.021364\n]\n \nRDX\n:\n \n0000000000000001\n \nRSI\n:\n \nffff88103f414760\n \nRDI\n:\n \nffff88103f4146c0\n\n\n[\n \n1713.022260\n]\n \nRBP\n:\n \nffff88101a34bec8\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \n0000000000000001\n\n\n[\n \n1713.023138\n]\n \nR10\n:\n \n0000000000000001\n \nR11\n:\n \nffffffffa0636b55\n \nR12\n:\n \nffff8820200253c0\n\n\n[\n \n1713.024000\n]\n \nR13\n:\n \nffff881022005000\n \nR14\n:\n \nffffffffa063b8e4\n \nR15\n:\n \nffffffffa063b8e4\n\n\n[\n \n1713.024839\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88103f400000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n1713.025676\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n1713.026481\n]\n \nCR2\n:\n \n00007f\nd77ef46000\n \nCR3\n:\n \n0000000001\n876000\n \nCR4\n:\n \n00000000001407f\n0\n\n\n[\n \n1713.027273\n]\n \nStack\n:\n\n\n[\n \n1713.028035\n]\n  \nffff881000000000\n \nffff881000300660\n \nffff882000000003\n \n0000000000000000\n\n\n[\n \n1713.028818\n]\n  \nffff881000000000\n \nffff881021eafc38\n \nffff881022005000\n \nffffffffa0636ae0\n\n\n[\n \n1713.029585\n]\n  \n0000000000000000\n \n0000000000000000\n \nffff88101a34bf48\n \nffffffff81085ec0\n\n\n[\n \n1713.030350\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1713.031098\n]\n  \n[\nffffffffa0636ae0\n]\n \n?\n \nfit_send_message_with_rdma_write_with_imm_request\n+\n0x3f0\n/\n0x3f0\n \n[\nfit\n]\n\n\n[\n \n1713.031875\n]\n  \n[\nffffffff81085ec0\n]\n \nkthread\n+\n0xc0\n/\n0xd0\n\n\n[\n \n1713.032641\n]\n  \n[\nffffffff81085e00\n]\n \n?\n \ninsert_kthread_work\n+\n0x40\n/\n0x40\n\n\n[\n \n1713.033407\n]\n  \n[\nffffffff815a94ac\n]\n \nret_from_fork\n+\n0x7c\n/\n0xb0\n\n\n[\n \n1713.034171\n]\n  \n[\nffffffff81085e00\n]\n \n?\n \ninsert_kthread_work\n+\n0x40\n/\n0x40\n\n\n\n\n\n\n\nSep 08\n\n\nCheck this log out:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n]---\n[  427.218569] STDOUT: ---[\nINFO:tensorflow:Graph was finalized.\n\n]---\n[  427.416043] BUG: unable to handle kernel NULL pointer dereference at           (null)\n[  427.424583] IP: [\nffffffff810748fb\n] fit_poll_recv_cq+0x5cb/0x860\n[  427.431370] mlx4_msi_x_interrupt(): IRQ: 27 CPU: 0\n[  427.436702] PGD 0\n[  427.438932] CQ_ERROR CQ overrun on CQN 000082\n[  427.443780] Oops: 0002 [#1] SMP PROCESSOR\n[  427.448240] event qp_event arrived\n[  427.452022] CPU: 6 PID: 18 Comm: FIT_RecvCQ-0 4.0.0-lego+ #23\n[  427.458421] event qp_event arrived\n[  427.462203] RIP: 0010:[\nffffffff810748fb\n]  [\nffffffff810748fb\n] fit_poll_recv_cq+0x5cb/0x860\n[  427.471704] RSP: 0000:ffff881023e3fe60  EFLAGS: 00010287\n[  427.477618] RAX: 0000000000000000 RBX: 000000002aaaaaab RCX: 0000000000000004\n[  427.485570] RDX: 0000000000000000 RSI: 0000000000000053 RDI: 0000000000000000\n[  427.493520] RBP: ffff881023e3fec0 R08: 0000000000000001 R09: ffff881039900000\n[  427.501470] R10: 0000000000000000 R11: ffff881039918000 R12: ffff8810398f2000\n[  427.509421] R13: 0000000000000000 R14: 0000000000000001 R15: ffff881023e25008\n[  427.517371] event qp_event arrived\n[  427.521153] FS:  0000000000000000(0000) GS:ffff88107fc60000(0000) knlGS:0000000000000000\n[  427.530169] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[  427.536569] CR2: 0000000000000000 CR3: 000000000117a000 CR4: 00000000000406a0\n[  427.544519] event qp_event arrived\n\n\n\n\n\nTrying to tune FIT\ns number polling threads. This could be the throughput/latency killer.\n\n\n128M\n\n\n\n\n\n\n\n\nP num_polling\n\n\nM worker\n\n\nM num_polling\n\n\nRuntime (s)\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n46.8s\n\n\n\n\n\n\n1\n\n\n4\n\n\n1\n\n\n\n\n\n\n\n\n\n\nSep 07\n\n\nSet up Infiniswap again. What a fucking crap code, and crash the kernel out of nowhere. crap crap crap.\n\n\nHmm, Linux will tune the CPU freq during runtime, will be higher than 2.4GHz. So disable it, make it a fair comparison with Lego.\n\n\nintel_pstate=disable.\n\n\nSep 06\n\n\nDid two optimizations on pcache, both are buffer management.\nEspecially the pcache rmap case. In both opts, we kind of use static/pre-allocated array to serve dynamic allocation.\n\n\nThis is a better solution than using kmem_cache, faster. kmem_cache will be a more general solution here.\n\n\nkmem_cache, FIFO queue (thpool buffer), static preallocated array (rmap, clflush)\n Buffer management is really a very important thing in system building. I should be aware at the beginning next time.\n\n\nThese changes are in commits:\n\n1\n2\n6e0cf6c5c64edbe445a27cf55f86ac51f8a897b3\n73377cafce95ffa0cfb155f77cac97456a5e4a71\n\n\n\n\n\nSep 05\n\n\nAlright. Besides some flaws/bugs in some kfree stuff, LegoOS now actually is very robust! Ran a quick git summary:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n project  : LegoOS\n repo age : 1 year, 11 months\n active   : 358 days\n commits  : 1540\n files    : 1161\n authors  :\n  1317  Yizhou Shan                  85.5%\n   120  root                         7.8%\n    36  hythzz                       2.3%\n    27  yilun                        1.8%\n    16  Yutong Huang                 1.0%\n    10  Build Android                0.6%\n     8  Yiying Zhang                 0.5%\n     4  sumukh1991                   0.3%\n     1  Yizhou SHan                  0.1%\n     1  Sumukh Hallymysore Ravindra  0.1%\n\n\n\n\n\n\nOf course, there are still PLENY room for improvement, and I know where. At this time, I really think we need something like kmem_cache, which is so fucking useful. It can probably further reduce much overhead.\n\n\nSep 04\n\n\nTrying the perset eviction list mechanism, instead of victim cache. The benefit of using this is: we will no longer be bottelnecked by victim cache anymore. Each faulting thread will do eviction/flush within its own context.\n\n\nFor 4 threads MNIST, I saw 3 seconds reduction.\n\n\nRemoved the bitmap, use per pcache set counter for quick reference.\n\n\nSep 03\n\n\nWith DEBUG_MM, try enable HAVE_FREE directory by directory\n\n\n-\n\n\n-\n\n\nupdate_wall_time+0x44 is where we call tsc_read. And this has been called many times (HZ per second). All of a sudden, the pointer got crashed. Who wrote to this code memory?? Remote RDMA?\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n[\n \n1052.470714\n]\n \ngeneral\n \nprotection\n \nfault\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n \n1052.477113\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n15\n \nComm\n:\n \nib_mad1\n \n4.0.0\n-\nlego\n+\n \n#\n509\n\n\n[\n \n1052.483125\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81015764\n]\n  \n[\nffffffff81015764\n]\n \nupdate_wall_time\n+\n0x44\n/\n0x6f0\n\n\n[\n \n1052.492530\n]\n \nRSP\n:\n \n0000\n:\nffff88103ad9fc88\n  \nEFLAGS\n:\n \n00010046\n\n\n[\n \n1052.498445\n]\n \nRAX\n:\n \n4510ff\nffffff8118\n \nRBX\n:\n \n0380ff\nffffffffff\n \nRCX\n:\n \n0000000000000001\n\n\n[\n \n1052.506396\n]\n \nRDX\n:\n \nffff88103ad9fd28\n \nRSI\n:\n \n0000000000000000\n \nRDI\n:\n \n4510ff\nffffff8118\n\n\n[\n \n1052.514346\n]\n \nRBP\n:\n \nffff88103ad9fcd0\n \nR08\n:\n \n000000000000001f\n \nR09\n:\n \n0000000000000000\n\n\n[\n \n1052.522298\n]\n \nR10\n:\n \n000000000000002\n9\n \nR11\n:\n \nffff881013f8e130\n \nR12\n:\n \naaff0000024a2677\n\n\n[\n \n1052.530248\n]\n \nR13\n:\n \n0000000000000000\n \nR14\n:\n \nffff88103ad85228\n \nR15\n:\n \nffff88103ae0c000\n\n\n[\n \n1052.538199\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc00000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n1052.547216\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n1052.553616\n]\n \nCR2\n:\n \n0000000000000000\n \nCR3\n:\n \n000000000117\nb000\n \nCR4\n:\n \n00000000000406\nb0\n\n\n[\n \n1052.561567\n]\n \nStack\n:\n\n\n[\n \n1052.563797\n]\n \n00000000000000\n86\n \nffff88107fc05d80\n \nffff88103ad85000\n \n0000000000000000\n\n\n[\n \n1052.571941\n]\n \nffff88107fc04980\n \n0000000000000000\n \n0000000000000000\n \nffff88103ad85228\n\n\n[\n \n1052.580085\n]\n \nffff88103ae0c000\n \nffff88103ad9fce8\n \nffffffff81017557\n \n000000003\nad9fe10\n\n\n[\n \n1052.588230\n]\n \nffff88103ad9fd10\n \nffffffff810067a4\n \nffffffff81088040\n \nffff88107fc05d80\n\n\n[\n \n1052.596375\n]\n \nffff88103ad85000\n \nffff88103ad9fdf8\n \nffffffff8100e8ea\n \nffff88103ad9fd28\n\n\n[\n \n1052.604520\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1052.607236\n]\n \nTSK\n\n\n[\n \n1052.609368\n]\n \n[\nffffffff81017557\n]\n \ntick_handle_periodic\n+\n0x67\n/\n0x70\n\n\n[\n \n1052.615961\n]\n \n[\nffffffff810067a4\n]\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n1052.622555\n]\n \n[\nffffffff8100e8ea\n]\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n1052.629633\n]\n \n[\nffffffff8107b488\n]\n \n?\n \n__schedule\n+\n0xf8\n/\n0x1e0\n\n\n[\n \n1052.635548\n]\n \n[\nffffffff8107b583\n]\n \nschedule\n+\n0x13\n/\n0x30\n\n\n[\n \n1052.640978\n]\n \n[\nffffffff8106c98e\n]\n \nib_mad_completion_handler\n+\n0x5de\n/\n0xc20\n\n\n[\n \n1052.648250\n]\n \n[\nffffffff8101de3b\n]\n \n?\n \ndequeue_task_rt\n+\n0x1b\n/\n0x180\n\n\n[\n \n1052.654648\n]\n \n[\nffffffff8106c3b0\n]\n \n?\n \nib_mad_send_done_handler\n.\nisra\n.22\n+\n0x4e0\n/\n0x4e0\n\n\n[\n \n1052.662793\n]\n \n[\nffffffff81022af6\n]\n \nkthread\n+\n0xf6\n/\n0x110\n\n\n[\n \n1052.668223\n]\n \n[\nffffffff81022a00\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n \n1052.674622\n]\n \n[\nffffffff8100eb72\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n[\n \n1052.680538\n]\n \nEOT\n\n\n[\n \n1052.682670\n]\n \nCode\n:\n \ndb\n \ne4\n \n16\n \n00\n \n79\n \n0\nd\n \nf3\n \n90\n \n80\n \n3\nd\n \nd0\n \ne4\n \n16\n \n00\n \n00\n \n7\ne\n \nf5\n \neb\n \nea\n \n48\n \n8\nb\n \n1\nd\n \nfd\n \nfa\n \n1f\n \n00\n \n48\n \n8\nb\n \n05\n \ne6\n \nfa\n \n1f\n \n00\n \n4\nc\n \n8\nb\n \n25\n \nf7\n \nfa\n \n1f\n \n00\n \n48\n \n89\n \nc7\n \nff\n \n50\n \n28\n \n49\n \n89\n \nc7\n \n48\n \n89\n \nd8\n \n4\nd\n \n29\n \ne7\n \n48\n \nd1\n \ne8\n \n49\n \n21\n \ndf\n \n48\n \nf7\n \nd0\n\n\n[\n \n1052.703711\n]\n \nRIP\n  \n[\nffffffff81015764\n]\n \nupdate_wall_time\n+\n0x44\n/\n0x6f0\n\n\n[\n \n1052.710498\n]\n  \nRSP\n \nffff88103ad9fc88", 
            "title": "Sep 2018"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-2018", 
            "text": "", 
            "title": "Sep 2018"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-20", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51 [     54.602054 ]   nr_pcache_pee_free :   0  [     54.602537 ]   nr_pcache_pee_free_kmalloc :   0  [   1468.765410 ]   mlx4_msi_x_interrupt () :   IRQ :   27   CPU :   1  [   1468.766956 ]   event   PORT_MNG_CHG   arrived  [   1468.768193 ]   mlx4_ib   handle_port_mgmt_change_event :   rereg    [   1468.813660 ]   ib_cache :   ib_cache_update () :   Updated   port   1   of   dev   0000 : 00 : 08.0  [   1468.815097 ]   ib_sa_event () :   TODO  [   1479.178651 ]   mlx4_msi_x_interrupt () :   IRQ :   27   CPU :   1  [   1479.180201 ]   event   PORT_MNG_CHG   arrived  [   1479.181430 ]   mlx4_ib   handle_port_mgmt_change_event :   rereg  [   1479.190813 ]   bad :   scheduling   from   the   idle   thread !  [   1479.192158 ]   CPU :   1   PID :   0   Comm :   swapper / 1   4.0.0 - lego +   # 146  [   1479.193622 ]   Stack :  [   1479.194408 ]   ffff88083fddf980   ffffffff8101eefc   ffff88083fc45d80   ffff88083fc45d80  [   1479.196826 ]   ffff88083fddf9a8   ffffffff8101ace4   00000001 810067 d4   ffff88083fe43000  [   1479.199226 ]   ffffffffffff0000   ffff88083fddf9e0   ffffffff81078bf6   ffffffff8100e8ea  [   1479.203615 ]   ffffffffffff0000   0000000000000000   ffff88083fe43000   ffff88083fe43000  [   1479.206532 ]   ffff88083fddf9f8   ffffffff81078ca3   7ff fffffffffffff   ffff88083fddfa68  [   1479.208791 ]   Call   Trace :  [   1479.209606 ]   TSK  [   1479.210322 ]   [ ffffffff8101ef08 ]   dequeue_task_idle + 0x48 / 0x60  [   1479.211726 ]   [ ffffffff8101ace4 ]   deactivate_task + 0x44 / 0x50  [   1479.213092 ]   [ ffffffff81078bf6 ]   __schedule + 0x146 / 0x1e0  [   1479.214410 ]   [ ffffffff8100e8ea ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [   1479.215960 ]   [ ffffffff81078ca3 ]   schedule + 0x13 / 0x30  [   1479.217211 ]   [ ffffffff810789da ]   schedule_timeout + 0x12a / 0x1a0  [   1479.218625 ]   [ ffffffff81079e54 ]   __down_common + 0xaa / 0x103  [   1479.219904 ]   [ ffffffff81079ec5 ]   __down + 0x18 / 0x1a  [   1479.221046 ]   [ ffffffff8101f24c ]   down + 0x3c / 0x40  [   1479.222163 ]   [ ffffffff8104dba7 ]   __mlx4_cmd + 0x1d7 / 0x3c0  [   1479.223397 ]   [ ffffffff810619de ]   mlx4_MAD_IFC + 0x22e / 0x490  [   1479.224666 ]   [ ffffffff8105d321 ]   __mlx4_ib_query_pkey + 0x181 / 0x240  [   1479.226045 ]   [ ffffffff8105d3f3 ]   mlx4_ib_query_pkey + 0x13 / 0x20  [   1479.227365 ]   [ ffffffff81064cb4 ]   ib_query_pkey + 0x14 / 0x20  [   1479.228617 ]   [ ffffffff810651a7 ]   ib_cache_update + 0x237 / 0x480  [   1479.229862 ]   [ ffffffff810657f8 ]   ib_cache_event + 0x28 / 0x30  [   1479.231026 ]   [ ffffffff81064bf0 ]   ib_dispatch_event + 0x40 / 0x70  [   1479.232222 ]   [ ffffffff810627c8 ]   handle_port_mgmt_change_event + 0x158 / 0x1c0  [   1479.233602 ]   [ ffffffff8105b5ac ]   mlx4_ib_event + 0x7c / 0xa0  [   1479.234744 ]   [ ffffffff8104ee55 ]   mlx4_dispatch_event + 0x65 / 0x90  [   1479.235968 ]   [ ffffffff8104f2c3 ]   mlx4_eq_int + 0x273 / 0x4f0  [   1479.237113 ]   [ ffffffff8104f616 ]   mlx4_msi_x_interrupt + 0x36 / 0x40  [   1479.238352 ]   [ ffffffff81017894 ]   handle_irq_event_percpu + 0x24 / 0xa0  [   1479.239584 ]   [ ffffffff81017938 ]   handle_irq_event + 0x28 / 0x50  [   1479.240696 ]   [ ffffffff810180fe ]   handle_edge_irq + 0x5e / 0xc0  [   1479.241794 ]   [ ffffffff810054c3 ]   do_IRQ + 0x43 / 0xd0  [   1479.242779 ]   [ ffffffff810067d4 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [   1479.243971 ]   [ ffffffff8100e0aa ]   common_interrupt + 0x6a / 0x6a  [   1479.245084 ]   [ ffffffff8101c6b0 ]   ?   cpu_idle + 0x10 / 0x30  [   1479.246123 ]   [ ffffffff81003425 ]   start_secondary_cpu + 0x55 / 0x60  [   1479.247278 ]   EOT", 
            "title": "Sep 20"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-17", 
            "text": "Can not believe I m wasting time on this crap X again.", 
            "title": "Sep 17"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-16", 
            "text": "Tests done today:     Setting  Log  nr_workers  Tracing (strace/counter/profiling)  Runtime (s)  pcache_flush_net (us)      TF-MNIST, Linux     13.2s     TF4-MNIST, 128MB  0916-w14-1  1  ON  avg 48.5s  9891    TF4-MNIST, 128MB  0916-w14-2  1  OFF  (46.1+44.6+45.5+45.7+44)/5 = 45.2s  N/A    TF4-MNIST, 128MB  0916-w14-4  4  ON  (43.4+44+43.9+42.6+42.1)/5=43.2  8351    TF4-MNIST, 128MB  0916-w14-3  4  OFF  (40.1+42.1+42.0+41.7+42.1)/5 = 41.6  N/A    TF4-Cifar, Linux     235.5s     TF4-Cifar, 128MB  0916-w14-5  4  OFF  (636.2+635.0+636.8+637.2+634.1)/5=635.8  N/A    TF4-Cifar, 128MB  0916-w14-6  1  OFF  (660.2+662.2+662.8+663.8+661+5)/5=663s  N/A    TF4-Cifar, 256MB  0916-w14-7  1  OFF  486s  N/A", 
            "title": "Sep 16"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-15", 
            "text": "DAMN.  Let us summarize today. Okay. Fixed the double-post-cqe issue. Hehe. The post part is the only fucking left code that I did not look into at fit_poll_recv_cq. And, ironically, there is no error checking for ib_post_recv(), which won t generate any error/warning.  error checking error checking  Anyway fuck it.  Today I created a new tag v0.0.9, hope we have a stable net. The RPC profile code is very stressing, and fit survived.  The following wanring is fixed by post rx_depth/2.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 [   1812.017204 ]   fit :   To   align   first   QPN ,   we   skipped :   # 72   # 72   # 73   # 74   # 75   # 76   # 77   # 78   # 79  [   1812.157570 ]   fit :   fit_post_receives_message () - 628   CPU   2   Fail   to   post   recv   conn_id :   12  [   1812.166013 ]   ------------ [   cut   here   ] ------------  [   1812.171152 ]   WARNING :   CPU :   2   PID :   16   at   net / lego / fit_internal . c : 629   fit_post_receives_message . isra .7 + 0xce / 0x100  [   1812.182302 ]   CPU :   2   PID :   16   Comm :   ib - initd   4.0.0 - lego +   # 95  [   1812.188314 ]   Stack :  [   1812.190544 ]   ffff880ff98bfd50   ffffffff8101299b   0000000000000 cff   0000000000000060  [   1812.198689 ]   0000000000000 d00   0000000000000100   ffff880ff98dc030   ffff880ff98bfd60  [   1812.206834 ]   ffffffff81012a8f   ffff880ff98bfdc8   ffffffff810743de   fffffff4fffffff4  [   1812.214978 ]   ffff880ff98bfd80   0000000000000000   0000000000000 cff   0000000000000000  [   1812.223124 ]   0000000000000000   ffff880ff98dc000   0000000000000000   000000000000000 c  [   1812.231269 ]   Call   Trace :  [   1812.233984 ]   TSK  [   1812.236116 ]   [ ffffffff810129a7 ]   __warn . constprop .0 + 0xa7 / 0x100  [   1812.242613 ]   [ ffffffff81012a8f ]   warn_slowpath_null + 0xf / 0x20  [   1812.248915 ]   [ ffffffff810743de ]   fit_post_receives_message . isra .7 + 0xce / 0x100  [   1812.256770 ]   [ ffffffff81076a1a ]   fit_add_newnode + 0xca / 0x170  [   1812.262974 ]   [ ffffffff81079d10 ]   fit_establish_conn + 0x7b0 / 0xaa0  [   1812.269568 ]   [ ffffffff81073ce8 ]   ?   ibv_add_one + 0x98 / 0x120  [   1812.275580 ]   [ ffffffff810741f0 ]   ?   ibapi_get_node_id + 0x20 / 0x20  [   1812.282076 ]   [ ffffffff81074258 ]   lego_ib_init + 0x68 / 0xf0  [   1812.287893 ]   [ ffffffff81023261 ]   kthread + 0x111 / 0x130  [   1812.293421 ]   [ ffffffff81023150 ]   ?   __kthread_parkme + 0x70 / 0x70  [   1812.299820 ]   [ ffffffff8100eaf2 ]   ret_from_fork + 0x22 / 0x30  [   1812.305735 ]   EOT  [   1812.307868 ]   --- [   end   trace   0000000000000000   ] ---", 
            "title": "Sep 15"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-11", 
            "text": "Got this log, 5 machine, p2s_open, S side has this issue. Damn.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72 [   1672.962279 ]                                                                                                                                                                                                                                  *****                                                                                                                                                                                                                                           *****   Fail   to   to   get   the   CQE   from   send_cq   after   20   seconds !                                                                                                                                                                                     *****   This   means   the   packet   was   lost   and   something   went   wrong                                                                                                                                                                                   *****   with   your   NIC ...  *****   connection_id :   11   dest   node :   0  *****  [   1673.061668 ]   ------------ [   cut   here   ] ------------  [   1673.074937 ]   WARNING :   CPU :   10   PID :   4624   at   / root / ys / LegoOS_2M / linux - modules / fit / fit_internal . c : 956   fit_internal_poll_sendcq + 0xda / 0x130   [ fit ]()  [   1673.101557 ]   Modules   linked   in :   storage ( OF )   fit ( OF )   xt_CHECKSUM   iptable_mangle   ipt_MASQUERADE   iptable_nat   nf_nat_ipv4   nf_nat   nf_conntrack_ipv4   nf_defrag_ipv4   xt_conntrack   nf_conntrack   ipt_REJECT   tun   bridge   stp   llc   ebtable_filter   ebtable  s   ip6table_filter   ip6_tables   iptable_filter   xprtrdma   sunrpc   ib_isert   iscsi_target_mod   ib_iser   libiscsi   scsi_transport_iscsi   ib_srpt   target_core_mod   ib_srp   scsi_transport_srp   scsi_tgt   ib_ipoib   rdma_ucm   ib_ucm   ib_uverbs   ib_umad   rdma_cm   ib_c  m   iw_cm   ib_addr   x86_pkg_temp_thermal   coretemp   kvm_intel   kvm   crc32_pclmul   ghash_clmulni_intel   aesni_intel   lrw   gf128mul   glue_helper   ipmi_devintf   ablk_helper   cryptd   ipmi_si   iTCO_wdt   ipmi_msghandler   iTCO_vendor_support   dcdbas   sg   pcspkr   shpchp \n  acpi_power_meter   lpc_ich   mfd_core   wmi   mperf   uinput   binfmt_misc   ip_tables   ext4   mbcache   jbd2   mlx4_ib  [   1673.182609 ]    ib_sa   ib_mad   ib_core   mlx4_en   sd_mod   crc_t10dif   mgag200   syscopyarea   sysfillrect   sysimgblt   i2c_algo_bit   drm_kms_helper   ttm   drm   ahci   crc32c_intel   libahci   mlx4_core   libata   tg3   nvme   megaraid_sas   ptp   i2c_core   pps_core   dm_mirror  dm_region_hash   dm_log   dm_mod  [   1673.222604 ]   CPU :   10   PID :   4624   Comm :   lego - storaged   Tainted :   GF         W    O   3.11.1 - vanilla   # 1  [   1673.235825 ]   Hardware   name :   Dell   Inc .   PowerEdge   R730 / 05 99 V5 ,   BIOS   1.5.4   10 / 002 / 2015  [   1673.248883 ]    000000000000000 9   ffff88102186b9f8   ffffffff8159a5a4   0000000000000000  [   1673.261795 ]    ffff88102186ba30   ffffffff810641bd   ffff882027180400   00000004 a817c800  [   1673.274499 ]    000001 80 dc3abde5   0000000000000000   0000000000000000   ffff88102186ba40  [   1673.287034 ]   Call   Trace :  [   1673.299259 ]    [ ffffffff8159a5a4 ]   dump_stack + 0x45 / 0x56  [   1673.311371 ]    [ ffffffff810641bd ]   warn_slowpath_common + 0x7d / 0xa0  [   1673.323268 ]    [ ffffffff8106429a ]   warn_slowpath_null + 0x1a / 0x20  [   1673.334892 ]    [ ffffffffa063669a ]   fit_internal_poll_sendcq + 0xda / 0x130   [ fit ]  [   1673.346348 ]    [ ffffffff81093e25 ]   ?   check_preempt_curr + 0x85 / 0xa0  [   1673.357575 ]    [ ffffffffa06367f7 ]   fit_send_message_with_rdma_write_with_imm_request + 0x107 / 0x3f0   [ fit ]  [   1673.368777 ]    [ ffffffff8107bde4 ]   ?   wake_up_worker + 0x24 / 0x30  [   1673.379741 ]    [ ffffffffa0636ee9 ]   fit_reply_message + 0x89 / 0xa0   [ fit ]  [   1673.390497 ]    [ ffffffffa063507b ]   ibapi_reply_message + 0x1b / 0x20   [ fit ]  [   1673.401039 ]    [ ffffffffa0646785 ]   handle_open_request + 0xa5 / 0xe0   [ storage ]  [   1673.411367 ]    [ ffffffffa0646106 ]   storage_manager + 0x106 / 0x300   [ storage ]  [   1673.421470 ]    [ ffffffffa0646000 ]   ?   0xffffffffa0645fff  [   1673.431297 ]    [ ffffffffa0646000 ]   ?   0xffffffffa0645fff  [   1673.440797 ]    [ ffffffff81085ec0 ]   kthread + 0xc0 / 0xd0  [   1673.450034 ]    [ ffffffff81085e00 ]   ?   insert_kthread_work + 0x40 / 0x40  [   1673.459063 ]    [ ffffffff815a94ac ]   ret_from_fork + 0x7c / 0xb0  [   1673.467837 ]    [ ffffffff81085e00 ]   ?   insert_kthread_work + 0x40 / 0x40  [   1673.476400 ]   --- [   end   trace   f9b19a31d409f910   ] ---  [   1695.867276 ]   storage_self_monitor () :   in_handler = 1  [   1695.875906 ]   handle_replica_flush :   0  [   1695.884613 ]   handle_replica_vma :   0  [   1695.893265 ]   handle_replica_read :   12740  [   1695.901920 ]   handle_replica_write :   0  [   1713.012565 ]   INFO :   rcu_sched   self - detected   stall   on   CPU   {   10 }    ( t = 60001   jiffies   g = 7646   c = 7645   q = 0 )  [   1713.013339 ]   sending   NMI   to   all   CPUs :  [   1713.013573 ]   INFO :   rcu_sched   detected   stalls   on   CPUs / tasks :   {   10 }   ( detected   by   15 ,   t = 60002   jiffies ,   g = 7646 ,   c = 7645 ,   q = 0 )  [   1713.014807 ]   NMI   backtrace   for   cpu   0  [   1713.015685 ]   CPU :   0   PID :   4591   Comm :   wq_handler   Tainted :   GF         W    O   3.11.1 - vanilla   # 1  [   1713.016624 ]   Hardware   name :   Dell   Inc .   PowerEdge   R730 / 05 99 V5 ,   BIOS   1.5.4   10 / 002 / 2015  [   1713.017575 ]   task :   ffff88201f193b40   ti :   ffff88101a34a000   task . ti :   ffff88101a34a000  [   1713.018530 ]   RIP :   0010 : [ ffffffffa0636b55 ]    [ ffffffffa0636b55 ]   waiting_queue_handler + 0x75 / 0x140   [ fit ]  [   1713.018530 ]   RIP :   0010 : [ ffffffffa0636b55 ]    [ ffffffffa0636b55 ]   waiting_queue_handler + 0x75 / 0x140   [ fit ]  [   1713.019512 ]   RSP :   001 8 : ffff88101a34be78    EFLAGS :   000002 96  [   1713.020444 ]   RAX :   00000000000 80080   RBX :   ffff8820200253f0   RCX :   ffff88201f193b40  [   1713.021364 ]   RDX :   0000000000000001   RSI :   ffff88103f414760   RDI :   ffff88103f4146c0  [   1713.022260 ]   RBP :   ffff88101a34bec8   R08 :   0000000000000000   R09 :   0000000000000001  [   1713.023138 ]   R10 :   0000000000000001   R11 :   ffffffffa0636b55   R12 :   ffff8820200253c0  [   1713.024000 ]   R13 :   ffff881022005000   R14 :   ffffffffa063b8e4   R15 :   ffffffffa063b8e4  [   1713.024839 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88103f400000 ( 0000 )   knlGS : 0000000000000000  [   1713.025676 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   1713.026481 ]   CR2 :   00007f d77ef46000   CR3 :   0000000001 876000   CR4 :   00000000001407f 0  [   1713.027273 ]   Stack :  [   1713.028035 ]    ffff881000000000   ffff881000300660   ffff882000000003   0000000000000000  [   1713.028818 ]    ffff881000000000   ffff881021eafc38   ffff881022005000   ffffffffa0636ae0  [   1713.029585 ]    0000000000000000   0000000000000000   ffff88101a34bf48   ffffffff81085ec0  [   1713.030350 ]   Call   Trace :  [   1713.031098 ]    [ ffffffffa0636ae0 ]   ?   fit_send_message_with_rdma_write_with_imm_request + 0x3f0 / 0x3f0   [ fit ]  [   1713.031875 ]    [ ffffffff81085ec0 ]   kthread + 0xc0 / 0xd0  [   1713.032641 ]    [ ffffffff81085e00 ]   ?   insert_kthread_work + 0x40 / 0x40  [   1713.033407 ]    [ ffffffff815a94ac ]   ret_from_fork + 0x7c / 0xb0  [   1713.034171 ]    [ ffffffff81085e00 ]   ?   insert_kthread_work + 0x40 / 0x40", 
            "title": "Sep 11"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-08", 
            "text": "Check this log out:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 ]---\n[  427.218569] STDOUT: ---[\nINFO:tensorflow:Graph was finalized.\n\n]---\n[  427.416043] BUG: unable to handle kernel NULL pointer dereference at           (null)\n[  427.424583] IP: [ ffffffff810748fb ] fit_poll_recv_cq+0x5cb/0x860\n[  427.431370] mlx4_msi_x_interrupt(): IRQ: 27 CPU: 0\n[  427.436702] PGD 0\n[  427.438932] CQ_ERROR CQ overrun on CQN 000082\n[  427.443780] Oops: 0002 [#1] SMP PROCESSOR\n[  427.448240] event qp_event arrived\n[  427.452022] CPU: 6 PID: 18 Comm: FIT_RecvCQ-0 4.0.0-lego+ #23\n[  427.458421] event qp_event arrived\n[  427.462203] RIP: 0010:[ ffffffff810748fb ]  [ ffffffff810748fb ] fit_poll_recv_cq+0x5cb/0x860\n[  427.471704] RSP: 0000:ffff881023e3fe60  EFLAGS: 00010287\n[  427.477618] RAX: 0000000000000000 RBX: 000000002aaaaaab RCX: 0000000000000004\n[  427.485570] RDX: 0000000000000000 RSI: 0000000000000053 RDI: 0000000000000000\n[  427.493520] RBP: ffff881023e3fec0 R08: 0000000000000001 R09: ffff881039900000\n[  427.501470] R10: 0000000000000000 R11: ffff881039918000 R12: ffff8810398f2000\n[  427.509421] R13: 0000000000000000 R14: 0000000000000001 R15: ffff881023e25008\n[  427.517371] event qp_event arrived\n[  427.521153] FS:  0000000000000000(0000) GS:ffff88107fc60000(0000) knlGS:0000000000000000\n[  427.530169] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[  427.536569] CR2: 0000000000000000 CR3: 000000000117a000 CR4: 00000000000406a0\n[  427.544519] event qp_event arrived   Trying to tune FIT s number polling threads. This could be the throughput/latency killer.  128M     P num_polling  M worker  M num_polling  Runtime (s)      1  1  1  46.8s    1  4  1", 
            "title": "Sep 08"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-07", 
            "text": "Set up Infiniswap again. What a fucking crap code, and crash the kernel out of nowhere. crap crap crap.  Hmm, Linux will tune the CPU freq during runtime, will be higher than 2.4GHz. So disable it, make it a fair comparison with Lego.  intel_pstate=disable.", 
            "title": "Sep 07"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-06", 
            "text": "Did two optimizations on pcache, both are buffer management.\nEspecially the pcache rmap case. In both opts, we kind of use static/pre-allocated array to serve dynamic allocation.  This is a better solution than using kmem_cache, faster. kmem_cache will be a more general solution here.  kmem_cache, FIFO queue (thpool buffer), static preallocated array (rmap, clflush)  Buffer management is really a very important thing in system building. I should be aware at the beginning next time.  These changes are in commits: 1\n2 6e0cf6c5c64edbe445a27cf55f86ac51f8a897b3\n73377cafce95ffa0cfb155f77cac97456a5e4a71", 
            "title": "Sep 06"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-05", 
            "text": "Alright. Besides some flaws/bugs in some kfree stuff, LegoOS now actually is very robust! Ran a quick git summary:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16  project  : LegoOS\n repo age : 1 year, 11 months\n active   : 358 days\n commits  : 1540\n files    : 1161\n authors  :\n  1317  Yizhou Shan                  85.5%\n   120  root                         7.8%\n    36  hythzz                       2.3%\n    27  yilun                        1.8%\n    16  Yutong Huang                 1.0%\n    10  Build Android                0.6%\n     8  Yiying Zhang                 0.5%\n     4  sumukh1991                   0.3%\n     1  Yizhou SHan                  0.1%\n     1  Sumukh Hallymysore Ravindra  0.1%   Of course, there are still PLENY room for improvement, and I know where. At this time, I really think we need something like kmem_cache, which is so fucking useful. It can probably further reduce much overhead.", 
            "title": "Sep 05"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-04", 
            "text": "Trying the perset eviction list mechanism, instead of victim cache. The benefit of using this is: we will no longer be bottelnecked by victim cache anymore. Each faulting thread will do eviction/flush within its own context.  For 4 threads MNIST, I saw 3 seconds reduction.  Removed the bitmap, use per pcache set counter for quick reference.", 
            "title": "Sep 04"
        }, 
        {
            "location": "/lego/log/log-09-2018/#sep-03", 
            "text": "With DEBUG_MM, try enable HAVE_FREE directory by directory", 
            "title": "Sep 03"
        }, 
        {
            "location": "/lego/log/log-09-2018/#-", 
            "text": "-  update_wall_time+0x44 is where we call tsc_read. And this has been called many times (HZ per second). All of a sudden, the pointer got crashed. Who wrote to this code memory?? Remote RDMA?  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 [   1052.470714 ]   general   protection   fault :   0000   [ # 1 ]   SMP   PROCESSOR  [   1052.477113 ]   CPU :   0   PID :   15   Comm :   ib_mad1   4.0.0 - lego +   # 509  [   1052.483125 ]   RIP :   0010 : [ ffffffff81015764 ]    [ ffffffff81015764 ]   update_wall_time + 0x44 / 0x6f0  [   1052.492530 ]   RSP :   0000 : ffff88103ad9fc88    EFLAGS :   00010046  [   1052.498445 ]   RAX :   4510ff ffffff8118   RBX :   0380ff ffffffffff   RCX :   0000000000000001  [   1052.506396 ]   RDX :   ffff88103ad9fd28   RSI :   0000000000000000   RDI :   4510ff ffffff8118  [   1052.514346 ]   RBP :   ffff88103ad9fcd0   R08 :   000000000000001f   R09 :   0000000000000000  [   1052.522298 ]   R10 :   000000000000002 9   R11 :   ffff881013f8e130   R12 :   aaff0000024a2677  [   1052.530248 ]   R13 :   0000000000000000   R14 :   ffff88103ad85228   R15 :   ffff88103ae0c000  [   1052.538199 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc00000 ( 0000 )   knlGS : 0000000000000000  [   1052.547216 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   1052.553616 ]   CR2 :   0000000000000000   CR3 :   000000000117 b000   CR4 :   00000000000406 b0  [   1052.561567 ]   Stack :  [   1052.563797 ]   00000000000000 86   ffff88107fc05d80   ffff88103ad85000   0000000000000000  [   1052.571941 ]   ffff88107fc04980   0000000000000000   0000000000000000   ffff88103ad85228  [   1052.580085 ]   ffff88103ae0c000   ffff88103ad9fce8   ffffffff81017557   000000003 ad9fe10  [   1052.588230 ]   ffff88103ad9fd10   ffffffff810067a4   ffffffff81088040   ffff88107fc05d80  [   1052.596375 ]   ffff88103ad85000   ffff88103ad9fdf8   ffffffff8100e8ea   ffff88103ad9fd28  [   1052.604520 ]   Call   Trace :  [   1052.607236 ]   TSK  [   1052.609368 ]   [ ffffffff81017557 ]   tick_handle_periodic + 0x67 / 0x70  [   1052.615961 ]   [ ffffffff810067a4 ]   apic_timer_interrupt + 0x54 / 0x90  [   1052.622555 ]   [ ffffffff8100e8ea ]   smp__apic_timer_interrupt + 0x6a / 0x70  [   1052.629633 ]   [ ffffffff8107b488 ]   ?   __schedule + 0xf8 / 0x1e0  [   1052.635548 ]   [ ffffffff8107b583 ]   schedule + 0x13 / 0x30  [   1052.640978 ]   [ ffffffff8106c98e ]   ib_mad_completion_handler + 0x5de / 0xc20  [   1052.648250 ]   [ ffffffff8101de3b ]   ?   dequeue_task_rt + 0x1b / 0x180  [   1052.654648 ]   [ ffffffff8106c3b0 ]   ?   ib_mad_send_done_handler . isra .22 + 0x4e0 / 0x4e0  [   1052.662793 ]   [ ffffffff81022af6 ]   kthread + 0xf6 / 0x110  [   1052.668223 ]   [ ffffffff81022a00 ]   ?   __kthread_parkme + 0x70 / 0x70  [   1052.674622 ]   [ ffffffff8100eb72 ]   ret_from_fork + 0x22 / 0x30  [   1052.680538 ]   EOT  [   1052.682670 ]   Code :   db   e4   16   00   79   0 d   f3   90   80   3 d   d0   e4   16   00   00   7 e   f5   eb   ea   48   8 b   1 d   fd   fa   1f   00   48   8 b   05   e6   fa   1f   00   4 c   8 b   25   f7   fa   1f   00   48   89   c7   ff   50   28   49   89   c7   48   89   d8   4 d   29   e7   48   d1   e8   49   21   df   48   f7   d0  [   1052.703711 ]   RIP    [ ffffffff81015764 ]   update_wall_time + 0x44 / 0x6f0  [   1052.710498 ]    RSP   ffff88103ad9fc88", 
            "title": "-"
        }, 
        {
            "location": "/lego/log/log-08-2018/", 
            "text": "Aug 2018\n\n\nAug 31\n\n\nOne major TODO\n\n\nCheck \ndo_handle_p2m_pcache_miss()\n. We MUST remove that mempcy, maybe by using another flag in thpool. This is just no acceptable.\n\n\nUgh\n\n\nFuck. Without debug_mm, there is still memory corruption.\n\n\nTry max_send_wr and number of QPs\n\n\nwithout lock_ib, with debug_mm.\nChange max_send_wr at all P M S.\n\n\n\n\nQP=4, max_send_wr = 1: always fail\n\n\nQP=4, max_send_wr = 256: always fail\n\n\nQP=24, max_send_wr = 1: succeed (0831-w14-18 0831-w14-20)\n\n\nQP=24, max_send_wr = 256: succeed (0831-w14-16 0831-w14-17)\n\n\n\n\nPay attention to the \n0831-w14-15\n\uff1a something wrong with our timekeeping code? QP=24, max_send_wr = 1 case.\n\n\nAfter Victim bug fix\n\n\nMNIST 4 threads\n\n\n\n\nWith lock_ib, debug_mm etc: 3 successful runs\n\n\nOnly with debug_mm: Well fit failed. Lost CQE.\n\n\n\n\nNow the debug scope is limited. Let me try the micro test suite, to stress ibapi_send_reply itself.\n\n\nPotential: read/write buffer.\n\n\nAug 30\n\n\nBe humble.\n\n\nIdentified victim bug.\n\n\nFinally. I thought it through, and with the help of this \n0830-w14-12\n.\nThe bug is in \nvictim_try_fill_pcache()\n, when there are multiple hits to the same victim. Since we released the \nusable_victim_lock\n after a hit. There might a be race case where: 1) CPU0 reached \ndec_and_test_filling\n, and passed to free the line. 2) CPU1 just got to the \nvictim_check_hit\n, and increment the fill counter to 1 again. When CPU1 finished filling, and do \ndec_and_test_filling\n, it will do the free again!!! What a double free.\n\n\nTomorrow, let me do the fix. Thought: adding more sync in victim_check_hit part. Basically we want to ensure only one CPU can do the final free.\n\n\nAfter adding pi_lock\n\n\nOkay. the pi_lock is added. Although it is mostly used by futex-pi and rt-mutex, we lego does not have these two guys. Therefore, it is only used by sched/core.c, exit.c, and kthread.c. 99% is in core.c\n\n\nThe purpose of having this back is to have the \nspin_lock_irqsave(\np-\npi_lock)\n back. Most scheduling code is not recursive, we have to disable interrupt. Of course we can use \nspin_lock_irqsave(\nrq-\nlock)\n instead of \nspin_lock(\nrq-\nlock)\n. But this is too dangerous at this stage. Porting based on Linux now is the fastest and safest way.\n\n\nThe importance of disabling interrupt in some kernel path!!\n\n\nGood. Now I\nm seeing now debuggable victim issue.\n\n\nClassical deadlock catched. Now, only two victims.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n[\n \n2819.068997\n]\n \nCPU14\n \nPID29\n \nAbort\n \nvictim\n \nalloc\n \n(\n20010\nms\n)\n \nnr_usable_victims\n:\n \n2.\n \nFrom\n \npset_idx\n:\n532\n \nnr_lru\n:\n63\n \nfault_uva\n:\n \n0x7fff98614000\n\n\n[\n \n2819.094409\n]\n \nCPU14\n \nPID29\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n \n[\n0\n]\n \ntotal\n:\n \n2\n\n\n[\n \n2819.114188\n]\n \nCPU14\n \nPID29\n  \nvictim\n[\n0\n]\n:\nffffffff810c2880\n \nrefcount\n:\n2\n \nnr_fill\n:\n1\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2819.133289\n]\n \nCPU14\n \nPID29\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffcc000000\n\n\n[\n \n2819.141723\n]\n \nCPU14\n \nPID29\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2819.149770\n]\n \nCPU14\n \nPID29\n  \nvictim\n[\n1\n]\n:\nffffffff810c2900\n \nrefcount\n:\n2\n \nnr_fill\n:\n1\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2819.168870\n]\n \nCPU14\n \nPID29\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffb0000000\n\n\n[\n \n2819.177306\n]\n \nCPU14\n \nPID29\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2819.185352\n]\n \nCPU14\n \nPID29\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n \n[\n0\n]\n\n\n\n[\n \n2819.081708\n]\n \nCPU16\n \nPID30\n \nAbort\n \nvictim\n \nalloc\n \n(\n20010\nms\n)\n \nnr_usable_victims\n:\n \n2.\n \nFrom\n \npset_idx\n:\n0\n \nnr_lru\n:\n63\n \nfault_uva\n:\n \n0x7fffcc000024\n\n\n[\n \n2819.209008\n]\n \nCPU16\n \nPID30\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n \n[\n1\n]\n \ntotal\n:\n \n2\n\n\n[\n \n2819.223358\n]\n \nCPU16\n \nPID30\n  \nvictim\n[\n0\n]\n:\nffffffff810c2880\n \nrefcount\n:\n2\n \nnr_fill\n:\n1\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2819.252443\n]\n \nCPU16\n \nPID30\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffcc000000\n\n\n[\n \n2819.260879\n]\n \nCPU16\n \nPID30\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2819.268926\n]\n \nCPU16\n \nPID30\n  \nvictim\n[\n1\n]\n:\nffffffff810c2900\n \nrefcount\n:\n2\n \nnr_fill\n:\n1\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2819.288026\n]\n \nCPU16\n \nPID30\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffb0000000\n\n\n[\n \n2819.296461\n]\n \nCPU16\n \nPID30\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2819.304508\n]\n \nCPU16\n \nPID30\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n \n[\n1\n]\n\n\n\n[\n \n2819.101391\n]\n \nCPU18\n \nPID31\n \nAbort\n \nvictim\n \nalloc\n \n(\n20010\nms\n)\n \nnr_usable_victims\n:\n \n2.\n \nFrom\n \npset_idx\n:\n15\n \nnr_lru\n:\n63\n \nfault_uva\n:\n \n0x7fff98c0f000\n\n\n[\n \n2819.328165\n]\n \nCPU18\n \nPID31\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n \n[\n2\n]\n \ntotal\n:\n \n2\n\n\n[\n \n2819.335146\n]\n \nCPU18\n \nPID31\n  \nvictim\n[\n0\n]\n:\nffffffff810c2880\n \nrefcount\n:\n1\n \nnr_fill\n:\n0\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2819.354246\n]\n \nCPU18\n \nPID31\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffcc000000\n\n\n[\n \n2819.362680\n]\n \nCPU18\n \nPID31\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2819.370728\n]\n \nCPU18\n \nPID31\n  \nvictim\n[\n1\n]\n:\nffffffff810c2900\n \nrefcount\n:\n2\n \nnr_fill\n:\n1\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2819.389828\n]\n \nCPU18\n \nPID31\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffb0000000\n\n\n[\n \n2819.398262\n]\n \nCPU18\n \nPID31\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2819.406310\n]\n \nCPU18\n \nPID31\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n \n[\n2\n]\n\n\n\n#\n\n\n# This guy grabbed the fill counter right before the first timout\n\n\n# That\ns why the above three timeout happen. And this one is 20s later\n\n\n# which equals to the timeout second.\n\n\n#\n\n\n[\n \n2839.327457\n]\n \nCPU12\n \nPID28\n \nAbort\n \nvictim\n \nalloc\n \n(\n20010\nms\n)\n \nnr_usable_victims\n:\n \n2.\n \nFrom\n \npset_idx\n:\n0\n \nnr_lru\n:\n63\n \nfault_uva\n:\n \n0x7fffb0000f00\n\n\n[\n \n2839.339964\n]\n \nCPU12\n \nPID28\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n \n[\n3\n]\n \ntotal\n:\n \n2\n\n\n[\n \n2839.346945\n]\n \nCPU12\n \nPID28\n  \nvictim\n[\n0\n]\n:\nffffffff810c2880\n \nrefcount\n:\n1\n \nnr_fill\n:\n0\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2839.366046\n]\n \nCPU12\n \nPID28\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffcc000000\n\n\n[\n \n2839.374480\n]\n \nCPU12\n \nPID28\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2839.382527\n]\n \nCPU12\n \nPID28\n  \nvictim\n[\n1\n]\n:\nffffffff810c2900\n \nrefcount\n:\n2\n \nnr_fill\n:\n1\n \nmax_fill\n:\n4\n \nlocked\n:\n0\n \nflags\n:(\n0x14e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n|\nfillfree\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff5a000\n\n\n[\n \n2839.401628\n]\n \nCPU12\n \nPID28\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffb0000000\n\n\n[\n \n2839.410062\n]\n \nCPU12\n \nPID28\n     \nrmap\n \nto\n \npset\n:\nffff88207ff5a000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n \n2839.418109\n]\n \nCPU12\n \nPID28\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n \n[\n3\n]\n\n\n\n\n\n\n\nrq-\nlock deadlock\n\n\nAlright. We had rq-\nlock deadlock issue. Basically, we missed the part of disabling interrupt. A timer interrupt will try to acquire the lock again. Then, bang we have a deadlock. Digging into the code, you will be able to find the cause easily. The root cause we removed all the pi_lock stuff, which actually have a lot irqsave usages\n Oh man, maybe it\ns time to add it back.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n[ 3367.835389] ------------------- cut here -------------------\n[ 3367.841504] Possible deadlock happend locker_cpu: 0\n[ 3367.846934] Current call stack:\n[ 3367.850425] CPU: 0 PID: 1 Comm: kernel_init 4.0.0-lego+ #437\n[ 3367.856726] Stack:\n[ 3367.858957] ffff88107ff0fa58 ffffffff8101f4b6 ffff88107fc05e00 00000004a817c800\n[ 3367.867101] ffff88107ff0fa80 ffffffff8101f52e ffff88107fc05e00 ffff88107ffb4000\n[ 3367.875246] 0000000000000000 ffff88107ff0faa0 ffffffff8101b1ae ffff88107fc04980\n[ 3367.883390] 0000000000000000 ffff88107ff0fab8 ffffffff810174f5 0000000000000286\n[ 3367.891535] ffff88107ff0fae0 ffffffff81006774 ffff88107ffb9000 ffff88107fc05e00\n[ 3367.899680] Call Trace:\n[ 3367.902396] \nTSK\n\n[ 3367.904528] [\nffffffff8101f4c2\n] report_deadlock+0x62/0x80\n[ 3367.910637] [\nffffffff8101f52e\n] debug_spin_lock+0x4e/0x60\n[ 3367.916745] [\nffffffff8101b1ae\n] scheduler_tick+0x2e/0x60\n[ 3367.922756] [\nffffffff810174f5\n] tick_handle_periodic+0x45/0x70\n[ 3367.929350] [\nffffffff81006774\n] apic_timer_interrupt+0x54/0x90\n[ 3367.935943] [\nffffffff8100e8aa\n] smp__apic_timer_interrupt+0x6a/0x70\n[ 3367.943021] [\nffffffff8101db99\n] ? enqueue_task_rt+0x149/0x250\n[ 3367.949518] [\nffffffff8105908a\n] ? __mlx4_write_mtt+0xea/0x140\n[ 3367.956014] [\nffffffff8101ad34\n] activate_task+0x44/0x50\n[ 3367.961929] [\nffffffff8101b667\n] ttwu_do_activate+0x27/0x50\n[ 3367.968134] [\nffffffff8101b89c\n] try_to_wake_up+0xdc/0x1f0\n[ 3367.974243] [\nffffffff8106cc20\n] ? ib_mad_send_done_handler.isra.22+0x4d0/0x4d0\n[ 3367.982388] [\nffffffff8101ba80\n] wake_up_process+0x10/0x20\n[ 3367.988497] [\nffffffff81023116\n] __kthread_create_on_node+0x146/0x230\n[ 3367.995671] [\nffffffff8102329f\n] kthread_create_on_node+0x2f/0x40\n[ 3368.002459] [\nffffffff81066873\n] ? ib_create_cq+0x23/0x60\n[ 3368.008470] [\nffffffff810695e1\n] ib_mad_init_device+0x1f1/0x7b0\n[ 3368.015064] [\nffffffff81067246\n] ib_register_device+0x5d6/0x690\n[ 3368.021657] [\nffffffff8105e9d3\n] mlx4_ib_add+0x653/0x780\n[ 3368.027571] [\nffffffff8105147d\n] mlx4_add_device+0x8d/0x130\n[ 3368.033777] [\nffffffff8105158c\n] mlx4_register_interface+0x6c/0xa0\n[ 3368.040661] [\nffffffff811dc660\n] mlx4_ib_init+0x10/0x20\n[ 3368.046478] [\nffffffff811dc619\n] mlx4_init+0x19/0x50\n[ 3368.052005] [\nffffffff811dc68d\n] ib_core_init+0x1d/0x30\n[ 3368.057823] [\nffffffff811db7f9\n] device_init+0x9/0x10\n[ 3368.063447] [\nffffffff8100030b\n] kernel_init+0x4b/0xc0\n[ 3368.069168] [\nffffffff8101b0ea\n] ? schedule_tail+0xa/0x40\n[ 3368.075178] [\nffffffff810002c0\n] ? 0xffffffff810002c0\n[ 3368.080803] [\nffffffff8100eb32\n] ret_from_fork+0x22/0x30\n[ 3368.086718] \nEOT\n\n\n\n\n\n\n\n0830-w14-1: I really don\nt know how this happen. The refcounter and fill counter should be enough to serialize..\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n[37722.177024] CPU20 PID31  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x12e)(allocated|usable|hasdata|waitflush|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.196623] CPU20 PID31     hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000\n[37722.204572] CPU20 PID31  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.224154] CPU20 PID31     rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63\n[37722.232299] CPU20 PID31     victim dumped because: PCACHE_BUG_ON_VICTIM(!VictimAllocated(v) || !VictimUsable(v) || !VictimFlushed(v) || VictimWriteback(v) || VictimLocked(v))\n[37722.254790] WARNING: CPU: 20 PID: 31 at managers/processor/pcache/victim.c:196 __put_victim_nolist+0xb8/0x140\n ffffffff8103e170[37722.453632] [\nffffffff8103c9c8\n] __put_victim_nolist+0xb8/0x140\n 0000000000000000[37722.461873] [\nffffffff8103db18\n] victim_try_fill_pcache+0x2f8/0x440\n\n[37722.265842] CPU10 PID20  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.291438] CPU10 PID20     hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000\n[37722.301616] CPU10 PID20  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.324206] CPU10 PID20     rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63\n[37722.332349] CPU10 PID20     victim dumped because: PCACHE_BUG_ON_VICTIM(victim_ref_count(v) == 0)\n[37722.350673] WARNING: CPU: 10 PID: 20 at ./include/processor/pcache_victim.h:127 __victim_flush_func+0x232/0x250\n[37722.363568] CPU: 10 PID: 20 Comm: kvictim_flushd 4.0.0-lego+ #435\n[37722.534003] [\nffffffff8103e152\n] __victim_flush_func+0x232/0x250\n[37722.547577] [\nffffffff8103e1d9\n] victim_flush_async+0x69/0xb0\n[37722.553975] [\nffffffff81022ec1\n] kthread+0x111/0x130\n[37722.565900] [\nffffffff8100eb32\n] ret_from_fork+0x22/0x30\n\n\n\n\n\nAug 29\n\n\nThe only thing left about core_IB is: ib_sa_query, which will be invoked when there is a mlx4 interrupts.\n\n\nNot sure if this is important.\n\n\nAnyway. Testing TF 4 threads MNIST again.\n\n\nWhen I enable SEQ_IBAPI\uff1a\n\n\n\n\n0829-w14-11 (0829-w09-11) succeed\n\n\n0829-w14-12: P side seems have deadlock. Let me enable DEBUG_SPINLOCK.\n\n\n\n\n0829-w14-13: SEQ_IBAPI, DEBUG_SPINLOCK, this is a very useful log:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n[\n  \n531.495545\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nINFO\n:\ntensorflow\n:\nloss\n \n=\n \n0.5256375\n,\n \nstep\n \n=\n \n101\n \n(\n25.166\n \nsec\n)\n\n\n\n]\n---\n\n\n[\n  \n531.624474\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000064\n\n\n[\n  \n531.633016\n]\n \nIP\n:\n \n[\nffffffff8103b60e\n]\n \n__put_victim_nolist\n+\n0xe\n/\n0xa0\n\n\n[\n  \n531.639803\n]\n \nPGD\n \n0\n\n\n[\n  \n531.642032\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n531.646493\n]\n \nCPU\n:\n \n10\n \nPID\n:\n \n20\n \nComm\n:\n \nkvictim_flushd\n \n4.0.0\n-\nlego\n+\n \n#\n426\n\n\n[\n  \n531.653279\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8103b60e\n]\n  \n[\nffffffff8103b60e\n]\n \n__put_victim_nolist\n+\n0xe\n/\n0xa0\n\n\n[\n  \n531.662781\n]\n \nRSP\n:\n \n0000\n:\nffff880fe392fde0\n  \nEFLAGS\n:\n \n00010006\n\n\n[\n  \n531.668696\n]\n \nRAX\n:\n \n0000000000000000\n \nRBX\n:\n \nffffffff810c2b00\n \nRCX\n:\n \nffffffff810c2b70\n\n\n[\n  \n531.676646\n]\n \nRDX\n:\n \nffffffff810c2b70\n \nRSI\n:\n \n0000007\naea3f42fa\n \nRDI\n:\n \nffffffff810c2b00\n\n\n[\n  \n531.684597\n]\n \nRBP\n:\n \nffff880fe392fdf0\n \nR08\n:\n \n000000000000001f\n \nR09\n:\n \n0000000000000002\n\n\n[\n  \n531.692548\n]\n \nR10\n:\n \n00000000\n80000000\n \nR11\n:\n \n00000000000664\nc3\n \nR12\n:\n \nffff88207ff57000\n\n\n[\n  \n531.700498\n]\n \nR13\n:\n \nffffffff810c2b60\n \nR14\n:\n \nffff880a72555000\n \nR15\n:\n \nffffffff810c2b48\n\n\n[\n  \n531.708449\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fca0000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n531.717466\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n531.723865\n]\n \nCR2\n:\n \n0000000000000064\n \nCR3\n:\n \n00000000011\nb9000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n  \n531.731816\n]\n \nStack\n:\n\n\n[\n  \n531.734046\n]\n \nffffffff810c2b00\n \nffff88207ff57000\n \nffff880fe392fe08\n \nffffffff8103bbea\n\n\n[\n  \n531.742190\n]\n \nffffffff810c2b00\n \nffff880fe392fe48\n \nffffffff8103c729\n \n00000000\n8103\nd7c2\n\n\n[\n  \n531.750335\n]\n \nffff880a72555060\n \nffff88107ff0fdc8\n \n0000000000000000\n \nffffffff8103c780\n\n\n[\n  \n531.758479\n]\n \n0000000000000000\n \nffff880fe392fe60\n \nffffffff8103c7e6\n \nffff880fe391c000\n\n\n[\n  \n531.766623\n]\n \nffff880fe392ff48\n \nffffffff81022e81\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n531.774768\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n531.777483\n]\n \nTSK\n\n\n[\n  \n531.779617\n]\n \n[\nffffffff8103bbea\n]\n \n__put_victim\n+\n0x4a\n/\n0x50\n\n\n[\n  \n531.785433\n]\n \n[\nffffffff8103c729\n]\n \n__victim_flush_func\n+\n0xb9\n/\n0x110\n\n\n[\n  \n531.792027\n]\n \n[\nffffffff8103c780\n]\n \n?\n \n__victim_flush_func\n+\n0x110\n/\n0x110\n\n\n[\n  \n531.798911\n]\n \n[\nffffffff8103c7e6\n]\n \nvictim_flush_async\n+\n0x66\n/\n0x90\n\n\n[\n  \n531.805310\n]\n \n[\nffffffff81022e81\n]\n \nkthread\n+\n0x111\n/\n0x130\n\n\n[\n  \n531.810836\n]\n \n[\nffffffff81022d70\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n  \n531.817236\n]\n \n[\nffffffff8100eb32\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n[\n  \n531.823151\n]\n \nEOT\n\n\n\n\n\n\n\n\n\n\n0829-w14-14: this looks like a double free, or concurrent eviction. But if you look into the evict code, we will check the Flushed flag. It means another eviction routine should have skipped this line, and will not pick this line to do eviction. Some other possibilities?\n\n\n\n\n\n\ncheck until 0829-w14-18\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n[\n \n1671.661424\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1671.666378\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n610\n/\nvictim_finish_insert\n()\n!\n\n\n[\n \n1671.675591\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1671.680339\n]\n \nCPU\n:\n \n20\n \nPID\n:\n \n31\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n426\n\n\n[\n \n1671.686351\n]\n \nStack\n:\n\n\n[\n \n1671.688581\n]\n \nffff880fbe76fda0\n \nffffffff810289b7\n \nffffffff00000008\n \nffff880fbe76fdb0\n\n\n[\n \n1671.696725\n]\n \nffff880fbe76fd68\n \nffffff0021475542\n \nffff88107fd45e00\n \nffff880fbe753000\n\n\n[\n \n1671.704870\n]\n \n0000000000000000\n \n0000000000000001\n \nffff880fbe76f9b0\n \nffffffff8101b1b7\n\n\n[\n \n1671.713015\n]\n \nffff88107fd44980\n \nffff880fbe76f9d8\n \nffffffff8101405f\n \n0000000000000000\n\n\n[\n \n1671.721160\n]\n \n0000000000000001\n \nffff880ff992a000\n \n0000000000000001\n \nffff880fbe76f9f0\n\n\n[\n \n1671.729304\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1671.732019\n]\n \nTSK\n\n\n[\n \n1671.734153\n]\n \n[\nffffffff810289c3\n]\n \npanic\n+\n0xc2\n/\n0x10a\n\n\n[\n \n1671.739388\n]\n \n[\nffffffff8101b1b7\n]\n \n?\n \nscheduler_tick\n+\n0x57\n/\n0x60\n\n\n[\n \n1671.745593\n]\n \n[\nffffffff8101405f\n]\n \n?\n \ngeneric_smp_call_function_single_interrupt\n+\n0x8f\n/\n0x160\n\n\n[\n \n1671.754611\n]\n \n[\nffffffff8100339e\n]\n \n?\n \ncall_function_interrupt\n+\n0x2e\n/\n0x40\n\n\n[\n \n1671.761688\n]\n \n[\nffffffff8100e9fa\n]\n \n?\n \nsmp__call_function_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n1671.769251\n]\n \n[\nffffffff8101f4bb\n]\n \n?\n \ndebug_spin_lock\n+\n0x1b\n/\n0x50\n\n\n[\n \n1671.775555\n]\n \n[\nffffffff81075efc\n]\n \n?\n \nfit_internal_poll_sendcq\n+\n0x6c\n/\n0x140\n\n\n[\n \n1671.782826\n]\n \n[\nffffffff81042039\n]\n \n?\n \nfind_next_bit\n+\n0x19\n/\n0x20\n\n\n[\n \n1671.788934\n]\n \n[\nffffffff8101f4bb\n]\n \n?\n \ndebug_spin_lock\n+\n0x1b\n/\n0x50\n\n\n[\n \n1671.795236\n]\n \n[\nffffffff8101dcac\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n \n1671.801248\n]\n \n[\nffffffff8101b1b7\n]\n \n?\n \nscheduler_tick\n+\n0x57\n/\n0x60\n\n\n[\n \n1671.807453\n]\n \n[\nffffffff810174d5\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n \n1671.814240\n]\n \n[\nffffffff81006774\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n1671.821029\n]\n \n[\nffffffff8100e8aa\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n1671.828300\n]\n \n[\nffffffff81012bc8\n]\n \n?\n \nprintk\n+\n0x118\n/\n0x1b0\n\n\n[\n \n1671.833924\n]\n \n[\nffffffff8103c161\n]\n \nvictim_finish_insert\n+\n0x171\n/\n0x180\n\n\n[\n \n1671.840711\n]\n \n[\nffffffff8103b2a2\n]\n \npcache_evict_line\n+\n0xf2\n/\n0x2e0\n\n\n[\n \n1671.847110\n]\n \n[\nffffffff81038d7c\n]\n \npcache_alloc\n+\n0x1ac\n/\n0x380\n\n\n[\n \n1671.853122\n]\n \n[\nffffffff8103a10c\n]\n \n?\n \npcache_add_rmap\n+\n0x7c\n/\n0x260\n\n\n[\n \n1671.859521\n]\n \n[\nffffffff810382bb\n]\n \ncommon_do_fill_page\n+\n0x2b\n/\n0x1e0\n\n\n[\n \n1671.866114\n]\n \n[\nffffffff81038631\n]\n \npcache_handle_fault\n+\n0x1c1\n/\n0x620\n\n\n[\n \n1671.872804\n]\n \n[\nffffffff81037fc0\n]\n \n?\n \npcache_meta_to_kva\n+\n0x30\n/\n0x30\n\n\n[\n \n1671.879398\n]\n \n[\nffffffff8101006f\n]\n \ndo_page_fault\n+\n0xaf\n/\n0x1c0\n\n\n[\n \n1671.885410\n]\n \n[\nffffffff8100dedf\n]\n \npage_fault\n+\n0x1f\n/\n0x30\n\n\n\n\n\n\n\n\n\n\n0829-w14-16: we got this by having debug_spinlock, and seq_ibapi. This is interesting and serious. I think our general C code is fine.. Should I go check the assembly part? This is the rq-\nlock? come on\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n[\n  \n683.748135\n]\n \n-------------------\n \ncut\n \nhere\n \n-------------------\n\n\n[\n  \n683.754252\n]\n \nPossible\n \ndeadlock\n \nhappend\n\n\n[\n  \n683.758323\n]\n \nCurrent\n \ncall\n \nstack\n:\n\n\n[\n  \n683.761815\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n39\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n428\n\n\n[\n  \n683.767728\n]\n \nStack\n:\n\n\n[\n  \n683.769959\n]\n \nffff880fc1c1fc38\n \nffffffff8101f48c\n \nffff88107fc45e00\n \nffff880fc1c1fc60\n\n\n[\n  \n683.778103\n]\n \nffffffff8101f4e4\n \nffff88107fc45e00\n \nffff880fc23fb000\n \n0000000000000000\n\n\n[\n  \n683.786247\n]\n \nffff880fc1c1fc80\n \nffffffff8101b18e\n \nffff88107fc44980\n \n0000000000000004\n\n\n[\n  \n683.794391\n]\n \nffff880fc1c1fc98\n \nffffffff810174d5\n \nffffffff8101dddb\n \nffff880fc1c1fcc0\n\n\n[\n  \n683.802537\n]\n \nffffffff81006774\n \nffff88107fc45e00\n \n00000004\na817c800\n \n000000\n9\na8a78c5e7\n\n\n[\n  \n683.810680\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n683.813396\n]\n \nTSK\n\n\n[\n  \n683.815528\n]\n \n[\nffffffff8101f498\n]\n \nreport_deadlock\n+\n0x58\n/\n0x60\n\n\n[\n  \n683.821637\n]\n \n[\nffffffff8101f4e4\n]\n \ndebug_spin_lock\n+\n0x44\n/\n0x50\n\n\n[\n  \n683.827745\n]\n \n[\nffffffff8101b18e\n]\n \nscheduler_tick\n+\n0x2e\n/\n0x60\n\n\n[\n  \n683.833758\n]\n \n[\nffffffff810174d5\n]\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n  \n683.840351\n]\n \n[\nffffffff8101dddb\n]\n \n?\n \ndequeue_task_rt\n+\n0x1b\n/\n0x180\n\n\n[\n  \n683.846750\n]\n \n[\nffffffff81006774\n]\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n  \n683.853343\n]\n \n[\nffffffff8100e8aa\n]\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n683.860421\n]\n \n[\nffffffff8101f4d1\n]\n \n?\n \ndebug_spin_lock\n+\n0x31\n/\n0x50\n\n\n[\n  \n683.866723\n]\n \n[\nffffffff8101b86e\n]\n \ntry_to_wake_up\n+\n0xce\n/\n0x1f0\n\n\n[\n  \n683.872832\n]\n \n[\nffffffff8101b9e4\n]\n \nwake_up_q\n+\n0x54\n/\n0xc0\n\n\n[\n  \n683.878358\n]\n \n[\nffffffff81028487\n]\n \ndo_futex\n+\n0x407\n/\n0x620\n\n\n[\n  \n683.883982\n]\n \n[\nffffffff8103a941\n]\n \n?\n \npcache_add_rmap\n+\n0xb1\n/\n0x600\n\n\n[\n  \n683.890381\n]\n \n[\nffffffff8102870c\n]\n \nsys_futex\n+\n0x6c\n/\n0x130\n\n\n[\n  \n683.896005\n]\n \n[\nffffffff8100ec66\n]\n \ndo_syscall_64\n+\n0x36\n/\n0xc0\n\n\n[\n  \n683.901919\n]\n \n[\nffffffff8100db6c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n\n\n\n\n\n\n\n\nAug 27\n\n\nThere a lot lost CQE cases. This one is about P-\nM-\nS. And M lost the CQE for the WQE sent to S.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n0827\n-\nw9\n-\n5\n\n\n[\n  \n963.304865\n]\n \nwatchdog\n:\n \nworker\n[\n0\n]\n \nCPU10\n \nstucked\n\n\n[\n  \n963.309712\n]\n \nwatchdog\n:\n  \ncommon_header\n \n[\nop\n=\n0x20000000\n \nsrc_nid\n:\n0\n]\n\n\n[\n  \n963.316210\n]\n \nCPU\n:\n \n10\n \nPID\n:\n \n20\n \nComm\n:\n \nthpool\n-\nworker0\n \n4.0.0\n-\nlego\n+\n \n#\n43\n\n\n[\n  \n963.322899\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8106ad51\n]\n  \n[\nffffffff8106ad51\n]\n \nfit_send_reply_with_rdma_write_with_imm\n+\n0x2a1\n/\n0x3a0\n\n\n[\n  \n963.334632\n]\n \nRSP\n:\n \n0000\n:\nffff88103ef3fc20\n  \nEFLAGS\n:\n \n000002\n87\n\n\n[\n  \n963.340547\n]\n \nRAX\n:\n \n00000000ff\nffb6d4\n \nRBX\n:\n \n000000000000000\nb\n \nRCX\n:\n \n0000000000001770\n\n\n[\n  \n963.348498\n]\n \nRDX\n:\n \n00000000ff\nffa70d\n \nRSI\n:\n \nfffffffffffff039\n \nRDI\n:\n \n0000000000000000\n\n\n[\n  \n963.356450\n]\n \nRBP\n:\n \nffff88103ef3fcc0\n \nR08\n:\n \n000000000000001f\n \nR09\n:\n \n0000000000000002\n\n\n[\n  \n963.364400\n]\n \nR10\n:\n \n00000000\n80000000\n \nR11\n:\n \n000077ff\n80000000\n \nR12\n:\n \n0000000000000000\n\n\n[\n  \n963.372352\n]\n \nR13\n:\n \nffff88103ef26738\n \nR14\n:\n \n00000000000\nb3d54\n \nR15\n:\n \nffff88103ef25008\n\n\n[\n  \n963.380303\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fca0000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n963.389320\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n963.395720\n]\n \nCR2\n:\n \n0000000000000000\n \nCR3\n:\n \n000000000116\na000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n  \n963.403671\n]\n \nStack\n:\n\n\n[\n  \n963.405901\n]\n \n00007ff\nf000b3d54\n \nffffffff800b3d54\n \nffff881000000004\n \nffff88103ef3fc78\n\n\n[\n  \n963.414045\n]\n \n0000000\n900000000\n \nffff881000000000\n \n0000100\n800000001\n \nffff88103d216000\n\n\n[\n  \n963.422191\n]\n \nffff88103eebae48\n \n800\nb3d540000011c\n \nffffff9b00000246\n \nffffea0000000001\n\n\n[\n  \n963.430337\n]\n \n000000103\nd216000\n \n0000000000010\nc00\n \n000000000000011\nc\n \n000000000000100\n8\n\n\n[\n  \n963.438481\n]\n \n000000000000011\nc\n \n000000000000100\n8\n \nffff88103eebae48\n \nffff88103ef3fd70\n\n\n[\n  \n963.446626\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n963.449342\n]\n \nTSK\n\n\n[\n  \n963.451475\n]\n \n[\nffffffff81067c80\n]\n \nibapi_send_reply_imm\n+\n0x50\n/\n0xd0\n\n\n[\n  \n963.458068\n]\n \n[\nffffffff8102e953\n]\n \n?\n \n__storage_read\n+\n0xc3\n/\n0x120\n\n\n[\n  \n963.464371\n]\n \n[\nffffffff8102e953\n]\n \n__storage_read\n+\n0xc3\n/\n0x120\n\n\n[\n  \n963.470480\n]\n \n[\nffffffff8102e9bf\n]\n \nstorage_read\n+\n0xf\n/\n0x50\n\n\n[\n  \n963.476201\n]\n \n[\nffffffff8102eab7\n]\n \nstorage_vma_fault\n+\n0xb7\n/\n0x130\n\n\n[\n  \n963.482600\n]\n \n[\nffffffff8103262f\n]\n \nhandle_lego_mm_fault\n+\n0x13f\n/\n0x4a0\n\n\n[\n  \n963.489389\n]\n \n[\nffffffff8102ecf4\n]\n \ncommon_handle_p2m_miss\n.\nisra\n.1\n+\n0x54\n/\n0xc0\n\n\n[\n  \n963.496855\n]\n \n[\nffffffff8102edc7\n]\n \nhandle_p2m_pcache_miss\n+\n0x67\n/\n0x2d0\n\n\n[\n  \n963.503739\n]\n \n[\nffffffff8102bf96\n]\n \nthpool_worker_func\n+\n0x296\n/\n0x3a0\n\n\n[\n  \n963.510332\n]\n \n[\nffffffff8102bd00\n]\n \n?\n \nhandle_bad_request\n+\n0x40\n/\n0x40\n\n\n[\n  \n963.516926\n]\n \n[\nffffffff81020ca6\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n  \n963.522357\n]\n \n[\nffffffff81020bb0\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n  \n963.528756\n]\n \n[\nffffffff8100e632\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n\n\n\nhmm, another on lost CQE happen at P.\nToday is weird, why we happen to have so many lost CQE today?\n\n\nThink about why CQE is not generated?\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n0827\n-\nw14\n-\n6\n\n\n[\n \n1185.835707\n]\n\n\n*****\n\n\n*****\n \nFail\n \nto\n \nto\n \nget\n \nthe\n \nCQE\n \nfrom\n \nsend_cq\n \nafter\n \n20\n \nseconds\n!\n\n\n*****\n \nThis\n \nmeans\n \nthe\n \npacket\n \nwas\n \nlost\n \nand\n \nsomething\n \nwent\n \nwrong\n\n\n*****\n \nwith\n \nyour\n \nNIC\n...\n\n\n*****\n \nconnection_id\n:\n \n7\n \ndest\n \nnode\n:\n \n1\n\n\n*****\n\n\n[\n \n1185.856465\n]\n \nIB\n \nStats\n:\n\n\n[\n \n1185.858985\n]\n     \nnr_ib_send_reply\n:\n            \n3452\n\n\n[\n \n1185.864221\n]\n     \nnr_bytes_tx\n:\n               \n506507\n\n\n[\n \n1185.869456\n]\n     \nnr_bytes_rx\n:\n              \n8981004\n\n\n[\n \n1185.874692\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1185.879829\n]\n \nWARNING\n:\n \nCPU\n:\n \n14\n \nPID\n:\n \n22\n \nat\n \nnet\n/\nlego\n/\nfit_internal\n.\nc\n:\n1108\n \nfit_internal_poll_sendcq\n+\n0xe5\n/\n0x140\n\n\n[\n \n1185.890399\n]\n \nCPU\n:\n \n14\n \nPID\n:\n \n22\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n356\n\n\n[\n \n1185.896410\n]\n \nStack\n:\n\n\n[\n \n1185.898640\n]\n \nffff88103c49fb30\n \nffffffff810126f5\n \nffff88103cb22000\n \n00000004\na817c800\n\n\n[\n \n1185.906784\n]\n \n0000010f7139214f\n \n0000000000000007\n \nffff88103c49fb40\n \nffffffff810127cf\n\n\n[\n \n1185.914927\n]\n \nffff88103c49fbf0\n \nffffffff810724b5\n \n000000023\ncb2c280\n \nffff88103cb2c1f8\n\n\n[\n \n1185.923072\n]\n \n00000000000002\n86\n \nffff88103c49fc18\n \nffff88103cb06000\n \nffff88103cb2c150\n\n\n[\n \n1185.931217\n]\n \n000000000000024\nb\n \nffff88108101c7dc\n \nffff88107fce5d80\n \nffff88103c46f000\n\n\n[\n \n1185.939360\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1185.942075\n]\n \nTSK\n\n\n[\n \n1185.944209\n]\n \n[\nffffffff81012701\n]\n \n__warn\n.\nconstprop\n.1\n+\n0x91\n/\n0xd0\n\n\n[\n \n1185.950607\n]\n \n[\nffffffff810127cf\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n \n1185.956909\n]\n \n[\nffffffff810724b5\n]\n \nfit_internal_poll_sendcq\n+\n0xe5\n/\n0x140\n\n\n[\n \n1185.963987\n]\n \n[\nffffffff81019dd5\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n \n1185.970192\n]\n \n[\nffffffff81072662\n]\n \nfit_send_message_with_rdma_write_with_imm_request\n+\n0x152\n/\n0x350\n\n\n[\n \n1185.979791\n]\n \n[\nffffffff810741ff\n]\n \nfit_send_reply_with_rdma_write_with_imm\n+\n0x25f\n/\n0x3a0\n\n\n[\n \n1185.988420\n]\n \n[\nffffffff810368c2\n]\n \n?\n \n__pcache_do_fill_page\n+\n0xc2\n/\n0x1d0\n\n\n[\n \n1185.995401\n]\n \n[\nffffffff810701e9\n]\n \nibapi_send_reply_timeout\n+\n0x79\n/\n0x120\n\n\n[\n \n1186.002479\n]\n \n[\nffffffff810368c2\n]\n \n?\n \n__pcache_do_fill_page\n+\n0xc2\n/\n0x1d0\n\n\n[\n \n1186.009459\n]\n \n[\nffffffff810368c2\n]\n \n__pcache_do_fill_page\n+\n0xc2\n/\n0x1d0\n\n\n[\n \n1186.016245\n]\n \n[\nffffffff81036ac4\n]\n \ncommon_do_fill_page\n+\n0xf4\n/\n0x1f0\n\n\n[\n \n1186.022839\n]\n \n[\nffffffff81036d80\n]\n \npcache_handle_fault\n+\n0x1c0\n/\n0x610\n\n\n[\n \n1186.029528\n]\n \n[\nffffffff81036800\n]\n \n?\n \n__pcache_do_zerofill_page\n+\n0x100\n/\n0x100\n\n\n[\n \n1186.036995\n]\n \n[\nffffffff8100fdff\n]\n \ndo_page_fault\n+\n0xaf\n/\n0x1c0\n\n\n[\n \n1186.043005\n]\n \n[\nffffffff8100dc1f\n]\n \npage_fault\n+\n0x1f\n/\n0x30\n\n\n\n\n\n\n\nAug 26\n\n\nOh well. I saw the same damn lost packet issue again. The issue can be desribed as: P use lite rpc to send a request to M. M processed the handled, and called rpc reply to sent back to P. M need to poll send_cq to poll completion. But M fail to get the CQE for the should-be-sent-out WQE.\n\n\nThis is tested with M\ns \nCONFIG_FIT_NOWAIT\n optimization, which is basically an optimization that M will not poll cq every time a reply was sent out, instead, do batch polling.\n\n\nThe following stack dump was reported by M side watchdog. It is not necessary mlx4_poll_cq\ns issue, since there is a while (1) loop at fit code. Oh well.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\nLog\n \nname\n:\n \n0826\n-\nw9\n-\n1\n\n\n\n[\n187736.669027\n]\n \nwatchdog\n:\n \nworker\n[\n0\n]\n \nCPU10\n \nstucked\n\n\n[\n187736.673972\n]\n \nwatchdog\n:\n  \ncommon_header\n \n[\nop\n=\n0x30000000\n \nsrc_nid\n:\n0\n]\n\n\n[\n187736.680566\n]\n \nCPU\n:\n \n10\n \nPID\n:\n \n20\n \nComm\n:\n \nthpool\n-\nworker0\n \n4.0.0\n-\nlego\n+\n \n#\n26\n\n\n[\n187736.687351\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff810522c3\n]\n  \n[\nffffffff810522c3\n]\n \nmlx4_ib_poll_cq\n+\n0x1d3\n/\n0x850\n\n\n[\n187736.696854\n]\n \nRSP\n:\n \n0000\n:\nffff88103ef3f750\n  \nEFLAGS\n:\n \n000002\n86\n\n\n[\n187736.702865\n]\n \nRAX\n:\n \n00000000ff\nfffff5\n \nRBX\n:\n \n0000000000000000\n \nRCX\n:\n \nffff88103ed6b050\n\n\n[\n187736.710913\n]\n \nRDX\n:\n \n00000000\n80630000\n \nRSI\n:\n \n0000000000000001\n \nRDI\n:\n \nffff88103edb0bf0\n\n\n[\n187736.718961\n]\n \nRBP\n:\n \nffff88103ef3f7b8\n \nR08\n:\n \n0000000000000020\n \nR09\n:\n \n0000000000000002\n\n\n[\n187736.727007\n]\n \nR10\n:\n \n0000000ff\nc53fddc\n \nR11\n:\n \n0000000040\nbf1040\n \nR12\n:\n \nffff88103ef3f7c8\n\n\n[\n187736.735055\n]\n \nR13\n:\n \n0000000000000000\n \nR14\n:\n \n0000000000000000\n \nR15\n:\n \nffff88103edb0bf0\n\n\n[\n187736.743104\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fca0000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n187736.752218\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n187736.758714\n]\n \nCR2\n:\n \n0000000000000000\n \nCR3\n:\n \n000000000116\na000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n187736.766762\n]\n \nStack\n:\n\n\n[\n187736.769089\n]\n \n0000000ff\nc53fddc\n \n0000000000000002\n \n0000000000000020\n \nffff88103edb0c98\n\n\n[\n187736.777331\n]\n \n00000000000002\n86\n \n00000000\n80630000\n \nffff88103ef3f7d0\n \n000063\n8000000018\n\n\n[\n187736.785572\n]\n \nffff88103edb0bf0\n \n0000000000000001\n \nffff88103ef25008\n \n0000000000000003\n\n\n[\n187736.793813\n]\n \n000000000000000\nc\n \nffff88103ef3fd30\n \nffffffff8106920c\n \nffff88103ef3fd54\n\n\n[\n187736.802054\n]\n \n0000000100000000\n \n0000000100000000\n \nffff88103edb07b0\n \nffff88103e81b008\n\n\n[\n187736.810296\n]\n \nCall\n \nTrace\n:\n\n\n[\n187736.813108\n]\n \nTSK\n\n\n[\n187736.815338\n]\n \n[\nffffffff8106920c\n]\n \nfit_internal_poll_sendcq\n+\n0x6c\n/\n0xe0\n\n\n[\n187736.822416\n]\n \n[\nffffffff8106ab2f\n]\n \n?\n \nfit_send_reply_with_rdma_write_with_imm\n+\n0x25f\n/\n0x3a0\n\n\n[\n187736.831336\n]\n \n[\nffffffff81033ff0\n]\n \n?\n \n_lego_copy_to_user\n+\n0x110\n/\n0x250\n\n\n[\n187736.838220\n]\n \n[\nffffffff81028d65\n]\n \n?\n \n__free_pages\n+\n0x25\n/\n0x30\n\n\n[\n187736.844329\n]\n \n[\nffffffff8102e981\n]\n \n?\n \n__storage_read\n+\n0xf1\n/\n0x120\n\n\n[\n187736.850728\n]\n \n[\nffffffff81019865\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n187736.857031\n]\n \n[\nffffffff810693d2\n]\n \n?\n \nfit_send_message_with_rdma_write_with_imm_request\n+\n0x152\n/\n0x350\n\n\n[\n187736.866920\n]\n \n[\nffffffff810693d2\n]\n \n?\n \nfit_send_message_with_rdma_write_with_imm_request\n+\n0x152\n/\n0x350\n\n\n[\n187736.876810\n]\n \n[\nffffffff8103043f\n]\n \n?\n \n__vma_adjust\n+\n0x38f\n/\n0x550\n\n\n[\n187736.883113\n]\n \n[\nffffffff81030944\n]\n \n?\n \nvma_merge\n+\n0x1a4\n/\n0x280\n\n\n[\n187736.889123\n]\n \n[\nffffffff81030f20\n]\n \n?\n \narch_get_unmapped_area_topdown\n+\n0xe0\n/\n0x220\n\n\n[\n187736.897075\n]\n \n[\nffffffff810693d2\n]\n \nfit_send_message_with_rdma_write_with_imm_request\n+\n0x152\n/\n0x350\n\n\n[\n187736.906771\n]\n \n[\nffffffff81069ab5\n]\n \nfit_ack_reply_callback\n+\n0x185\n/\n0x1e0\n\n\n[\n187736.913848\n]\n \n[\nffffffff8102f129\n]\n \n?\n \nhandle_p2m_flush_one\n+\n0x69\n/\n0x160\n\n\n[\n187736.920830\n]\n \n[\nffffffff8102bde0\n]\n \nthpool_worker_func\n+\n0xe0\n/\n0x3a0\n\n\n[\n187736.927424\n]\n \n[\nffffffff8102bd00\n]\n \n?\n \nhandle_bad_request\n+\n0x40\n/\n0x40\n\n\n[\n187736.934113\n]\n \n[\nffffffff81020ca6\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n187736.939639\n]\n \n[\nffffffff81020bb0\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n187736.946137\n]\n \n[\nffffffff8100e632\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n\n\n\nAug 22\n\n\nDamn it!!! After so much effort verifying we had a solid IB stack, we still has memory corruption and deadlock issues. Fuck!\n\n\nOne thing at a time, simple stuff first. Okay, tomorrow first add DEBUG_SPINLOCK to detect possible deadlocks. This, could help to identify some buggy code. After this, I will spend some time looking into the LITE, it\ns fucking HEAVY. I do found a lot issues during summer.\n\n\nPersonally, I\nm not feeling good this days. I treat someone with love and respect, but there is not too much in return. Yeahyeahyeah, I know how this works. It\ns just sad that sometimes you just have a BAD timing. I\nve went through too much things in 2018, good and bad. I care sooo much about the people I love, family and others. I feel this is good, of course. Anyway, it is supposed to be a Lego dump, that no one probably interested in.", 
            "title": "Aug 2018"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-2018", 
            "text": "", 
            "title": "Aug 2018"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-31", 
            "text": "", 
            "title": "Aug 31"
        }, 
        {
            "location": "/lego/log/log-08-2018/#one-major-todo", 
            "text": "Check  do_handle_p2m_pcache_miss() . We MUST remove that mempcy, maybe by using another flag in thpool. This is just no acceptable.", 
            "title": "One major TODO"
        }, 
        {
            "location": "/lego/log/log-08-2018/#ugh", 
            "text": "Fuck. Without debug_mm, there is still memory corruption.", 
            "title": "Ugh"
        }, 
        {
            "location": "/lego/log/log-08-2018/#try-max_send_wr-and-number-of-qps", 
            "text": "without lock_ib, with debug_mm.\nChange max_send_wr at all P M S.   QP=4, max_send_wr = 1: always fail  QP=4, max_send_wr = 256: always fail  QP=24, max_send_wr = 1: succeed (0831-w14-18 0831-w14-20)  QP=24, max_send_wr = 256: succeed (0831-w14-16 0831-w14-17)   Pay attention to the  0831-w14-15 \uff1a something wrong with our timekeeping code? QP=24, max_send_wr = 1 case.", 
            "title": "Try max_send_wr and number of QPs"
        }, 
        {
            "location": "/lego/log/log-08-2018/#after-victim-bug-fix", 
            "text": "MNIST 4 threads   With lock_ib, debug_mm etc: 3 successful runs  Only with debug_mm: Well fit failed. Lost CQE.   Now the debug scope is limited. Let me try the micro test suite, to stress ibapi_send_reply itself.  Potential: read/write buffer.", 
            "title": "After Victim bug fix"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-30", 
            "text": "Be humble.", 
            "title": "Aug 30"
        }, 
        {
            "location": "/lego/log/log-08-2018/#identified-victim-bug", 
            "text": "Finally. I thought it through, and with the help of this  0830-w14-12 .\nThe bug is in  victim_try_fill_pcache() , when there are multiple hits to the same victim. Since we released the  usable_victim_lock  after a hit. There might a be race case where: 1) CPU0 reached  dec_and_test_filling , and passed to free the line. 2) CPU1 just got to the  victim_check_hit , and increment the fill counter to 1 again. When CPU1 finished filling, and do  dec_and_test_filling , it will do the free again!!! What a double free.  Tomorrow, let me do the fix. Thought: adding more sync in victim_check_hit part. Basically we want to ensure only one CPU can do the final free.", 
            "title": "Identified victim bug."
        }, 
        {
            "location": "/lego/log/log-08-2018/#after-adding-pi_lock", 
            "text": "Okay. the pi_lock is added. Although it is mostly used by futex-pi and rt-mutex, we lego does not have these two guys. Therefore, it is only used by sched/core.c, exit.c, and kthread.c. 99% is in core.c  The purpose of having this back is to have the  spin_lock_irqsave( p- pi_lock)  back. Most scheduling code is not recursive, we have to disable interrupt. Of course we can use  spin_lock_irqsave( rq- lock)  instead of  spin_lock( rq- lock) . But this is too dangerous at this stage. Porting based on Linux now is the fastest and safest way.  The importance of disabling interrupt in some kernel path!!  Good. Now I m seeing now debuggable victim issue.  Classical deadlock catched. Now, only two victims.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 [   2819.068997 ]   CPU14   PID29   Abort   victim   alloc   ( 20010 ms )   nr_usable_victims :   2.   From   pset_idx : 532   nr_lru : 63   fault_uva :   0x7fff98614000  [   2819.094409 ]   CPU14   PID29     --     Start   Dump   Victim   Cache   [ 0 ]   total :   2  [   2819.114188 ]   CPU14   PID29    victim [ 0 ] : ffffffff810c2880   refcount : 2   nr_fill : 1   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2819.133289 ]   CPU14   PID29       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffcc000000  [   2819.141723 ]   CPU14   PID29       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2819.149770 ]   CPU14   PID29    victim [ 1 ] : ffffffff810c2900   refcount : 2   nr_fill : 1   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2819.168870 ]   CPU14   PID29       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffb0000000  [   2819.177306 ]   CPU14   PID29       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2819.185352 ]   CPU14   PID29     --     End   Dump   Victim   Cache   [ 0 ]  [   2819.081708 ]   CPU16   PID30   Abort   victim   alloc   ( 20010 ms )   nr_usable_victims :   2.   From   pset_idx : 0   nr_lru : 63   fault_uva :   0x7fffcc000024  [   2819.209008 ]   CPU16   PID30     --     Start   Dump   Victim   Cache   [ 1 ]   total :   2  [   2819.223358 ]   CPU16   PID30    victim [ 0 ] : ffffffff810c2880   refcount : 2   nr_fill : 1   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2819.252443 ]   CPU16   PID30       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffcc000000  [   2819.260879 ]   CPU16   PID30       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2819.268926 ]   CPU16   PID30    victim [ 1 ] : ffffffff810c2900   refcount : 2   nr_fill : 1   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2819.288026 ]   CPU16   PID30       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffb0000000  [   2819.296461 ]   CPU16   PID30       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2819.304508 ]   CPU16   PID30     --     End   Dump   Victim   Cache   [ 1 ]  [   2819.101391 ]   CPU18   PID31   Abort   victim   alloc   ( 20010 ms )   nr_usable_victims :   2.   From   pset_idx : 15   nr_lru : 63   fault_uva :   0x7fff98c0f000  [   2819.328165 ]   CPU18   PID31     --     Start   Dump   Victim   Cache   [ 2 ]   total :   2  [   2819.335146 ]   CPU18   PID31    victim [ 0 ] : ffffffff810c2880   refcount : 1   nr_fill : 0   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2819.354246 ]   CPU18   PID31       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffcc000000  [   2819.362680 ]   CPU18   PID31       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2819.370728 ]   CPU18   PID31    victim [ 1 ] : ffffffff810c2900   refcount : 2   nr_fill : 1   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2819.389828 ]   CPU18   PID31       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffb0000000  [   2819.398262 ]   CPU18   PID31       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2819.406310 ]   CPU18   PID31     --     End   Dump   Victim   Cache   [ 2 ]  #  # This guy grabbed the fill counter right before the first timout  # That s why the above three timeout happen. And this one is 20s later  # which equals to the timeout second.  #  [   2839.327457 ]   CPU12   PID28   Abort   victim   alloc   ( 20010 ms )   nr_usable_victims :   2.   From   pset_idx : 0   nr_lru : 63   fault_uva :   0x7fffb0000f00  [   2839.339964 ]   CPU12   PID28     --     Start   Dump   Victim   Cache   [ 3 ]   total :   2  [   2839.346945 ]   CPU12   PID28    victim [ 0 ] : ffffffff810c2880   refcount : 1   nr_fill : 0   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2839.366046 ]   CPU12   PID28       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffcc000000  [   2839.374480 ]   CPU12   PID28       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2839.382527 ]   CPU12   PID28    victim [ 1 ] : ffffffff810c2900   refcount : 2   nr_fill : 1   max_fill : 4   locked : 0   flags :( 0x14e )( allocated | usable | hasdata | flushed | fillfree )   pcm :            ( null )   pset : ffff88207ff5a000  [   2839.401628 ]   CPU12   PID28       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffb0000000  [   2839.410062 ]   CPU12   PID28       rmap   to   pset : ffff88207ff5a000   set_idx :   0   nr_lru : 63  [   2839.418109 ]   CPU12   PID28     --     End   Dump   Victim   Cache   [ 3 ]", 
            "title": "After adding pi_lock"
        }, 
        {
            "location": "/lego/log/log-08-2018/#rq-lock-deadlock", 
            "text": "Alright. We had rq- lock deadlock issue. Basically, we missed the part of disabling interrupt. A timer interrupt will try to acquire the lock again. Then, bang we have a deadlock. Digging into the code, you will be able to find the cause easily. The root cause we removed all the pi_lock stuff, which actually have a lot irqsave usages  Oh man, maybe it s time to add it back.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42 [ 3367.835389] ------------------- cut here -------------------\n[ 3367.841504] Possible deadlock happend locker_cpu: 0\n[ 3367.846934] Current call stack:\n[ 3367.850425] CPU: 0 PID: 1 Comm: kernel_init 4.0.0-lego+ #437\n[ 3367.856726] Stack:\n[ 3367.858957] ffff88107ff0fa58 ffffffff8101f4b6 ffff88107fc05e00 00000004a817c800\n[ 3367.867101] ffff88107ff0fa80 ffffffff8101f52e ffff88107fc05e00 ffff88107ffb4000\n[ 3367.875246] 0000000000000000 ffff88107ff0faa0 ffffffff8101b1ae ffff88107fc04980\n[ 3367.883390] 0000000000000000 ffff88107ff0fab8 ffffffff810174f5 0000000000000286\n[ 3367.891535] ffff88107ff0fae0 ffffffff81006774 ffff88107ffb9000 ffff88107fc05e00\n[ 3367.899680] Call Trace:\n[ 3367.902396]  TSK \n[ 3367.904528] [ ffffffff8101f4c2 ] report_deadlock+0x62/0x80\n[ 3367.910637] [ ffffffff8101f52e ] debug_spin_lock+0x4e/0x60\n[ 3367.916745] [ ffffffff8101b1ae ] scheduler_tick+0x2e/0x60\n[ 3367.922756] [ ffffffff810174f5 ] tick_handle_periodic+0x45/0x70\n[ 3367.929350] [ ffffffff81006774 ] apic_timer_interrupt+0x54/0x90\n[ 3367.935943] [ ffffffff8100e8aa ] smp__apic_timer_interrupt+0x6a/0x70\n[ 3367.943021] [ ffffffff8101db99 ] ? enqueue_task_rt+0x149/0x250\n[ 3367.949518] [ ffffffff8105908a ] ? __mlx4_write_mtt+0xea/0x140\n[ 3367.956014] [ ffffffff8101ad34 ] activate_task+0x44/0x50\n[ 3367.961929] [ ffffffff8101b667 ] ttwu_do_activate+0x27/0x50\n[ 3367.968134] [ ffffffff8101b89c ] try_to_wake_up+0xdc/0x1f0\n[ 3367.974243] [ ffffffff8106cc20 ] ? ib_mad_send_done_handler.isra.22+0x4d0/0x4d0\n[ 3367.982388] [ ffffffff8101ba80 ] wake_up_process+0x10/0x20\n[ 3367.988497] [ ffffffff81023116 ] __kthread_create_on_node+0x146/0x230\n[ 3367.995671] [ ffffffff8102329f ] kthread_create_on_node+0x2f/0x40\n[ 3368.002459] [ ffffffff81066873 ] ? ib_create_cq+0x23/0x60\n[ 3368.008470] [ ffffffff810695e1 ] ib_mad_init_device+0x1f1/0x7b0\n[ 3368.015064] [ ffffffff81067246 ] ib_register_device+0x5d6/0x690\n[ 3368.021657] [ ffffffff8105e9d3 ] mlx4_ib_add+0x653/0x780\n[ 3368.027571] [ ffffffff8105147d ] mlx4_add_device+0x8d/0x130\n[ 3368.033777] [ ffffffff8105158c ] mlx4_register_interface+0x6c/0xa0\n[ 3368.040661] [ ffffffff811dc660 ] mlx4_ib_init+0x10/0x20\n[ 3368.046478] [ ffffffff811dc619 ] mlx4_init+0x19/0x50\n[ 3368.052005] [ ffffffff811dc68d ] ib_core_init+0x1d/0x30\n[ 3368.057823] [ ffffffff811db7f9 ] device_init+0x9/0x10\n[ 3368.063447] [ ffffffff8100030b ] kernel_init+0x4b/0xc0\n[ 3368.069168] [ ffffffff8101b0ea ] ? schedule_tail+0xa/0x40\n[ 3368.075178] [ ffffffff810002c0 ] ? 0xffffffff810002c0\n[ 3368.080803] [ ffffffff8100eb32 ] ret_from_fork+0x22/0x30\n[ 3368.086718]  EOT    0830-w14-1: I really don t know how this happen. The refcounter and fill counter should be enough to serialize..  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 [37722.177024] CPU20 PID31  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x12e)(allocated|usable|hasdata|waitflush|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.196623] CPU20 PID31     hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000\n[37722.204572] CPU20 PID31  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.224154] CPU20 PID31     rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63\n[37722.232299] CPU20 PID31     victim dumped because: PCACHE_BUG_ON_VICTIM(!VictimAllocated(v) || !VictimUsable(v) || !VictimFlushed(v) || VictimWriteback(v) || VictimLocked(v))\n[37722.254790] WARNING: CPU: 20 PID: 31 at managers/processor/pcache/victim.c:196 __put_victim_nolist+0xb8/0x140\n ffffffff8103e170[37722.453632] [ ffffffff8103c9c8 ] __put_victim_nolist+0xb8/0x140\n 0000000000000000[37722.461873] [ ffffffff8103db18 ] victim_try_fill_pcache+0x2f8/0x440\n\n[37722.265842] CPU10 PID20  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.291438] CPU10 PID20     hit[0] owner:22 m_nid:1 rep_nid:1 addr: 0x2c33000\n[37722.301616] CPU10 PID20  victim:ffffffff810c2880 index:0 refcount:0 nr_fill:0 max_fill:4 locked:0 flags:(0x14e)(allocated|usable|hasdata|flushed|fillfree) pcm:          (null) pset:ffff88207ff5b980\n[37722.324206] CPU10 PID20     rmap to pset:ffff88207ff5b980 set_idx: 51 nr_lru:63\n[37722.332349] CPU10 PID20     victim dumped because: PCACHE_BUG_ON_VICTIM(victim_ref_count(v) == 0)\n[37722.350673] WARNING: CPU: 10 PID: 20 at ./include/processor/pcache_victim.h:127 __victim_flush_func+0x232/0x250\n[37722.363568] CPU: 10 PID: 20 Comm: kvictim_flushd 4.0.0-lego+ #435\n[37722.534003] [ ffffffff8103e152 ] __victim_flush_func+0x232/0x250\n[37722.547577] [ ffffffff8103e1d9 ] victim_flush_async+0x69/0xb0\n[37722.553975] [ ffffffff81022ec1 ] kthread+0x111/0x130\n[37722.565900] [ ffffffff8100eb32 ] ret_from_fork+0x22/0x30", 
            "title": "rq-&gt;lock deadlock"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-29", 
            "text": "The only thing left about core_IB is: ib_sa_query, which will be invoked when there is a mlx4 interrupts.  Not sure if this is important.  Anyway. Testing TF 4 threads MNIST again.  When I enable SEQ_IBAPI\uff1a   0829-w14-11 (0829-w09-11) succeed  0829-w14-12: P side seems have deadlock. Let me enable DEBUG_SPINLOCK.   0829-w14-13: SEQ_IBAPI, DEBUG_SPINLOCK, this is a very useful log:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 [    531.495545 ]   STDOUT :   --- [  INFO : tensorflow : loss   =   0.5256375 ,   step   =   101   ( 25.166   sec )  ] ---  [    531.624474 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000064  [    531.633016 ]   IP :   [ ffffffff8103b60e ]   __put_victim_nolist + 0xe / 0xa0  [    531.639803 ]   PGD   0  [    531.642032 ]   Oops :   0002   [ # 1 ]   SMP   PROCESSOR  [    531.646493 ]   CPU :   10   PID :   20   Comm :   kvictim_flushd   4.0.0 - lego +   # 426  [    531.653279 ]   RIP :   0010 : [ ffffffff8103b60e ]    [ ffffffff8103b60e ]   __put_victim_nolist + 0xe / 0xa0  [    531.662781 ]   RSP :   0000 : ffff880fe392fde0    EFLAGS :   00010006  [    531.668696 ]   RAX :   0000000000000000   RBX :   ffffffff810c2b00   RCX :   ffffffff810c2b70  [    531.676646 ]   RDX :   ffffffff810c2b70   RSI :   0000007 aea3f42fa   RDI :   ffffffff810c2b00  [    531.684597 ]   RBP :   ffff880fe392fdf0   R08 :   000000000000001f   R09 :   0000000000000002  [    531.692548 ]   R10 :   00000000 80000000   R11 :   00000000000664 c3   R12 :   ffff88207ff57000  [    531.700498 ]   R13 :   ffffffff810c2b60   R14 :   ffff880a72555000   R15 :   ffffffff810c2b48  [    531.708449 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fca0000 ( 0000 )   knlGS : 0000000000000000  [    531.717466 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    531.723865 ]   CR2 :   0000000000000064   CR3 :   00000000011 b9000   CR4 :   00000000000406 a0  [    531.731816 ]   Stack :  [    531.734046 ]   ffffffff810c2b00   ffff88207ff57000   ffff880fe392fe08   ffffffff8103bbea  [    531.742190 ]   ffffffff810c2b00   ffff880fe392fe48   ffffffff8103c729   00000000 8103 d7c2  [    531.750335 ]   ffff880a72555060   ffff88107ff0fdc8   0000000000000000   ffffffff8103c780  [    531.758479 ]   0000000000000000   ffff880fe392fe60   ffffffff8103c7e6   ffff880fe391c000  [    531.766623 ]   ffff880fe392ff48   ffffffff81022e81   0000000000000000   0000000000000000  [    531.774768 ]   Call   Trace :  [    531.777483 ]   TSK  [    531.779617 ]   [ ffffffff8103bbea ]   __put_victim + 0x4a / 0x50  [    531.785433 ]   [ ffffffff8103c729 ]   __victim_flush_func + 0xb9 / 0x110  [    531.792027 ]   [ ffffffff8103c780 ]   ?   __victim_flush_func + 0x110 / 0x110  [    531.798911 ]   [ ffffffff8103c7e6 ]   victim_flush_async + 0x66 / 0x90  [    531.805310 ]   [ ffffffff81022e81 ]   kthread + 0x111 / 0x130  [    531.810836 ]   [ ffffffff81022d70 ]   ?   __kthread_parkme + 0x70 / 0x70  [    531.817236 ]   [ ffffffff8100eb32 ]   ret_from_fork + 0x22 / 0x30  [    531.823151 ]   EOT      0829-w14-14: this looks like a double free, or concurrent eviction. But if you look into the evict code, we will check the Flushed flag. It means another eviction routine should have skipped this line, and will not pick this line to do eviction. Some other possibilities?    check until 0829-w14-18  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 [   1671.661424 ]   ------------ [   cut   here   ] ------------  [   1671.666378 ]   BUG :   failure   at   managers / processor / pcache / victim . c : 610 / victim_finish_insert () !  [   1671.675591 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1671.680339 ]   CPU :   20   PID :   31   Comm :   python   4.0.0 - lego +   # 426  [   1671.686351 ]   Stack :  [   1671.688581 ]   ffff880fbe76fda0   ffffffff810289b7   ffffffff00000008   ffff880fbe76fdb0  [   1671.696725 ]   ffff880fbe76fd68   ffffff0021475542   ffff88107fd45e00   ffff880fbe753000  [   1671.704870 ]   0000000000000000   0000000000000001   ffff880fbe76f9b0   ffffffff8101b1b7  [   1671.713015 ]   ffff88107fd44980   ffff880fbe76f9d8   ffffffff8101405f   0000000000000000  [   1671.721160 ]   0000000000000001   ffff880ff992a000   0000000000000001   ffff880fbe76f9f0  [   1671.729304 ]   Call   Trace :  [   1671.732019 ]   TSK  [   1671.734153 ]   [ ffffffff810289c3 ]   panic + 0xc2 / 0x10a  [   1671.739388 ]   [ ffffffff8101b1b7 ]   ?   scheduler_tick + 0x57 / 0x60  [   1671.745593 ]   [ ffffffff8101405f ]   ?   generic_smp_call_function_single_interrupt + 0x8f / 0x160  [   1671.754611 ]   [ ffffffff8100339e ]   ?   call_function_interrupt + 0x2e / 0x40  [   1671.761688 ]   [ ffffffff8100e9fa ]   ?   smp__call_function_interrupt + 0x6a / 0x70  [   1671.769251 ]   [ ffffffff8101f4bb ]   ?   debug_spin_lock + 0x1b / 0x50  [   1671.775555 ]   [ ffffffff81075efc ]   ?   fit_internal_poll_sendcq + 0x6c / 0x140  [   1671.782826 ]   [ ffffffff81042039 ]   ?   find_next_bit + 0x19 / 0x20  [   1671.788934 ]   [ ffffffff8101f4bb ]   ?   debug_spin_lock + 0x1b / 0x50  [   1671.795236 ]   [ ffffffff8101dcac ]   ?   task_tick_rt + 0x2c / 0xd0  [   1671.801248 ]   [ ffffffff8101b1b7 ]   ?   scheduler_tick + 0x57 / 0x60  [   1671.807453 ]   [ ffffffff810174d5 ]   ?   tick_handle_periodic + 0x45 / 0x70  [   1671.814240 ]   [ ffffffff81006774 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [   1671.821029 ]   [ ffffffff8100e8aa ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [   1671.828300 ]   [ ffffffff81012bc8 ]   ?   printk + 0x118 / 0x1b0  [   1671.833924 ]   [ ffffffff8103c161 ]   victim_finish_insert + 0x171 / 0x180  [   1671.840711 ]   [ ffffffff8103b2a2 ]   pcache_evict_line + 0xf2 / 0x2e0  [   1671.847110 ]   [ ffffffff81038d7c ]   pcache_alloc + 0x1ac / 0x380  [   1671.853122 ]   [ ffffffff8103a10c ]   ?   pcache_add_rmap + 0x7c / 0x260  [   1671.859521 ]   [ ffffffff810382bb ]   common_do_fill_page + 0x2b / 0x1e0  [   1671.866114 ]   [ ffffffff81038631 ]   pcache_handle_fault + 0x1c1 / 0x620  [   1671.872804 ]   [ ffffffff81037fc0 ]   ?   pcache_meta_to_kva + 0x30 / 0x30  [   1671.879398 ]   [ ffffffff8101006f ]   do_page_fault + 0xaf / 0x1c0  [   1671.885410 ]   [ ffffffff8100dedf ]   page_fault + 0x1f / 0x30      0829-w14-16: we got this by having debug_spinlock, and seq_ibapi. This is interesting and serious. I think our general C code is fine.. Should I go check the assembly part? This is the rq- lock? come on   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 [    683.748135 ]   -------------------   cut   here   -------------------  [    683.754252 ]   Possible   deadlock   happend  [    683.758323 ]   Current   call   stack :  [    683.761815 ]   CPU :   4   PID :   39   Comm :   python   4.0.0 - lego +   # 428  [    683.767728 ]   Stack :  [    683.769959 ]   ffff880fc1c1fc38   ffffffff8101f48c   ffff88107fc45e00   ffff880fc1c1fc60  [    683.778103 ]   ffffffff8101f4e4   ffff88107fc45e00   ffff880fc23fb000   0000000000000000  [    683.786247 ]   ffff880fc1c1fc80   ffffffff8101b18e   ffff88107fc44980   0000000000000004  [    683.794391 ]   ffff880fc1c1fc98   ffffffff810174d5   ffffffff8101dddb   ffff880fc1c1fcc0  [    683.802537 ]   ffffffff81006774   ffff88107fc45e00   00000004 a817c800   000000 9 a8a78c5e7  [    683.810680 ]   Call   Trace :  [    683.813396 ]   TSK  [    683.815528 ]   [ ffffffff8101f498 ]   report_deadlock + 0x58 / 0x60  [    683.821637 ]   [ ffffffff8101f4e4 ]   debug_spin_lock + 0x44 / 0x50  [    683.827745 ]   [ ffffffff8101b18e ]   scheduler_tick + 0x2e / 0x60  [    683.833758 ]   [ ffffffff810174d5 ]   tick_handle_periodic + 0x45 / 0x70  [    683.840351 ]   [ ffffffff8101dddb ]   ?   dequeue_task_rt + 0x1b / 0x180  [    683.846750 ]   [ ffffffff81006774 ]   apic_timer_interrupt + 0x54 / 0x90  [    683.853343 ]   [ ffffffff8100e8aa ]   smp__apic_timer_interrupt + 0x6a / 0x70  [    683.860421 ]   [ ffffffff8101f4d1 ]   ?   debug_spin_lock + 0x31 / 0x50  [    683.866723 ]   [ ffffffff8101b86e ]   try_to_wake_up + 0xce / 0x1f0  [    683.872832 ]   [ ffffffff8101b9e4 ]   wake_up_q + 0x54 / 0xc0  [    683.878358 ]   [ ffffffff81028487 ]   do_futex + 0x407 / 0x620  [    683.883982 ]   [ ffffffff8103a941 ]   ?   pcache_add_rmap + 0xb1 / 0x600  [    683.890381 ]   [ ffffffff8102870c ]   sys_futex + 0x6c / 0x130  [    683.896005 ]   [ ffffffff8100ec66 ]   do_syscall_64 + 0x36 / 0xc0  [    683.901919 ]   [ ffffffff8100db6c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25", 
            "title": "Aug 29"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-27", 
            "text": "There a lot lost CQE cases. This one is about P- M- S. And M lost the CQE for the WQE sent to S.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 0827 - w9 - 5  [    963.304865 ]   watchdog :   worker [ 0 ]   CPU10   stucked  [    963.309712 ]   watchdog :    common_header   [ op = 0x20000000   src_nid : 0 ]  [    963.316210 ]   CPU :   10   PID :   20   Comm :   thpool - worker0   4.0.0 - lego +   # 43  [    963.322899 ]   RIP :   0010 : [ ffffffff8106ad51 ]    [ ffffffff8106ad51 ]   fit_send_reply_with_rdma_write_with_imm + 0x2a1 / 0x3a0  [    963.334632 ]   RSP :   0000 : ffff88103ef3fc20    EFLAGS :   000002 87  [    963.340547 ]   RAX :   00000000ff ffb6d4   RBX :   000000000000000 b   RCX :   0000000000001770  [    963.348498 ]   RDX :   00000000ff ffa70d   RSI :   fffffffffffff039   RDI :   0000000000000000  [    963.356450 ]   RBP :   ffff88103ef3fcc0   R08 :   000000000000001f   R09 :   0000000000000002  [    963.364400 ]   R10 :   00000000 80000000   R11 :   000077ff 80000000   R12 :   0000000000000000  [    963.372352 ]   R13 :   ffff88103ef26738   R14 :   00000000000 b3d54   R15 :   ffff88103ef25008  [    963.380303 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fca0000 ( 0000 )   knlGS : 0000000000000000  [    963.389320 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    963.395720 ]   CR2 :   0000000000000000   CR3 :   000000000116 a000   CR4 :   00000000000406 a0  [    963.403671 ]   Stack :  [    963.405901 ]   00007ff f000b3d54   ffffffff800b3d54   ffff881000000004   ffff88103ef3fc78  [    963.414045 ]   0000000 900000000   ffff881000000000   0000100 800000001   ffff88103d216000  [    963.422191 ]   ffff88103eebae48   800 b3d540000011c   ffffff9b00000246   ffffea0000000001  [    963.430337 ]   000000103 d216000   0000000000010 c00   000000000000011 c   000000000000100 8  [    963.438481 ]   000000000000011 c   000000000000100 8   ffff88103eebae48   ffff88103ef3fd70  [    963.446626 ]   Call   Trace :  [    963.449342 ]   TSK  [    963.451475 ]   [ ffffffff81067c80 ]   ibapi_send_reply_imm + 0x50 / 0xd0  [    963.458068 ]   [ ffffffff8102e953 ]   ?   __storage_read + 0xc3 / 0x120  [    963.464371 ]   [ ffffffff8102e953 ]   __storage_read + 0xc3 / 0x120  [    963.470480 ]   [ ffffffff8102e9bf ]   storage_read + 0xf / 0x50  [    963.476201 ]   [ ffffffff8102eab7 ]   storage_vma_fault + 0xb7 / 0x130  [    963.482600 ]   [ ffffffff8103262f ]   handle_lego_mm_fault + 0x13f / 0x4a0  [    963.489389 ]   [ ffffffff8102ecf4 ]   common_handle_p2m_miss . isra .1 + 0x54 / 0xc0  [    963.496855 ]   [ ffffffff8102edc7 ]   handle_p2m_pcache_miss + 0x67 / 0x2d0  [    963.503739 ]   [ ffffffff8102bf96 ]   thpool_worker_func + 0x296 / 0x3a0  [    963.510332 ]   [ ffffffff8102bd00 ]   ?   handle_bad_request + 0x40 / 0x40  [    963.516926 ]   [ ffffffff81020ca6 ]   kthread + 0xf6 / 0x120  [    963.522357 ]   [ ffffffff81020bb0 ]   ?   __kthread_parkme + 0x70 / 0x70  [    963.528756 ]   [ ffffffff8100e632 ]   ret_from_fork + 0x22 / 0x30    hmm, another on lost CQE happen at P.\nToday is weird, why we happen to have so many lost CQE today?  Think about why CQE is not generated?   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 0827 - w14 - 6  [   1185.835707 ]  *****  *****   Fail   to   to   get   the   CQE   from   send_cq   after   20   seconds !  *****   This   means   the   packet   was   lost   and   something   went   wrong  *****   with   your   NIC ...  *****   connection_id :   7   dest   node :   1  *****  [   1185.856465 ]   IB   Stats :  [   1185.858985 ]       nr_ib_send_reply :              3452  [   1185.864221 ]       nr_bytes_tx :                 506507  [   1185.869456 ]       nr_bytes_rx :                8981004  [   1185.874692 ]   ------------ [   cut   here   ] ------------  [   1185.879829 ]   WARNING :   CPU :   14   PID :   22   at   net / lego / fit_internal . c : 1108   fit_internal_poll_sendcq + 0xe5 / 0x140  [   1185.890399 ]   CPU :   14   PID :   22   Comm :   python   4.0.0 - lego +   # 356  [   1185.896410 ]   Stack :  [   1185.898640 ]   ffff88103c49fb30   ffffffff810126f5   ffff88103cb22000   00000004 a817c800  [   1185.906784 ]   0000010f7139214f   0000000000000007   ffff88103c49fb40   ffffffff810127cf  [   1185.914927 ]   ffff88103c49fbf0   ffffffff810724b5   000000023 cb2c280   ffff88103cb2c1f8  [   1185.923072 ]   00000000000002 86   ffff88103c49fc18   ffff88103cb06000   ffff88103cb2c150  [   1185.931217 ]   000000000000024 b   ffff88108101c7dc   ffff88107fce5d80   ffff88103c46f000  [   1185.939360 ]   Call   Trace :  [   1185.942075 ]   TSK  [   1185.944209 ]   [ ffffffff81012701 ]   __warn . constprop .1 + 0x91 / 0xd0  [   1185.950607 ]   [ ffffffff810127cf ]   warn_slowpath_null + 0xf / 0x20  [   1185.956909 ]   [ ffffffff810724b5 ]   fit_internal_poll_sendcq + 0xe5 / 0x140  [   1185.963987 ]   [ ffffffff81019dd5 ]   ?   scheduler_tick + 0x55 / 0x60  [   1185.970192 ]   [ ffffffff81072662 ]   fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350  [   1185.979791 ]   [ ffffffff810741ff ]   fit_send_reply_with_rdma_write_with_imm + 0x25f / 0x3a0  [   1185.988420 ]   [ ffffffff810368c2 ]   ?   __pcache_do_fill_page + 0xc2 / 0x1d0  [   1185.995401 ]   [ ffffffff810701e9 ]   ibapi_send_reply_timeout + 0x79 / 0x120  [   1186.002479 ]   [ ffffffff810368c2 ]   ?   __pcache_do_fill_page + 0xc2 / 0x1d0  [   1186.009459 ]   [ ffffffff810368c2 ]   __pcache_do_fill_page + 0xc2 / 0x1d0  [   1186.016245 ]   [ ffffffff81036ac4 ]   common_do_fill_page + 0xf4 / 0x1f0  [   1186.022839 ]   [ ffffffff81036d80 ]   pcache_handle_fault + 0x1c0 / 0x610  [   1186.029528 ]   [ ffffffff81036800 ]   ?   __pcache_do_zerofill_page + 0x100 / 0x100  [   1186.036995 ]   [ ffffffff8100fdff ]   do_page_fault + 0xaf / 0x1c0  [   1186.043005 ]   [ ffffffff8100dc1f ]   page_fault + 0x1f / 0x30", 
            "title": "Aug 27"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-26", 
            "text": "Oh well. I saw the same damn lost packet issue again. The issue can be desribed as: P use lite rpc to send a request to M. M processed the handled, and called rpc reply to sent back to P. M need to poll send_cq to poll completion. But M fail to get the CQE for the should-be-sent-out WQE.  This is tested with M s  CONFIG_FIT_NOWAIT  optimization, which is basically an optimization that M will not poll cq every time a reply was sent out, instead, do batch polling.  The following stack dump was reported by M side watchdog. It is not necessary mlx4_poll_cq s issue, since there is a while (1) loop at fit code. Oh well.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42 Log   name :   0826 - w9 - 1  [ 187736.669027 ]   watchdog :   worker [ 0 ]   CPU10   stucked  [ 187736.673972 ]   watchdog :    common_header   [ op = 0x30000000   src_nid : 0 ]  [ 187736.680566 ]   CPU :   10   PID :   20   Comm :   thpool - worker0   4.0.0 - lego +   # 26  [ 187736.687351 ]   RIP :   0010 : [ ffffffff810522c3 ]    [ ffffffff810522c3 ]   mlx4_ib_poll_cq + 0x1d3 / 0x850  [ 187736.696854 ]   RSP :   0000 : ffff88103ef3f750    EFLAGS :   000002 86  [ 187736.702865 ]   RAX :   00000000ff fffff5   RBX :   0000000000000000   RCX :   ffff88103ed6b050  [ 187736.710913 ]   RDX :   00000000 80630000   RSI :   0000000000000001   RDI :   ffff88103edb0bf0  [ 187736.718961 ]   RBP :   ffff88103ef3f7b8   R08 :   0000000000000020   R09 :   0000000000000002  [ 187736.727007 ]   R10 :   0000000ff c53fddc   R11 :   0000000040 bf1040   R12 :   ffff88103ef3f7c8  [ 187736.735055 ]   R13 :   0000000000000000   R14 :   0000000000000000   R15 :   ffff88103edb0bf0  [ 187736.743104 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fca0000 ( 0000 )   knlGS : 0000000000000000  [ 187736.752218 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [ 187736.758714 ]   CR2 :   0000000000000000   CR3 :   000000000116 a000   CR4 :   00000000000406 a0  [ 187736.766762 ]   Stack :  [ 187736.769089 ]   0000000ff c53fddc   0000000000000002   0000000000000020   ffff88103edb0c98  [ 187736.777331 ]   00000000000002 86   00000000 80630000   ffff88103ef3f7d0   000063 8000000018  [ 187736.785572 ]   ffff88103edb0bf0   0000000000000001   ffff88103ef25008   0000000000000003  [ 187736.793813 ]   000000000000000 c   ffff88103ef3fd30   ffffffff8106920c   ffff88103ef3fd54  [ 187736.802054 ]   0000000100000000   0000000100000000   ffff88103edb07b0   ffff88103e81b008  [ 187736.810296 ]   Call   Trace :  [ 187736.813108 ]   TSK  [ 187736.815338 ]   [ ffffffff8106920c ]   fit_internal_poll_sendcq + 0x6c / 0xe0  [ 187736.822416 ]   [ ffffffff8106ab2f ]   ?   fit_send_reply_with_rdma_write_with_imm + 0x25f / 0x3a0  [ 187736.831336 ]   [ ffffffff81033ff0 ]   ?   _lego_copy_to_user + 0x110 / 0x250  [ 187736.838220 ]   [ ffffffff81028d65 ]   ?   __free_pages + 0x25 / 0x30  [ 187736.844329 ]   [ ffffffff8102e981 ]   ?   __storage_read + 0xf1 / 0x120  [ 187736.850728 ]   [ ffffffff81019865 ]   ?   scheduler_tick + 0x55 / 0x60  [ 187736.857031 ]   [ ffffffff810693d2 ]   ?   fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350  [ 187736.866920 ]   [ ffffffff810693d2 ]   ?   fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350  [ 187736.876810 ]   [ ffffffff8103043f ]   ?   __vma_adjust + 0x38f / 0x550  [ 187736.883113 ]   [ ffffffff81030944 ]   ?   vma_merge + 0x1a4 / 0x280  [ 187736.889123 ]   [ ffffffff81030f20 ]   ?   arch_get_unmapped_area_topdown + 0xe0 / 0x220  [ 187736.897075 ]   [ ffffffff810693d2 ]   fit_send_message_with_rdma_write_with_imm_request + 0x152 / 0x350  [ 187736.906771 ]   [ ffffffff81069ab5 ]   fit_ack_reply_callback + 0x185 / 0x1e0  [ 187736.913848 ]   [ ffffffff8102f129 ]   ?   handle_p2m_flush_one + 0x69 / 0x160  [ 187736.920830 ]   [ ffffffff8102bde0 ]   thpool_worker_func + 0xe0 / 0x3a0  [ 187736.927424 ]   [ ffffffff8102bd00 ]   ?   handle_bad_request + 0x40 / 0x40  [ 187736.934113 ]   [ ffffffff81020ca6 ]   kthread + 0xf6 / 0x120  [ 187736.939639 ]   [ ffffffff81020bb0 ]   ?   __kthread_parkme + 0x70 / 0x70  [ 187736.946137 ]   [ ffffffff8100e632 ]   ret_from_fork + 0x22 / 0x30", 
            "title": "Aug 26"
        }, 
        {
            "location": "/lego/log/log-08-2018/#aug-22", 
            "text": "Damn it!!! After so much effort verifying we had a solid IB stack, we still has memory corruption and deadlock issues. Fuck!  One thing at a time, simple stuff first. Okay, tomorrow first add DEBUG_SPINLOCK to detect possible deadlocks. This, could help to identify some buggy code. After this, I will spend some time looking into the LITE, it s fucking HEAVY. I do found a lot issues during summer.  Personally, I m not feeling good this days. I treat someone with love and respect, but there is not too much in return. Yeahyeahyeah, I know how this works. It s just sad that sometimes you just have a BAD timing. I ve went through too much things in 2018, good and bad. I care sooo much about the people I love, family and others. I feel this is good, of course. Anyway, it is supposed to be a Lego dump, that no one probably interested in.", 
            "title": "Aug 22"
        }, 
        {
            "location": "/lego/log/log-04-2018/", 
            "text": "April 2018\n\n\n05/04 Fri\n\n\nWe made it. We\nve done our part, now, it depends on reviewers. Please, be mercy, our hardworking deserves something good.\n\n\n04/29 Sun\n\n\nRolling.\n\n\n04/26 Thus\n\n\nFix the victim pte_same issue in SMP race cases. SMP is really pain in the ass, how many times? But\n another victim ref count bug show up in SMP.\nFirst log in 0426-w15-\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n0426\n-\nw15\n-\n1\n/\n3\n\n\n\n[\n  \n206.381646\n]\n \nCPU12\n \nPID28\n  \nvictim\n:\nffff88207ff69120\n \nindex\n:\n4\n \nrefcount\n:\n0\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\n0x2e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff72000\n\n\n[\n  \n206.416658\n]\n \nCPU12\n \nPID28\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffd0000000\n\n\n[\n  \n206.433431\n]\n \nCPU12\n \nPID28\n  \nvictim\n:\nffff88207ff69120\n \nindex\n:\n4\n \nrefcount\n:\n0\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\n0x4e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff72000\n\n\n[\n  \n206.468429\n]\n \nCPU12\n \nPID28\n     \nrmap\n \nto\n \npset\n:\nffff88207ff72000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n  \n206.484425\n]\n \nCPU12\n \nPID28\n     \nvictim\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_VICTIM\n(\n!\nVictimAllocated\n(\nv\n)\n \n||\n \n!\nVictimUsable\n(\nv\n)\n \n||\n \n!\nVictimFlushed\n(\nv\n)\n \n||\n \nVictimWriteback\n(\nv\n)\n \n||\n \nVictimLocked\n(\nv\n))\n\n\n[\n  \n206.543952\n]\n \nCPU\n:\n \n12\n \nPID\n:\n \n28\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n274\n\n\n[\n  \n206.521849\n]\n \nWARNING\n:\n \nCPU\n:\n \n12\n \nPID\n:\n \n28\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n196\n \n__put_victim_nolist\n+\n0xa5\n/\n0xd0\n\n\n[\n  \n206.722631\n]\n \n[\nffffffff8103b555\n]\n \n__put_victim_nolist\n+\n0xa5\n/\n0xd0\n\n\n[\n  \n206.729127\n]\n \n[\nffffffff8103c419\n]\n \nvictim_try_fill_pcache\n+\n0x2d9\n/\n0x460\n\n\n[\n  \n206.736107\n]\n \n[\nffffffff8103b740\n]\n \n?\n \nvictim_insert_hit_entry\n+\n0x170\n/\n0x170\n\n\n[\n  \n206.743378\n]\n \n[\nffffffff810371ea\n]\n \npcache_handle_fault\n+\n0x18a\n/\n0x750\n\n\n\n[\n  \n206.399206\n]\n \nCPU8\n \nPID19\n  \nvictim\n:\nffff88207ff69120\n \nindex\n:\n4\n \nrefcount\n:\n0\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\n0x4e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff72000\n\n\n[\n  \n206.425092\n]\n \nCPU8\n \nPID19\n     \nhit\n[\n0\n]\n \nowner\n:\n21\n \nm_nid\n:\n1\n \nrep_nid\n:\n1\n \naddr\n:\n \n0x7fffd0000000\n\n\n[\n  \n206.450977\n]\n \nCPU8\n \nPID19\n  \nvictim\n:\nffff88207ff69120\n \nindex\n:\n4\n \nrefcount\n:\n0\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\n0x4e\n)(\nallocated\n|\nusable\n|\nhasdata\n|\nflushed\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207ff72000\n\n\n[\n  \n206.476475\n]\n \nCPU8\n \nPID19\n     \nrmap\n \nto\n \npset\n:\nffff88207ff72000\n \nset_idx\n:\n \n0\n \nnr_lru\n:\n63\n\n\n[\n  \n206.501779\n]\n \nCPU8\n \nPID19\n     \nvictim\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_VICTIM\n(\nvictim_ref_count\n(\nv\n)\n \n==\n \n0\n)\n\n\n[\n  \n206.549963\n]\n \nCPU\n:\n \n8\n \nPID\n:\n \n19\n \nComm\n:\n \nkvictim_flushd\n \n4.0.0\n-\nlego\n+\n \n#\n274\n\n\n[\n  \n206.532803\n]\n \nWARNING\n:\n \nCPU\n:\n \n8\n \nPID\n:\n \n19\n \nat\n \n.\n/\ninclude\n/\nprocessor\n/\npcache_victim\n.\nh\n:\n119\n \n__victim_flush_func\n+\n0x1e4\n/\n0x1f0\n\n\n\n\n\n\n\n04/25\n\n\nStay humble. Be real.\n\n\n04/22 Sun\n\n\nTesting. Hardworking!\n\n\n04/21 Sat\n\n\nAnother major bug report in 0421-w15-19. Rmapped corrupted. lock issue?\n\n\nFixed. It is handle_m2m_fork bug.\n\n1\npcache_miss_error\n+\n0x20\n\n\n\n\n\n\nKeep it going.\n\n\nI can not remember how many times I have seen this bug issue. And I have no idea.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n[\n  \n714.144354\n]\n \nIP\n:\n \n[\nffffffffffff8100\n]\n \n0xffffffffffff8100\n\n\n[\n  \n714.150171\n]\n \nPGD\n \n115\nc067\n \nPUD\n \n115e067\n \nPMD\n \n0\n\n\n[\n  \n714.154729\n]\n \nOops\n:\n \n0010\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n714.159189\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n15\n \nComm\n:\n \nib_mad_completi\n \n4.0.0\n-\nlego\n+\n \n#\n245\n\n\n[\n  \n714.165976\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \nffffffffffff8100\n\n\n[\n  \n714.173732\n]\n \nIP\n:\n \n[\nffffffffffff8100\n]\n \n0xffffffffffff8100\n\n\n[\n  \n714.179549\n]\n \nPGD\n \n115\nc067\n \nPUD\n \n115e067\n \nPMD\n \n0\n\n\n[\n  \n714.184106\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffffffff8100\n]\n  \n[\nffffffffffff8100\n]\n \n0xffffffffffff8100\n\n\n[\n  \n714.192638\n]\n \nRSP\n:\n \n0000\n:\nffff88103e88fc80\n  \nEFLAGS\n:\n \n00010046\n\n\n[\n  \n714.198552\n]\n \nRAX\n:\n \n6e82000000000098\n \nRBX\n:\n \n7\nb0bffffffffffff\n \nRCX\n:\n \n0000000000000001\n\n\n[\n  \n714.206503\n]\n \nRDX\n:\n \nffff88103e88fd28\n \nRSI\n:\n \n0000000000000000\n \nRDI\n:\n \n44\nc0ffffffff8116\n\n\n[\n  \n714.214453\n]\n \nRBP\n:\n \nffff88103e88fcd0\n \nR08\n:\n \n000000000000001f\n \nR09\n:\n \nffff88103e8643c0\n\n\n[\n  \n714.222403\n]\n \nR10\n:\n \nffff88103e88fe68\n \nR11\n:\n \n0000000000000001\n \nR12\n:\n \na9670000018d71ba\n\n\n[\n  \n714.230354\n]\n \nR13\n:\n \n0000000000000000\n \nR14\n:\n \nffff88103e85d0f8\n \nR15\n:\n \nffff88103dd58000\n\n\n[\n  \n714.238304\n]\n \nOops\n:\n \n0010\n \n[\n#\n2\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n714.242763\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc00000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n714.251781\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n714.258180\n]\n \nCR2\n:\n \nffffffffffff8100\n \nCR3\n:\n \n000000000115\n9000\n \nCR4\n:\n \n00000000000406\nb0\n\n\n[\n  \n714.266130\n]\n \nCPU\n:\n \n10\n \nPID\n:\n \n20\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n245\n\n\n[\n  \n714.272141\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffffffff8100\n]\n  \n[\nffffffffffff8100\n]\n \n0xffffffffffff8100\n\n\n[\n  \n714.280673\n]\n \nRSP\n:\n \n001\n8\n:\nffff88103dd8fe10\n  \nEFLAGS\n:\n \n00010202\n\n\n[\n  \n714.286588\n]\n \nRAX\n:\n \nffff88101fa54270\n \nRBX\n:\n \n00000000000\nc92a6\n \nRCX\n:\n \n0000000000000002\n\n\n[\n  \n714.294538\n]\n \nRDX\n:\n \n00000000ff\nffffff\n \nRSI\n:\n \n0000000000000000\n \nRDI\n:\n \n44\nc0ffffffff8116\n\n\n[\n  \n714.302488\n]\n \nRBP\n:\n \nffff88103dd8fe20\n \nR08\n:\n \nffff88101fa6f000\n \nR09\n:\n \nffff88101fa54400\n\n\n[\n  \n714.310439\n]\n \nR10\n:\n \nffff880000000000\n \nR11\n:\n \n00000000407e9\nc00\n \nR12\n:\n \nffff88101fa54000\n\n\n[\n  \n714.318389\n]\n \nR13\n:\n \nffff88103dd68000\n \nR14\n:\n \nffff88101fa60000\n \nR15\n:\n \nffff88101fa54000\n\n\n[\n  \n714.326339\n]\n \nStack\n:\n\n\n[\n  \n714.328569\n]\n \nFS\n:\n  \n00007ff\nff7fdf740\n(\n0000\n)\n \nGS\n:\nffff88107fca0000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n714.337585\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n714.343984\n]\n \nCR2\n:\n \nffffffffffff8100\n \nCR3\n:\n \n000000103\ndd9a000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n  \n714.351936\n]\n \nStack\n:\n\n\n[\n  \n714.354165\n]\n \nffffffff810157f9\n \n00000000003\nd0f00\n \nffff88103dd8fec0\n \nffffffff8101dde5\n\n\n[\n  \n714.362309\n]\n \nffff88103dd8fe68\n \nffffffff81036788\n \n000000000000003\n8\n \n000000000000003\n8\n\n\n[\n  \n714.370453\n]\n \n00007ff\nfd89a79c0\n \nffff88101fa541c0\n \nffff88101fa54188\n \n0000000000000000\n\n\n[\n  \n714.378598\n]\n \n000000101f\na60000\n \n00007ff\nfd89a79d0\n \n00007ff\nfd89a7700\n \n0000000000000000\n\n\n[\n  \n714.386742\n]\n \n00007ff\nfd89a6fb0\n \nffff88103dd8ff58\n \n000000000000003\n8\n \n00000000003\nd0f00\n\n\n[\n  \n714.394886\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n714.397600\n]\n \nffffffff81014f37\n \n00000000000000\n86\n \nffff88107fc05d80\n \nffff88103e864000\n\n\n[\n  \n714.405745\n]\n \n0000000000000000\n \nffff88107fc04980\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n714.413889\n]\n \nffff88103e85d0f8\n \nffff88103dd58000\n \nffff88103e88fce8\n \nffffffff81016bb7\n\n\n[\n  \n714.422034\n]\n \n000000007f\nc05d80\n \nffff88103e88fd10\n \nffffffff81006754\n \nffffffffffff0000\n\n\n[\n  \n714.430177\n]\n \nffff88107fc05d80\n \nffff88103e864000\n \nffff88103e88fe00\n \nffffffff8100e4ea\n\n\n[\n  \n714.438321\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n714.441037\n]\n \nTSK\n\n\n[\n  \n714.443169\n]\n \n[\nffffffff810157f9\n]\n \n?\n \nktime_get\n+\n0x19\n/\n0x60\n\n\n[\n  \n714.448890\n]\n \n[\nffffffff8101dde5\n]\n \ncopy_process\n+\n0x2c5\n/\n0x1170\n\n\n[\n  \n714.454998\n]\n \n[\nffffffff81036788\n]\n \n?\n \nstrace_printflags\n+\n0x88\n/\n0xc0\n\n\n[\n  \n714.461495\n]\n \nTSK\n\n\n[\n  \n714.463627\n]\n \n[\nffffffff81014f37\n]\n \n?\n \nupdate_wall_time\n+\n0x47\n/\n0x6b0\n\n\n[\n  \n714.470123\n]\n \n[\nffffffff81016bb7\n]\n \ntick_handle_periodic\n+\n0x67\n/\n0x70\n\n\n[\n  \n714.476716\n]\n \n[\nffffffff81006754\n]\n \napic_timer_interrupt\n+\n0x55\n/\n0x90\n\n\n[\n  \n714.483309\n]\n \n[\nffffffff8101ecb6\n]\n \ndo_fork\n+\n0x26\n/\n0x160\n\n\n[\n  \n714.488738\n]\n \n[\nffffffff8101eea9\n]\n \nsys_clone\n+\n0x29\n/\n0x30\n\n\n[\n  \n714.494265\n]\n \n[\nffffffff8100e8ad\n]\n \ndo_syscall_64\n+\n0x3d\n/\n0xd0\n\n\n[\n  \n714.500180\n]\n \n[\nffffffff8100d7ac\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n  \n714.507257\n]\n \n[\nffffffff8100e4ea\n]\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n714.514335\n]\n \nEOT\n\n\n\n\n\n\n04/20 Fri\n\n\nGlad TF finally working now!\n\n\nKeep seeing this message from kernel. It have been many many times. Very deterministic.\n\n1\nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \nffffffffffff8100\n\n\n\n\n\n\n04/19 Thur\n\n\n\n\nPatched clflush to use tgid, n_nid directly without task_struct.\n\n\n\n\nIn a 256M excache today (0419-w15-4), a timeout happen first, which will be handled as segfault to kill all threads. In an eviction-\nvictim_prepare_hits, the get_memory_nodes() encounter the NULL again. Looks like the thread_group-\nmm got cleared before.\n\n\n\n\n\n\n04/18 Wed\n\n\n\n\nTry best to fix the pipe bug. (I found it by using my old way of debugging. By writing a function that test if PTE is corrupted or not. I put that function around the sycall enter/exit. So it help to find which syscall corrupt memory. I have used this stupid technique to find so many hard-to-find memory corruption bugs.....)\n\n\ndo_close_on_exec\n\n\ndup2\n\n\n\n\n\n\n\n\nRe-read Yutong\ns patch again. It touches a lot handler code. This has to be verified before using any nowait reply.\n\n\n\n\n\n\npipe\ns wakeup may have issue?\n\n\n\n\n\n\n0418-w15-41. 39sec\n\n\n\n\n\n\n04/17 Tue\n\n\nChecking list:\n\n\n\n\n-pcache: ibapi use va or pa, does it matter?-\n\n\nNo, I change it to use the VA. Then we don\nt have the need to use PA reply any more.\n\n\n\n\n\n\n=ib_mad, does it really corrupt Memory=\n\n\nStill not sure. Should be something come from the \nib_poll_cq\n.\n\n\n\n\n\n\nM side per PTE lock, check if the lock is really the same lock!\n\n\n-Mail I20. Check CPT.-\n\n\nDist-VMA\n\n\nFirst make sure, TF+no-dist-vma work on my own setting. Though sometimes random bug happen (I doubt it is IB).\n\n\nThen turn on dist-vma\n\n\nw/wo zerofill\n\n\nw/wo kfree\n\n\nw/wo all-zero Debug.\n\n\nw/wo M side per PTE lock\n\n\n\n\n\n\n\n\n\n\nChange most handlers to use TX buffer. Reduce the random mismatched reply case.\n\n\nP side watchdog patch: what to print\n\n\n\n\n-It looks like it is more easier to have bug when I turn on those debug counter printing. I probably should check those buffer mgmt. All next test have zerofill:-\n\n\n\n\nw  print\n\n\nF 0417-w15-2(rmap_walk list_for_each_entrry #GP)\n\n\nF 0417-w15-3(pcache_copy_page_range corrupted PTE)\n\n\nF 0417-w15-4(fit_poll_cq+0x39 ib_poll_cq() \n)\n\n\nF 0417-w15-5(pcache_copy_page_range corrupted PTE)\n\n\n\n\n\n\nwo strace exit:\n\n\nS 0417-w15-6(each 100 step take ~39s/ Linux is ~34s)\n\n\nS 0417-w15-7(filling shuffle data, that works)\n\n\nF 0417-w15-8(pcache_copy_page_range+0x5d1)\n\n\nF 0417-w15-9(rmap_walk+0x47 #GP)\n\n\n\n\n\n\ndisable strace:\n\n\nF 0417-w15-10(pcache_copy_page_range+0x5d1)\n\n\n\n\n\n\nConclusion\n\n\nit has nothing to do with the strace thing.\n\n\nmost of them fail around \nnr_reqs=19103\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the pcache_copy_page_range always happen, after some fork, execve.\n\n\n\n\nw strace (fork, vfork, clone, execve)\n\n\nF 0417-w15-11 (pcache_cp_pg_range). Understand its flow. Back to make sure P side per PTE lock is correct. If it is pcache_cp fault, it always fail at \nnr_reqs=19103\n. And it is: 1) python fork, 2) execve sh.\n\n\nS 0417-w15-12. With global PTE lock. Passed the failed stage above.\n\n\nF 0417-W15-13. With global PTE lock. Failed at pcache_cp. Same place. (Since global PTE lock also fail, so it is not the lock issue. Still someone write to wrong memory.)\n\n\nF 0417-w15-14. With global PTE lock. Same place. Found that I printed a misleading debug info. Modified a little bit to print the actual pte content. Hope can get some valid info next round.\n\n\nF 0417-w15-15. Same place. \ncopy\n:\n \naddr\n:\n \n0x7fffdca07000\n,\n \nptecont\n:\n \n0x8800000000000\n. \nzap\n:\n \nptent\n:\n \n0x340\n \naddress\n:\n \n0x7fffdca08000\n.\n\n\nF 0417-w15-16. Well. BUG in ib_mad_send handler. I add the same checking in ib_mad_receive. This is really just used to catch it. Not fixing it.\n\n\nF 0417-w15-17. Again, \naddr\n:\n \n0x7fffdc207000\n,\n \nptecont\n:\n \n0x8800000000000\n\n\nF 0417-w15-18. \naddr\n:\n \n0x7fffdca07000\n,\n \nptecont\n:\n \n0x8800000000000\n\n\n\n\n\n\nConclusion\n\n\nOnly these two addresses\n\n\naddr: 0x7fffdca07000, ptecont: 0x8800000000000\n\n\npte:ffff88103ea87038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000)\n\n\naddr: 0x7fffdc207000, ptecont: 0x8800000000000\n\n\npte:ffff88103ea97038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000)\n\n\n\n\n\n\n\n\n\n\n\n\nBug found. In pipe_read/write. It somehow corrupted memory. Damn.\n\n\n\n\n\n\n-Another first thing, check this weird log.. :\nHmm, this log should be fine. mad_post is after recv_done_handler. So even if we detect corrupted memory in handler, it has nothing to do with mad_post. The root cause should come from ib_poll_cq, that is where we pass wc to, and where the wc.wr_id was filled in.-\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n[\n \n3850.911144\n]\n \nib_mad_recv_done_handler\n()\n:\n \nc1\n:\n \n2060\n \nc2\n:\n \n12\n \nwc\n-\nwr_id\n:\n \n0xffff88103eea1398\n\n\n[\n \n3850.921881\n]\n \nib_mad_post_receive_mads\n()\n:\n \nc1\n:\n \n2060\n \nc2\n:\n \n13\n \nrecv_wr\n.\nwr_id\n:\n \n0xffff88103eea1008\n \nrecv_queue\n:\n \nffff88103ee42520\n\n\n[\n \n3850.933620\n]\n \nib_mad_completion_handler\n \n2377\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n13\n\n\n[\n \n3850.942346\n]\n \nib_mad_completion_handler\n \n2383\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n14\n\n\n[\n \n3850.951266\n]\n \nib_mad_recv_done_handler\n()\n:\n \nc1\n:\n \n2061\n \nc2\n:\n \n13\n \nwc\n-\nwr_id\n:\n \n0xffff88103eea1560\n\n\n[\n \n3850.961999\n]\n \nib_mad_post_receive_mads\n()\n:\n \nc1\n:\n \n2061\n \nc2\n:\n \n14\n \nrecv_wr\n.\nwr_id\n:\n \n0xffff88103eea11d0\n \nrecv_queue\n:\n \nffff88103ee42520\n\n\n[\n \n3850.973737\n]\n \nib_mad_completion_handler\n \n2377\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n14\n\n\n[\n \n3851.257563\n]\n \nib_mad_completion_handler\n \n2383\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n15\n\n\n[\n \n3851.266295\n]\n \nib_mad_recv_done_handler\n()\n:\n \nc1\n:\n \n2062\n \nc2\n:\n \n14\n \nwc\n-\nwr_id\n:\n \n0xffff88103eea1728\n\n\n[\n \n3851.277029\n]\n \nib_mad_post_receive_mads\n()\n:\n \nc1\n:\n \n2062\n \nc2\n:\n \n15\n \nrecv_wr\n.\nwr_id\n:\n \n0xffff88103eea1398\n \nrecv_queue\n:\n \nffff88103ee42520\n\n\n[\n \n3851.288767\n]\n \nib_mad_completion_handler\n \n2377\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n15\n\n\n[\n \n3851.297493\n]\n \nib_mad_completion_handler\n \n2383\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n16\n\n\n[\n \n3851.306413\n]\n \nib_mad_recv_done_handler\n()\n:\n \nc1\n:\n \n2063\n \nc2\n:\n \n15\n \nwc\n-\nwr_id\n:\n \n0xffff88103eea18f0\n\n\n[\n \n3851.317147\n]\n \nib_mad_post_receive_mads\n()\n:\n \nc1\n:\n \n2063\n \nc2\n:\n \n16\n \nrecv_wr\n.\nwr_id\n:\n \n0xffff88103eea1560\n \nrecv_queue\n:\n \nffff88103ee42520\n\n\n[\n \n3851.328886\n]\n \nib_mad_completion_handler\n \n2377\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n16\n\n\n[\n \n3851.903180\n]\n \nib_mad_completion_handler\n \n2383\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n17\n\n\n[\n \n3851.911913\n]\n \nib_mad_recv_done_handler\n()\n:\n \nc1\n:\n \n2064\n \nc2\n:\n \n16\n \nwc\n-\nwr_id\n:\n \n0xffff88103eea1ab8\n\n\n[\n \n3851.922646\n]\n \nib_mad_post_receive_mads\n()\n:\n \nc1\n:\n \n2064\n \nc2\n:\n \n17\n \nrecv_wr\n.\nwr_id\n:\n \n0xffff88103eea1728\n \nrecv_queue\n:\n \nffff88103ee42520\n\n\n[\n \n3851.934384\n]\n \nib_mad_completion_handler\n \n2377\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n17\n\n\n[\n \n3851.943110\n]\n \nib_mad_completion_handler\n \n2383\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n18\n\n\n[\n \n3851.952030\n]\n \nib_mad_recv_done_handler\n()\n:\n \nc1\n:\n \n2065\n \nc2\n:\n \n17\n \nwc\n-\nwr_id\n:\n \n0xffff88103eea1c80\n\n\n[\n \n3851.962764\n]\n \nib_mad_post_receive_mads\n()\n:\n \nc1\n:\n \n2065\n \nc2\n:\n \n18\n \nrecv_wr\n.\nwr_id\n:\n \n0xffff88103eea18f0\n \nrecv_queue\n:\n \nffff88103ee42520\n\n\n[\n \n3851.974502\n]\n \nib_mad_completion_handler\n \n2377\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n18\n\n\n[\n \n3864.723128\n]\n \n***\n  \nFIT\n \nlayer\n \nready\n \nto\n \ngo\n!\n\n\n[\n \n3864.727206\n]\n \n***\n\n\n[\n \n3867.339488\n]\n \nProcessor\n \nLLC\n \nConfigurations\n:\n\n\n[\n \n3867.343760\n]\n     \nPhysStart\n:\n         \n0x100000000\n\n\n[\n \n3867.348705\n]\n     \nVirtStart\n:\n         \n0xffff880100000000\n\n\n[\n \n3867.354329\n]\n     \nRegistered\n \nSize\n:\n   \n0x400000000\n\n\n[\n \n3867.359274\n]\n     \nActual\n \nUsed\n \nSize\n:\n  \n0x208000000\n\n\n[\n \n3867.364219\n]\n     \nNR\n \ncachelines\n:\n     \n2097152\n\n\n[\n \n3867.368776\n]\n     \nAssociativity\n:\n     \n8\n\n\n[\n \n3867.372751\n]\n     \nNR\n \nSets\n:\n           \n262144\n\n\n[\n \n3867.377210\n]\n     \nCacheline\n \nsize\n:\n    \n4096\n \nB\n\n\n[\n \n3867.381672\n]\n     \nMetadata\n \nsize\n:\n     \n64\n \nB\n\n\n[\n \n3867.385937\n]\n     \nNR\n \ncacheline\n \nbits\n:\n \n12\n \n[\n \n0\n \n-\n \n11\n]\n \n0x0000000000000fff\n\n\n[\n \n3867.392821\n]\n     \nNR\n \nset\n-\nindex\n \nbits\n:\n \n18\n \n[\n12\n \n-\n \n29\n]\n \n0x000000003ffff000\n\n\n[\n \n3867.399705\n]\n     \nNR\n \ntag\n \nbits\n:\n       \n34\n \n[\n30\n \n-\n \n63\n]\n \n0xffffffffc0000000\n\n\n[\n \n3867.406588\n]\n     \nNR\n \npages\n \nfor\n \ndata\n:\n \n2097152\n\n\n[\n \n3867.411147\n]\n     \nNR\n \npages\n \nfor\n \nmeta\n:\n \n32768\n\n\n[\n \n3867.415509\n]\n     \nCacheline\n \n(\npa\n)\n \nrange\n:\n   \n[\n       \n0x100000000\n \n-\n        \n0x2ffffffff\n]\n\n\n[\n \n3867.423848\n]\n     \nMetadata\n \n(\npa\n)\n \nrange\n:\n    \n[\n       \n0x300000000\n \n-\n        \n0x307ffffff\n]\n\n\n[\n \n3867.432186\n]\n     \nCacheline\n \n(\nva\n)\n \nrange\n:\n   \n[\n0xffff880100000000\n \n-\n \n0xffff8802ffffffff\n]\n\n\n[\n \n3867.440524\n]\n     \nMetadata\n \n(\nva\n)\n \nrange\n:\n    \n[\n  \nffff880300000000\n \n-\n \n0xffff880307ffffff\n]\n\n\n[\n \n3867.448862\n]\n     \npcache_set_map\n(\n064\nB\n)\n:\n   \n[\n  \nffff88207ec00000\n \n-\n \n0xffff88207fbfffff\n]\n\n\n[\n \n3867.457201\n]\n     \nWay\n \ncache\n \nstride\n:\n  \n0x40000000\n\n\n[\n \n3867.462048\n]\n     \nMemmap\n \n$\n \nsemantic\n:\n       \nmemblock\n \nreserved\n\n\n[\n \n3867.468156\n]\n     \nNR\n \nvictim\n \n$\n \nentries\n:\n     \n8\n\n\n[\n \n3867.472725\n]\n \nnewpid\n:\n \n1\n \nhome\n:\n1\n \nreplica\n:\n \n1\n\n\n[\n \n3867.476980\n]\n \np2m_fork\n(\ncpu0\n)\n:\n \nI\n \ncur\n:\n1\n-\nkernel_init\n \nnew\n:\n20\n\n\n[\n \n3867.482718\n]\n \np2m_fork\n(\ncpu0\n)\n:\n \nO\n \nsucceed\n \ncur\n:\n1\n-\nkernel_init\n \nnew\n:\n20\n\n\n[\n \n3867.489197\n]\n \nProcessor\n:\n \nProcessor\n \nmanager\n \nis\n \nrunning\n.\n\n\n[\n \n3867.494724\n]\n \nOnline\n \nCPU\n:\n \n0\n,\n2\n,\n4\n,\n6\n,\n8\n,\n10\n,\n12\n,\n14\n,\n16\n,\n18\n,\n20\n,\n22\n\n\n[\n \n3867.500444\n]\n \nActive\n \nCPU\n:\n \n0\n,\n2\n,\n6\n,\n10\n,\n12\n,\n14\n,\n16\n,\n18\n,\n20\n,\n22\n\n\n[\n \n3867.505777\n]\n   \n[\n0\n]\n \nThread\n[\nkvictim_flushd\n:\n19\n]\n \npinned\n \nat\n \nCPU\n \n8\n\n\n[\n \n3867.511982\n]\n   \n[\n1\n]\n \nThread\n[\nrecvpollcq\n:\n17\n]\n \npinned\n \nat\n \nCPU\n \n4\n\n\n[\n \n3867.539217\n]\n \ndo_close_on_exec\n()\n:\n \nTODO\n,\n \nnot\n \nimplemented\n.\n\n\n[\n \n3867.549209\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nBefore\n \nexecv\n^\nV\n\n\n]\n---\n\n\n[\n \n3867.553870\n]\n \nSTDOUT\n:\n \n---\n[\n\n\n\ne\n\n\n---\n\n\n[\n \n3867.557880\n]\n \nnewpid\n:\n \n20\n \nhome\n:\n1\n \nreplica\n:\n \n1\n\n\n[\n \n3867.562248\n]\n \np2m_fork\n(\ncpu10\n)\n:\n \nI\n \ncur\n:\n20\n-\nexe\n.\no\n \nnew\n:\n21\n\n\n[\n \n3867.567560\n]\n \np2m_fork\n(\ncpu10\n)\n:\n \nO\n \nsucceed\n \ncur\n:\n20\n-\nexe\n.\no\n \nnew\n:\n21\n\n\n[\n \n3867.573670\n]\n \nCPU12\n \nPID21\n \nsys_execve\n\n\n[\n \n3867.578681\n]\n \ndo_close_on_exec\n()\n:\n \nTODO\n,\n \nnot\n \nimplemented\n.\n\n\n[\n \n3867.584215\n]\n \nCPU12\n \nPID21\n \nsys_execve\n \n=\n \n0\n,\n \n0x0\n\n\n[\n \n3867.599867\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \n000000040\n8446080\n\n\n[\n \n3867.607436\n]\n \nIP\n:\n \n[\nffffffff8101bbbf\n]\n \ntask_tick_rt\n+\n0x1f\n/\n0xd0\n\n\n\n\n\n\n\n\n\n\n04/16 Mon\n\n\nMake dist-vma work with TF first. Tough work.\n\n\n0416-w14-7\n: 1) do_wp_page triggered, 2) dealock on per pte lock. This really should not happen. It is single worker. Basically means the page-\nlock is not intialized. Probabaly our per PTE lock implementation is wrong.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n[\n \n5220.250552\n]\n \nhb\n:\n \nworker\n[\n0\n]\n \nCPU\n \n4\n \nstucked\n\n\n[\n \n5220.254819\n]\n \nhb\n:\n  \ncommon_header\n \n[\nop\n=\n0x20000000\n \nsrc_nid\n:\n0\n]\n\n\n[\n \n5220.260734\n]\n \nhb\n:\n  \nmsg\n \n[\npid\n=\n21\n,\ntgid\n=\n21\n,\nflags\n=\n0x51\n,\nvaddr\n=\n0x7fff7b7fdfb8\n]\n\n\n[\n \n5220.267911\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n31\n \nComm\n:\n \nthpool\n-\nworker0\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n237\n\n\n[\n \n5220.274890\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81031aa3\n]\n  \n[\nffffffff81031aa3\n]\n \nhandle_lego_mm_fault\n+\n0x373\n/\n0x4f0\n\n\n\nhandle_lego_mm_fault\n+\n0x373\n/\n0x4ee\n:\n                                                                                                                                                                                   \n\narch_spin_lock\n \nat\n \narch\n/\nx86\n/\ninclude\n/\nasm\n/\nspinlock\n.\nh\n:\n21\n                                                                                                                                                                \n \n(\ninlined\n \nby\n)\n \nspin_lock\n \nat\n \ninclude\n/\nlego\n/\nspinlock\n.\nh\n:\n72\n                                                                                                                                                               \n \n(\ninlined\n \nby\n)\n \ndo_anonymous_page\n \nat\n \nmanagers\n/\nmemory\n/\nvm\n/\nfault\n.\nc\n:\n115\n                                                                                                                                                   \n \n(\ninlined\n \nby\n)\n \nhandle_pte_fault\n \nat\n \nmanagers\n/\nmemory\n/\nvm\n/\nfault\n.\nc\n:\n142\n                                                                                                                                                    \n \n(\ninlined\n \nby\n)\n \nhandle_lego_mm_fault\n \nat\n \nmanagers\n/\nmemory\n/\nvm\n/\nfault\n.\nc\n:\n225\n\n\n\n\n\n\nA IB bug during normal run (P M S TF), this is REALLY weird:\n\n1\n2\n3\n4\n5\n6\n7\n[\n  \n395.259560\n]\n \nCPU12\n \nPID21\n \nsys_execve\n\n\n[\n  \n395.263345\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n00000000000001\na0\n\n\n[\n  \n395.272068\n]\n \nIP\n:\n \n[\nffffffff81064c09\n]\n \nfit_poll_cq\n+\n0x39\n/\n0x530\n\n\n\nfit_poll_cq\n+\n0x39\n/\n0x523\n:\ngit\n:(\ntest_vma\n)]\n \n$\n \n.\n/\nscripts\n/\nfaddr2line\n \nvmImage\n  \nfit_poll_cq\n+\n0x39\n\n\nib_poll_cq\n \nat\n \ninclude\n/\nrdma\n/\nib_verbs\n.\nh\n:\n1614\n\n \n(\ninlined\n \nby\n)\n \nfit_poll_cq\n \nat\n \nnet\n/\nlego\n/\nfit_internal\n.\nc\n:\n1671\n\n\n\n\n\n\nCatch the ib_mad bug once.. and mlx4_error follows. I added more checking to where the mad_queue was assigned.\n\n1\n2\n3\n4\n[\n  \n787.471385\n]\n \nib_mad_completion_handler\n \n2365\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n15\n\n\n[\n  \n787.480124\n]\n \nBUG\n!\n \nmad_list\n:\n \nffff88103eea1728\n \nmad_queue\n:\n           \n(\nnull\n)\n\n\n[\n  \n787.487491\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n787.492630\n]\n \nWARNING\n:\n \nCPU\n:\n \n0\n \nPID\n:\n \n15\n \nat\n \ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n1909\n \nib_mad_completion_handler\n+\n0xa56\n/\n0xab0\n\n\n\n\n\n\n04/15 Sun\n\n\nTrying TF myself.\n\n\nHad a bug report on 0415-w15-5, on fork, execve etc.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n[\n  \n317.436811\n]\n \nnewpid\n:\n \n22\n \nhome\n:\n1\n \nreplica\n:\n \n1\n\n\n[\n  \n317.477701\n]\n \npte\n:\nffff88103e94a038\n \npfn\n:\n0x0\n \nflags\n:()\n\n\n[\n  \n317.482752\n]\n \npte\n \ndumped\n \nbecause\n:\n \ncorrupted\n\n\n[\n  \n317.487213\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n317.492352\n]\n \nWARNING\n:\n \nCPU\n:\n \n14\n \nPID\n:\n \n22\n \nat\n \nmanagers\n/\nprocessor\n/\npgtable\n.\nc\n:\n365\n \npcache_copy_page_range\n+\n0x5d1\n/\n0x6c0\n\n\n[\n  \n317.503213\n]\n \nCPU\n:\n \n14\n \nPID\n:\n \n22\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n93\n\n\n[\n  \n317.552082\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n317.554799\n]\n \nTSK\n\n\n[\n  \n317.556930\n]\n \n[\nffffffff810123a1\n]\n \n__warn\n.\nconstprop\n.0\n+\n0x91\n/\n0xd0\n\n\n[\n  \n317.563330\n]\n \n[\nffffffff8101246f\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n  \n317.569634\n]\n \n[\nffffffff8102d401\n]\n \npcache_copy_page_range\n+\n0x5d1\n/\n0x6c0\n\n\n[\n  \n317.576615\n]\n \n[\nffffffff81037ed7\n]\n \nfork_dup_pcache\n+\n0x27\n/\n0x30\n\n\n[\n  \n317.582723\n]\n \n[\nffffffff8101e514\n]\n \ncopy_process\n+\n0xcf4\n/\n0x1140\n\n\n[\n  \n317.588833\n]\n \n[\nffffffff8101e986\n]\n \ndo_fork\n+\n0x26\n/\n0x160\n\n\n[\n  \n317.594264\n]\n \n[\nffffffff8101eb89\n]\n \nsys_clone\n+\n0x29\n/\n0x30\n\n\n[\n  \n317.599789\n]\n \n[\nffffffff8100e66d\n]\n \ndo_syscall_64\n+\n0x3d\n/\n0xd0\n\n\n[\n  \n317.605705\n]\n \n[\nffffffff8100d56c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n  \n317.612782\n]\n \nEOT\n\n\n[\n  \n317.614917\n]\n \n---\n[\n \nend\n \ntrace\n \n0000000000000000\n \n]\n---\n\n\n[\n  \n317.625561\n]\n \np2m_fork\n(\ncpu14\n)\n:\n \nI\n \ncur\n:\n22\n-\npython\n \nnew\n:\n36\n\n\n[\n  \n330.209312\n]\n \np2m_fork\n(\ncpu14\n)\n:\n \nO\n \nsucceed\n \ncur\n:\n22\n-\npython\n \nnew\n:\n36\n\n\n\n\n[\n  \n330.310909\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n330.315864\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n804\n/\npcache_zap_pte\n()\n!\n\n\n[\n  \n330.324302\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n  \n330.329050\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n36\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n93\n\n\n[\n  \n330.377824\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n330.380540\n]\n \nTSK\n\n\n[\n  \n330.382672\n]\n \n[\nffffffff81026493\n]\n \npanic\n+\n0xc2\n/\n0x105\n\n\n[\n  \n330.387908\n]\n \n[\nffffffff8101bbcc\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n  \n330.393920\n]\n \n[\nffffffff81019245\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n  \n330.400126\n]\n \n[\nffffffff810168f5\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n  \n330.406913\n]\n \n[\nffffffff81006684\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n  \n330.413700\n]\n \n[\nffffffff8100e2aa\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n330.420973\n]\n \n[\nffffffff810125ad\n]\n \n?\n \nprintk\n+\n0x11d\n/\n0x1b0\n\n\n[\n  \n330.426597\n]\n \n[\nffffffff810375bc\n]\n \npcache_zap_pte\n+\n0x14c\n/\n0x190\n\n\n[\n  \n330.432802\n]\n \n[\nffffffff81035db0\n]\n \n?\n \n__pcache_remove_rmap_one\n+\n0x70\n/\n0x70\n\n\n[\n  \n330.439978\n]\n \n[\nffffffff8102cd25\n]\n \nunmap_page_range\n+\n0x325\n/\n0x3f0\n\n\n[\n  \n330.446379\n]\n \n[\nffffffff8102ce0e\n]\n \nrelease_pgtable\n+\n0x1e\n/\n0x40\n\n\n[\n  \n330.452487\n]\n \n[\nffffffff81037ef8\n]\n \npcache_process_exit\n+\n0x18\n/\n0x20\n\n\n[\n  \n330.458984\n]\n \n[\nffffffff8101d3c4\n]\n \nmmput\n+\n0x34\n/\n0xb0\n\n\n[\n  \n330.464123\n]\n \n[\nffffffff8102c38d\n]\n \ndo_execve\n+\n0x42d\n/\n0x760\n\n\n[\n  \n330.469845\n]\n \n[\nffffffff8102c6c9\n]\n \nsys_execve\n+\n0x9\n/\n0x10\n\n\n[\n  \n330.475371\n]\n \n[\nffffffff8100e66d\n]\n \ndo_syscall_64\n+\n0x3d\n/\n0xd0\n\n\n[\n  \n330.481286\n]\n \n[\nffffffff8100d56c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n  \n330.488364\n]\n \nEOT\n\n\n[\n  \n330.490501\n]\n \n---\n[\n \nend\n \nKernel\n \npanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n\n\n\none more\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n[\n  \n369.223161\n]\n \nnewpid\n:\n \n22\n \nhome\n:\n1\n \nreplica\n:\n \n1\n\n\n[\n  \n369.264307\n]\n \npte\n:\nffff88103ea41038\n \n(\n0x0\n)\n \npfn\n:\n0x0\n \nflags\n:()\n\n\n[\n  \n369.269938\n]\n \npte\n \ndumped\n \nbecause\n:\n \ncorrupted\n\n\n[\n  \n369.274399\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n369.279538\n]\n \nWARNING\n:\n \nCPU\n:\n \n14\n \nPID\n:\n \n22\n \nat\n \nmanagers\n/\nprocessor\n/\npgtable\n.\nc\n:\n365\n \npcache_copy_page_range\n+\n0x5d1\n/\n0x6c0\n\n\n[\n  \n369.290398\n]\n \nCPU\n:\n \n14\n \nPID\n:\n \n22\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n+\n \n#\n94\n\n\n[\n  \n369.296310\n]\n \nStack\n:\n\n\n[\n  \n369.341976\n]\n \nTSK\n\n\n[\n  \n369.344107\n]\n \n[\nffffffff810123a1\n]\n \n__warn\n.\nconstprop\n.0\n+\n0x91\n/\n0xd0\n\n\n[\n  \n369.350508\n]\n \n[\nffffffff8101246f\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n  \n369.356809\n]\n \n[\nffffffff8102d401\n]\n \npcache_copy_page_range\n+\n0x5d1\n/\n0x6c0\n\n\n[\n  \n369.363790\n]\n \n[\nffffffff81037f07\n]\n \nfork_dup_pcache\n+\n0x27\n/\n0x30\n\n\n[\n  \n369.369897\n]\n \n[\nffffffff8101e514\n]\n \ncopy_process\n+\n0xcf4\n/\n0x1140\n\n\n[\n  \n369.376006\n]\n \n[\nffffffff8101e986\n]\n \ndo_fork\n+\n0x26\n/\n0x160\n\n\n[\n  \n369.381435\n]\n \n[\nffffffff8101eb89\n]\n \nsys_clone\n+\n0x29\n/\n0x30\n\n\n[\n  \n369.386960\n]\n \n[\nffffffff8100e66d\n]\n \ndo_syscall_64\n+\n0x3d\n/\n0xd0\n\n\n[\n  \n369.392875\n]\n \n[\nffffffff8100d56c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n  \n369.399952\n]\n \nEOT\n\n\n[\n  \n369.402086\n]\n \n---\n[\n \nend\n \ntrace\n \n0000000000000000\n \n]\n---\n\n\n\n[\n  \n369.412750\n]\n \np2m_fork\n(\ncpu14\n)\n:\n \nI\n \ncur\n:\n22\n-\npython\n \nnew\n:\n36\n\n\n[\n  \n369.418215\n]\n \np2m_fork\n(\ncpu14\n)\n:\n \nO\n \nsucceed\n \ncur\n:\n22\n-\npython\n \nnew\n:\n36\n\n\n[\n  \n369.500829\n]\n \nptent\n:\n \n0x340\n \naddress\n:\n \n0x7fffe2408000\n\n\n[\n  \n369.505783\n]\n \npte\n:\nffff88103dbe5040\n \n(\n0x340\n)\n \npfn\n:\n0x0\n \nflags\n:(\ndirty\n|\nglobal\n|\nsoftw1\n)\n\n\n[\n  \n369.513637\n]\n \npte\n \ndumped\n \nbecause\n:\n \ncorrupted\n\n\n[\n  \n369.518095\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n369.523236\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n808\n/\npcache_zap_pte\n()\n!\n\n\n[\n  \n369.531672\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n\n\n\n\n04/14 Sat\n\n\n\n\nCheck if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm. Will they do harm\n\n\nImplement replication\n\n\nAdd IB counter\n\n\n\n\n04/13 Fri\n\n\nPatched M side pgtable to use per PTE/PMD lock. So thpool in M will not be bottlnecked by the page_table_lock.\n\n\nib_mad_recv_done_handler may corrupt memory, again.\n\n\nSomehow, during testing of this patch. Running with MT-Phoenix 1GB, the P side has reported bad pgd entries. I\nm using fork+execve way. The child(phoenix) already exit. This msg is printed when parent exit_mm. The pgd table should either be 0, or valid pud va. Memory corruption happened\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n[\n \n2551.687806\n]\n \nKernel\n \nstrace\n\n\n[\n \n2551.690715\n]\n \nTask\n:\n \n21\n:\n21\n \nnr_accumulated_threads\n:\n \n1\n\n\n[\n \n2551.696327\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n2551.703704\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n2551.712141\n]\n  \n98.63\n   \n66.942660568\n    \n66942661\n         \n1\n         \n0\n \nsys_wait4\n\n\n[\n \n2551.719898\n]\n   \n0.45\n    \n0.457060789\n      \n457061\n         \n1\n         \n0\n \nsys_clone\n\n\n[\n \n2551.727654\n]\n   \n0.20\n    \n0.204320071\n       \n51081\n         \n4\n         \n0\n \nsys_brk\n\n\n[\n \n2551.735216\n]\n   \n0.40\n    \n0.040378189\n       \n40379\n         \n1\n         \n0\n \nsys_mmap\n\n\n[\n \n2551.742876\n]\n   \n0.13\n    \n0.013682424\n        \n4561\n         \n3\n         \n0\n \nsys_write\n\n\n[\n \n2551.750633\n]\n   \n0.10\n    \n0.000001039\n           \n2\n         \n1\n         \n0\n \nsys_newfstat\n\n\n[\n \n2551.758681\n]\n   \n0.88\n    \n0.000000888\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n2551.767114\n]\n   \n0.79\n    \n0.000000792\n           \n1\n         \n2\n         \n0\n \nsys_futex\n\n\n[\n \n2551.774871\n]\n   \n0.77\n    \n0.000000770\n           \n1\n         \n1\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n2551.783501\n]\n   \n0.54\n    \n0.000000548\n           \n1\n         \n1\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n2551.791742\n]\n   \n0.49\n    \n0.000000499\n           \n1\n         \n1\n         \n0\n \nsys_newuname\n\n\n[\n \n2551.799789\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n1\n         \n0\n \nsys_getrlimit\n\n\n[\n \n2551.807933\n]\n   \n0.19\n    \n0.000000195\n           \n1\n         \n1\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n2551.816659\n]\n   \n0.19\n    \n0.000000190\n           \n1\n         \n1\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n2551.825386\n]\n   \n0.18\n    \n0.000000181\n           \n1\n         \n1\n         \n0\n \nsys_ioctl\n\n\n[\n \n2551.833143\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n2551.841577\n]\n \n100.00\n   \n67.658107612\n                    \n22\n         \n0\n \ntotal\n\n\n[\n \n2551.848945\n]\n\n\n[\n \n2551.850591\n]\n\n\n[\n \n2551.852240\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n \n2551.855924\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n \n2551.865621\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n \n2551.875317\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000204992\n                \n58\n              \n3535\n\n\n[\n \n2551.885014\n]\n     \noff\n     \n__do_kmalloc_node\n       \n0.300783843\n            \n281501\n              \n1069\n\n\n[\n \n2551.894709\n]\n     \noff\n     \n__pcache_zerofill\n       \n0.009844770\n             \n16558\n               \n595\n\n\n[\n \n2551.904404\n]\n     \noff\n           \npcache_miss\n      \n54.414457906\n            \n257869\n            \n211016\n\n\n[\n \n2551.914100\n]\n     \noff\n          \npcache_flush\n       \n0.000000000\n                 \n0\n                 \n0\n\n\n[\n \n2551.923795\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n \n2551.933490\n]\n\n\n[\n \n2552.074985\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956028\n(\nffffffff81146ca0\n)\n\n\n[\n \n2552.084206\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956030\n(\nffff88103e956030\n)\n\n\n[\n \n2552.093611\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956038\n(\nffff88103e956030\n)\n\n\n[\n \n2552.103016\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956048\n(\nffff88103cc48740\n)\n\n\n[\n \n2552.112421\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956050\n(\n00000000000001\nc0\n)\n\n\n[\n \n2552.121825\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956058\n(\nffff88103eea2008\n)\n\n\n[\n \n2552.131230\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956060\n(\nffff88103eea17d8\n)\n\n\n[\n \n2552.140635\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956068\n(\nffff88103ee42520\n)\n\n\n[\n \n2552.150040\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9560e8\n(\n000000103e9560\nf0\n)\n\n\n[\n \n2552.159444\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956118\n(\n010200\n8081018101\n)\n\n\n[\n \n2552.168849\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956120\n(\n3\nc010b0012000000\n)\n\n\n[\n \n2552.178254\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956128\n(\n0000000000001100\n)\n\n\n[\n \n2552.187659\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956138\n(\n00000000ff\nffffff\n)\n\n\n[\n \n2552.197064\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956158\n(\n0307\n8\na2402010101\n)\n\n\n[\n \n2552.206467\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956160\n(\n0307\n8\na2453946600\n)\n\n\n[\n \n2552.215872\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956168\n(\n0307\n8\na2450946600\n)\n\n\n[\n \n2552.225277\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956170\n(\n0310\n800051946600\n)\n\n\n[\n \n2552.234682\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956178\n(\nc902000100000000\n)\n\n\n[\n \n2552.244088\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956198\n(\nbfd0cc054a122000\n)\n\n\n[\n \n2552.253492\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561a0\n(\n0000000000\n98\nb9c8\n)\n\n\n[\n \n2552.262897\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561a8\n(\nbfe0fe0610914e01\n)\n\n\n[\n \n2552.272302\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561b0\n(\n000000000050f\n2\nc7\n)\n\n\n[\n \n2552.281706\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561b8\n(\nbfd9a30000ec5100\n)\n\n\n[\n \n2552.291111\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561c0\n(\nbffc91d40f20f2c7\n)\n\n\n[\n \n2552.300516\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561c8\n(\n0f\n20\ncd054a20f2c7\n)\n\n\n[\n \n2552.309920\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561d0\n(\n1094\nedcf0f60edcf\n)\n\n\n[\n \n2552.319325\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e9561d8\n(\n0000000000000100\n)\n\n\n[\n \n2552.328730\n]\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\npgtable\n.\nh\n:\n579\n:\n \nbad\n \npgd\n \nffff88103e956218\n(\n0000000000005\naa5\n)\n\n\n[\n \n2552.338151\n]\n \nnr_pgfault\n:\n \n26\n\n\n\n\n\n\nSecond run, saw this invalid pointer deference again! Combined with the above log, I think ib_mad is definitely corrupting memory! I have to take a look.\n\n1\nqp_info = mad_list-\nmad_queue-\nqp_info;\n\n\n\n\n\nPatching the handlers to use tx buffer.\n\n\nPatched.\n\n\nOnce race condition: pcache_handle_miss use the page itself as the reply buffer. Assume later on, it changes to use nowait reply. When the reply is buffered in the queue and has not been sent. Another munmap comes in and invalidate this area, then the page will be freed. The data is invalidate.\n\n\nBut this case seems abnormal. The application will not do so I guess.\n\n\nCheck if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm.\n\n\nTonight task. Think about how to do the VMA replication, how to combine with the $ line replicaiton.\n\n\n04/12 Thur\n\n\nPatched zerofill. All done.\n\n\nTesting new driver fix with Phoenix\n- 1\nst\n run, the mismatch reply is still there. mmap() replied address is different from the one printed. So segfault follows. (0412-w15-4)\n- 2st run, 3st run, succeed.\n\n\n0412-w15-9 0412-w14-9  First time testing phoenix with zerofill (no net). Somehow, P has pcache timeout, but M\ns watchdog show there is no pending requests. This happen once before I remember\n\n\n0412-w15-10. Have not seen this ib mad thing for a long time. Indeed somewhere is wrong.\n\n1\n2\n3\n[  297.794969] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15\n[  297.803706] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020\n[  297.812431] IP: [\nffffffff81058937\n] ib_mad_completion_handler+0xc7/0x810\n\n\n\n\n2\n\n\n04/11 Wed\n\n\nAdding anon first touch opt.\n\n\n0411-p/m-9\n: this log indicate M does not have any unhandled requests, but P side has 1 \n__pcache_fill\n timeout. It seems the message is lost somewhere.\n\n\n0411-p/m-11\n: catch one with the debug msg Yiying added. She says the M side send queue has 2 reqs. But poll does not return any error. Weird.\n\n\nHelp debugging IB issue.\n\n\n04/10 Tue\n\n\nFound. IB stuck. Damn.\n\n1\n2\n3\n4\n5\n6\n7\n8\n[ 2240.294960] RIP: 0010:[\nffffffff8104a6d8\n]  [\nffffffff8104a6d8\n] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2242.694733] RIP: 0010:[\nffffffff8104a6d8\n]  [\nffffffff8104a6d8\n] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2245.094524] RIP: 0010:[\nffffffff8104a6e3\n]  [\nffffffff8104a6e3\n] mlx4_ib_poll_cq+0x383/0x6a0\n[ 2247.494306] RIP: 0010:[\nffffffff8104a6d8\n]  [\nffffffff8104a6d8\n] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2249.894088] RIP: 0010:[\nffffffff8104a6d8\n]  [\nffffffff8104a6d8\n] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2252.293870] RIP: 0010:[\nffffffff8104a6d8\n]  [\nffffffff8104a6d8\n] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2254.693651] RIP: 0010:[\nffffffff8104a6d8\n]  [\nffffffff8104a6d8\n] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2257.093431] RIP: 0010:[\nffffffff8104a6e3\n]  [\nffffffff8104a6e3\n] mlx4_ib_poll_cq+0x383/0x6a0\n\n\n\n\n\n04/09 Mon\n\n\nthpool testing. 4 workers. MT-phoenix:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n[\nroot\n@\nwuklab05\n \nys\n]\n#\n \ncat\n \n040\n9\n-\np\n \n|\n \ngrep\n \n__munmap\n\n\n[\n  \n227.054974\n]\n \nCPU14\n \nPID22\n \nstrace__munmap\n([\n0x7fffb0ba9000\n \n-\n \n0x7fffb4000000\n],\n \n54882304\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.093466\n]\n \nCPU16\n \nPID23\n \nstrace__munmap\n([\n0x7fffab7ff000\n \n-\n \n0x7fffac000000\n],\n \n8392704\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.102773\n]\n \nCPU14\n \nPID22\n \nstrace__munmap\n([\n0x7fffb8000000\n \n-\n \n0x7fffb8ba9000\n],\n \n12226560\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.141265\n]\n \nCPU18\n \nPID24\n \nstrace__munmap\n([\n0x7fffa8000000\n \n-\n \n0x7fffac000000\n],\n \n67108864\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.150669\n]\n \nCPU16\n \nPID23\n \nstrace__munmap\n([\n0x7fffb0000000\n \n-\n \n0x7fffb37ff000\n],\n \n58716160\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.218248\n]\n \nCPU22\n \nPID26\n \nstrace__munmap\n([\n0x7fffa0000000\n \n-\n \n0x7fffa4000000\n],\n \n67108864\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.285826\n]\n \nCPU2\n \nPID28\n \nstrace__munmap\n([\n0x7fff98000000\n \n-\n \n0x7fff9c000000\n],\n \n67108864\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.440567\n]\n \nCPU14\n \nPID31\n \nstrace__munmap\n([\n0x7fff8a7fd000\n \n-\n \n0x7fff8c000000\n],\n \n25178112\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.449972\n]\n \nCPU12\n \nPID30\n \nstrace__munmap\n([\n0x7fff88000000\n \n-\n \n0x7fff8c000000\n],\n \n67108864\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.459376\n]\n \nCPU14\n \nPID31\n \nstrace__munmap\n([\n0x7fff90000000\n \n-\n \n0x7fff927fd000\n],\n \n41930752\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.490109\n]\n \nCPU18\n \nPID33\n \nstrace__munmap\n([\n0x7fff80000000\n \n-\n \n0x7fff84000000\n],\n \n67108864\n)\n \n=\n \n0\n,\n \n0x0\n\n\n[\n  \n227.723140\n]\n \nword_count\n-\npthr\n[\n29\n]\n:\n \nsegfault\n \nat\n \n0x4e842010\n \nip\n \n0000000000420354\n \nsp\n \n00007ff\nfb17f9bc0\n \nerror\n \n6\n\n\n0x4e842010\n\n\n\n\n\n\n\n\n\nPrint mmap on M, if segfault. Printed, the \n0x4e842010\n is never a valid address. thpool makes Memory side SMP. Probably bring some issues.\n\n\n\n\nFound:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nP\n\n\nCPU22\n \nPID26\n \nstrace__mmap\n(\naddr\n=\n0x0\n,\n \nlen\n=\n0xfb000\n,\n \nprot\n(\n0x3\n)\n=\nPROT_READ\n|\nPROT_WRITE\n,\n \nflags\n(\n0x22\n)\n=\nMAP_PRIVATE\n|\nMAP_ANONYMOUS\n,\n \nfd\n=\n18446744073709551615\n(\n \n),\n \noff\n=\n0x0\n)\n \n=\n \n1317351432\n,\n \n0x4e853008\n\n\nword_count\n-\npthr\n[\n26\n]\n:\n \nsegfault\n \nat\n \n0x4e853010\n \nip\n \n0000000000420354\n \nsp\n \n00007ff\nf972a8bc0\n \nerror\n \n6\n\n\n\nM\n\n\n[\n  \n583.120615\n]\n   \n00400000\n-\n004\nd9000\n \nr\n-\nxp\n \n00000000\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\npthread\n\n\n[\n  \n583.131578\n]\n   \n006\nd9000\n-\n006\ndc000\n \nrw\n-\np\n \n000\nd9000\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\npthread\n\n\n[\n  \n583.142729\n]\n   \n006\ndc000\n-\n00755000\n \nrw\n-\np\n \n00000000\n \n[\nheap\n]\n\n\n[\n  \n583.148254\n]\n   \n7ff\nf529c9000\n-\n7ff\nfb93aa000\n \nrw\n-\np\n \n00000000\n\n\n[\n  \n583.153974\n]\n   \n7ff\nfb93aa000\n-\n7ff\nff7fff000\n \nrw\n-\np\n \n00000000\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_1GB\n.\ntxt\n\n\n[\n  \n583.167355\n]\n   \n7ff\nffffde000\n-\n7ff\nffffff000\n \nrw\n-\np\n \n00000000\n \n[\nstack\n]\n\n\n[\n  \n583.173753\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n583.178892\n]\n \nWARNING\n:\n \nCPU\n:\n \n4\n \nPID\n:\n \n31\n \nat\n \nmanagers\n/\nmemory\n/\nhandle_pcache\n/\nfault\n.\nc\n:\n55\n \nhandle_p2m_pcache_miss\n+\n0x18e\n/\n0x1d0\n\n\n[\n  \n583.190430\n]\n \nsrc_nid\n:\n0\n,\npid\n:\n21\n,\nvaddr\n:\n0x4e853010\n\n\n[\n  \n583.195279\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n31\n \nComm\n:\n \nthpool\n-\nworker0\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n90\n\n\n\n\n\n\nConfirmed. I printed added a number to mmap requests. And the compare the results of both P and M. The data is wrong. Btw, I\nm only running 1 worker thread at M, which makes it single thread handling. So, I\nm going to, 1) first use kmalloc to get the reply buffer, and 2) revert back the IB MAX_OUT config, remove the #ifdef COMP_MEMORY. See if it is this patch\ns issue.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nP\n:\n\n\nCPU18\n \nPID24\n \nstrace__mmap\n(\n30\n \n..)\n \n=\n \n-\n1325940736\n,\n \n0x7fffb0f7c000\n\n\nCPU22\n \nPID26\n \nstrace__mmap\n(\n31\n \n..)\n \n=\n \n2144269992\n,\n \n0x7fcef6a8\n\n\n\nM\n:\n\n\n...\n\n\nhandle_p2m_mmap\n()\n:\n \n30\n \n7ff\nfb0f7c000\n\n\nhandle_p2m_mmap\n()\n:\n \n31\n \n7ff\nfb0efe000\n\n\n...\n\n\n\n\n\n\nAnyway, this is \ntemporary\n fixed by using kmalloced reply buffer.\n\n\nSpent whole afternoon and whole night. Finally figure out why timeout happen in P. It is because somewhere in the middle, M has 1 or more requests stucked/unhandled. Deadlock happen in the middle.\n\n\nLike this one. 5 requests queued waiting, 1 is being handled. And that 1 handler stuck. And it is handle_pcache_miss. Now, I need to find out where it stuck!\n\n1\nthpool-worker0 nr_queued: 5 1\n\n\n\n\n\nOh, I really hope we can have some \nsoft/hw lockdep, watchdog stuff\n. This should make out life much much much much much easier!\n\n\n04/08 Sun\n\n\nTrying the fit_nowait patch.\n\n\n\n\nFirst try fit_nowait patch, without any chanegs to other code. See if this patch can work.\n\n\nSecond, modify pcache to use reply_message_nowait. See if this can work. and improve performance.\n\n\nThird, if 2) can improve, perf. Move on to modify thpool patch.\n\n\n\n\n1\nst\n, P fail at ib_mad, during boot:\n\n1\n2\n3\n4\n5\n6\n7\n[\n  \n349.239220\n]\n \nOnline\n \nCPU\n:\n \n0\n,\n2\n,\n4\n,\n6\n,\n8\n,\n10\n,\n12\n,\n14\n,\n16\n,\n18\n,\n20\n,\n22\n\n\n[\n  \n349.244940\n]\n \nActive\n \nCPU\n:\n \n0\n,\n2\n,\n6\n,\n10\n,\n12\n,\n14\n,\n16\n,\n18\n,\n20\n,\n22\n\n\n[\n  \n349.250272\n]\n   \n[\n0\n]\n \nThread\n[\nkvictim_flushd\n:\n19\n]\n \npinned\n \nat\n \nCPU\n \n8\n\n\n[\n  \n349.256478\n]\n   \n[\n1\n]\n \nThread\n[\nrecvpollcq\n:\n17\n]\n \npinned\n \nat\n \nCPU\n \n4\n\n\n[\n  \n356.188819\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n13\n\n\n[\n  \n356.197545\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000020\n\n\n[\n  \n356.206270\n]\n \nIP\n:\n \n[\nffffffff81058287\n]\n \nib_mad_completion_handler\n+\n0xc7\n/\n0x810\n\n\n\n\n\n\n2st run, P side, config MAX_OUT to 1. Then single-thread pheonix with 1GB data finished. But forgot to turn on the profile point. Run one more time.\n\n\n3st run. Same with 2st run setting. But with profile on. Bug shows. Ugh. I still think it is because of ib_mad_handler. It must write to wrong memory locations, and corrupt things randomly.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n[\n  \n456.237913\n]\n \ndo_close_on_exec\n()\n:\n \nTODO\n,\n \nnot\n \nimplemented\n.\n\n\n...\n\n\n[\n  \n456.263274\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \n00000002f\n4\nbfbf58\n\n\n[\n  \n456.270843\n]\n \nIP\n:\n \n[\nffffffff8101bbff\n]\n \ntask_tick_rt\n+\n0x1f\n/\n0xd0\n\n\n[\n  \n456.277048\n]\n \nPGD\n \n0\n\n\n[\n  \n456.279279\n]\n \nThread\n \noverran\n \nstack\n,\n \nor\n \nstack\n \ncorrupted\n\n\n[\n  \n456.284804\n]\n \nOops\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n456.289265\n]\n \nCPU\n:\n \n10\n \nPID\n:\n \n20\n \nComm\n:\n \nkevict_sweepd\n \n4.0.0\n-\nlego\n+\n \n#\n40\n\n\n[\n  \n456.295858\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8101bbff\n]\n  \n[\nffffffff8101bbff\n]\n \ntask_tick_rt\n+\n0x1f\n/\n0xd0\n\n\n\n\n\n\n4st run, succeed. But it looks like the perf is very bad. Oh. but 99% of the pcache miss are file-backed, which will go to storage. So the number is actually doubled.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nWith\n \nfit_nowait\n \npatch\n:\n\n\n[\n  \n308.660051\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n308.663734\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n308.673431\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n308.683128\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000130715\n                \n53\n              \n2467\n\n\n[\n  \n308.692824\n]\n     \noff\n     \n__do_kmalloc_node\n       \n0.097344056\n            \n265647\n               \n367\n\n\n[\n  \n308.702521\n]\n     \noff\n           \npcache_miss\n       \n4.504660891\n            \n258211\n             \n17446\n\n\n[\n  \n308.712218\n]\n     \noff\n          \npcache_flush\n       \n0.000000000\n                 \n0\n                 \n0\n\n\n[\n  \n308.721914\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\n5st run. Just run large malloc test. Looks better than yesterday\ns result. But I\nm using 15 as P today, instead of 13. So, let me try one more time to see if it is the machine.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nWith\n \nfit_nowait\n \npatch\n:\n\n\n[\n  \n674.382592\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n674.386277\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n674.395974\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n674.405670\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000130838\n                \n53\n              \n2469\n\n\n[\n  \n674.415366\n]\n     \noff\n     \n__do_kmalloc_node\n       \n1.604700641\n           \n1584917\n              \n1013\n\n\n[\n  \n674.425062\n]\n     \noff\n           \npcache_miss\n       \n6.467938547\n            \n786571\n              \n8223\n\n\n[\n  \n674.434758\n]\n     \noff\n          \npcache_flush\n       \n3.342783614\n            \n262225\n             \n12748\n\n\n[\n  \n674.444455\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n674.554497\n]\n \nnr_pgfault\n:\n \n786513\n\n\n[\n  \n674.557706\n]\n \nnr_clflush\n:\n \n262225\n\n\n[\n  \n674.561099\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n674.564299\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n674.567887\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n674.571668\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n674.577195\n]\n \nnr_pcache_fill_from_memory\n:\n \n786511\n\n\n[\n  \n674.582139\n]\n \nnr_pcache_fill_from_victim\n:\n \n2\n\n\n\n\n\n\n6st run. Looks like the above fit_nowait can have 400ns improvement. But how come? I did not even change the pcache handling to use ibapi_nowait!!! Maybe random variation. Let me run more.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nWithout\n \nfit_nowait\n \npatches\n\n\n[\n  \n428.546738\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n428.550424\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n428.560119\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n428.569815\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000131140\n                \n53\n              \n2475\n\n\n[\n  \n428.579510\n]\n     \noff\n     \n__do_kmalloc_node\n       \n1.758704197\n           \n1331927\n              \n1321\n\n\n[\n  \n428.589205\n]\n     \noff\n           \npcache_miss\n       \n6.807601189\n            \n786575\n              \n8655\n\n\n[\n  \n428.598899\n]\n     \noff\n          \npcache_flush\n       \n3.699044847\n            \n262227\n             \n14107\n\n\n[\n  \n428.608594\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n428.618289\n]\n\n\n[\n  \n428.718670\n]\n \nnr_pgfault\n:\n \n786515\n\n\n[\n  \n428.721878\n]\n \nnr_clflush\n:\n \n262227\n\n\n[\n  \n428.725272\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n428.728470\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n428.732058\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n428.735840\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n428.741365\n]\n \nnr_pcache_fill_from_memory\n:\n \n786515\n\n\n[\n  \n428.746310\n]\n \nnr_pcache_fill_from_victim\n:\n \n0\n\n\n\n\n\n\n7\nth\n run. without fit_nowait.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nwithout\n \nfit_nowait\n.\n\n\n[\n  \n901.223090\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n901.226775\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n901.236472\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n901.246168\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000130802\n                \n53\n              \n2468\n\n\n[\n  \n901.255865\n]\n     \noff\n     \n__do_kmalloc_node\n       \n1.862575608\n           \n1331923\n              \n1399\n\n\n[\n  \n901.265560\n]\n     \noff\n           \npcache_miss\n       \n6.814540477\n            \n786572\n              \n8664\n\n\n[\n  \n901.275257\n]\n     \noff\n          \npcache_flush\n       \n3.699187003\n            \n262224\n             \n14107\n\n\n[\n  \n901.284953\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\n8\nth\n run. without fit_nowait.\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n321.514564\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n321.518250\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n321.527945\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n321.537639\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000130934\n                \n53\n              \n2471\n\n\n[\n  \n321.547335\n]\n     \noff\n     \n__do_kmalloc_node\n       \n2.216772665\n           \n1331939\n              \n1665\n\n\n[\n  \n321.557031\n]\n     \noff\n           \npcache_miss\n       \n6.806060415\n            \n786573\n              \n8653\n\n\n[\n  \n321.566726\n]\n     \noff\n          \npcache_flush\n       \n3.725455841\n            \n262231\n             \n14207\n\n\n[\n  \n321.576421\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\n9\nth\n run. with fit_nowait\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n374.847912\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n374.851597\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n374.861293\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n374.870989\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000130858\n                \n53\n              \n2470\n\n\n[\n  \n374.880684\n]\n     \noff\n     \n__do_kmalloc_node\n       \n1.485304454\n           \n1331934\n              \n1116\n\n\n[\n  \n374.890381\n]\n     \noff\n           \npcache_miss\n       \n6.615317677\n            \n786582\n              \n8411\n\n\n[\n  \n374.900076\n]\n     \noff\n          \npcache_flush\n       \n3.508328900\n            \n262234\n             \n13379\n\n\n[\n  \n374.909772\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\n10\nth\n run, with fit_nowait\n\n1\n2\n3\n4\n5\n6\n7\n8\n[  225.211058] Kernel Profile Points\n[  225.214743]  status                  name             total                nr            avg.ns\n[  225.224440] -------  --------------------  ----------------  ----------------  ----------------\n[  225.234137]     off      flush_tlb_others       0.000131029                53              2473\n[  225.243833]     off     __do_kmalloc_node       1.211421872           1331984               910  \n[  225.253529]     off           pcache_miss       6.583096125            786574              8370\n[  225.263226]     off          pcache_flush       3.464430818            262227             13212\n[  225.272922] -------  --------------------  ----------------  ----------------  ----------------\n\n\n\n\n\nSum:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nwith fit_nowait:\n\n[  225.253529]     off           pcache_miss       6.583096125            786574              8370\n[  225.263226]     off          pcache_flush       3.464430818            262227             13212\n\n[  374.890381]     off           pcache_miss       6.615317677            786582              8411\n[  374.900076]     off          pcache_flush       3.508328900            262234             13379\n\n[  674.425062]     off           pcache_miss       6.467938547            786571              8223\n[  674.434758]     off          pcache_flush       3.342783614            262225             12748\n\nWithout fit_nowait:\n[  428.589205]     off           pcache_miss       6.807601189            786575              8655\n[  428.598899]     off          pcache_flush       3.699044847            262227             14107\n\n[  901.265560]     off           pcache_miss       6.814540477            786572              8664\n[  901.275257]     off          pcache_flush       3.699187003            262224             14107\n\n[  321.557031]     off           pcache_miss       6.806060415            786573              8653\n[  321.566726]     off          pcache_flush       3.725455841            262231             14207\n\n\n\n\n\n04/07 Sat\n\n\nWell, now we finished all the profiling stuff. Continue on other work.\n\n\nNow I like listening Jazz while coding. Amazing Jazz, really good.\n\n\nOnce again, ib_mad_completion_handler bug will happen. During application run, or even after application exit.\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n465.835447\n]\n \nnr_mremap_pset_diff\n:\n \n0\n\n\n[\n  \n477.086886\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n21\n\n\n[\n  \n477.095620\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000020\n\n\n[\n  \n477.104345\n]\n \nIP\n:\n \n[\nffffffff81058277\n]\n \nib_mad_completion_handler\n+\n0xc7\n/\n0x810\n\n\n\nib_mad_completion_handler\n+\n0xc7\n/\n0x808\n:\n\n\nib_mad_recv_done_handler\n \nat\n \ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n1899\n\n \n(\ninlined\n \nby\n)\n \nib_mad_completion_handler\n \nat\n \ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n2345\n\n\n\n\n\n\nAfter remove net from pcache miss:\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n465.572131\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n465.575815\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n465.585510\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n465.595206\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000000000\n                 \n0\n                 \n0\n\n\n[\n  \n465.604901\n]\n     \noff\n     \n__do_kmalloc_node\n       \n0.656371295\n           \n1762220\n               \n373\n\n\n[\n  \n465.614597\n]\n     \noff\n           \npcache_miss\n       \n7.172572671\n            \n786596\n              \n9119\n\n\n[\n  \n465.624291\n]\n     \noff\n          \npcache_flush\n       \n3.698294960\n            \n262251\n             \n14103\n\n\n[\n  \n465.633987\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\nAfter remove net from pcache flush:\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n684.984000\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n  \n684.987683\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n  \n684.997379\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n  \n685.007074\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000000000\n                 \n0\n                 \n0\n\n\n[\n  \n685.016770\n]\n     \noff\n     \n__do_kmalloc_node\n       \n0.627372836\n           \n1500543\n               \n419\n\n\n[\n  \n685.026464\n]\n     \noff\n           \npcache_miss\n       \n7.128702028\n            \n786596\n              \n9063\n\n\n[\n  \n685.036159\n]\n     \noff\n          \npcache_flush\n       \n3.660772506\n            \n262251\n             \n13960\n\n\n[\n  \n685.045855\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\nmalloc, miss, flush are too slow. Especially the flush, how can it take 13.9us?\n\n\nIt must be our handlers! lego_copy_to_user stuff.\n\n\n04/06 Fri\n\n\nWell.\nNow we have in-kernel strace, in-kernel readprofile. Yummy.\n\n\n04/05 Thur\n\n\nDiscussion with Yilun.\n1. munmap+nr_pgfault figure: count number of pgfaults between munmap, it should be an interesting figure.\n2. track number of pgfault at: since there is no eviction, so any mmaped area at M should only have exactly one pcache fetch.\n3. I probably want to use per-cpu counter.\n\n\nAnyway, continue strace work first. Finished.\n\n\n04/04 Wed\n\n\nSTRACE Performance\n\n\nTF has very bad performance. It is either due to the syscall or pcache. Now I\nm adding facilities to track syscall activities, including average latency, total time.\n\n\nBasic utilities of strace are done. But I somehow need to change the design of multithread strace. Previously, I naively make the thread group keep some info, and let all other threads use that info to do bookkeeping.\n\n\nBut this is really hard and not accurate. We first need to make sure we are running on a non-preemptable kernel, so the per-cpu time tracking will be accurate. Besides, we also need to make sure threads do not migrate because of syscalls such as sched_setaffinity.\n\n\nOh, well, so I though I have to use per-thread strace_info. The first design I thought is: accumulating the counter of one thread to its thread group leader, when it exit. But this is slightly complex, and will affect the thread group leader runtime.\n\n\nSo the second solution I came up is let all threads within a process, chain their straec_info together. And normal thread does not need to accumulate the counter. It can just exit. While the thread group leader exit, it walk through the chain to accumulate the counters. This is simpler. Besides, the strace_info of dead thread is safe. No one will touch it.\n\n\nYeh! Let us do this tomorrow. We will have a robust kernel version strace.\n\n\nSM Heartbeat\n\n\nContinue run some experiments on yesterday\ns case.\n\n\nOne we sure is SM will keep sending requests to HCA. And it looks like it does not send in a very deterministic interval:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n[\n \n1224.034898\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n15\n\n\n[\n \n1224.130616\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n15\n\n\n[\n \n1224.222189\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n16\n\n\n[\n \n1224.417181\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n16\n\n\n\n[\n \n1393.159845\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n17\n\n\n[\n \n1393.255546\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n17\n\n\n[\n \n1393.347132\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n18\n\n\n[\n \n1393.538972\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n18\n\n\n\n[\n \n1449.437542\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n19\n\n\n[\n \n1449.533248\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n19\n\n\n[\n \n1449.624833\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n20\n\n\n[\n \n1449.722512\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n20\n\n\n\n[\n \n4322.423624\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n21\n\n\n[\n \n4322.519328\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n21\n\n\n[\n \n4322.610914\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n22\n\n\n[\n \n4322.708594\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n22\n\n\n[\n \n4350.750574\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n23\n\n\n[\n \n4350.846278\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n23\n\n\n[\n \n4350.937863\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n24\n\n\n[\n \n4351.035543\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n24\n\n\n\n[\n \n4519.690559\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n25\n\n\n[\n \n4519.786262\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n25\n\n\n[\n \n4519.877848\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n26\n\n\n[\n \n4519.975527\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n26\n\n\n\n[\n \n4576.396279\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n27\n\n\n[\n \n4576.491979\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n27\n\n\n[\n \n4576.583565\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n28\n\n\n[\n \n4576.681245\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n28\n\n\n\n[\n \n4942.886820\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n29\n\n\n[\n \n4942.982523\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n29\n\n\n[\n \n4943.074108\n]\n \nib_mad_completion_handler\n \n2344\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n30\n\n\n[\n \n4943.171789\n]\n \nib_mad_completion_handler\n \n2338\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n30\n\n\n\n\n\n\n04/03 Tue\n\n\nBUG BUG BUG\n\n\nFinished basic replication mechanism last night.\n\n\nToday merged several patches. And both Yilun and I think there is something wrong with \nib_mad_completion_handler\n. It seems it will break things behind our back.\n\n\nThis is one bug catched today:\n\n\nib_mad_completion_handler\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nAt\n \nvery\n \nearly\n \nstage\n:\n\n\n\n[\n \n1174.406177\n]\n \nnewpid\n:\n \n20\n \nhome\n:\n1\n \nreplica\n:\n \n1\n\n\n[\n \n1174.452983\n]\n \np2m_fork\n(\ncpu10\n)\n:\n \nI\n \ncur\n:\n20\n-\nexe\n.\no\n \nnew\n:\n21\n\n\n[\n \n1177.462795\n]\n \nib_mad_completion_handler\n \n2324\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n22\n\n\n[\n \n1177.556502\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000020\n\n\n[\n \n1177.650101\n]\n \nIP\n:\n \n[\nffffffff81059104\n]\n \nib_mad_completion_handler\n+\n0xb4\n/\n0x8a0\n\n\n\n.\n/\nscripts\n/\nfaddr2line\n \nvmImage\n  \nib_mad_completion_handler\n+\n0xb4\n\n\nib_mad_completion_handler\n+\n0xb4\n/\n0x899\n:\n\n\nib_mad_recv_done_handler\n \nat\n \ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n1899\n\n \n(\ninlined\n \nby\n)\n \nib_mad_completion_handler\n \nat\n \ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n2325\n\n\n\nib_mad_recv_done_handler\n()\n:\n\n\n1899\n:\n \nqp_info\n \n=\n \nmad_list\n-\nmad_queue\n-\nqp_info\n;\n\n\n\n\n\n\n\nA more scared one after I changed ib_mad_completion_handler. Note that recvcq is the only thread running on cpu4:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n[\n  \n863.887705\n]\n \np2m_fork\n(\ncpu10\n)\n:\n \nI\n \ncur\n:\n20\n-\nexe\n.\no\n \nnew\n:\n21\n\n\n[\n  \n868.478424\n]\n \np2m_fork\n(\ncpu10\n)\n:\n \nO\n \nsucceed\n \ncur\n:\n20\n-\nexe\n.\no\n \nnew\n:\n21\n\n\n[\n  \n868.541991\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n000000000000000\n8\n\n\n[\n  \n868.635569\n]\n \nIP\n:\n \n[\nffffffff810656d4\n]\n \n__schedule\n+\n0x94\n/\n0x1e0\n\n\n[\n  \n868.701090\n]\n \nPGD\n \n0\n\n\n[\n  \n868.725010\n]\n \ngeneral\n \nprotection\n \nfault\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n868.793651\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n17\n \nComm\n:\n \nrecvpollcq\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n737\n\n\n\nSource\n:\n\n\nclear_tsk_need_resched\n(\nprev\n);\n\n\n\n\n\n\nEven this one for Phoenix:\n\n1\n2\n3\n4\n5\n6\n[\n  \n763.442043\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000010\n\n\n[\n  \n763.535636\n]\n \nIP\n:\n \n[\nffffffff81018d6f\n]\n \ntask_curr\n+\n0xf\n/\n0x30\n\n\n[\n  \n763.598035\n]\n \nPGD\n \n103e956067\n \nPUD\n \n103e964067\n \nPMD\n \n0\n\n\n[\n  \n763.653154\n]\n \nOops\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n763.700992\n]\n \nCPU\n:\n \n12\n \nPID\n:\n \n21\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n740\n\n\n[\n  \n763.777950\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81018d6f\n]\n  \n[\nffffffff81018d6f\n]\n \ntask_curr\n+\n0xf\n/\n0x30\n\n\n\n\n\n\nThis NEVER happen before. And this part of code should be correct. We\nve ran a\nlot things.. I doubt if recent IB merge corrupt things.\n\n\nfit_poll_cq\n\n\nAnother one:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n[\n  \n690.401626\n]\n \nstat\n:\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_1GB\n.\ntxt\n\n\n[\n  \n690.507742\n]\n \nSYSC_close\n()\n \nCPU12\n \nPID\n:\n21\n \n[\nfd\n:\n \n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n713.899884\n]\n \nib_mad_completion_handler\n \n2337\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n21\n\n\n[\n  \n713.995606\n]\n \nib_mad_completion_handler\n \n2331\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n21\n\n\n[\n  \n714.087185\n]\n \nib_mad_completion_handler\n \n2337\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n22\n\n\n[\n  \n714.184871\n]\n \nib_mad_completion_handler\n \n2331\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n22\n\n\n[\n  \n742.078102\n]\n \nib_mad_completion_handler\n \n2337\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n23\n\n\n[\n  \n742.173810\n]\n \nib_mad_completion_handler\n \n2331\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n23\n\n\n[\n  \n742.265399\n]\n \nib_mad_completion_handler\n \n2337\n \ngot\n \nsuccessful\n \nrecv\n \ncq\n \nop\n \n128\n \nmad_got_one\n \n24\n\n\n[\n  \n742.363085\n]\n \nib_mad_completion_handler\n \n2331\n \ngot\n \nsuccessful\n \nsend\n \ncq\n \nop\n \n0\n \nmad_got_one\n \n24\n\n\n[\n  \n847.063372\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n21\n\n\n[\n  \n847.116511\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n847.170590\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n7\n \nas\n \n12\n\n\n[\n  \n847.230909\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n847.284988\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n847.339067\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n847.393146\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1832\n\n\n[\n  \n847.457624\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1833\n\n\n[\n  \n847.522103\n]\n \nfit_poll_cq\n:\n \nconnection\n \n7\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n1\n\n\n[\n  \n847.589701\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1834\n\n\n[\n  \n847.654179\n]\n \nfit_poll_cq\n:\n \nconnection\n \n7\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n  \n847.725938\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1835\n\n\n[\n  \n847.790416\n]\n \nfit_poll_cq\n:\n \nconnection\n \n7\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n  \n847.862174\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n847.916252\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n847.970331\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n848.024410\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n848.078490\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1836\n\n\n[\n  \n848.142967\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1837\n\n\n[\n  \n848.207446\n]\n \nfit_poll_cq\n:\n \nconnection\n \n7\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n1\n\n\n[\n  \n848.275044\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1838\n\n\n[\n  \n848.339523\n]\n \nfit_poll_cq\n:\n \nconnection\n \n7\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n  \n848.411281\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1839\n\n\n[\n  \n848.475760\n]\n \nfit_poll_cq\n:\n \nconnection\n \n7\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n  \n848.547517\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n848.601596\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n848.655675\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n848.709753\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n  \n848.763832\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1840\n\n\n\n[\n  \n848.828313\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n           \n(\nnull\n)\n\n\n[\n  \n848.921908\n]\n \nIP\n:\n \n[\nffffffff8106346d\n]\n \nfit_poll_cq\n+\n0x4ad\n/\n0x510\n\n\n[\n  \n848.989507\n]\n \nPGD\n \n0\n\n\n[\n  \n849.013426\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n849.061265\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n17\n \nComm\n:\n \nrecvpollcq\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n744\n\n\n[\n  \n849.131983\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8106346d\n]\n  \n[\nffffffff8106346d\n]\n \nfit_poll_cq\n+\n0x4ad\n/\n0x510\n\n\n[\n  \n849.228700\n]\n \nRSP\n:\n \n0000\n:\nffff88103e813d88\n  \nEFLAGS\n:\n \n00010246\n\n\n[\n  \n849.292139\n]\n \nRAX\n:\n \n000000000000100\n8\n \nRBX\n:\n \nffff88103effbad0\n \nRCX\n:\n \n0000000000000000\n\n\n[\n  \n849.377418\n]\n \nRDX\n:\n \n0000000000000000\n \nRSI\n:\n \nffffffff811d46e0\n \nRDI\n:\n \nffffffff811dbc08\n\n\n[\n  \n849.462695\n]\n \nRBP\n:\n \nffff88103e813ea8\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \n0000000000000000\n\n\n[\n  \n849.547973\n]\n \nR10\n:\n \n0000000000000002\n \nR11\n:\n \n0000000000000004\n \nR12\n:\n \n0000000000000000\n\n\n[\n  \n849.633251\n]\n \nR13\n:\n \nffff88103e801008\n \nR14\n:\n \n0000000000000004\n \nR15\n:\n \nffff88103e813da0\n\n\n[\n  \n849.718529\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc40000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n849.815246\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n849.883884\n]\n \nCR2\n:\n \n0000000000000000\n \nCR3\n:\n \n000000000113\nd000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n  \n849.969163\n]\n \nStack\n:\n\n\n[\n  \n849.993082\n]\n \nffffffff81003299\n \n000001\nb03e813da0\n \n0000000000000004\n \n0000000000000730\n\n\n[\n  \n850.080440\n]\n \n000000\n8100000005\n \n00001008000000f\n9\n \nffff88103eff8c50\n \n002\nc222040000000\n\n\n[\n  \n850.167798\n]\n \n0010004000000002\n \nffff88107fc20000\n \n0000000000000731\n \nffffffff00000005\n\n\n[\n  \n850.255156\n]\n \nffff8810000000f9\n \nffff88103eff8c50\n \n0000000000000000\n \nffff88103e813e38\n\n\n[\n  \n850.342513\n]\n \nffffffff81019854\n \n0000000000000732\n \nffff881000000005\n \nffff8810000000f9\n\n\n[\n  \n850.429871\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n850.458992\n]\n \nTSK\n\n\n[\n  \n850.481870\n]\n \n[\nffffffff81003299\n]\n \n?\n \nnative_smp_send_reschedule\n+\n0x39\n/\n0x50\n\n\n[\n  \n850.560909\n]\n \n[\nffffffff81019854\n]\n \n?\n \ntry_to_wake_up\n+\n0xe4\n/\n0x1f0\n\n\n[\n  \n850.628506\n]\n \n[\nffffffff81065708\n]\n \n?\n \n__schedule\n+\n0xf8\n/\n0x1e0\n\n\n[\n  \n850.691945\n]\n \n[\nffffffff810634d0\n]\n \n?\n \nfit_poll_cq\n+\n0x510\n/\n0x510\n\n\n[\n  \n850.757464\n]\n \n[\nffffffff810634e4\n]\n \nfit_poll_cq_pass\n+\n0x14\n/\n0x30\n\n\n[\n  \n850.824021\n]\n \n[\nffffffff81020636\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n  \n850.882260\n]\n \n[\nffffffff81020540\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n  \n850.950898\n]\n \n[\nffffffff8100e572\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n/* handle normal reply */\n\n\n...\n\n\nmemcpy\n((\nvoid\n \n*\n)\nctx\n-\nreply_ready_indicators\n[\nreply_indicator_index\n],\n \nlength\n,\n \nsizeof\n(\nint\n));\n\n\n...\n\n\n(\nThis\n \nis\n \na\n \nbad\n \nmemcpy\n:\n \nreply_indicator_index\n,\n \nctx\n,\n \netc\n \nshould\n \nbe\n \nchecked\n.)\n\n\n\n\n\n\nIB Spec: QP, CQE, WQE, SEND\n\n\nThe channel adapter detects the WQE posting and accesses the WQE.\nThe channel adapter interprets the command, validates the WQE\u2019s virtual 12\naddresses, translates it to physical addresses, and accesses the data.\nThe outgoing message buffer is split into one or more packets. To each packet the channel adapter adds a transport header (sequence numbers, opcode, etc.). If the destination resides on a remote subnet the channel adapter adds a network header (source \n destination GIDs). The channel adapter then adds the local route header and calculates both the variant\nand invariant checksums.\n\n\nFor a Send operation, the QP retrieves the address of\nthe receive buffer from the next WQE on its receive queue, translates it to physical addresses, and accesses memory writing the data. If this is not\nthe last packet of the message, the QP saves the current write location in 38 its context and waits for the next packet at which time it continues writing\nthe receive buffer until it receives a packet that indicates it is the last packet of the operation. It then updates the receive WQE, retires it, and sends an acknowledge message to the originator.\n\n\nWhen the originator receives an acknowledgment, it creates a CQE on the 5\nCQ and retires the WQE from the send queue.\n\n\nA QP can have multiple outstanding messages at any one time but the 8\ntarget always acknowledges in the order sent, thus WQEs are retired in the order that they are posted.\n\n\n04/02 Mon\n\n\nPatching storage replica handler, able to finish today.\n\n\n04/01 Sun\n\n\nAnyway. Summary of the day: replication at M almost done. Only flush part left. Storage also need a handler. But we still need code to recover.\n\n\nI\nm tired. :-( A month to go.\n\n\nRecord a IB error. Using wuklab12 (P) and wuklab14(M+RAMFS), running usr/pcache_conflic.o:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\nP\n\n\n[\n30801.296160\n]\n \nibapi_send_reply\n()\n \nCPU\n:\n8\n \nPID\n:\n19\n \ntimeout\n \n(\n30010\n \nms\n),\n \ncaller\n:\n \nclflush_one\n+\n0x1c9\n/\n0x370\n\n\n[\n30938.564843\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n21\n\n\n[\n30938.617988\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30938.672068\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n6\n \nas\n \n12\n\n\n[\n30938.732389\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30938.786470\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30938.840551\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30938.894632\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1584\n\n\n[\n30938.959112\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1585\n\n\n[\n30939.023593\n]\n \nfit_poll_cq\n:\n \nconnection\n \n6\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n1\n\n\n[\n30939.091194\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1586\n\n\n[\n30939.155676\n]\n \nfit_poll_cq\n:\n \nconnection\n \n6\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n30939.227436\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1587\n\n\n[\n30939.291917\n]\n \nfit_poll_cq\n:\n \nconnection\n \n6\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n30939.363678\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30939.417759\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30939.471839\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30939.525921\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30939.580002\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1588\n\n\n[\n30939.644483\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n           \n(\nnull\n)\n\n\n[\n30939.738083\n]\n \nIP\n:\n \n[\nffffffff81062fcd\n]\n \nfit_poll_cq\n+\n0x4ad\n/\n0x510\n\n\n[\n30939.805684\n]\n \nPGD\n \n0\n\n\n[\n30939.829604\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n30939.877445\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n17\n \nComm\n:\n \nrecvpollcq\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n715\n\n\n[\n30939.948166\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81062fcd\n]\n  \n[\nffffffff81062fcd\n]\n \nfit_poll_cq\n+\n0x4ad\n/\n0x510\n\n\n\nfit_poll_cq\n \nat\n \nnet\n/\nlego\n/\nfit_internal\n.\nc\n:\n1734\n\n\nmemcpy\n((\nvoid\n \n*\n)\nctx\n-\nreply_ready_indicators\n[\nreply_indicator_index\n],\n \nlength\n,\n \nsizeof\n(\nint\n));\n\n\n\nM\n\n\n[\n30913.642698\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n21\n\n\n[\n30913.695839\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30913.749919\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n1\n \nas\n \n12\n\n\n[\n30913.810236\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30913.864315\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30913.918395\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n30913.972474\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n305\n\n\n[\n30914.035912\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n306", 
            "title": "April 2018"
        }, 
        {
            "location": "/lego/log/log-04-2018/#april-2018", 
            "text": "", 
            "title": "April 2018"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0504-fri", 
            "text": "We made it. We ve done our part, now, it depends on reviewers. Please, be mercy, our hardworking deserves something good.", 
            "title": "05/04 Fri"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0429-sun", 
            "text": "Rolling.", 
            "title": "04/29 Sun"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0426-thus", 
            "text": "Fix the victim pte_same issue in SMP race cases. SMP is really pain in the ass, how many times? But  another victim ref count bug show up in SMP.\nFirst log in 0426-w15-   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 0426 - w15 - 1 / 3  [    206.381646 ]   CPU12   PID28    victim : ffff88207ff69120   index : 4   refcount : 0   nr_fill : 0   locked : 0   flags :( 0x2e )( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207ff72000  [    206.416658 ]   CPU12   PID28       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffd0000000  [    206.433431 ]   CPU12   PID28    victim : ffff88207ff69120   index : 4   refcount : 0   nr_fill : 0   locked : 0   flags :( 0x4e )( allocated | usable | hasdata | flushed )   pcm :            ( null )   pset : ffff88207ff72000  [    206.468429 ]   CPU12   PID28       rmap   to   pset : ffff88207ff72000   set_idx :   0   nr_lru : 63  [    206.484425 ]   CPU12   PID28       victim   dumped   because :   PCACHE_BUG_ON_VICTIM ( ! VictimAllocated ( v )   ||   ! VictimUsable ( v )   ||   ! VictimFlushed ( v )   ||   VictimWriteback ( v )   ||   VictimLocked ( v ))  [    206.543952 ]   CPU :   12   PID :   28   Comm :   python   4.0.0 - lego +   # 274  [    206.521849 ]   WARNING :   CPU :   12   PID :   28   at   managers / processor / pcache / victim . c : 196   __put_victim_nolist + 0xa5 / 0xd0  [    206.722631 ]   [ ffffffff8103b555 ]   __put_victim_nolist + 0xa5 / 0xd0  [    206.729127 ]   [ ffffffff8103c419 ]   victim_try_fill_pcache + 0x2d9 / 0x460  [    206.736107 ]   [ ffffffff8103b740 ]   ?   victim_insert_hit_entry + 0x170 / 0x170  [    206.743378 ]   [ ffffffff810371ea ]   pcache_handle_fault + 0x18a / 0x750  [    206.399206 ]   CPU8   PID19    victim : ffff88207ff69120   index : 4   refcount : 0   nr_fill : 0   locked : 0   flags :( 0x4e )( allocated | usable | hasdata | flushed )   pcm :            ( null )   pset : ffff88207ff72000  [    206.425092 ]   CPU8   PID19       hit [ 0 ]   owner : 21   m_nid : 1   rep_nid : 1   addr :   0x7fffd0000000  [    206.450977 ]   CPU8   PID19    victim : ffff88207ff69120   index : 4   refcount : 0   nr_fill : 0   locked : 0   flags :( 0x4e )( allocated | usable | hasdata | flushed )   pcm :            ( null )   pset : ffff88207ff72000  [    206.476475 ]   CPU8   PID19       rmap   to   pset : ffff88207ff72000   set_idx :   0   nr_lru : 63  [    206.501779 ]   CPU8   PID19       victim   dumped   because :   PCACHE_BUG_ON_VICTIM ( victim_ref_count ( v )   ==   0 )  [    206.549963 ]   CPU :   8   PID :   19   Comm :   kvictim_flushd   4.0.0 - lego +   # 274  [    206.532803 ]   WARNING :   CPU :   8   PID :   19   at   . / include / processor / pcache_victim . h : 119   __victim_flush_func + 0x1e4 / 0x1f0", 
            "title": "04/26 Thus"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0425", 
            "text": "Stay humble. Be real.", 
            "title": "04/25"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0422-sun", 
            "text": "Testing. Hardworking!", 
            "title": "04/22 Sun"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0421-sat", 
            "text": "Another major bug report in 0421-w15-19. Rmapped corrupted. lock issue?  Fixed. It is handle_m2m_fork bug. 1 pcache_miss_error + 0x20    Keep it going.  I can not remember how many times I have seen this bug issue. And I have no idea.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57 [    714.144354 ]   IP :   [ ffffffffffff8100 ]   0xffffffffffff8100  [    714.150171 ]   PGD   115 c067   PUD   115e067   PMD   0  [    714.154729 ]   Oops :   0010   [ # 1 ]   SMP   PROCESSOR  [    714.159189 ]   CPU :   0   PID :   15   Comm :   ib_mad_completi   4.0.0 - lego +   # 245  [    714.165976 ]   BUG :   unable   to   handle   kernel   paging   request   at   ffffffffffff8100  [    714.173732 ]   IP :   [ ffffffffffff8100 ]   0xffffffffffff8100  [    714.179549 ]   PGD   115 c067   PUD   115e067   PMD   0  [    714.184106 ]   RIP :   0010 : [ ffffffffffff8100 ]    [ ffffffffffff8100 ]   0xffffffffffff8100  [    714.192638 ]   RSP :   0000 : ffff88103e88fc80    EFLAGS :   00010046  [    714.198552 ]   RAX :   6e82000000000098   RBX :   7 b0bffffffffffff   RCX :   0000000000000001  [    714.206503 ]   RDX :   ffff88103e88fd28   RSI :   0000000000000000   RDI :   44 c0ffffffff8116  [    714.214453 ]   RBP :   ffff88103e88fcd0   R08 :   000000000000001f   R09 :   ffff88103e8643c0  [    714.222403 ]   R10 :   ffff88103e88fe68   R11 :   0000000000000001   R12 :   a9670000018d71ba  [    714.230354 ]   R13 :   0000000000000000   R14 :   ffff88103e85d0f8   R15 :   ffff88103dd58000  [    714.238304 ]   Oops :   0010   [ # 2 ]   SMP   PROCESSOR  [    714.242763 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc00000 ( 0000 )   knlGS : 0000000000000000  [    714.251781 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    714.258180 ]   CR2 :   ffffffffffff8100   CR3 :   000000000115 9000   CR4 :   00000000000406 b0  [    714.266130 ]   CPU :   10   PID :   20   Comm :   python   4.0.0 - lego +   # 245  [    714.272141 ]   RIP :   0010 : [ ffffffffffff8100 ]    [ ffffffffffff8100 ]   0xffffffffffff8100  [    714.280673 ]   RSP :   001 8 : ffff88103dd8fe10    EFLAGS :   00010202  [    714.286588 ]   RAX :   ffff88101fa54270   RBX :   00000000000 c92a6   RCX :   0000000000000002  [    714.294538 ]   RDX :   00000000ff ffffff   RSI :   0000000000000000   RDI :   44 c0ffffffff8116  [    714.302488 ]   RBP :   ffff88103dd8fe20   R08 :   ffff88101fa6f000   R09 :   ffff88101fa54400  [    714.310439 ]   R10 :   ffff880000000000   R11 :   00000000407e9 c00   R12 :   ffff88101fa54000  [    714.318389 ]   R13 :   ffff88103dd68000   R14 :   ffff88101fa60000   R15 :   ffff88101fa54000  [    714.326339 ]   Stack :  [    714.328569 ]   FS :    00007ff ff7fdf740 ( 0000 )   GS : ffff88107fca0000 ( 0000 )   knlGS : 0000000000000000  [    714.337585 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    714.343984 ]   CR2 :   ffffffffffff8100   CR3 :   000000103 dd9a000   CR4 :   00000000000406 a0  [    714.351936 ]   Stack :  [    714.354165 ]   ffffffff810157f9   00000000003 d0f00   ffff88103dd8fec0   ffffffff8101dde5  [    714.362309 ]   ffff88103dd8fe68   ffffffff81036788   000000000000003 8   000000000000003 8  [    714.370453 ]   00007ff fd89a79c0   ffff88101fa541c0   ffff88101fa54188   0000000000000000  [    714.378598 ]   000000101f a60000   00007ff fd89a79d0   00007ff fd89a7700   0000000000000000  [    714.386742 ]   00007ff fd89a6fb0   ffff88103dd8ff58   000000000000003 8   00000000003 d0f00  [    714.394886 ]   Call   Trace :  [    714.397600 ]   ffffffff81014f37   00000000000000 86   ffff88107fc05d80   ffff88103e864000  [    714.405745 ]   0000000000000000   ffff88107fc04980   0000000000000000   0000000000000000  [    714.413889 ]   ffff88103e85d0f8   ffff88103dd58000   ffff88103e88fce8   ffffffff81016bb7  [    714.422034 ]   000000007f c05d80   ffff88103e88fd10   ffffffff81006754   ffffffffffff0000  [    714.430177 ]   ffff88107fc05d80   ffff88103e864000   ffff88103e88fe00   ffffffff8100e4ea  [    714.438321 ]   Call   Trace :  [    714.441037 ]   TSK  [    714.443169 ]   [ ffffffff810157f9 ]   ?   ktime_get + 0x19 / 0x60  [    714.448890 ]   [ ffffffff8101dde5 ]   copy_process + 0x2c5 / 0x1170  [    714.454998 ]   [ ffffffff81036788 ]   ?   strace_printflags + 0x88 / 0xc0  [    714.461495 ]   TSK  [    714.463627 ]   [ ffffffff81014f37 ]   ?   update_wall_time + 0x47 / 0x6b0  [    714.470123 ]   [ ffffffff81016bb7 ]   tick_handle_periodic + 0x67 / 0x70  [    714.476716 ]   [ ffffffff81006754 ]   apic_timer_interrupt + 0x55 / 0x90  [    714.483309 ]   [ ffffffff8101ecb6 ]   do_fork + 0x26 / 0x160  [    714.488738 ]   [ ffffffff8101eea9 ]   sys_clone + 0x29 / 0x30  [    714.494265 ]   [ ffffffff8100e8ad ]   do_syscall_64 + 0x3d / 0xd0  [    714.500180 ]   [ ffffffff8100d7ac ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [    714.507257 ]   [ ffffffff8100e4ea ]   smp__apic_timer_interrupt + 0x6a / 0x70  [    714.514335 ]   EOT", 
            "title": "04/21 Sat"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0420-fri", 
            "text": "Glad TF finally working now!  Keep seeing this message from kernel. It have been many many times. Very deterministic. 1 BUG :   unable   to   handle   kernel   paging   request   at   ffffffffffff8100", 
            "title": "04/20 Fri"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0419-thur", 
            "text": "Patched clflush to use tgid, n_nid directly without task_struct.", 
            "title": "04/19 Thur"
        }, 
        {
            "location": "/lego/log/log-04-2018/#in-a-256m-excache-today-0419-w15-4-a-timeout-happen-first-which-will-be-handled-as-segfault-to-kill-all-threads-in-an-eviction-victim_prepare_hits-the-get_memory_nodes-encounter-the-null-again-looks-like-the-thread_group-mm-got-cleared-before", 
            "text": "", 
            "title": "In a 256M excache today (0419-w15-4), a timeout happen first, which will be handled as segfault to kill all threads. In an eviction-&gt;victim_prepare_hits, the get_memory_nodes() encounter the NULL again. Looks like the thread_group-&gt;mm got cleared before."
        }, 
        {
            "location": "/lego/log/log-04-2018/#0418-wed", 
            "text": "Try best to fix the pipe bug. (I found it by using my old way of debugging. By writing a function that test if PTE is corrupted or not. I put that function around the sycall enter/exit. So it help to find which syscall corrupt memory. I have used this stupid technique to find so many hard-to-find memory corruption bugs.....)  do_close_on_exec  dup2     Re-read Yutong s patch again. It touches a lot handler code. This has to be verified before using any nowait reply.    pipe s wakeup may have issue?    0418-w15-41. 39sec", 
            "title": "04/18 Wed"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0417-tue", 
            "text": "Checking list:   -pcache: ibapi use va or pa, does it matter?-  No, I change it to use the VA. Then we don t have the need to use PA reply any more.    =ib_mad, does it really corrupt Memory=  Still not sure. Should be something come from the  ib_poll_cq .    M side per PTE lock, check if the lock is really the same lock!  -Mail I20. Check CPT.-  Dist-VMA  First make sure, TF+no-dist-vma work on my own setting. Though sometimes random bug happen (I doubt it is IB).  Then turn on dist-vma  w/wo zerofill  w/wo kfree  w/wo all-zero Debug.  w/wo M side per PTE lock      Change most handlers to use TX buffer. Reduce the random mismatched reply case.  P side watchdog patch: what to print   -It looks like it is more easier to have bug when I turn on those debug counter printing. I probably should check those buffer mgmt. All next test have zerofill:-   w  print  F 0417-w15-2(rmap_walk list_for_each_entrry #GP)  F 0417-w15-3(pcache_copy_page_range corrupted PTE)  F 0417-w15-4(fit_poll_cq+0x39 ib_poll_cq()  )  F 0417-w15-5(pcache_copy_page_range corrupted PTE)    wo strace exit:  S 0417-w15-6(each 100 step take ~39s/ Linux is ~34s)  S 0417-w15-7(filling shuffle data, that works)  F 0417-w15-8(pcache_copy_page_range+0x5d1)  F 0417-w15-9(rmap_walk+0x47 #GP)    disable strace:  F 0417-w15-10(pcache_copy_page_range+0x5d1)    Conclusion  it has nothing to do with the strace thing.  most of them fail around  nr_reqs=19103       Why the pcache_copy_page_range always happen, after some fork, execve.   w strace (fork, vfork, clone, execve)  F 0417-w15-11 (pcache_cp_pg_range). Understand its flow. Back to make sure P side per PTE lock is correct. If it is pcache_cp fault, it always fail at  nr_reqs=19103 . And it is: 1) python fork, 2) execve sh.  S 0417-w15-12. With global PTE lock. Passed the failed stage above.  F 0417-W15-13. With global PTE lock. Failed at pcache_cp. Same place. (Since global PTE lock also fail, so it is not the lock issue. Still someone write to wrong memory.)  F 0417-w15-14. With global PTE lock. Same place. Found that I printed a misleading debug info. Modified a little bit to print the actual pte content. Hope can get some valid info next round.  F 0417-w15-15. Same place.  copy :   addr :   0x7fffdca07000 ,   ptecont :   0x8800000000000 .  zap :   ptent :   0x340   address :   0x7fffdca08000 .  F 0417-w15-16. Well. BUG in ib_mad_send handler. I add the same checking in ib_mad_receive. This is really just used to catch it. Not fixing it.  F 0417-w15-17. Again,  addr :   0x7fffdc207000 ,   ptecont :   0x8800000000000  F 0417-w15-18.  addr :   0x7fffdca07000 ,   ptecont :   0x8800000000000    Conclusion  Only these two addresses  addr: 0x7fffdca07000, ptecont: 0x8800000000000  pte:ffff88103ea87038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000)  addr: 0x7fffdc207000, ptecont: 0x8800000000000  pte:ffff88103ea97038 (0x8800000000000) pfn:0x0 flags:(0x8800000000000)       Bug found. In pipe_read/write. It somehow corrupted memory. Damn.    -Another first thing, check this weird log.. :\nHmm, this log should be fine. mad_post is after recv_done_handler. So even if we detect corrupted memory in handler, it has nothing to do with mad_post. The root cause should come from ib_poll_cq, that is where we pass wc to, and where the wc.wr_id was filled in.-   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72 [   3850.911144 ]   ib_mad_recv_done_handler () :   c1 :   2060   c2 :   12   wc - wr_id :   0xffff88103eea1398  [   3850.921881 ]   ib_mad_post_receive_mads () :   c1 :   2060   c2 :   13   recv_wr . wr_id :   0xffff88103eea1008   recv_queue :   ffff88103ee42520  [   3850.933620 ]   ib_mad_completion_handler   2377   got   successful   send   cq   op   0   mad_got_one   13  [   3850.942346 ]   ib_mad_completion_handler   2383   got   successful   recv   cq   op   128   mad_got_one   14  [   3850.951266 ]   ib_mad_recv_done_handler () :   c1 :   2061   c2 :   13   wc - wr_id :   0xffff88103eea1560  [   3850.961999 ]   ib_mad_post_receive_mads () :   c1 :   2061   c2 :   14   recv_wr . wr_id :   0xffff88103eea11d0   recv_queue :   ffff88103ee42520  [   3850.973737 ]   ib_mad_completion_handler   2377   got   successful   send   cq   op   0   mad_got_one   14  [   3851.257563 ]   ib_mad_completion_handler   2383   got   successful   recv   cq   op   128   mad_got_one   15  [   3851.266295 ]   ib_mad_recv_done_handler () :   c1 :   2062   c2 :   14   wc - wr_id :   0xffff88103eea1728  [   3851.277029 ]   ib_mad_post_receive_mads () :   c1 :   2062   c2 :   15   recv_wr . wr_id :   0xffff88103eea1398   recv_queue :   ffff88103ee42520  [   3851.288767 ]   ib_mad_completion_handler   2377   got   successful   send   cq   op   0   mad_got_one   15  [   3851.297493 ]   ib_mad_completion_handler   2383   got   successful   recv   cq   op   128   mad_got_one   16  [   3851.306413 ]   ib_mad_recv_done_handler () :   c1 :   2063   c2 :   15   wc - wr_id :   0xffff88103eea18f0  [   3851.317147 ]   ib_mad_post_receive_mads () :   c1 :   2063   c2 :   16   recv_wr . wr_id :   0xffff88103eea1560   recv_queue :   ffff88103ee42520  [   3851.328886 ]   ib_mad_completion_handler   2377   got   successful   send   cq   op   0   mad_got_one   16  [   3851.903180 ]   ib_mad_completion_handler   2383   got   successful   recv   cq   op   128   mad_got_one   17  [   3851.911913 ]   ib_mad_recv_done_handler () :   c1 :   2064   c2 :   16   wc - wr_id :   0xffff88103eea1ab8  [   3851.922646 ]   ib_mad_post_receive_mads () :   c1 :   2064   c2 :   17   recv_wr . wr_id :   0xffff88103eea1728   recv_queue :   ffff88103ee42520  [   3851.934384 ]   ib_mad_completion_handler   2377   got   successful   send   cq   op   0   mad_got_one   17  [   3851.943110 ]   ib_mad_completion_handler   2383   got   successful   recv   cq   op   128   mad_got_one   18  [   3851.952030 ]   ib_mad_recv_done_handler () :   c1 :   2065   c2 :   17   wc - wr_id :   0xffff88103eea1c80  [   3851.962764 ]   ib_mad_post_receive_mads () :   c1 :   2065   c2 :   18   recv_wr . wr_id :   0xffff88103eea18f0   recv_queue :   ffff88103ee42520  [   3851.974502 ]   ib_mad_completion_handler   2377   got   successful   send   cq   op   0   mad_got_one   18  [   3864.723128 ]   ***    FIT   layer   ready   to   go !  [   3864.727206 ]   ***  [   3867.339488 ]   Processor   LLC   Configurations :  [   3867.343760 ]       PhysStart :           0x100000000  [   3867.348705 ]       VirtStart :           0xffff880100000000  [   3867.354329 ]       Registered   Size :     0x400000000  [   3867.359274 ]       Actual   Used   Size :    0x208000000  [   3867.364219 ]       NR   cachelines :       2097152  [   3867.368776 ]       Associativity :       8  [   3867.372751 ]       NR   Sets :             262144  [   3867.377210 ]       Cacheline   size :      4096   B  [   3867.381672 ]       Metadata   size :       64   B  [   3867.385937 ]       NR   cacheline   bits :   12   [   0   -   11 ]   0x0000000000000fff  [   3867.392821 ]       NR   set - index   bits :   18   [ 12   -   29 ]   0x000000003ffff000  [   3867.399705 ]       NR   tag   bits :         34   [ 30   -   63 ]   0xffffffffc0000000  [   3867.406588 ]       NR   pages   for   data :   2097152  [   3867.411147 ]       NR   pages   for   meta :   32768  [   3867.415509 ]       Cacheline   ( pa )   range :     [         0x100000000   -          0x2ffffffff ]  [   3867.423848 ]       Metadata   ( pa )   range :      [         0x300000000   -          0x307ffffff ]  [   3867.432186 ]       Cacheline   ( va )   range :     [ 0xffff880100000000   -   0xffff8802ffffffff ]  [   3867.440524 ]       Metadata   ( va )   range :      [    ffff880300000000   -   0xffff880307ffffff ]  [   3867.448862 ]       pcache_set_map ( 064 B ) :     [    ffff88207ec00000   -   0xffff88207fbfffff ]  [   3867.457201 ]       Way   cache   stride :    0x40000000  [   3867.462048 ]       Memmap   $   semantic :         memblock   reserved  [   3867.468156 ]       NR   victim   $   entries :       8  [   3867.472725 ]   newpid :   1   home : 1   replica :   1  [   3867.476980 ]   p2m_fork ( cpu0 ) :   I   cur : 1 - kernel_init   new : 20  [   3867.482718 ]   p2m_fork ( cpu0 ) :   O   succeed   cur : 1 - kernel_init   new : 20  [   3867.489197 ]   Processor :   Processor   manager   is   running .  [   3867.494724 ]   Online   CPU :   0 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 , 18 , 20 , 22  [   3867.500444 ]   Active   CPU :   0 , 2 , 6 , 10 , 12 , 14 , 16 , 18 , 20 , 22  [   3867.505777 ]     [ 0 ]   Thread [ kvictim_flushd : 19 ]   pinned   at   CPU   8  [   3867.511982 ]     [ 1 ]   Thread [ recvpollcq : 17 ]   pinned   at   CPU   4  [   3867.539217 ]   do_close_on_exec () :   TODO ,   not   implemented .  [   3867.549209 ]   STDOUT :   --- [  Before   execv ^ V  ] ---  [   3867.553870 ]   STDOUT :   --- [  e  ---  [   3867.557880 ]   newpid :   20   home : 1   replica :   1  [   3867.562248 ]   p2m_fork ( cpu10 ) :   I   cur : 20 - exe . o   new : 21  [   3867.567560 ]   p2m_fork ( cpu10 ) :   O   succeed   cur : 20 - exe . o   new : 21  [   3867.573670 ]   CPU12   PID21   sys_execve  [   3867.578681 ]   do_close_on_exec () :   TODO ,   not   implemented .  [   3867.584215 ]   CPU12   PID21   sys_execve   =   0 ,   0x0  [   3867.599867 ]   BUG :   unable   to   handle   kernel   paging   request   at   000000040 8446080  [   3867.607436 ]   IP :   [ ffffffff8101bbbf ]   task_tick_rt + 0x1f / 0xd0", 
            "title": "04/17 Tue"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0416-mon", 
            "text": "Make dist-vma work with TF first. Tough work.  0416-w14-7 : 1) do_wp_page triggered, 2) dealock on per pte lock. This really should not happen. It is single worker. Basically means the page- lock is not intialized. Probabaly our per PTE lock implementation is wrong.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 [   5220.250552 ]   hb :   worker [ 0 ]   CPU   4   stucked  [   5220.254819 ]   hb :    common_header   [ op = 0x20000000   src_nid : 0 ]  [   5220.260734 ]   hb :    msg   [ pid = 21 , tgid = 21 , flags = 0x51 , vaddr = 0x7fff7b7fdfb8 ]  [   5220.267911 ]   CPU :   4   PID :   31   Comm :   thpool - worker0   4.0.0 - lego - ys +   # 237  [   5220.274890 ]   RIP :   0010 : [ ffffffff81031aa3 ]    [ ffffffff81031aa3 ]   handle_lego_mm_fault + 0x373 / 0x4f0  handle_lego_mm_fault + 0x373 / 0x4ee :                                                                                                                                                                                     arch_spin_lock   at   arch / x86 / include / asm / spinlock . h : 21                                                                                                                                                                 \n  ( inlined   by )   spin_lock   at   include / lego / spinlock . h : 72                                                                                                                                                                \n  ( inlined   by )   do_anonymous_page   at   managers / memory / vm / fault . c : 115                                                                                                                                                    \n  ( inlined   by )   handle_pte_fault   at   managers / memory / vm / fault . c : 142                                                                                                                                                     \n  ( inlined   by )   handle_lego_mm_fault   at   managers / memory / vm / fault . c : 225    A IB bug during normal run (P M S TF), this is REALLY weird: 1\n2\n3\n4\n5\n6\n7 [    395.259560 ]   CPU12   PID21   sys_execve  [    395.263345 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   00000000000001 a0  [    395.272068 ]   IP :   [ ffffffff81064c09 ]   fit_poll_cq + 0x39 / 0x530  fit_poll_cq + 0x39 / 0x523 : git :( test_vma )]   $   . / scripts / faddr2line   vmImage    fit_poll_cq + 0x39  ib_poll_cq   at   include / rdma / ib_verbs . h : 1614 \n  ( inlined   by )   fit_poll_cq   at   net / lego / fit_internal . c : 1671    Catch the ib_mad bug once.. and mlx4_error follows. I added more checking to where the mad_queue was assigned. 1\n2\n3\n4 [    787.471385 ]   ib_mad_completion_handler   2365   got   successful   recv   cq   op   128   mad_got_one   15  [    787.480124 ]   BUG !   mad_list :   ffff88103eea1728   mad_queue :             ( null )  [    787.487491 ]   ------------ [   cut   here   ] ------------  [    787.492630 ]   WARNING :   CPU :   0   PID :   15   at   drivers / infiniband / core / mad . c : 1909   ib_mad_completion_handler + 0xa56 / 0xab0", 
            "title": "04/16 Mon"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0415-sun", 
            "text": "Trying TF myself.  Had a bug report on 0415-w15-5, on fork, execve etc.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48 [    317.436811 ]   newpid :   22   home : 1   replica :   1  [    317.477701 ]   pte : ffff88103e94a038   pfn : 0x0   flags :()  [    317.482752 ]   pte   dumped   because :   corrupted  [    317.487213 ]   ------------ [   cut   here   ] ------------  [    317.492352 ]   WARNING :   CPU :   14   PID :   22   at   managers / processor / pgtable . c : 365   pcache_copy_page_range + 0x5d1 / 0x6c0  [    317.503213 ]   CPU :   14   PID :   22   Comm :   python   4.0.0 - lego +   # 93  [    317.552082 ]   Call   Trace :  [    317.554799 ]   TSK  [    317.556930 ]   [ ffffffff810123a1 ]   __warn . constprop .0 + 0x91 / 0xd0  [    317.563330 ]   [ ffffffff8101246f ]   warn_slowpath_null + 0xf / 0x20  [    317.569634 ]   [ ffffffff8102d401 ]   pcache_copy_page_range + 0x5d1 / 0x6c0  [    317.576615 ]   [ ffffffff81037ed7 ]   fork_dup_pcache + 0x27 / 0x30  [    317.582723 ]   [ ffffffff8101e514 ]   copy_process + 0xcf4 / 0x1140  [    317.588833 ]   [ ffffffff8101e986 ]   do_fork + 0x26 / 0x160  [    317.594264 ]   [ ffffffff8101eb89 ]   sys_clone + 0x29 / 0x30  [    317.599789 ]   [ ffffffff8100e66d ]   do_syscall_64 + 0x3d / 0xd0  [    317.605705 ]   [ ffffffff8100d56c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [    317.612782 ]   EOT  [    317.614917 ]   --- [   end   trace   0000000000000000   ] ---  [    317.625561 ]   p2m_fork ( cpu14 ) :   I   cur : 22 - python   new : 36  [    330.209312 ]   p2m_fork ( cpu14 ) :   O   succeed   cur : 22 - python   new : 36  [    330.310909 ]   ------------ [   cut   here   ] ------------  [    330.315864 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 804 / pcache_zap_pte () !  [    330.324302 ]   Kernel   Panic   -   not   syncing :   BUG !  [    330.329050 ]   CPU :   0   PID :   36   Comm :   python   4.0.0 - lego +   # 93  [    330.377824 ]   Call   Trace :  [    330.380540 ]   TSK  [    330.382672 ]   [ ffffffff81026493 ]   panic + 0xc2 / 0x105  [    330.387908 ]   [ ffffffff8101bbcc ]   ?   task_tick_rt + 0x2c / 0xd0  [    330.393920 ]   [ ffffffff81019245 ]   ?   scheduler_tick + 0x55 / 0x60  [    330.400126 ]   [ ffffffff810168f5 ]   ?   tick_handle_periodic + 0x45 / 0x70  [    330.406913 ]   [ ffffffff81006684 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [    330.413700 ]   [ ffffffff8100e2aa ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [    330.420973 ]   [ ffffffff810125ad ]   ?   printk + 0x11d / 0x1b0  [    330.426597 ]   [ ffffffff810375bc ]   pcache_zap_pte + 0x14c / 0x190  [    330.432802 ]   [ ffffffff81035db0 ]   ?   __pcache_remove_rmap_one + 0x70 / 0x70  [    330.439978 ]   [ ffffffff8102cd25 ]   unmap_page_range + 0x325 / 0x3f0  [    330.446379 ]   [ ffffffff8102ce0e ]   release_pgtable + 0x1e / 0x40  [    330.452487 ]   [ ffffffff81037ef8 ]   pcache_process_exit + 0x18 / 0x20  [    330.458984 ]   [ ffffffff8101d3c4 ]   mmput + 0x34 / 0xb0  [    330.464123 ]   [ ffffffff8102c38d ]   do_execve + 0x42d / 0x760  [    330.469845 ]   [ ffffffff8102c6c9 ]   sys_execve + 0x9 / 0x10  [    330.475371 ]   [ ffffffff8100e66d ]   do_syscall_64 + 0x3d / 0xd0  [    330.481286 ]   [ ffffffff8100d56c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [    330.488364 ]   EOT  [    330.490501 ]   --- [   end   Kernel   panic   -   not   syncing :   BUG !   \none more  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 [    369.223161 ]   newpid :   22   home : 1   replica :   1  [    369.264307 ]   pte : ffff88103ea41038   ( 0x0 )   pfn : 0x0   flags :()  [    369.269938 ]   pte   dumped   because :   corrupted  [    369.274399 ]   ------------ [   cut   here   ] ------------  [    369.279538 ]   WARNING :   CPU :   14   PID :   22   at   managers / processor / pgtable . c : 365   pcache_copy_page_range + 0x5d1 / 0x6c0  [    369.290398 ]   CPU :   14   PID :   22   Comm :   python   4.0.0 - lego +   # 94  [    369.296310 ]   Stack :  [    369.341976 ]   TSK  [    369.344107 ]   [ ffffffff810123a1 ]   __warn . constprop .0 + 0x91 / 0xd0  [    369.350508 ]   [ ffffffff8101246f ]   warn_slowpath_null + 0xf / 0x20  [    369.356809 ]   [ ffffffff8102d401 ]   pcache_copy_page_range + 0x5d1 / 0x6c0  [    369.363790 ]   [ ffffffff81037f07 ]   fork_dup_pcache + 0x27 / 0x30  [    369.369897 ]   [ ffffffff8101e514 ]   copy_process + 0xcf4 / 0x1140  [    369.376006 ]   [ ffffffff8101e986 ]   do_fork + 0x26 / 0x160  [    369.381435 ]   [ ffffffff8101eb89 ]   sys_clone + 0x29 / 0x30  [    369.386960 ]   [ ffffffff8100e66d ]   do_syscall_64 + 0x3d / 0xd0  [    369.392875 ]   [ ffffffff8100d56c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [    369.399952 ]   EOT  [    369.402086 ]   --- [   end   trace   0000000000000000   ] ---  [    369.412750 ]   p2m_fork ( cpu14 ) :   I   cur : 22 - python   new : 36  [    369.418215 ]   p2m_fork ( cpu14 ) :   O   succeed   cur : 22 - python   new : 36  [    369.500829 ]   ptent :   0x340   address :   0x7fffe2408000  [    369.505783 ]   pte : ffff88103dbe5040   ( 0x340 )   pfn : 0x0   flags :( dirty | global | softw1 )  [    369.513637 ]   pte   dumped   because :   corrupted  [    369.518095 ]   ------------ [   cut   here   ] ------------  [    369.523236 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 808 / pcache_zap_pte () !  [    369.531672 ]   Kernel   Panic   -   not   syncing :   BUG !", 
            "title": "04/15 Sun"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0414-sat", 
            "text": "Check if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm. Will they do harm  Implement replication  Add IB counter", 
            "title": "04/14 Sat"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0413-fri", 
            "text": "Patched M side pgtable to use per PTE/PMD lock. So thpool in M will not be bottlnecked by the page_table_lock.", 
            "title": "04/13 Fri"
        }, 
        {
            "location": "/lego/log/log-04-2018/#ib_mad_recv_done_handler-may-corrupt-memory-again", 
            "text": "Somehow, during testing of this patch. Running with MT-Phoenix 1GB, the P side has reported bad pgd entries. I m using fork+execve way. The child(phoenix) already exit. This msg is printed when parent exit_mm. The pgd table should either be 0, or valid pud va. Memory corruption happened   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62 [   2551.687806 ]   Kernel   strace  [   2551.690715 ]   Task :   21 : 21   nr_accumulated_threads :   1  [   2551.696327 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   2551.703704 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   2551.712141 ]    98.63     66.942660568      66942661           1           0   sys_wait4  [   2551.719898 ]     0.45      0.457060789        457061           1           0   sys_clone  [   2551.727654 ]     0.20      0.204320071         51081           4           0   sys_brk  [   2551.735216 ]     0.40      0.040378189         40379           1           0   sys_mmap  [   2551.742876 ]     0.13      0.013682424          4561           3           0   sys_write  [   2551.750633 ]     0.10      0.000001039             2           1           0   sys_newfstat  [   2551.758681 ]     0.88      0.000000888             1           2           0   sys_rt_sigaction  [   2551.767114 ]     0.79      0.000000792             1           2           0   sys_futex  [   2551.774871 ]     0.77      0.000000770             1           1           0   sys_rt_sigprocmask  [   2551.783501 ]     0.54      0.000000548             1           1           0   sys_arch_prctl  [   2551.791742 ]     0.49      0.000000499             1           1           0   sys_newuname  [   2551.799789 ]     0.46      0.000000469             1           1           0   sys_getrlimit  [   2551.807933 ]     0.19      0.000000195             1           1           0   sys_set_tid_address  [   2551.816659 ]     0.19      0.000000190             1           1           0   sys_set_robust_list  [   2551.825386 ]     0.18      0.000000181             1           1           0   sys_ioctl  [   2551.833143 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   2551.841577 ]   100.00     67.658107612                      22           0   total  [   2551.848945 ]  [   2551.850591 ]  [   2551.852240 ]   Kernel   Profile   Points  [   2551.855924 ]    status                    name               total                  nr              avg . ns  [   2551.865621 ]   -------    --------------------    ----------------    ----------------    ----------------  [   2551.875317 ]       off        flush_tlb_others         0.000204992                  58                3535  [   2551.885014 ]       off       __do_kmalloc_node         0.300783843              281501                1069  [   2551.894709 ]       off       __pcache_zerofill         0.009844770               16558                 595  [   2551.904404 ]       off             pcache_miss        54.414457906              257869              211016  [   2551.914100 ]       off            pcache_flush         0.000000000                   0                   0  [   2551.923795 ]   -------    --------------------    ----------------    ----------------    ----------------  [   2551.933490 ]  [   2552.074985 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956028 ( ffffffff81146ca0 )  [   2552.084206 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956030 ( ffff88103e956030 )  [   2552.093611 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956038 ( ffff88103e956030 )  [   2552.103016 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956048 ( ffff88103cc48740 )  [   2552.112421 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956050 ( 00000000000001 c0 )  [   2552.121825 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956058 ( ffff88103eea2008 )  [   2552.131230 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956060 ( ffff88103eea17d8 )  [   2552.140635 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956068 ( ffff88103ee42520 )  [   2552.150040 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9560e8 ( 000000103e9560 f0 )  [   2552.159444 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956118 ( 010200 8081018101 )  [   2552.168849 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956120 ( 3 c010b0012000000 )  [   2552.178254 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956128 ( 0000000000001100 )  [   2552.187659 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956138 ( 00000000ff ffffff )  [   2552.197064 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956158 ( 0307 8 a2402010101 )  [   2552.206467 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956160 ( 0307 8 a2453946600 )  [   2552.215872 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956168 ( 0307 8 a2450946600 )  [   2552.225277 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956170 ( 0310 800051946600 )  [   2552.234682 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956178 ( c902000100000000 )  [   2552.244088 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956198 ( bfd0cc054a122000 )  [   2552.253492 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561a0 ( 0000000000 98 b9c8 )  [   2552.262897 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561a8 ( bfe0fe0610914e01 )  [   2552.272302 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561b0 ( 000000000050f 2 c7 )  [   2552.281706 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561b8 ( bfd9a30000ec5100 )  [   2552.291111 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561c0 ( bffc91d40f20f2c7 )  [   2552.300516 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561c8 ( 0f 20 cd054a20f2c7 )  [   2552.309920 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561d0 ( 1094 edcf0f60edcf )  [   2552.319325 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e9561d8 ( 0000000000000100 )  [   2552.328730 ]   . / arch / x86 / include / asm / pgtable . h : 579 :   bad   pgd   ffff88103e956218 ( 0000000000005 aa5 )  [   2552.338151 ]   nr_pgfault :   26    Second run, saw this invalid pointer deference again! Combined with the above log, I think ib_mad is definitely corrupting memory! I have to take a look. 1 qp_info = mad_list- mad_queue- qp_info;", 
            "title": "ib_mad_recv_done_handler may corrupt memory, again."
        }, 
        {
            "location": "/lego/log/log-04-2018/#patching-the-handlers-to-use-tx-buffer", 
            "text": "Patched.  Once race condition: pcache_handle_miss use the page itself as the reply buffer. Assume later on, it changes to use nowait reply. When the reply is buffered in the queue and has not been sent. Another munmap comes in and invalidate this area, then the page will be freed. The data is invalidate.  But this case seems abnormal. The application will not do so I guess.", 
            "title": "Patching the handlers to use tx buffer."
        }, 
        {
            "location": "/lego/log/log-04-2018/#check-if-page-table-pages-page-themselves-are-freed-in-munmap-at-both-p-and-m-need-to-confirm", 
            "text": "", 
            "title": "Check if page table pages, page themselves are freed in munmap, at both P and M. Need to confirm."
        }, 
        {
            "location": "/lego/log/log-04-2018/#tonight-task-think-about-how-to-do-the-vma-replication-how-to-combine-with-the-line-replicaiton", 
            "text": "", 
            "title": "Tonight task. Think about how to do the VMA replication, how to combine with the $ line replicaiton."
        }, 
        {
            "location": "/lego/log/log-04-2018/#0412-thur", 
            "text": "Patched zerofill. All done.  Testing new driver fix with Phoenix\n- 1 st  run, the mismatch reply is still there. mmap() replied address is different from the one printed. So segfault follows. (0412-w15-4)\n- 2st run, 3st run, succeed.  0412-w15-9 0412-w14-9  First time testing phoenix with zerofill (no net). Somehow, P has pcache timeout, but M s watchdog show there is no pending requests. This happen once before I remember  0412-w15-10. Have not seen this ib mad thing for a long time. Indeed somewhere is wrong. 1\n2\n3 [  297.794969] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15\n[  297.803706] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020\n[  297.812431] IP: [ ffffffff81058937 ] ib_mad_completion_handler+0xc7/0x810  \n2", 
            "title": "04/12 Thur"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0411-wed", 
            "text": "Adding anon first touch opt.  0411-p/m-9 : this log indicate M does not have any unhandled requests, but P side has 1  __pcache_fill  timeout. It seems the message is lost somewhere.  0411-p/m-11 : catch one with the debug msg Yiying added. She says the M side send queue has 2 reqs. But poll does not return any error. Weird.  Help debugging IB issue.", 
            "title": "04/11 Wed"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0410-tue", 
            "text": "Found. IB stuck. Damn. 1\n2\n3\n4\n5\n6\n7\n8 [ 2240.294960] RIP: 0010:[ ffffffff8104a6d8 ]  [ ffffffff8104a6d8 ] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2242.694733] RIP: 0010:[ ffffffff8104a6d8 ]  [ ffffffff8104a6d8 ] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2245.094524] RIP: 0010:[ ffffffff8104a6e3 ]  [ ffffffff8104a6e3 ] mlx4_ib_poll_cq+0x383/0x6a0\n[ 2247.494306] RIP: 0010:[ ffffffff8104a6d8 ]  [ ffffffff8104a6d8 ] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2249.894088] RIP: 0010:[ ffffffff8104a6d8 ]  [ ffffffff8104a6d8 ] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2252.293870] RIP: 0010:[ ffffffff8104a6d8 ]  [ ffffffff8104a6d8 ] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2254.693651] RIP: 0010:[ ffffffff8104a6d8 ]  [ ffffffff8104a6d8 ] mlx4_ib_poll_cq+0x378/0x6a0\n[ 2257.093431] RIP: 0010:[ ffffffff8104a6e3 ]  [ ffffffff8104a6e3 ] mlx4_ib_poll_cq+0x383/0x6a0", 
            "title": "04/10 Tue"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0409-mon", 
            "text": "thpool testing. 4 workers. MT-phoenix:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 [ root @ wuklab05   ys ] #   cat   040 9 - p   |   grep   __munmap  [    227.054974 ]   CPU14   PID22   strace__munmap ([ 0x7fffb0ba9000   -   0x7fffb4000000 ],   54882304 )   =   0 ,   0x0  [    227.093466 ]   CPU16   PID23   strace__munmap ([ 0x7fffab7ff000   -   0x7fffac000000 ],   8392704 )   =   0 ,   0x0  [    227.102773 ]   CPU14   PID22   strace__munmap ([ 0x7fffb8000000   -   0x7fffb8ba9000 ],   12226560 )   =   0 ,   0x0  [    227.141265 ]   CPU18   PID24   strace__munmap ([ 0x7fffa8000000   -   0x7fffac000000 ],   67108864 )   =   0 ,   0x0  [    227.150669 ]   CPU16   PID23   strace__munmap ([ 0x7fffb0000000   -   0x7fffb37ff000 ],   58716160 )   =   0 ,   0x0  [    227.218248 ]   CPU22   PID26   strace__munmap ([ 0x7fffa0000000   -   0x7fffa4000000 ],   67108864 )   =   0 ,   0x0  [    227.285826 ]   CPU2   PID28   strace__munmap ([ 0x7fff98000000   -   0x7fff9c000000 ],   67108864 )   =   0 ,   0x0  [    227.440567 ]   CPU14   PID31   strace__munmap ([ 0x7fff8a7fd000   -   0x7fff8c000000 ],   25178112 )   =   0 ,   0x0  [    227.449972 ]   CPU12   PID30   strace__munmap ([ 0x7fff88000000   -   0x7fff8c000000 ],   67108864 )   =   0 ,   0x0  [    227.459376 ]   CPU14   PID31   strace__munmap ([ 0x7fff90000000   -   0x7fff927fd000 ],   41930752 )   =   0 ,   0x0  [    227.490109 ]   CPU18   PID33   strace__munmap ([ 0x7fff80000000   -   0x7fff84000000 ],   67108864 )   =   0 ,   0x0  [    227.723140 ]   word_count - pthr [ 29 ] :   segfault   at   0x4e842010   ip   0000000000420354   sp   00007ff fb17f9bc0   error   6  0x4e842010     Print mmap on M, if segfault. Printed, the  0x4e842010  is never a valid address. thpool makes Memory side SMP. Probably bring some issues.   Found:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 P  CPU22   PID26   strace__mmap ( addr = 0x0 ,   len = 0xfb000 ,   prot ( 0x3 ) = PROT_READ | PROT_WRITE ,   flags ( 0x22 ) = MAP_PRIVATE | MAP_ANONYMOUS ,   fd = 18446744073709551615 (   ),   off = 0x0 )   =   1317351432 ,   0x4e853008  word_count - pthr [ 26 ] :   segfault   at   0x4e853010   ip   0000000000420354   sp   00007ff f972a8bc0   error   6  M  [    583.120615 ]     00400000 - 004 d9000   r - xp   00000000   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread  [    583.131578 ]     006 d9000 - 006 dc000   rw - p   000 d9000   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread  [    583.142729 ]     006 dc000 - 00755000   rw - p   00000000   [ heap ]  [    583.148254 ]     7ff f529c9000 - 7ff fb93aa000   rw - p   00000000  [    583.153974 ]     7ff fb93aa000 - 7ff ff7fff000   rw - p   00000000   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt  [    583.167355 ]     7ff ffffde000 - 7ff ffffff000   rw - p   00000000   [ stack ]  [    583.173753 ]   ------------ [   cut   here   ] ------------  [    583.178892 ]   WARNING :   CPU :   4   PID :   31   at   managers / memory / handle_pcache / fault . c : 55   handle_p2m_pcache_miss + 0x18e / 0x1d0  [    583.190430 ]   src_nid : 0 , pid : 21 , vaddr : 0x4e853010  [    583.195279 ]   CPU :   4   PID :   31   Comm :   thpool - worker0   4.0.0 - lego - ys +   # 90    Confirmed. I printed added a number to mmap requests. And the compare the results of both P and M. The data is wrong. Btw, I m only running 1 worker thread at M, which makes it single thread handling. So, I m going to, 1) first use kmalloc to get the reply buffer, and 2) revert back the IB MAX_OUT config, remove the #ifdef COMP_MEMORY. See if it is this patch s issue. 1\n2\n3\n4\n5\n6\n7\n8\n9 P :  CPU18   PID24   strace__mmap ( 30   ..)   =   - 1325940736 ,   0x7fffb0f7c000  CPU22   PID26   strace__mmap ( 31   ..)   =   2144269992 ,   0x7fcef6a8  M :  ...  handle_p2m_mmap () :   30   7ff fb0f7c000  handle_p2m_mmap () :   31   7ff fb0efe000  ...    Anyway, this is  temporary  fixed by using kmalloced reply buffer.  Spent whole afternoon and whole night. Finally figure out why timeout happen in P. It is because somewhere in the middle, M has 1 or more requests stucked/unhandled. Deadlock happen in the middle.  Like this one. 5 requests queued waiting, 1 is being handled. And that 1 handler stuck. And it is handle_pcache_miss. Now, I need to find out where it stuck! 1 thpool-worker0 nr_queued: 5 1   Oh, I really hope we can have some  soft/hw lockdep, watchdog stuff . This should make out life much much much much much easier!", 
            "title": "04/09 Mon"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0408-sun", 
            "text": "Trying the fit_nowait patch.   First try fit_nowait patch, without any chanegs to other code. See if this patch can work.  Second, modify pcache to use reply_message_nowait. See if this can work. and improve performance.  Third, if 2) can improve, perf. Move on to modify thpool patch.   1 st , P fail at ib_mad, during boot: 1\n2\n3\n4\n5\n6\n7 [    349.239220 ]   Online   CPU :   0 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 , 18 , 20 , 22  [    349.244940 ]   Active   CPU :   0 , 2 , 6 , 10 , 12 , 14 , 16 , 18 , 20 , 22  [    349.250272 ]     [ 0 ]   Thread [ kvictim_flushd : 19 ]   pinned   at   CPU   8  [    349.256478 ]     [ 1 ]   Thread [ recvpollcq : 17 ]   pinned   at   CPU   4  [    356.188819 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   13  [    356.197545 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000020  [    356.206270 ]   IP :   [ ffffffff81058287 ]   ib_mad_completion_handler + 0xc7 / 0x810    2st run, P side, config MAX_OUT to 1. Then single-thread pheonix with 1GB data finished. But forgot to turn on the profile point. Run one more time.  3st run. Same with 2st run setting. But with profile on. Bug shows. Ugh. I still think it is because of ib_mad_handler. It must write to wrong memory locations, and corrupt things randomly. 1\n2\n3\n4\n5\n6\n7\n8\n9 [    456.237913 ]   do_close_on_exec () :   TODO ,   not   implemented .  ...  [    456.263274 ]   BUG :   unable   to   handle   kernel   paging   request   at   00000002f 4 bfbf58  [    456.270843 ]   IP :   [ ffffffff8101bbff ]   task_tick_rt + 0x1f / 0xd0  [    456.277048 ]   PGD   0  [    456.279279 ]   Thread   overran   stack ,   or   stack   corrupted  [    456.284804 ]   Oops :   0000   [ # 1 ]   SMP   PROCESSOR  [    456.289265 ]   CPU :   10   PID :   20   Comm :   kevict_sweepd   4.0.0 - lego +   # 40  [    456.295858 ]   RIP :   0010 : [ ffffffff8101bbff ]    [ ffffffff8101bbff ]   task_tick_rt + 0x1f / 0xd0    4st run, succeed. But it looks like the perf is very bad. Oh. but 99% of the pcache miss are file-backed, which will go to storage. So the number is actually doubled. 1\n2\n3\n4\n5\n6\n7\n8\n9 With   fit_nowait   patch :  [    308.660051 ]   Kernel   Profile   Points  [    308.663734 ]    status                    name               total                  nr              avg . ns  [    308.673431 ]   -------    --------------------    ----------------    ----------------    ----------------  [    308.683128 ]       off        flush_tlb_others         0.000130715                  53                2467  [    308.692824 ]       off       __do_kmalloc_node         0.097344056              265647                 367  [    308.702521 ]       off             pcache_miss         4.504660891              258211               17446  [    308.712218 ]       off            pcache_flush         0.000000000                   0                   0  [    308.721914 ]   -------    --------------------    ----------------    ----------------    ----------------    5st run. Just run large malloc test. Looks better than yesterday s result. But I m using 15 as P today, instead of 13. So, let me try one more time to see if it is the machine.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 With   fit_nowait   patch :  [    674.382592 ]   Kernel   Profile   Points  [    674.386277 ]    status                    name               total                  nr              avg . ns  [    674.395974 ]   -------    --------------------    ----------------    ----------------    ----------------  [    674.405670 ]       off        flush_tlb_others         0.000130838                  53                2469  [    674.415366 ]       off       __do_kmalloc_node         1.604700641             1584917                1013  [    674.425062 ]       off             pcache_miss         6.467938547              786571                8223  [    674.434758 ]       off            pcache_flush         3.342783614              262225               12748  [    674.444455 ]   -------    --------------------    ----------------    ----------------    ----------------  [    674.554497 ]   nr_pgfault :   786513  [    674.557706 ]   nr_clflush :   262225  [    674.561099 ]   nr_pgfault_wp :   0  [    674.564299 ]   nr_pgfault_wp_cow :   0  [    674.567887 ]   nr_pgfault_wp_reuse :   0  [    674.571668 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    674.577195 ]   nr_pcache_fill_from_memory :   786511  [    674.582139 ]   nr_pcache_fill_from_victim :   2    6st run. Looks like the above fit_nowait can have 400ns improvement. But how come? I did not even change the pcache handling to use ibapi_nowait!!! Maybe random variation. Let me run more.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 Without   fit_nowait   patches  [    428.546738 ]   Kernel   Profile   Points  [    428.550424 ]    status                    name               total                  nr              avg . ns  [    428.560119 ]   -------    --------------------    ----------------    ----------------    ----------------  [    428.569815 ]       off        flush_tlb_others         0.000131140                  53                2475  [    428.579510 ]       off       __do_kmalloc_node         1.758704197             1331927                1321  [    428.589205 ]       off             pcache_miss         6.807601189              786575                8655  [    428.598899 ]       off            pcache_flush         3.699044847              262227               14107  [    428.608594 ]   -------    --------------------    ----------------    ----------------    ----------------  [    428.618289 ]  [    428.718670 ]   nr_pgfault :   786515  [    428.721878 ]   nr_clflush :   262227  [    428.725272 ]   nr_pgfault_wp :   0  [    428.728470 ]   nr_pgfault_wp_cow :   0  [    428.732058 ]   nr_pgfault_wp_reuse :   0  [    428.735840 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    428.741365 ]   nr_pcache_fill_from_memory :   786515  [    428.746310 ]   nr_pcache_fill_from_victim :   0    7 th  run. without fit_nowait. 1\n2\n3\n4\n5\n6\n7\n8\n9 without   fit_nowait .  [    901.223090 ]   Kernel   Profile   Points  [    901.226775 ]    status                    name               total                  nr              avg . ns  [    901.236472 ]   -------    --------------------    ----------------    ----------------    ----------------  [    901.246168 ]       off        flush_tlb_others         0.000130802                  53                2468  [    901.255865 ]       off       __do_kmalloc_node         1.862575608             1331923                1399  [    901.265560 ]       off             pcache_miss         6.814540477              786572                8664  [    901.275257 ]       off            pcache_flush         3.699187003              262224               14107  [    901.284953 ]   -------    --------------------    ----------------    ----------------    ----------------    8 th  run. without fit_nowait. 1\n2\n3\n4\n5\n6\n7\n8 [    321.514564 ]   Kernel   Profile   Points  [    321.518250 ]    status                    name               total                  nr              avg . ns  [    321.527945 ]   -------    --------------------    ----------------    ----------------    ----------------  [    321.537639 ]       off        flush_tlb_others         0.000130934                  53                2471  [    321.547335 ]       off       __do_kmalloc_node         2.216772665             1331939                1665  [    321.557031 ]       off             pcache_miss         6.806060415              786573                8653  [    321.566726 ]       off            pcache_flush         3.725455841              262231               14207  [    321.576421 ]   -------    --------------------    ----------------    ----------------    ----------------    9 th  run. with fit_nowait 1\n2\n3\n4\n5\n6\n7\n8 [    374.847912 ]   Kernel   Profile   Points  [    374.851597 ]    status                    name               total                  nr              avg . ns  [    374.861293 ]   -------    --------------------    ----------------    ----------------    ----------------  [    374.870989 ]       off        flush_tlb_others         0.000130858                  53                2470  [    374.880684 ]       off       __do_kmalloc_node         1.485304454             1331934                1116  [    374.890381 ]       off             pcache_miss         6.615317677              786582                8411  [    374.900076 ]       off            pcache_flush         3.508328900              262234               13379  [    374.909772 ]   -------    --------------------    ----------------    ----------------    ----------------    10 th  run, with fit_nowait 1\n2\n3\n4\n5\n6\n7\n8 [  225.211058] Kernel Profile Points\n[  225.214743]  status                  name             total                nr            avg.ns\n[  225.224440] -------  --------------------  ----------------  ----------------  ----------------\n[  225.234137]     off      flush_tlb_others       0.000131029                53              2473\n[  225.243833]     off     __do_kmalloc_node       1.211421872           1331984               910  \n[  225.253529]     off           pcache_miss       6.583096125            786574              8370\n[  225.263226]     off          pcache_flush       3.464430818            262227             13212\n[  225.272922] -------  --------------------  ----------------  ----------------  ----------------   Sum:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 with fit_nowait:\n\n[  225.253529]     off           pcache_miss       6.583096125            786574              8370\n[  225.263226]     off          pcache_flush       3.464430818            262227             13212\n\n[  374.890381]     off           pcache_miss       6.615317677            786582              8411\n[  374.900076]     off          pcache_flush       3.508328900            262234             13379\n\n[  674.425062]     off           pcache_miss       6.467938547            786571              8223\n[  674.434758]     off          pcache_flush       3.342783614            262225             12748\n\nWithout fit_nowait:\n[  428.589205]     off           pcache_miss       6.807601189            786575              8655\n[  428.598899]     off          pcache_flush       3.699044847            262227             14107\n\n[  901.265560]     off           pcache_miss       6.814540477            786572              8664\n[  901.275257]     off          pcache_flush       3.699187003            262224             14107\n\n[  321.557031]     off           pcache_miss       6.806060415            786573              8653\n[  321.566726]     off          pcache_flush       3.725455841            262231             14207", 
            "title": "04/08 Sun"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0407-sat", 
            "text": "Well, now we finished all the profiling stuff. Continue on other work.  Now I like listening Jazz while coding. Amazing Jazz, really good.  Once again, ib_mad_completion_handler bug will happen. During application run, or even after application exit. 1\n2\n3\n4\n5\n6\n7\n8 [    465.835447 ]   nr_mremap_pset_diff :   0  [    477.086886 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   21  [    477.095620 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000020  [    477.104345 ]   IP :   [ ffffffff81058277 ]   ib_mad_completion_handler + 0xc7 / 0x810  ib_mad_completion_handler + 0xc7 / 0x808 :  ib_mad_recv_done_handler   at   drivers / infiniband / core / mad . c : 1899 \n  ( inlined   by )   ib_mad_completion_handler   at   drivers / infiniband / core / mad . c : 2345    After remove net from pcache miss: 1\n2\n3\n4\n5\n6\n7\n8 [    465.572131 ]   Kernel   Profile   Points  [    465.575815 ]    status                    name               total                  nr              avg . ns  [    465.585510 ]   -------    --------------------    ----------------    ----------------    ----------------  [    465.595206 ]       off        flush_tlb_others         0.000000000                   0                   0  [    465.604901 ]       off       __do_kmalloc_node         0.656371295             1762220                 373  [    465.614597 ]       off             pcache_miss         7.172572671              786596                9119  [    465.624291 ]       off            pcache_flush         3.698294960              262251               14103  [    465.633987 ]   -------    --------------------    ----------------    ----------------    ----------------    After remove net from pcache flush: 1\n2\n3\n4\n5\n6\n7\n8 [    684.984000 ]   Kernel   Profile   Points  [    684.987683 ]    status                    name               total                  nr              avg . ns  [    684.997379 ]   -------    --------------------    ----------------    ----------------    ----------------  [    685.007074 ]       off        flush_tlb_others         0.000000000                   0                   0  [    685.016770 ]       off       __do_kmalloc_node         0.627372836             1500543                 419  [    685.026464 ]       off             pcache_miss         7.128702028              786596                9063  [    685.036159 ]       off            pcache_flush         3.660772506              262251               13960  [    685.045855 ]   -------    --------------------    ----------------    ----------------    ----------------    malloc, miss, flush are too slow. Especially the flush, how can it take 13.9us?  It must be our handlers! lego_copy_to_user stuff.", 
            "title": "04/07 Sat"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0406-fri", 
            "text": "Well.\nNow we have in-kernel strace, in-kernel readprofile. Yummy.", 
            "title": "04/06 Fri"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0405-thur", 
            "text": "Discussion with Yilun.\n1. munmap+nr_pgfault figure: count number of pgfaults between munmap, it should be an interesting figure.\n2. track number of pgfault at: since there is no eviction, so any mmaped area at M should only have exactly one pcache fetch.\n3. I probably want to use per-cpu counter.  Anyway, continue strace work first. Finished.", 
            "title": "04/05 Thur"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0404-wed", 
            "text": "", 
            "title": "04/04 Wed"
        }, 
        {
            "location": "/lego/log/log-04-2018/#strace-performance", 
            "text": "TF has very bad performance. It is either due to the syscall or pcache. Now I m adding facilities to track syscall activities, including average latency, total time.  Basic utilities of strace are done. But I somehow need to change the design of multithread strace. Previously, I naively make the thread group keep some info, and let all other threads use that info to do bookkeeping.  But this is really hard and not accurate. We first need to make sure we are running on a non-preemptable kernel, so the per-cpu time tracking will be accurate. Besides, we also need to make sure threads do not migrate because of syscalls such as sched_setaffinity.  Oh, well, so I though I have to use per-thread strace_info. The first design I thought is: accumulating the counter of one thread to its thread group leader, when it exit. But this is slightly complex, and will affect the thread group leader runtime.  So the second solution I came up is let all threads within a process, chain their straec_info together. And normal thread does not need to accumulate the counter. It can just exit. While the thread group leader exit, it walk through the chain to accumulate the counters. This is simpler. Besides, the strace_info of dead thread is safe. No one will touch it.  Yeh! Let us do this tomorrow. We will have a robust kernel version strace.", 
            "title": "STRACE Performance"
        }, 
        {
            "location": "/lego/log/log-04-2018/#sm-heartbeat", 
            "text": "Continue run some experiments on yesterday s case.  One we sure is SM will keep sending requests to HCA. And it looks like it does not send in a very deterministic interval:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 [   1224.034898 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   15  [   1224.130616 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   15  [   1224.222189 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   16  [   1224.417181 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   16  [   1393.159845 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   17  [   1393.255546 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   17  [   1393.347132 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   18  [   1393.538972 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   18  [   1449.437542 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   19  [   1449.533248 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   19  [   1449.624833 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   20  [   1449.722512 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   20  [   4322.423624 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   21  [   4322.519328 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   21  [   4322.610914 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   22  [   4322.708594 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   22  [   4350.750574 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   23  [   4350.846278 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   23  [   4350.937863 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   24  [   4351.035543 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   24  [   4519.690559 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   25  [   4519.786262 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   25  [   4519.877848 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   26  [   4519.975527 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   26  [   4576.396279 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   27  [   4576.491979 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   27  [   4576.583565 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   28  [   4576.681245 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   28  [   4942.886820 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   29  [   4942.982523 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   29  [   4943.074108 ]   ib_mad_completion_handler   2344   got   successful   recv   cq   op   128   mad_got_one   30  [   4943.171789 ]   ib_mad_completion_handler   2338   got   successful   send   cq   op   0   mad_got_one   30", 
            "title": "SM Heartbeat"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0403-tue", 
            "text": "", 
            "title": "04/03 Tue"
        }, 
        {
            "location": "/lego/log/log-04-2018/#bug-bug-bug", 
            "text": "Finished basic replication mechanism last night.  Today merged several patches. And both Yilun and I think there is something wrong with  ib_mad_completion_handler . It seems it will break things behind our back.  This is one bug catched today:", 
            "title": "BUG BUG BUG"
        }, 
        {
            "location": "/lego/log/log-04-2018/#ib_mad_completion_handler", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 At   very   early   stage :  [   1174.406177 ]   newpid :   20   home : 1   replica :   1  [   1174.452983 ]   p2m_fork ( cpu10 ) :   I   cur : 20 - exe . o   new : 21  [   1177.462795 ]   ib_mad_completion_handler   2324   got   successful   recv   cq   op   128   mad_got_one   22  [   1177.556502 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000020  [   1177.650101 ]   IP :   [ ffffffff81059104 ]   ib_mad_completion_handler + 0xb4 / 0x8a0  . / scripts / faddr2line   vmImage    ib_mad_completion_handler + 0xb4  ib_mad_completion_handler + 0xb4 / 0x899 :  ib_mad_recv_done_handler   at   drivers / infiniband / core / mad . c : 1899 \n  ( inlined   by )   ib_mad_completion_handler   at   drivers / infiniband / core / mad . c : 2325  ib_mad_recv_done_handler () :  1899 :   qp_info   =   mad_list - mad_queue - qp_info ;    A more scared one after I changed ib_mad_completion_handler. Note that recvcq is the only thread running on cpu4:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 [    863.887705 ]   p2m_fork ( cpu10 ) :   I   cur : 20 - exe . o   new : 21  [    868.478424 ]   p2m_fork ( cpu10 ) :   O   succeed   cur : 20 - exe . o   new : 21  [    868.541991 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   000000000000000 8  [    868.635569 ]   IP :   [ ffffffff810656d4 ]   __schedule + 0x94 / 0x1e0  [    868.701090 ]   PGD   0  [    868.725010 ]   general   protection   fault :   0000   [ # 1 ]   SMP   PROCESSOR  [    868.793651 ]   CPU :   4   PID :   17   Comm :   recvpollcq   4.0.0 - lego - ys +   # 737  Source :  clear_tsk_need_resched ( prev );    Even this one for Phoenix: 1\n2\n3\n4\n5\n6 [    763.442043 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000010  [    763.535636 ]   IP :   [ ffffffff81018d6f ]   task_curr + 0xf / 0x30  [    763.598035 ]   PGD   103e956067   PUD   103e964067   PMD   0  [    763.653154 ]   Oops :   0000   [ # 1 ]   SMP   PROCESSOR  [    763.700992 ]   CPU :   12   PID :   21   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 740  [    763.777950 ]   RIP :   0010 : [ ffffffff81018d6f ]    [ ffffffff81018d6f ]   task_curr + 0xf / 0x30    This NEVER happen before. And this part of code should be correct. We ve ran a\nlot things.. I doubt if recent IB merge corrupt things.", 
            "title": "ib_mad_completion_handler"
        }, 
        {
            "location": "/lego/log/log-04-2018/#fit_poll_cq", 
            "text": "Another one:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77 [    690.401626 ]   stat :   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt  [    690.507742 ]   SYSC_close ()   CPU12   PID : 21   [ fd :   4 ]   -   [ / sys / devices / system / cpu / online ]  [    713.899884 ]   ib_mad_completion_handler   2337   got   successful   recv   cq   op   128   mad_got_one   21  [    713.995606 ]   ib_mad_completion_handler   2331   got   successful   send   cq   op   0   mad_got_one   21  [    714.087185 ]   ib_mad_completion_handler   2337   got   successful   recv   cq   op   128   mad_got_one   22  [    714.184871 ]   ib_mad_completion_handler   2331   got   successful   send   cq   op   0   mad_got_one   22  [    742.078102 ]   ib_mad_completion_handler   2337   got   successful   recv   cq   op   128   mad_got_one   23  [    742.173810 ]   ib_mad_completion_handler   2331   got   successful   send   cq   op   0   mad_got_one   23  [    742.265399 ]   ib_mad_completion_handler   2337   got   successful   recv   cq   op   128   mad_got_one   24  [    742.363085 ]   ib_mad_completion_handler   2331   got   successful   send   cq   op   0   mad_got_one   24  [    847.063372 ]   mlx4_ib_handle_error_cqe   syndrome   21  [    847.116511 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    847.170590 ]   send   request   failed   at   connection   7   as   12  [    847.230909 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    847.284988 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    847.339067 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    847.393146 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1832  [    847.457624 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1833  [    847.522103 ]   fit_poll_cq :   connection   7   Recv   weird   event   as   - 1  [    847.589701 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1834  [    847.654179 ]   fit_poll_cq :   connection   7   Recv   weird   event   as   - 30704  [    847.725938 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1835  [    847.790416 ]   fit_poll_cq :   connection   7   Recv   weird   event   as   - 30704  [    847.862174 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    847.916252 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    847.970331 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    848.024410 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    848.078490 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1836  [    848.142967 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1837  [    848.207446 ]   fit_poll_cq :   connection   7   Recv   weird   event   as   - 1  [    848.275044 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1838  [    848.339523 ]   fit_poll_cq :   connection   7   Recv   weird   event   as   - 30704  [    848.411281 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1839  [    848.475760 ]   fit_poll_cq :   connection   7   Recv   weird   event   as   - 30704  [    848.547517 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    848.601596 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    848.655675 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    848.709753 ]   mlx4_ib_handle_error_cqe   syndrome   5  [    848.763832 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1840  [    848.828313 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at             ( null )  [    848.921908 ]   IP :   [ ffffffff8106346d ]   fit_poll_cq + 0x4ad / 0x510  [    848.989507 ]   PGD   0  [    849.013426 ]   Oops :   0002   [ # 1 ]   SMP   PROCESSOR  [    849.061265 ]   CPU :   4   PID :   17   Comm :   recvpollcq   4.0.0 - lego - ys +   # 744  [    849.131983 ]   RIP :   0010 : [ ffffffff8106346d ]    [ ffffffff8106346d ]   fit_poll_cq + 0x4ad / 0x510  [    849.228700 ]   RSP :   0000 : ffff88103e813d88    EFLAGS :   00010246  [    849.292139 ]   RAX :   000000000000100 8   RBX :   ffff88103effbad0   RCX :   0000000000000000  [    849.377418 ]   RDX :   0000000000000000   RSI :   ffffffff811d46e0   RDI :   ffffffff811dbc08  [    849.462695 ]   RBP :   ffff88103e813ea8   R08 :   0000000000000000   R09 :   0000000000000000  [    849.547973 ]   R10 :   0000000000000002   R11 :   0000000000000004   R12 :   0000000000000000  [    849.633251 ]   R13 :   ffff88103e801008   R14 :   0000000000000004   R15 :   ffff88103e813da0  [    849.718529 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc40000 ( 0000 )   knlGS : 0000000000000000  [    849.815246 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    849.883884 ]   CR2 :   0000000000000000   CR3 :   000000000113 d000   CR4 :   00000000000406 a0  [    849.969163 ]   Stack :  [    849.993082 ]   ffffffff81003299   000001 b03e813da0   0000000000000004   0000000000000730  [    850.080440 ]   000000 8100000005   00001008000000f 9   ffff88103eff8c50   002 c222040000000  [    850.167798 ]   0010004000000002   ffff88107fc20000   0000000000000731   ffffffff00000005  [    850.255156 ]   ffff8810000000f9   ffff88103eff8c50   0000000000000000   ffff88103e813e38  [    850.342513 ]   ffffffff81019854   0000000000000732   ffff881000000005   ffff8810000000f9  [    850.429871 ]   Call   Trace :  [    850.458992 ]   TSK  [    850.481870 ]   [ ffffffff81003299 ]   ?   native_smp_send_reschedule + 0x39 / 0x50  [    850.560909 ]   [ ffffffff81019854 ]   ?   try_to_wake_up + 0xe4 / 0x1f0  [    850.628506 ]   [ ffffffff81065708 ]   ?   __schedule + 0xf8 / 0x1e0  [    850.691945 ]   [ ffffffff810634d0 ]   ?   fit_poll_cq + 0x510 / 0x510  [    850.757464 ]   [ ffffffff810634e4 ]   fit_poll_cq_pass + 0x14 / 0x30  [    850.824021 ]   [ ffffffff81020636 ]   kthread + 0xf6 / 0x120  [    850.882260 ]   [ ffffffff81020540 ]   ?   __kthread_parkme + 0x70 / 0x70  [    850.950898 ]   [ ffffffff8100e572 ]   ret_from_fork + 0x22 / 0x30  /* handle normal reply */  ...  memcpy (( void   * ) ctx - reply_ready_indicators [ reply_indicator_index ],   length ,   sizeof ( int ));  ...  ( This   is   a   bad   memcpy :   reply_indicator_index ,   ctx ,   etc   should   be   checked .)", 
            "title": "fit_poll_cq"
        }, 
        {
            "location": "/lego/log/log-04-2018/#ib-spec-qp-cqe-wqe-send", 
            "text": "The channel adapter detects the WQE posting and accesses the WQE.\nThe channel adapter interprets the command, validates the WQE\u2019s virtual 12\naddresses, translates it to physical addresses, and accesses the data.\nThe outgoing message buffer is split into one or more packets. To each packet the channel adapter adds a transport header (sequence numbers, opcode, etc.). If the destination resides on a remote subnet the channel adapter adds a network header (source   destination GIDs). The channel adapter then adds the local route header and calculates both the variant\nand invariant checksums.  For a Send operation, the QP retrieves the address of\nthe receive buffer from the next WQE on its receive queue, translates it to physical addresses, and accesses memory writing the data. If this is not\nthe last packet of the message, the QP saves the current write location in 38 its context and waits for the next packet at which time it continues writing\nthe receive buffer until it receives a packet that indicates it is the last packet of the operation. It then updates the receive WQE, retires it, and sends an acknowledge message to the originator.  When the originator receives an acknowledgment, it creates a CQE on the 5\nCQ and retires the WQE from the send queue.  A QP can have multiple outstanding messages at any one time but the 8\ntarget always acknowledges in the order sent, thus WQEs are retired in the order that they are posted.", 
            "title": "IB Spec: QP, CQE, WQE, SEND"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0402-mon", 
            "text": "Patching storage replica handler, able to finish today.", 
            "title": "04/02 Mon"
        }, 
        {
            "location": "/lego/log/log-04-2018/#0401-sun", 
            "text": "Anyway. Summary of the day: replication at M almost done. Only flush part left. Storage also need a handler. But we still need code to recover.  I m tired. :-( A month to go.  Record a IB error. Using wuklab12 (P) and wuklab14(M+RAMFS), running usr/pcache_conflic.o:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39 P  [ 30801.296160 ]   ibapi_send_reply ()   CPU : 8   PID : 19   timeout   ( 30010   ms ),   caller :   clflush_one + 0x1c9 / 0x370  [ 30938.564843 ]   mlx4_ib_handle_error_cqe   syndrome   21  [ 30938.617988 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30938.672068 ]   send   request   failed   at   connection   6   as   12  [ 30938.732389 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30938.786470 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30938.840551 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30938.894632 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1584  [ 30938.959112 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1585  [ 30939.023593 ]   fit_poll_cq :   connection   6   Recv   weird   event   as   - 1  [ 30939.091194 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1586  [ 30939.155676 ]   fit_poll_cq :   connection   6   Recv   weird   event   as   - 30704  [ 30939.227436 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1587  [ 30939.291917 ]   fit_poll_cq :   connection   6   Recv   weird   event   as   - 30704  [ 30939.363678 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30939.417759 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30939.471839 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30939.525921 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30939.580002 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1588  [ 30939.644483 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at             ( null )  [ 30939.738083 ]   IP :   [ ffffffff81062fcd ]   fit_poll_cq + 0x4ad / 0x510  [ 30939.805684 ]   PGD   0  [ 30939.829604 ]   Oops :   0002   [ # 1 ]   SMP   PROCESSOR  [ 30939.877445 ]   CPU :   4   PID :   17   Comm :   recvpollcq   4.0.0 - lego - ys +   # 715  [ 30939.948166 ]   RIP :   0010 : [ ffffffff81062fcd ]    [ ffffffff81062fcd ]   fit_poll_cq + 0x4ad / 0x510  fit_poll_cq   at   net / lego / fit_internal . c : 1734  memcpy (( void   * ) ctx - reply_ready_indicators [ reply_indicator_index ],   length ,   sizeof ( int ));  M  [ 30913.642698 ]   mlx4_ib_handle_error_cqe   syndrome   21  [ 30913.695839 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30913.749919 ]   send   request   failed   at   connection   1   as   12  [ 30913.810236 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30913.864315 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30913.918395 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 30913.972474 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   305  [ 30914.035912 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   306", 
            "title": "04/01 Sun"
        }, 
        {
            "location": "/lego/log/log-03-2018/", 
            "text": "March 2018\n\n\n03/31 Sat\n\n\nStay humble. Be real.\n\n\n03/30 Fri\n\n\nOur scheduling, or IB do have issues. I must revisit this.\n\n\nThe case is: in P, we boot only 12 cores, and three of them are used by flush, sweep, and IB. So there are 9 cores left for user. Phoenix create 24 threads. During the run, a lot ib timeout will happen. If we have a good scheduling, this should never happen. I probably need to check more on this.\n\n\nAnyway. Today I reorganized the opcode things. And now I\nm adding the final large piece of Lego: replication. It should be much simpler than the pcache part. I will first write down what code I need to add, e.g., opcode, handler, buffer mgmt etc.\n\n\nEnd of day. Want to write down some simple thoughts on building system. Building system is fun, but you have to know that devil is in the details. And, you may end up debugging for many many hours on a very very little issue. But that is how it is. Building system does not mean you are always working on fantastic beautiful ideas. It is always about those little bugs, little things, trivial fixes, that make your system robust and usable. For example, the patch Yilun sent me today is about handling special cases of stat and lseek. The patch does not improve any performance or adding fancy features, it is a minor fix to make user progam run. But this enable us to run TF. I think it is a great patch and it stands for 90% of building systems in middle or late stage.\n\n\nOf course, there are other trivial things on building systems: 1) initialize every possible used variables, can be local variables, malloced buffers. 2) have decent cleanup, which is a counterpart of your initialization, like dequeue list, decrease counter etc. 3) Clear coding style, write code for others, for yourself when you read the code two weeks later. This one is hard, need experience. But can be learned. I think Yilun and Yutong both improved a lot during this project. Me? I learned this from NVM emulator protect. It is a painful one, but also a valuable one. 4) Decent protect source file organization. 5) Remember, draw, the connections between subsystems. By adding this new feature to this subsystem A, will it broke subsystem B, which is using subsystem A. Something like this. 6) clear mind on lock usage, multithread issue. This is the most difficult one. I would say I learned this by coding pcache, or mm. I would say, mm is the most difficult multithread issue one can encounter.\n\n\n03/26 Mon\n\n\nSpent several days on replication design. Now I\nm back on coding and debuging track.\n\n\nFixed a bug introduced by per-pte lock. A one hided by previous one big giant page table lock.\n\n\nAlso add an option to boot socket 0 only if Processor is configured. This is because pcache is normally registered at socket 0, if we schedule user threads to sockets other than socket 0, that will have bad performance.\n\n\n03/22 Thur\n\n\nClear Registers for execve()\n\n\nWant to figure out execve problem today.\n\n\n\n\nCheck if pcache is clean after process_exit.\n\n\nCheck if pgtable is clean.\n\n\n\n\nWell. Checked, both are clean.\n\n\nThe bug looks like the return of main, evevntually does not go to library\ns exit. Is it because library pages are not loaded properly? Since the number of pgfault equals to normal setting, I guess it may originate from Memory side.\n\n\nTLB is also flushed, so TLB should not be a hidden issue.\n\n\nGoing to check checsum. Well, checsum is okay too.\n\n\nSyscall execve will change ip, sp, flags registers. So it will use \niretq\n instead of \nsysexit\n to return to userspace.\n\n\nGot an insteresting IB bug after execve. The CPU5 seems fail to return to userspace, and the CPU0 has the IB bug followed:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n[\n \n1201.940681\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \nseq\n.\no\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n609\n\n\n[\n \n1202.006200\n]\n \nRIP\n:\n \n0033\n:\n[\n0000000000401\nd1d\n]\n  \n[\n0000000000401\nd1d\n]\n \n0x401d1d\n\n\n[\n \n1202.087320\n]\n \nRSP\n:\n \n002\nb\n:\n00007ff\nfffffedb0\n  \nEFLAGS\n:\n \n00000200\n\n\n[\n \n1202.150760\n]\n \nRAX\n:\n \n0000000000000000\n \nRBX\n:\n \n00000000004002e0\n \nRCX\n:\n \n000000000043\nb2c7\n\n\n[\n \n1202.236041\n]\n \nRDX\n:\n \n00007ff\nfffffedc8\n \nRSI\n:\n \n00007ff\nfffffeb40\n \nRDI\n:\n \n000000000048f9f\n0\n\n\n[\n \n1202.321320\n]\n \nRBP\n:\n \n00007ff\nfffffeb60\n \nR08\n:\n \n00000000006\nba4a0\n \nR09\n:\n \n00000000006\nbc880\n\n\n[\n \n1202.406601\n]\n \nR10\n:\n \n000000000000000f\n \nR11\n:\n \n0000000000000246\n \nR12\n:\n \n0000000000000000\n\n\n[\n \n1202.491881\n]\n \nR13\n:\n \n0000000000401\n930\n \nR14\n:\n \n0000000000401\n9\nc0\n \nR15\n:\n \n0000000000000006\n\n\n[\n \n1202.577161\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88207fc40000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n1202.673880\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n1202.742521\n]\n \nCR2\n:\n \n000000000042\nc9a0\n \nCR3\n:\n \n000000207f\nc2f000\n \nCR4\n:\n \n00000000000406\na0\n\n\n\n[\n \n1220.465601\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000020\n\n\n[\n \n1220.557225\n]\n \nIP\n:\n \n[\nffffffff810591ef\n]\n \nib_mad_completion_handler\n+\n0x6f\n/\n0x7c0\n\n\n[\n \n1220.638344\n]\n \nPGD\n \n0\n\n\n[\n \n1220.662265\n]\n \nOops\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n \n1220.710105\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n27\n \nComm\n:\n \nib_mad_completi\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n609\n\n\n[\n \n1220.786025\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff810591ef\n]\n  \n[\nffffffff810591ef\n]\n \nib_mad_completion_handler\n+\n0x6f\n/\n0x7c0\n\n\n[\n \n1220.896265\n]\n \nRSP\n:\n \n0000\n:\nffff88103eea7e30\n  \nEFLAGS\n:\n \n00010246\n\n\n[\n \n1220.959704\n]\n \nRAX\n:\n \n0000000000000000\n \nRBX\n:\n \nffff88103eeac728\n \nRCX\n:\n \n0000000000000001\n\n\n[\n \n1221.044985\n]\n \nRDX\n:\n \n000000002\n8000000\n \nRSI\n:\n \nffff88103ee8f000\n \nRDI\n:\n \nffff88107ff841d8\n\n\n[\n \n1221.130265\n]\n \nRBP\n:\n \nffff88103eea7ec0\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \nffff88103eea03c0\n\n\n[\n \n1221.215545\n]\n \nR10\n:\n \nffff88103eea7ea0\n \nR11\n:\n \n0000000000000001\n \nR12\n:\n \nffff88103ee8c3f0\n\n\n[\n \n1221.300825\n]\n \nR13\n:\n \nffff88103ee8c4e8\n \nR14\n:\n \nffff88103eeac620\n \nR15\n:\n \nffff88103eeac5f8\n\n\n[\n \n1221.386106\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc00000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n1221.482825\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n1221.551466\n]\n \nCR2\n:\n \n0000000000000020\n \nCR3\n:\n \n000000000113\nd000\n \nCR4\n:\n \n00000000000406\nb0\n\n\n[\n \n1221.636746\n]\n \nStack\n:\n\n\n[\n \n1221.660666\n]\n \nffff88103eeaac10\n \nffff881000000001\n \nffff88103eeaac10\n \nffff88103eeaab50\n\n\n[\n \n1221.748026\n]\n \nffff88107fc05d80\n \nffff88103eea0000\n \nffff88103eeac728\n \n000000\n8000000000\n\n\n[\n \n1221.835386\n]\n \n0000012\n83\neea7ea8\n \nffff88103ee8c9a8\n \n000000007f\ncf2000\n \nffff000000000000\n\n\n[\n \n1221.922746\n]\n \nffff88107fcf0000\n \nffff88207ff6cbd8\n \nffff88107fcf76e8\n \nffff88103ee8c3f0\n\n\n[\n \n1222.010106\n]\n \nffffffff81059180\n \n0000000000000000\n \nffff88103eea7f48\n \nffffffff81020866\n\n\n[\n \n1222.097466\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1222.126586\n]\n \nTSK\n\n\n[\n \n1222.149466\n]\n \n[\nffffffff81059180\n]\n \n?\n \nib_mad_send_done_handler\n.\nisra\n.21\n+\n0x1d0\n/\n0x1d0\n\n\n[\n \n1222.236826\n]\n \n[\nffffffff81020866\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n \n1222.295066\n]\n \n[\nffffffff81020770\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n \n1222.363707\n]\n \n[\nffffffff8100e4b2\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n\n\n\n1\n2\n3\n4\n5\n[\nroot\n@\nwuklab12\n:\n \nLegoOS\n \ngit\n:(\nmaster\n)]\n \n$\n \naddr2line\n \n-\ne\n \nvmImage\n  \n-\ni\n \nffffffff810591ef\n\n\n/\nroot\n/\nys\n/\nLegoOS\n/\ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n1899\n\n\n/\nroot\n/\nys\n/\nLegoOS\n/\ndrivers\n/\ninfiniband\n/\ncore\n/\nmad\n.\nc\n:\n2324\n\n\n\nIt\n \nis\n \nib_mad_recv_done_handler\n()\n\n\n\n\n\n\n\nWell\n\n\nEventually, at 22:09, I figured out..\n\n\nAfter I cleaned up all registers (except IP, SP, CS, SS, FLAGS) within start_thread, the execve\ned program can run to end successfully.\n\n\nI did not clear the registers because linux does not clear it. I thought this is fine. Glibc should clear it anyway, right?\n\n\nBut anyway, this works.\n\n\n03/21 Wed\n\n\n\n\nTask 1: add some checking in ib, flush, sweep thread. 1) If cpu changed, 2) if nr_threads on this core \n 1.\n\n\n\n\nHad an issue while testing: execve(). I ran a exec.o first, then do execve to run seq.o:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\nwuklab13\n \n0321\n-\n10\n\n\n[\n  \n970.380252\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nuname\n()\n:\n\n\n\n---\n\n\n[\n  \n970.431212\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x44605d\n \nflags\n:\n0x150\n\n\n[\n \n1101.862429\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n21\n\n\n[\n \n1101.915570\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1101.969649\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n12\n\n\n[\n \n1102.029968\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1102.084046\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1102.138125\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1102.192203\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1054\n\n\n[\n \n1102.256681\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1055\n\n\n[\n \n1102.321160\n]\n \ncsum\n:\n \n442\na97c0\n,\n \nreply\n-\ncsum\n:\n \n2\nd352c33\n\n\n[\n \n1102.377319\n]\n \nfit_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n1\n\n\n[\n \n1102.444916\n]\n \npcache\n:\nffff880180011180\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880100446000\n\n\n[\n \n1102.558273\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1056\n\n\n[\n \n1102.622751\n]\n \npcache\n \ndumped\n \nbecause\n:\n \ncsum\n \nmismatch\n\n\n[\n \n1102.677871\n]\n \nfit_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n \n1102.749627\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1102.804746\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1057\n\n\n[\n \n1102.869225\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nfault\n.\nc\n:\n237\n/\n__pcache_do_fill_page\n()\n!\n\n\n[\n \n1102.968022\n]\n \nfit_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n \n1103.039780\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1103.090739\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \nseq\n.\no\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n599\n\n\n[\n \n1103.156256\n]\n \nStack\n:\n\n\n[\n \n1103.180177\n]\n \nffff88103e85be18\n \nffffffff8102676c\n \nffffffff00000008\n \nffff88103e85be28\n\n\n[\n \n1103.267533\n]\n \nffff88103e85bde0\n \n0000000021475542\n \n00000000000002\n96\n \nffff88103e85ba10\n\n\n[\n \n1103.354892\n]\n \nffffffff810195c5\n \nffff88207fc44980\n \n0000000000000005\n \nffff88103e85ba28\n\n\n[\n \n1103.442249\n]\n \nffffffff81016c75\n \nffff88103e85ba40\n \nffff88103e85ba50\n \nffffffff810065d4\n\n\n[\n \n1103.529607\n]\n \nffffffff811d36e0\n \n000000000000003\n9\n \nffffffff81081718\n \nffff88103e85bb80\n\n\n[\n \n1103.616964\n]\n \nCall\n \nTrace\n:\n\n\n\n\n\n\n03/20 Tue\n\n\n\n\nTask 1: calculate failure numbers\n\n\nTask 2: read 0319-4 Log\n\n\nTask 3: opt pte lock\n\n\n\n\nHmm, I finished the per-pte per-pmd lock patch. I think it works.\nBut I do found an issue. When I run MT+2GB, it will create 24 threads. Since I marked 3 CPUs inactive, so all new 24 threads will be scheduled to other cores (I may need to check this!). At some point, Lego P either stuck, or a lot ibapi_send_reply timeout.\n\n\nWhen I change the cpu_online to may \n0-6\n, it finished. When I change it to \n0-18\n, also succeed. I really doubt if actually those pinned threads are not pinned. Need to check.\n\n\nIB Bug again\n\n\nRunning MT-phoenix, 2GB, somehow crashed in the middle:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n[\n60095.857381\n]\n \nSYSC_close\n()\n \nCPU6\n \nPID\n:\n33\n \n[\nfd\n:\n \n4\n]\n \n-\n \n[\n/\nproc\n/\nstat\n]\n\n\n\n[\n60286.127359\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n21\n\n\n[\n60286.180503\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.234582\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n12\n\n\n[\n60286.294903\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.348981\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.403062\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.457141\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.511221\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60286.570500\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1056\n\n\n[\n60286.634980\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.689059\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1057\n\n\n[\n60286.753539\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60286.812819\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1058\n\n\n[\n60286.877298\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60286.931378\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1059\n\n\n[\n60286.995857\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60287.055138\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.109217\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.163297\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.217376\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.271456\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.325536\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60287.384815\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1060\n\n\n[\n60287.449294\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.503375\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n           \n(\nnull\n)\n\n\n[\n60287.596973\n]\n \nIP\n:\n \n[\nffffffff81063ffd\n]\n \nfit_poll_cq\n+\n0x4dd\n/\n0x530\n\n\n[\n60287.664574\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60287.723853\n]\n \nPGD\n \n0\n\n\n[\n60287.747772\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60287.801852\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nPREEMPT\n \nSMP\n \nPROCESSOR\n\n\n[\n60287.858013\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60287.917292\n]\n \nCPU\n:\n \n2\n \nPID\n:\n \n29\n \nComm\n:\n \nrecvpollcq\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n569\n\n\n[\n60287.988010\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81063ffd\n]\n  \n[\nffffffff81063ffd\n]\n \nfit_poll_cq\n+\n0x4dd\n/\n0x530\n\n\n[\n60288.084731\n]\n \nRSP\n:\n \n0000\n:\nffff88103e84fd88\n  \nEFLAGS\n:\n \n00010206\n\n\n[\n60288.148170\n]\n \nRAX\n:\n \n000000000000100\n8\n \nRBX\n:\n \nffff88103e848438\n \nRCX\n:\n \n0000000000000014\n\n\n[\n60288.233450\n]\n \nRDX\n:\n \n0000000000000000\n \nRSI\n:\n \nffffffff811d36e0\n \nRDI\n:\n \nffffffff811dac08\n\n\n[\n60288.318728\n]\n \nRBP\n:\n \nffff88103e84fea8\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \n0000000000000000\n\n\n[\n60288.404008\n]\n \nR10\n:\n \n0000000000000002\n \nR11\n:\n \n0000000000000004\n \nR12\n:\n \n0000000000000000\n\n\n[\n60288.489288\n]\n \nR13\n:\n \nffff88207fd6e008\n \nR14\n:\n \n0000000000000004\n \nR15\n:\n \nffff88103e84fda0\n\n\n[\n60288.574568\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60288.628647\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc20000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n60288.725367\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n60288.794006\n]\n \nCR2\n:\n \n0000000000000000\n \nCR3\n:\n \n000000000113\nd000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n60288.879285\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60288.938565\n]\n \nStack\n:\n\n\n[\n60288.962484\n]\n \nffffffff810031d9\n \n000\n801\nd43e84fda0\n \n0000000000000007\n \n0000000000000424\n\n\n[\n60289.049844\n]\n \n000000\n8100000005\n \n00001008000000f\n9\n \nffff88103e848868\n \n00616e6440000014\n\n\n[\n60289.137204\n]\n \n0020004000000002\n \nffff88207fc00000\n \n0000000000000425\n \n000000\n8100000005\n\n\n[\n60289.224563\n]\n \n00001008000000f\n9\n \nffff88103e848868\n \n007370654000000\nd\n \n0010004000000002\n\n\n[\n60289.311922\n]\n \nffffffff81010000\n \n0000000000000426\n \n000000\n8100000005\n \n00001008000000f\n9\n\n\n[\n60289.399282\n]\n \nCall\n \nTrace\n:\n\n\n[\n60289.428402\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60289.482482\n]\n \nTSK\n\n\n[\n60289.505361\n]\n \n[\nffffffff810031d9\n]\n \n?\n \nnative_smp_send_reschedule\n+\n0x39\n/\n0x50\n\n\n[\n60289.584400\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n5\n\n\n[\n60289.643680\n]\n \n[\nffffffff81010000\n]\n \n?\n \n__ioremap_caller\n+\n0x170\n/\n0x570\n\n\n[\n60289.714400\n]\n \n[\nffffffff81060000\n]\n \n?\n \ncm_work_handler\n+\n0x270\n/\n0x1450\n\n\n[\n60289.785119\n]\n \n[\nffffffff81064050\n]\n \n?\n \nfit_poll_cq\n+\n0x530\n/\n0x530\n\n\n[\n60289.850639\n]\n \n[\nffffffff81064064\n]\n \nfit_poll_cq_pass\n+\n0x14\n/\n0x30\n\n\n[\n60289.917198\n]\n \n[\nffffffff81020c06\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n60289.975438\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n60290.029518\n]\n \n[\nffffffff81020b10\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n60290.098157\n]\n \n[\nffffffff8100e722\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n\n\n\nUuh:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n[\n \n1002.803051\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n1\n\n\n[\n \n1002.855153\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1002.909232\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1002.963310\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n1003.017390\n]\n \nfit_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n1\n)\n \nfor\n \nwr_id\n \n512\n\n\n[\n \n1003.080829\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000200\n\n\n[\n \n1003.174425\n]\n \nIP\n:\n \n[\nffffffff8105d499\n]\n \nfit_poll_cq\n+\n0x179\n/\n0x510\n\n\n[\n \n1003.242024\n]\n \nPGD\n \n0\n\n\n[\n \n1003.265943\n]\n \nOops\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nMEMORY\n\n\n[\n \n1003.310661\n]\n \nCPU\n:\n \n2\n \nPID\n:\n \n29\n \nComm\n:\n \nrecvpollcq\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n149\n\n\n[\n \n1003.381380\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8105d499\n]\n  \n[\nffffffff8105d499\n]\n \nfit_poll_cq\n+\n0x179\n/\n0x510\n\n\n[\n \n1003.478098\n]\n \nRSP\n:\n \n0000\n:\nffff88104e84fd88\n  \nEFLAGS\n:\n \n00010246\n\n\n[\n \n1003.541537\n]\n \nRAX\n:\n \nffff880000000000\n \nRBX\n:\n \nffff88104e848008\n \nRCX\n:\n \n00000000000000\n80\n\n\n[\n \n1003.626814\n]\n \nRDX\n:\n \n0000000000000200\n \nRSI\n:\n \nffffffff811c76e0\n \nRDI\n:\n \nffffffff811d0988\n\n\n[\n \n1003.712092\n]\n \nRBP\n:\n \nffff88104e84fea8\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \n0000000000000000\n\n\n[\n \n1003.797369\n]\n \nR10\n:\n \n0000000000000002\n \nR11\n:\n \n0000000000000004\n \nR12\n:\n \n0000000000000000\n\n\n[\n \n1003.882648\n]\n \nR13\n:\n \nffff88207ff75008\n \nR14\n:\n \n0000000000000004\n \nR15\n:\n \nffff88104e84fda0\n\n\n[\n \n1003.967925\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc20000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n1004.064644\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n1004.133282\n]\n \nCR2\n:\n \n0000000000000200\n \nCR3\n:\n \n0000000001131000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n \n1004.218559\n]\n \nStack\n:\n\n\n[\n \n1004.242479\n]\n \nffffffff810031a9\n \nffff88104e84fda0\n \nffffffff81018ef4\n \n0000000000000200\n\n\n[\n \n1004.329837\n]\n \n000000\n8000000001\n \n0000004\n8000000\nd7\n \nffff88104e848c98\n \n00000000\n81019302\n\n\n[\n \n1004.417194\n]\n \n0014000000000000\n \nffff88207fc00000\n \n0000000000000201\n \nffffffff00000005\n\n\n[\n \n1004.504552\n]\n \nffff8810000000f9\n \nffff88104e848c98\n \n0000000000000000\n \nffff88104e84fe38\n\n\n[\n \n1004.591910\n]\n \nffffffff810195a4\n \n0000000000000202\n \nffff881000000005\n \nffff8810000000f9\n\n\n[\n \n1004.679268\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1004.708388\n]\n \nTSK\n\n\n[\n \n1004.731267\n]\n \n[\nffffffff810031a9\n]\n \n?\n \nnative_smp_send_reschedule\n+\n0x39\n/\n0x50\n\n\n[\n \n1004.810305\n]\n \n[\nffffffff81018ef4\n]\n \n?\n \nresched_curr\n+\n0x34\n/\n0x40\n\n\n[\n \n1004.874783\n]\n \n[\nffffffff810195a4\n]\n \n?\n \ntry_to_wake_up\n+\n0xe4\n/\n0x1f0\n\n\n[\n \n1004.942382\n]\n \n[\nffffffff8105f458\n]\n \n?\n \n__schedule\n+\n0xf8\n/\n0x1e0\n\n\n[\n \n1005.005820\n]\n \n[\nffffffff8105d830\n]\n \n?\n \nfit_poll_cq\n+\n0x510\n/\n0x510\n\n\n[\n \n1005.071338\n]\n \n[\nffffffff8105d844\n]\n \nfit_poll_cq_pass\n+\n0x14\n/\n0x30\n\n\n[\n \n1005.137897\n]\n \n[\nffffffff8101fdc6\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n \n1005.196135\n]\n \n[\nffffffff8101fcd0\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n \n1005.264773\n]\n \n[\nffffffff8100e472\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n\n\n\n03/19 Mon\n\n\nNot too many days left!!! Got to design full replication mechanism and algorithm today.\n\n\nMerged pull request for \npipe\n, \npipe2\n and \n/dev/null\n from Yilun. Our simple file op mgmt concerns me. I left a note at kernel/fork.c.\n\n\nGot a bug report from Yilun, syscall execv failed. To be honest, I\nve never tried this syscall, always call it directly within kernel.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n[\n  \n943.650712\n]\n \nCPU6\n \nPID17\n \nsys_execve\n+\n0x0\n/\n0x10\n\n\n[\n  \n943.701899\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \n00000000004\n90523\n\n\n[\n  \n943.702776\n]\n \nIP\n:\n \n[\nffffffff8103db86\n]\n \nstrrchr\n+\n0x6\n/\n0x20\n\n\n[\n  \n943.711501\n]\n \nPGD\n \n0\n\n\n[\n  \n943.711911\n]\n \nOops\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n943.712433\n]\n \nCPU\n:\n \n6\n \nPID\n:\n \n17\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n+\n \n#\n64\n\n\n[\n  \n943.713126\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8103db86\n]\n  \n[\nffffffff8103db86\n]\n \nstrrchr\n+\n0x6\n/\n0x20\n\n\n[\n  \n943.714090\n]\n \nRSP\n:\n \n001\n8\n:\nffff88083e4bfe98\n  \nEFLAGS\n:\n \n00010246\n\n\n[\n  \n943.714724\n]\n \nRAX\n:\n \n0000000000000000\n \nRBX\n:\n \nffff88083e4b3780\n \nRCX\n:\n \n0000000000000000\n\n\n[\n  \n943.715511\n]\n \nRDX\n:\n \n00000000ff\nffffff\n \nRSI\n:\n \n000000000000002f\n \nRDI\n:\n \n00000000004\n90523\n\n\n[\n  \n943.716297\n]\n \nRBP\n:\n \nffff88083e4bfe98\n \nR08\n:\n \n0000160000000000\n \nR09\n:\n \nffff88083e4b8400\n\n\n[\n  \n943.717085\n]\n \nR10\n:\n \nffff880000000000\n \nR11\n:\n \n6\ndb6db6db6db6db7\n \nR12\n:\n \nffff88083e4b8000\n\n\n[\n  \n943.717871\n]\n \nR13\n:\n \nffff88083e4e6290\n \nR14\n:\n \n00000000004\n90523\n \nR15\n:\n \nffff88083e4b3920\n\n\n[\n  \n943.718683\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88083fd80000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n943.719650\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n943.720319\n]\n \nCR2\n:\n \n00000000004\n90523\n \nCR3\n:\n \n000000083e4\ne7000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n  \n943.721106\n]\n \nStack\n:\n\n\n[\n  \n943.721459\n]\n \nffff88083e4bff18\n \nffffffff8102c6bf\n \nffff880800000000\n \n0000000000000e10\n\n\n[\n  \n943.722541\n]\n \n00007ff\nfffffedb0\n \n0000000000400\nd0d\n \nffff88083e4c0000\n \n00000000004\n90523\n\n\n[\n  \n943.723624\n]\n \nffff88083e4b9008\n \n00007ff\nfffffed30\n \n000000\n8400000084\n \nffff88083e4bff58\n\n\n[\n  \n943.724706\n]\n \n000000000000003\nb\n \n0000000000401\n9\nd0\n \n0000000000401\na60\n \n0000000000000000\n\n\n[\n  \n943.725789\n]\n \nffff88083e4bff28\n \nffffffff8102c989\n \nffff88083e4bff48\n \nffffffff8100e5f5\n\n\n[\n  \n943.726870\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n943.727260\n]\n \nTSK\n\n\n[\n  \n943.727619\n]\n \n[\nffffffff8102c6bf\n]\n \ndo_execve\n+\n0x4af\n/\n0x770\n\n\n[\n  \n943.728236\n]\n \n[\nffffffff8102c989\n]\n \nsys_execve\n+\n0x9\n/\n0x10\n\n\n[\n  \n943.728868\n]\n \n[\nffffffff8100e5f5\n]\n \ndo_syscall_64\n+\n0x45\n/\n0xd0\n\n\n[\n  \n943.729499\n]\n \n[\nffffffff8100d4ec\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n  \n943.730222\n]\n \nEOT\n\n\n[\n  \n943.730570\n]\n \nCode\n:\n \nd2\n \n74\n \n18\n \n40\n \n38\n \nf2\n \n89\n \nf1\n \n75\n \n06\n \neb\n \n0f\n \n38\n \nca\n \n74\n \n0\nb\n \n48\n \n83\n \nc0\n \n01\n \n0f\n \nb6\n \n10\n \n84\n \nd2\n \n75\n \nf1\n \n5\nd\n \nc3\n \n0f\n \n1f\n \n84\n \n00\n \n00\n \n00\n \n00\n \n00\n \n55\n \n31\n \nc0\n \n48\n \n89\n \ne5\n \n0f\n \nb6\n \n17\n \n40\n \n38\n \nf2\n \n48\n \n0f\n \n44\n \nc7\n \n48\n \n83\n \nc7\n \n01\n \n84\n \nd2\n \n75\n \nee\n \n5\nd\n \nc3\n \n66\n\n\n[\n  \n943.735455\n]\n \nRIP\n  \n[\nffffffff8103db86\n]\n \nstrrchr\n+\n0x6\n/\n0x20\n\n\n[\n  \n943.736120\n]\n  \nRSP\n \nffff88083e4bfe98\n\n\n[\n  \n943.736598\n]\n \nCR2\n:\n \n00000000004\n90523\n\n\n\n\n\n\nIt is \nsetup_new_exec() -\n set_task_comm()\n. I passed the user pointer to \nset_task_comm()\n, which I should pass a kernel pointer.\n\n\nAnd I actually found we missed a function: \ndo_close_on_exec()\n. I also add a note above.\n\n\nRandom IB Bug\n\n\nAnother weird bug after pathing loader. Actually, I tried the same setting twice, the second time it works. I guess this is some random IB bug. (Setting: 1P, 1M, 1S. Running a simple exec.c testing program, this have not reach that point yet.)\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\nwuklab13\n \n031\n9\n-\n2\n\n\n[\n  \n496.288272\n]\n \np2m_fork\n(\ncpu0\n)\n:\n \nI\n \ncur\n:\n1\n-\nkernel_init\n \nnew\n:\n31\n\n\n[\n  \n496.349624\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000004\n\n\n[\n  \n496.443216\n]\n \nIP\n:\n \n[\nffffffff81064935\n]\n \nfit_send_reply_with_rdma_write_with_imm\n+\n0x65\n/\n0x3b0\n\n\n[\n  \n496.538892\n]\n \nPGD\n \n0\n\n\n[\n  \n496.562811\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nPREEMPT\n \nSMP\n \nPROCESSOR\n\n\n[\n  \n496.618968\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n1\n \nComm\n:\n \nkernel_init\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n559\n\n\n[\n  \n496.689684\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81064935\n]\n  \n[\nffffffff81064935\n]\n \nfit_send_reply_with_rdma_write_with_imm\n+\n0x65\n/\n0x3b0\n\n\n[\n  \n496.814478\n]\n \nRSP\n:\n \n0000\n:\nffff88107fcf7d00\n  \nEFLAGS\n:\n \n00010202\n\n\n[\n  \n496.877915\n]\n \nRAX\n:\n \n000000000000004\nc\n \nRBX\n:\n \n0000000000000004\n \nRCX\n:\n \n000000000000002\nc\n\n\n[\n  \n496.963190\n]\n \nRDX\n:\n \n0000000000000004\n \nRSI\n:\n \n0000000000000001\n \nRDI\n:\n \nffff88207ff6d008\n\n\n[\n  \n497.048466\n]\n \nRBP\n:\n \nffff88107fcf7d98\n \nR08\n:\n \nffff88107fcf7e3c\n \nR09\n:\n \n0000000000000004\n\n\n[\n  \n497.133742\n]\n \nR10\n:\n \nffffffff81145fe0\n \nR11\n:\n \n000000000000001\nc\n \nR12\n:\n \n000000000000002\nc\n\n\n[\n  \n497.219018\n]\n \nR13\n:\n \n0000000000000001\n \nR14\n:\n \nffff88107fcf7e40\n \nR15\n:\n \nffff88207ff6d008\n\n\n[\n  \n497.304293\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc00000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n497.401009\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n497.469645\n]\n \nCR2\n:\n \n0000000000000004\n \nCR3\n:\n \n000000000113\nd000\n \nCR4\n:\n \n00000000000406\nb0\n\n\n[\n  \n497.554922\n]\n \nStack\n:\n\n\n[\n  \n497.578840\n]\n \nffff88107fcf7d08\n \n0000000000000000\n \n00000000000002\n82\n \nffffffff81077b10\n\n\n[\n  \n497.666195\n]\n \n000000000000003\na\n \n000000047f\ncf7e18\n \nffff88107fcf7e3c\n \nffff88107fd5ed88\n\n\n[\n  \n497.753552\n]\n \n000000010000002\nc\n \nffffff9b00000040\n \n0000000000000034\n \nffffffff81145fe0\n\n\n[\n  \n497.840906\n]\n \nffff88107fcf7db0\n \n00000000000002\n97\n \nffff88107fd5ed88\n \n000000000000002\nc\n\n\n[\n  \n497.928263\n]\n \nffff88107fcf7e3c\n \nffff88107fcf7e40\n \n000000000000003\n9\n \nffff88107fcf7dc8\n\n\n[\n  \n498.015618\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n498.044736\n]\n \nTSK\n\n\n[\n  \n498.067615\n]\n \n[\nffffffff810622ff\n]\n \nibapi_send_reply_timeout\n+\n0x3f\n/\n0x50\n\n\n[\n  \n498.142492\n]\n \n[\nffffffff8103b0d4\n]\n \n?\n \nnet_send_reply_timeout\n+\n0x94\n/\n0x132\n\n\n[\n  \n498.218408\n]\n \n[\nffffffff8103b0d4\n]\n \nnet_send_reply_timeout\n+\n0x94\n/\n0x132\n\n\n[\n  \n498.292244\n]\n \n[\nffffffff8102c683\n]\n \np2m_fork\n+\n0xd3\n/\n0x200\n\n\n[\n  \n498.351521\n]\n \n[\nffffffff8101f490\n]\n \ndo_fork\n+\n0xf0\n/\n0x150\n\n\n[\n  \n498.409758\n]\n \n[\nffffffff8101f514\n]\n \nkernel_thread\n+\n0x24\n/\n0x30\n\n\n[\n  \n498.473195\n]\n \n[\nffffffff8115bf21\n]\n \nprocessor_manager_init\n+\n0x21\n/\n0x50\n\n\n[\n  \n498.545991\n]\n \n[\nffffffff81000354\n]\n \nkernel_init\n+\n0x94\n/\n0x120\n\n\n[\n  \n498.608388\n]\n \n[\nffffffff810002c0\n]\n \n?\n \n0xffffffff810002c0\n\n\n[\n  \n498.668706\n]\n \n[\nffffffff81019b0a\n]\n \n?\n \nschedule_tail\n+\n0xa\n/\n0x40\n\n\n[\n  \n498.733182\n]\n \n[\nffffffff810002c0\n]\n \n?\n \n0xffffffff810002c0\n\n\n[\n  \n498.793499\n]\n \n[\nffffffff8100e762\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n[\n  \n498.856936\n]\n \nEOT\n\n\n\n\n\n\n03/18 Sun\n\n\nGot a bug report after enable preempt and sweep thread\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n[\n  \n582.545444\n]\n \npcache\n:\nffff8801812cb680\n \nmapcount\n:\n1\n \nrefcount\n:\n2\n \nflags\n:(\nlocked\n|\nallocated\n|\nusable\n|\nvalid\n|\nreclaim\n)\n \nkva\n:\n \nffff88014b2da000\n\n\n[\n  \n582.678677\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_PCM\n(\npcache_mapped\n(\npcm\n))\n\n\n[\n  \n582.758760\n]\n \nrmap\n:\nffff88207e5e37e8\n \nflags\n:\n0x0\n \nowner\n-\ntgid\n:\n33\n \nuser_va\n:\n0x7fff0b2da000\n \nptep\n:\nffff88207e4a86d0\n\n\n[\n  \n582.870046\n]\n \npte\n:\nffff88207e4a86d0\n \npfn\n:\n0x0\n \nflags\n:()\n\n\n[\n  \n582.926210\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n582.981333\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n604\n/\nvictim_finish_insert\n()\n!\n\n\n[\n  \n583.080137\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n...\n\n\n...\n\n\n[\n  \n588.847239\n]\n \nnr_pgfault\n:\n \n591101\n\n\n[\n  \n588.883641\n]\n \nnr_clflush\n:\n \n66176\n\n\n[\n  \n588.919003\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n588.953325\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n588.991806\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n589.032368\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n589.091651\n]\n \nnr_pcache_fill_from_memory\n:\n \n587057\n\n\n[\n  \n589.144694\n]\n \nnr_pcache_fill_from_victim\n:\n \n4038\n\n\n[\n  \n589.195656\n]\n \nnr_pcache_eviction_triggered\n:\n \n439562\n\n\n[\n  \n589.250780\n]\n \nnr_pcache_eviction_eagain_freeable\n:\n \n373382\n\n\n[\n  \n589.312143\n]\n \nnr_pcache_eviction_eagain_concurrent\n:\n \n0\n\n\n[\n  \n589.370386\n]\n \nnr_pcache_eviction_failure_find\n:\n \n0\n\n\n[\n  \n589.423429\n]\n \nnr_pcache_eviction_failure_evict\n:\n \n0\n\n\n[\n  \n589.477512\n]\n \nnr_pcache_eviction_succeed\n:\n \n66176\n\n\n[\n  \n589.529514\n]\n \nnr_victim_eviction_triggered\n:\n \n733361\n\n\n[\n  \n589.584638\n]\n \nnr_victim_eviction_eagain\n:\n \n671227\n\n\n[\n  \n589.636640\n]\n \nnr_victim_eviction_succeed\n:\n \n62134\n\n\n[\n  \n589.688642\n]\n \nnr_victim_prepare_insert\n:\n \n66180\n\n\n[\n  \n589.738566\n]\n \nnr_victim_finish_insert\n:\n \n66176\n\n\n[\n  \n589.787447\n]\n \nnr_victim_flush_submitted\n:\n \n66176\n\n\n[\n  \n589.838411\n]\n \nnr_victim_flush_finished\n:\n \n66176\n\n\n[\n  \n589.888332\n]\n \nnr_victim_flush_async_run\n:\n \n0\n\n\n[\n  \n589.935135\n]\n \nnr_victim_flush_sync\n:\n \n0\n\n\n[\n  \n589.976738\n]\n \nnr_sweep_run\n:\n \n50580\n\n\n[\n  \n590.014179\n]\n \nnr_sweep_nr_pset\n:\n \n116770383\n\n\n[\n  \n590.059943\n]\n \nnr_sweep_nr_moved_pcm\n:\n \n100686435\n\n\n\n\n\n\nThis is an interesting bug. Two threads, one doing munmap or mremap, one doing eviction. They are using the same pcm. munmap and mremap will use \npte_get_and_clear()\n to get the pcm.  While eviction will call \npcache_try_to_unamp\n, which will further call \nrmap_get_locked_pte()\n, in which we check if the pte is none, if it is, then we know this is under munmap or mremap, then we skip. This is absolutely wrong. When \npcache_try_to_unamp\n is called by eviction, it should always unmap ALL rmap. The above case is triggered because both two threads skip the final \n__pcache_remove_rmap\n.\n\n\nHmm, looks like open/close filename is wrong. I need to check.\n\n\nLast Log from MT+2GB, computation finished:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\nwuklab13\n \n031\n8\n-\n10\n\n\n[\n  \n627.280016\n]\n\n\n****\n    \nERROR\n:\n\n\n***\n     \ncurrent\n:\n \n32\n:\nkevict_sweepd\n \ncaller\n:\n           \n(\nnull\n)\n\n\n****\n    \n[\npte\n \n==\n \nrmap\n-\npage_table\n]\n \n \n[\npcache_pfn\n \n!=\n \npte_pfn\n]\n\n\n****\n    \nrmap\n-\nowner_process\n:\n \nword_count\n-\npthr\n \nuva\n:\n \n0x7fff78f52000\n \nptep\n:\n \nffff88107e87fa90\n,\n \nrmap\n-\npage_table\n:\n \nffff88107e87fa90\n\n\n****\n    \npcache_pfn\n:\n \n0x168f52\n,\n \npte_pfn\n:\n \n0x178f52\n\n\n\n[\n  \n627.624239\n]\n \nrmap\n:\nffff88107dc73740\n \nflags\n:\n0x0\n \nowner\n-\ntgid\n:\n33\n \nuser_va\n:\n0x7fff78f52000\n \nptep\n:\nffff88107e87fa90\n\n\n[\n  \n627.735513\n]\n \npte\n:\nffff88107e87fa90\n \npfn\n:\n0x0\n \nflags\n:()\n\n\n[\n  \n627.791670\n]\n \npcache_rmap\n \ndumped\n \nbecause\n:\n \nCorrupted\n \nRMAP\n\n\n[\n  \n627.853026\n]\n \npcache\n:\nffff880181a3d480\n \nmapcount\n:\n1\n \nrefcount\n:\n2\n \nflags\n:(\nlocked\n|\nallocated\n|\nusable\n|\nvalid\n)\n \nkva\n:\n \nffff880168f52000\n\n\n[\n  \n627.979901\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nCorrupted\n \nRMAP\n\n\n[\n  \n628.036057\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n628.091175\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n109\n/\nreport_bad_rmap\n()\n!\n\n\n[\n  \n628.182691\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n  \n628.233647\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \nkevict_sweepd\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n543\n\n\n[\n  \n628.307483\n]\n \nStack\n:\n\n\n[\n  \n628.331401\n]\n \nffff88107e85bd00\n \nffffffff81026d24\n \n000000000000000\n8\n \nffff88107e85bd10\n\n\n[\n  \n628.418756\n]\n \nffff88107e85bcc8\n \n0000000021475542\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n628.506113\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n628.593468\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n628.680823\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n628.768179\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n628.797299\n]\n \nTSK\n\n\n[\n  \n628.820176\n]\n \n[\nffffffff81026d30\n]\n \npanic\n+\n0xc2\n/\n0x102\n\n\n[\n  \n628.876334\n]\n \n[\nffffffff8101c6ac\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n  \n628.940811\n]\n \n[\nffffffff8101c6ac\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n  \n629.005288\n]\n \n[\nffffffff81019bfc\n]\n \n?\n \nscheduler_tick\n+\n0x5c\n/\n0x70\n\n\n[\n  \n629.071843\n]\n \n[\nffffffff81017195\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n  \n629.144639\n]\n \n[\nffffffff81006704\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n  \n629.217436\n]\n \n[\nffffffff8100e4da\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n629.295432\n]\n \n[\nffffffff81012d94\n]\n \n?\n \nprintk\n+\n0x124\n/\n0x1c0\n\n\n[\n  \n629.355748\n]\n \n[\nffffffff8103ad1f\n]\n \nreport_bad_rmap\n+\n0x144\n/\n0x144\n\n\n[\n  \n629.423345\n]\n \n[\nffffffff81031046\n]\n \npcache_referenced_trylock_one\n+\n0x1c6\n/\n0x2c0\n\n\n[\n  \n629.505500\n]\n \n[\nffffffff8100e4da\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n629.583497\n]\n \n[\nffffffff810328a1\n]\n \nrmap_walk\n+\n0x71\n/\n0xe0\n\n\n[\n  \n629.642774\n]\n \n[\nffffffff81033329\n]\n \npcache_referenced_trylock\n+\n0x59\n/\n0xd0\n\n\n\n\n\n\n03/17 Sat\n\n\nI\nm too tired today.\n\n\nCoding side, I will only optimize sweep. Besides, I will book tickets for Iceland trip.\n\n\n03/16 Friday\n\n\nTask 1\n: Add physical memory counter. It is a per-zone based counter, even though there is also some global counters. In Linux, per-cpu counter is first accumlated, global counter is updated only when per-cpu ones overflow. Lego\ns initial version save the trouble of per-cpu counter, I only port one global counter today, because I\nm not quite confident about our percpu_alloc\n\n\nAnway, the info is reported in the format of \nmanager_sysinfo\n. Do note this is different from the oirginal \nsysinfo\n structure, which is used by sysinfo syscall.\n\n\nTask 2\n: Patch get_random_number and /dev/urandom /dev/random. Others wrote the code, but he did not stick to the tradition of format naming. So I have to rewrite some of them. Sigh.\n\n\nTask 3\n: optimize sweep\n\n\n03/15 Thur\n\n\nForgot to write the log yesterday. I actually solved the major bug, the refcount and eviction one. That is really nasty. I basically used pte lock, pcache_lock, and refcount to synchronize between eviction routine and other users such as munmap, mremap, write-protected-handler.\n\n\nI\nm really not sure if this mode can be reproduced if I have any other similar systems. But I\nm glad that I find a way to do this.\n\n\nToday I got few tasks going on. First merge storage syscall branch, then add sched_yield syscall, add zone/node counters, and probably patch get_random_number.\n\n\nTask 1\n: Merge Yilun\ns storage pull request, has bunch syscalls. I\nm reviewing now.\n\n\n\n\ntruncate\n\n\nftruncate\n\n\ngetdents\n\n\ngetcwd\n\n\nmkdir\n\n\nrmdir\n\n\ncreat\n\n\nunlink\n\n\nunlinkat\n\n\nreadlink\n\n\nstatfs\n\n\nsync\n\n\n\n\nTask 2\n: Add \nsched_yield()\n. Fairly simple.\n\n\nTask 3\n: Add physical memory counter. Fairly complex. The underlying is built long time ago. Need to pick up some. Well some facts:\n\n\n\n\npg_data_t (and zone) is allcoated by alloc_node_data if NUMA is configured.\n\n\nall zones are built and initliazed in memory_init() in Lego\n\n\nstats are reset to 0 when pg_data_t allocated (DUH?). Played directly in page_alloc.c\n\n\n\n\nHave to continue tomorrow.\n\n\nTask 4\n: Patch get_random_number and /dev/urandom\n\n\n03/13 Wed\n\n\nThe slow victim flush issue is solved by pinning the thread to a core and remove that core from active_cpu mask.\n\n\nToday I\nm going to solve the SMP object issue. I\nm hoping by solving this, we can have a complete working pcache and victim cache.\n\n\nContinue yesterday\ns log:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nwuklab13\n \n0313\n-\n12\n\n\n[\n \n1073.616269\n]\n \npcache\n:\nffff880180777a80\n \nmapcount\n:\n0\n \nrefcount\n:\n3\n \nflags\n:(\nlocked\n|\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff88011ddea000\n\n\n[\n \n1073.734941\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n \ntsk\n:\n \n32\n \nuser_va\n:\n \n0x7fff4ddea000\n\n\n[\n \n1073.822304\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict\n/\nref\n \nbug\n\n\n\n[\n \n1073.987667\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nevict\n.\nc\n:\n301\n/\npcache_evict_line\n()\n!\n\n\n\n[\n \n1074.082308\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n763\n/\npcache_zap_pte\n()\n!\n\n\n[\n \n1074.172789\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1074.223751\n]\n \nCPU\n:\n \n23\n \nPID\n:\n \n50\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n476\n\n\n\n\n\n\n\n\n\n\n\n\nTime\n\n\nCPU0\n\n\nCPU1\n\n\n\n\n\n\n\n\n\n\n0\n\n\npcache_evict_line()\n\n\nzap_pte_range()\n\n\n\n\n\n\n1\n\n\nfind @pcm to evict\n\n\nprepare to unmap pte which points to @pcm\n\n\n\n\n\n\n2\n\n\nlock_pcache()\n\n\n..\n\n\n\n\n\n\n3\n\n\npcache_try_to_unmap()\n\n\npte_offset_lock()\n\n\n\n\n\n\n4\n\n\ntry to lock pte\n\n\npcache_zap_pte()\n\n\n\n\n\n\n5\n\n\n..spin..\n\n\ntrylock_pcache (failed)\n\n\n\n\n\n\n6\n\n\n..spin..\n\n\nunlock pte\n\n\n\n\n\n\n7\n\n\nlock pte\n\n\ntrylock pcache\n\n\n\n\n\n\n8\n\n\nclear pte\n\n\n..spin..\n\n\n\n\n\n\n9\n\n\nunlock pte\n\n\n..spin..\n\n\n\n\n\n\n10\n\n\nunlock pcache\n\n\n..spin..\n\n\n\n\n\n\n11\n\n\n..\n\n\nlock pcache\n\n\n\n\n\n\n12\n\n\n..\n\n\nlock pte\n\n\n\n\n\n\n13\n\n\n..\n\n\nHERE, should check if pte changed!\n\n\n\n\n\n\n\n\nHuh, patched both eviction and other code. Use refcount, pcache lock, pte lock to synchronize between all users. Make sure a going-to-be-evicted pcm will not be used by others. And others will not have a chance to use such line.\n\n\n03/12 Tue\n\n\nContinue victim cache. The current conclusion is victim has a unbalanced input and output rate. That is why some cores timeout and abort.\n\n\nGot some more clean log. The log told us that the flushd_victim is too slow at flushing content. Next I going to print the current flush queue content. Make sure that they are really not flushed. If so, I want to add code to flush sync.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n[\n  \n318.193591\n]\n \nCPU4\n \nPID\n:\n54\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207f81d340\n,\n \npset_idx\n:\n1869\n,\n \nnr_lru\n:\n7\n\n\n[\n  \n318.330986\n]\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n     \n--\n\n\n[\n  \n318.388190\n]\n   \n--\n   \nCPU4\n \n[\nword_count\n-\npthr\n][\npid\n=\n54\n,\n \ntgid\n=\n32\n]\n \n--\n\n\n[\n  \n318.456835\n]\n \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d200\n\n\n[\n  \n318.627406\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90748000\n\n\n[\n  \n318.707492\n]\n     \npset\n:\nffff88207f81d200\n \nset_idx\n:\n \n1864\n \nnr_lru\n:\n8\n\n\n[\n  \n318.775096\n]\n\n\n[\n  \n318.792778\n]\n \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d240\n\n\n[\n  \n318.963349\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90749000\n\n\n[\n  \n319.043435\n]\n     \npset\n:\nffff88207f81d240\n \nset_idx\n:\n \n1865\n \nnr_lru\n:\n8\n\n\n[\n  \n319.111040\n]\n\n\n[\n  \n319.128721\n]\n \nvictim\n:\nffff88207ff71090\n \nindex\n:\n2\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d180\n\n\n[\n  \n319.299292\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90746000\n\n\n[\n  \n319.379378\n]\n     \npset\n:\nffff88207f81d180\n \nset_idx\n:\n \n1862\n \nnr_lru\n:\n8\n\n\n[\n  \n319.446983\n]\n\n\n[\n  \n319.464664\n]\n \nvictim\n:\nffff88207ff710d8\n \nindex\n:\n3\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d280\n\n\n[\n  \n319.635237\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074a000\n\n\n[\n  \n319.715321\n]\n     \npset\n:\nffff88207f81d280\n \nset_idx\n:\n \n1866\n \nnr_lru\n:\n8\n\n\n[\n  \n319.782927\n]\n\n\n[\n  \n319.800608\n]\n \nvictim\n:\nffff88207ff71120\n \nindex\n:\n4\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d140\n\n\n[\n  \n319.971179\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90745000\n\n\n[\n  \n320.051265\n]\n     \npset\n:\nffff88207f81d140\n \nset_idx\n:\n \n1861\n \nnr_lru\n:\n8\n\n\n[\n  \n320.118870\n]\n\n\n[\n  \n320.136551\n]\n \nvictim\n:\nffff88207ff71168\n \nindex\n:\n5\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d300\n\n\n[\n  \n320.307123\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074c000\n\n\n[\n  \n320.387208\n]\n     \npset\n:\nffff88207f81d300\n \nset_idx\n:\n \n1868\n \nnr_lru\n:\n8\n\n\n[\n  \n320.454813\n]\n\n\n[\n  \n320.472494\n]\n \nvictim\n:\nffff88207ff711b0\n \nindex\n:\n6\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d1c0\n\n\n[\n  \n320.643066\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90747000\n\n\n[\n  \n320.723152\n]\n     \npset\n:\nffff88207f81d1c0\n \nset_idx\n:\n \n1863\n \nnr_lru\n:\n8\n\n\n[\n  \n320.790756\n]\n\n\n[\n  \n320.808438\n]\n \nvictim\n:\nffff88207ff711f8\n \nindex\n:\n7\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d2c0\n\n\n[\n  \n320.979009\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074b000\n\n\n[\n  \n321.059096\n]\n     \npset\n:\nffff88207f81d2c0\n \nset_idx\n:\n \n1867\n \nnr_lru\n:\n8\n\n\n[\n  \n321.126700\n]\n\n\n[\n  \n321.144381\n]\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n       \n--\n\n\n[\n  \n321.200545\n]\n \nCPU4\n \nPID\n:\n54\n \nfail\n \nto\n \nallocate\n \npcache\n \nor\n \nvictim\n \ncache\n \nlines\n.\n\n\n[\n  \n321.278552\n]\n \nword_count\n-\npthr\n[\n54\n]\n:\n \nsegfault\n \nat\n \n0x74d000\n \nip\n \n00000000004024\n9\nd\n \nsp\n \n00007ff\nf7674cd80\n \nerror\n \n6\n\n\n[\n  \n321.511925\n]\n \nnr_pgfault\n:\n \n551908\n\n\n[\n  \n321.546357\n]\n \nnr_clflush\n:\n \n33449\n\n\n[\n  \n321.581718\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n321.616040\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n321.654523\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n321.695087\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n321.754371\n]\n \nnr_pcache_fill_from_memory\n:\n \n546067\n\n\n[\n  \n321.807414\n]\n \nnr_pcache_fill_from_victim\n:\n \n5750\n\n\n[\n  \n321.858378\n]\n \nnr_pcache_eviction_triggered\n:\n \n38689\n\n\n[\n  \n321.912461\n]\n \nnr_pcache_eviction_eagain\n:\n \n5239\n\n\n[\n  \n321.962385\n]\n \nnr_pcache_eviction_succeed\n:\n \n33449\n\n\n[\n  \n322.014389\n]\n \nnr_victim_eviction_triggered\n:\n \n41887455\n\n\n[\n  \n322.071592\n]\n \nnr_victim_eviction_eagain\n:\n \n41859764\n\n\n[\n  \n322.125676\n]\n \nnr_victim_eviction_succeed\n:\n \n27691\n\n\n[\n  \n322.177680\n]\n \nnr_victim_prepare_insert\n:\n \n33450\n\n\n[\n  \n322.227603\n]\n \nnr_victim_finish_insert\n:\n \n33449\n\n\n[\n  \n322.276487\n]\n \nnr_victim_flush_submitted\n:\n \n33449\n\n\n[\n  \n322.327451\n]\n \nnr_victim_flush_finished\n:\n \n33449\n\n\n[\n  \n322.377374\n]\n \nnr_victim_flush_async_run\n:\n \n26989\n\n\n[\n  \n322.428338\n]\n \nnr_victim_flush_sync\n:\n \n0\n\n\n\n\n\n\n\nYes, this victims are truly not being flushed. They are inside the flush_queue. No bug, hoo! Just some performance coding issues. But god why the flushd does not get a chance to run in 10 seconds? Hmm\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n[\n \n5520.236187\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n \ntsk\n:\n \n32\n \nuser_va\n:\n \n0x7fff464fa000\n\n\n[\n \n5530.404269\n]\n \nCPU4\n \nPID\n:\n54\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207f81d340\n,\n \npset_idx\n:\n1869\n,\n \nnr_lru\n:\n7\n\n\n[\n \n5530.541664\n]\n \nCPU4\n \nPID54\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n \n[\n0\n]\n\n\n[\n \n5530.606147\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d1c0\n\n\n[\n \n5530.789194\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90747000\n\n\n[\n \n5530.880717\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d1c0\n \nset_idx\n:\n \n1863\n \nnr_lru\n:\n8\n\n\n[\n \n5530.968080\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d280\n\n\n[\n \n5531.151128\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074a000\n\n\n[\n \n5531.242652\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d280\n \nset_idx\n:\n \n1866\n \nnr_lru\n:\n8\n\n\n[\n \n5531.330015\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71090\n \nindex\n:\n2\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d300\n\n\n[\n \n5531.513063\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074c000\n\n\n[\n \n5531.604586\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d300\n \nset_idx\n:\n \n1868\n \nnr_lru\n:\n8\n\n\n[\n \n5531.691950\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff710d8\n \nindex\n:\n3\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d2c0\n\n\n[\n \n5531.874997\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074b000\n\n\n[\n \n5531.966521\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d2c0\n \nset_idx\n:\n \n1867\n \nnr_lru\n:\n8\n\n\n[\n \n5532.053885\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71120\n \nindex\n:\n4\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d200\n\n\n[\n \n5532.236932\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90748000\n\n\n[\n \n5532.328456\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d200\n \nset_idx\n:\n \n1864\n \nnr_lru\n:\n8\n\n\n[\n \n5532.415819\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71168\n \nindex\n:\n5\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d240\n\n\n[\n \n5532.598867\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90749000\n\n\n[\n \n5532.690390\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d240\n \nset_idx\n:\n \n1865\n \nnr_lru\n:\n8\n\n\n[\n \n5532.777753\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff711b0\n \nindex\n:\n6\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d180\n\n\n[\n \n5532.960802\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90746000\n\n\n[\n \n5533.052325\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d180\n \nset_idx\n:\n \n1862\n \nnr_lru\n:\n8\n\n\n[\n \n5533.139689\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff711f8\n \nindex\n:\n7\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d140\n\n\n[\n \n5533.322736\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff90745000\n\n\n[\n \n5533.414259\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d140\n \nset_idx\n:\n \n1861\n \nnr_lru\n:\n8\n\n\n[\n \n5533.501623\n]\n \nCPU4\n \nPID54\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n \n[\n0\n]\n\n\n\n[\n \n5533.566106\n]\n \nCPU4\n \nPID54\n   \n--\n  \nStart\n \nDump\n \nVictim\n \nFlush\n \nQueue\n \n[\n0\n]\n\n\n[\n \n5533.635789\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff711f8\n \nindex\n:\n7\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d140\n\n\n[\n \n5533.818837\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff711b0\n \nindex\n:\n6\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d180\n\n\n[\n \n5534.001884\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d1c0\n\n\n[\n \n5534.184931\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71120\n \nindex\n:\n4\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d200\n\n\n[\n \n5534.367978\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71168\n \nindex\n:\n5\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d240\n\n\n[\n \n5534.551025\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d280\n\n\n[\n \n5534.734074\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff710d8\n \nindex\n:\n3\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d2c0\n\n\n[\n \n5534.917120\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71090\n \nindex\n:\n2\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d300\n\n\n[\n \n5535.100168\n]\n \nCPU4\n \nPID54\n   \n--\n  \nEnd\n \nDump\n \nVictim\n \nFlush\n \nQueue\n \n[\n0\n]\n\n\n\n[\n \n5535.169851\n]\n \nCPU4\n \nPID\n:\n54\n \nfail\n \nto\n \nallocate\n \npcache\n \nor\n \nvictim\n \ncache\n \nlines\n.\n\n\n[\n \n5535.247854\n]\n \nword_count\n-\npthr\n[\n54\n]\n:\n \nsegfault\n \nat\n \n0x74d000\n \nip\n \n00000000004024\n9\nd\n \nsp\n \n00007ff\nf7674cd80\n \nerror\n \n6\n\n\n[\n \n5535.480513\n]\n \nnr_pgfault\n:\n \n549578\n\n\n[\n \n5535.514943\n]\n \nnr_clflush\n:\n \n31822\n\n\n[\n \n5535.550304\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n \n5535.584625\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n \n5535.623107\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n \n5535.663669\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n \n5535.722952\n]\n \nnr_pcache_fill_from_memory\n:\n \n544279\n\n\n[\n \n5535.775993\n]\n \nnr_pcache_fill_from_victim\n:\n \n5201\n\n\n[\n \n5535.826955\n]\n \nnr_pcache_eviction_triggered\n:\n \n37437\n\n\n[\n \n5535.881038\n]\n \nnr_pcache_eviction_eagain\n:\n \n5614\n\n\n[\n \n5535.930960\n]\n \nnr_pcache_eviction_succeed\n:\n \n31822\n\n\n[\n \n5535.982963\n]\n \nnr_victim_eviction_triggered\n:\n \n42000029\n\n\n[\n \n5536.040165\n]\n \nnr_victim_eviction_eagain\n:\n \n41973416\n\n\n[\n \n5536.094247\n]\n \nnr_victim_eviction_succeed\n:\n \n26613\n\n\n[\n \n5536.146249\n]\n \nnr_victim_prepare_insert\n:\n \n31823\n\n\n[\n \n5536.196171\n]\n \nnr_victim_finish_insert\n:\n \n31822\n\n\n[\n \n5536.245052\n]\n \nnr_victim_flush_submitted\n:\n \n31822\n\n\n[\n \n5536.296015\n]\n \nnr_victim_flush_finished\n:\n \n31822\n\n\n[\n \n5536.345937\n]\n \nnr_victim_flush_async_run\n:\n \n26718\n\n\n[\n \n5536.396899\n]\n \nnr_victim_flush_sync\n:\n \n0\n\n\n\n\n\n\nHmm, got some interesting bug, which never happened before. We did a \nunmap\n before \nfinish_insert\n, so the mapcount must be zero. Since we have the \nReclaim\n set for the candidate. But it looks like other code does not too much about the Reclaim bit. I need to check.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n[\n \n1009.676839\n]\n \nvictim_flush_async\n \nCPU4\n \njobs\n \n1\n\n\n[\n \n1009.725830\n]\n \nvictim_flush_async\n \nCPU4\n \njobs\n \n1\n\n\n[\n \n1009.774423\n]\n \nvictim_flush_async\n \nCPU4\n \njobs\n \n1\n\n\n[\n \n1009.823147\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n \ntsk\n:\n \n32\n \nuser_va\n:\n \n0x7fff465fc000\n\n\n[\n \n1009.910479\n]\n \npcache\n:\nffff88018098d740\n \nmapcount\n:\n1\n \nrefcount\n:\n3\n \nflags\n:(\nlocked\n|\nallocated\n|\nusable\n|\nvalid\n|\nreclaim\n)\n \nkva\n:\n \nffff88012635d000\n\n\n[\n \n1010.045652\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_PCM\n(\npcache_mapped\n(\npcm\n))\n\n\n[\n \n1010.125725\n]\n \nvictim_flush_async\n \nCPU4\n \njobs\n \n1\n\n\n[\n \n1010.174602\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1010.229717\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n601\n/\nvictim_finish_insert\n()\n!\n\n\n[\n \n1010.328509\n]\n \nvictim_flush_async\n \nCPU4\n \njobs\n \n1\n\n\n[\n \n1010.377385\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1010.428341\n]\n \nCPU\n:\n \n20\n \nPID\n:\n \n47\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n468\n\n\n[\n \n1010.505294\n]\n \nStack\n:\n\n\n[\n \n1010.529212\n]\n \nffff881f2040fe08\n \nffffffff810259f4\n \n000000000000000\n8\n \nffff881f2040fe18\n\n\n[\n \n1010.616565\n]\n \nffff881f2040fdd0\n \n0000000021475542\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1010.703918\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1010.791270\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1010.878623\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1010.965976\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1010.995095\n]\n \nTSK\n\n\n[\n \n1011.017972\n]\n \n[\nffffffff81025a00\n]\n \npanic\n+\n0xc2\n/\n0x102\n\n\n[\n \n1011.074127\n]\n \n[\nffffffff81063a8a\n]\n \n?\n \nclient_internal_poll_sendcq\n+\n0x2a\n/\n0x80\n\n\n[\n \n1011.154202\n]\n \n[\nffffffff81063c2d\n]\n \n?\n \nclient_send_message_with_rdma_write_with_imm_request\n+\n0x14d\n/\n0x360\n\n\n[\n \n1011.262351\n]\n \n[\nffffffff8101bffc\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n \n1011.326827\n]\n \n[\nffffffff81019755\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n \n1011.393382\n]\n \n[\nffffffff81016e25\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n \n1011.466175\n]\n \n[\nffffffff810066e4\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n1011.538969\n]\n \n[\nffffffff8100e4aa\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n1011.616964\n]\n \n[\nffffffff81012cfd\n]\n \n?\n \nprintk\n+\n0x11d\n/\n0x1b0\n\n\n[\n \n1011.677279\n]\n \n[\nffffffff81032a19\n]\n \nvictim_finish_insert\n+\n0x89\n/\n0x230\n\n\n[\n \n1011.749032\n]\n \n[\nffffffff81031a99\n]\n \npcache_evict_line\n+\n0x79\n/\n0x280\n\n\n[\n \n1011.817667\n]\n \n[\nffffffff8102f00a\n]\n \npcache_alloc\n+\n0x23a\n/\n0x340\n\n\n[\n \n1011.882141\n]\n \n[\nffffffff8102e4da\n]\n \ncommon_do_fill_page\n+\n0x2a\n/\n0x1b0\n\n\n[\n \n1011.952856\n]\n \n[\nffffffff8102e160\n]\n \n?\n \npcache_meta_to_kva\n+\n0x30\n/\n0x30\n\n\n[\n \n1012.023570\n]\n \n[\nffffffff8102e802\n]\n \npcache_handle_fault\n+\n0x1a2\n/\n0x660\n\n\n[\n \n1012.095324\n]\n \n[\nffffffff810102b2\n]\n \ndo_page_fault\n+\n0xa2\n/\n0x1a0\n\n\n[\n \n1012.159799\n]\n \n[\nffffffff8100dadf\n]\n \npage_fault\n+\n0x1f\n/\n0x30\n\n\n\n\n\n\nInteresting. Memory consistency issue? Actually, I\nm not sure if it is the \nv-\nflags = 0\n issue. Others use atomic bit operations to play with this flag, while the reset is a simple store. I checked the list operations,  all of them are protected by spinlock. So the below should never happen in theory. I\nm changing the \nv-\nflags = 0\n to \nsmp_store_mb(v-\nflags, 0)\n, which is a \nxchg\n in x86. Same for pcache.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n[\n \n1773.814490\n]\n \nCPU17\n \nPID44\n  \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n1\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\n          \n(\nnull\n)\n\n\n[\n \n1773.979705\n]\n \nCPU17\n \nPID44\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff95b1c000\n\n\n[\n \n1774.072260\n]\n \nCPU17\n \nPID44\n     \nrmap\n \nto\n \npset\n:\nffff88207f96c700\n \nset_idx\n:\n \n23324\n \nnr_lru\n:\n8\n\n\n[\n \n1774.161694\n]\n \nCPU17\n \nPID44\n     \nvictim\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_VICTIM\n(\n!\nVictimUsable\n(\nv\n))\n\n\n[\n \n1774.259451\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1774.314567\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n231\n/\nfind_victim_to_evict\n()\n!\n\n\n[\n \n1774.413363\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1774.464320\n]\n \nCPU\n:\n \n17\n \nPID\n:\n \n44\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n47\n\n\n...\n\n\n[\n \n1781.363348\n]\n \nnr_pcache_fill_from_victim\n:\n \n2\n\n\n\n\n\n\nDid another run. I added an explicit \nwake_up_victim_flushd\n if victim failed to evict any line. But this fails with IB failure..\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n[\n \n2336.950087\n]\n \nCPU4\n \nPID\n:\n54\n \nAbort\n \nvictim\n \nalloc\n \n(\n20010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207f81d340\n,\n \npset_idx\n:\n1869\n,\n \nnr_lru\n:\n7\n\n\n[\n \n2337.087474\n]\n \nCPU4\n \nPID54\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n \n[\n0\n]\n\n\n[\n \n2337.151955\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d280\n\n\n[\n \n2337.334999\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074a000\n\n\n[\n \n2337.426521\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d280\n \nset_idx\n:\n \n1866\n \nnr_lru\n:\n8\n\n\n[\n \n2337.513883\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d2c0\n\n\n[\n \n2337.696927\n]\n \nCPU4\n \nPID54\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff9074b000\n\n\n[\n \n2337.788450\n]\n \nCPU4\n \nPID54\n     \nrmap\n \nto\n \npset\n:\nffff88207f81d2c0\n \nset_idx\n:\n \n1867\n \nnr_lru\n:\n8\n\n\n...\n\n\n...\n\n\n[\n \n2340.111861\n]\n \nCPU4\n \nPID54\n   \n--\n  \nStart\n \nDump\n \nVictim\n \nFlush\n \nQueue\n \n[\n0\n]\n\n\n[\n \n2340.181543\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71090\n \nindex\n:\n2\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d140\n\n\n[\n \n2340.364587\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71120\n \nindex\n:\n4\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d180\n\n\n[\n \n2340.547632\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff711f8\n \nindex\n:\n7\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d1c0\n\n\n[\n \n2340.730675\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71168\n \nindex\n:\n5\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d200\n\n\n[\n \n2340.913720\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff711b0\n \nindex\n:\n6\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d240\n\n\n[\n \n2341.096763\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d280\n\n\n[\n \n2341.279808\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d2c0\n\n\n[\n \n2341.462851\n]\n \nCPU4\n \nPID54\n  \nvictim\n:\nffff88207ff710d8\n \nindex\n:\n3\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n|\nwaitflush\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f81d300\n\n\n[\n \n2341.645895\n]\n \nCPU4\n \nPID54\n   \n--\n  \nEnd\n \nDump\n \nVictim\n \nFlush\n \nQueue\n \n[\n0\n]\n\n\n\n[\n \n2341.715577\n]\n \nCPU4\n \nPID\n:\n54\n \nfail\n \nto\n \nallocate\n \npcache\n \nor\n \nvictim\n \ncache\n \nlines\n.\n\n\n[\n \n2341.793579\n]\n \nword_count\n-\npthr\n[\n54\n]\n:\n \nsegfault\n \nat\n \n0x74d000\n \nip\n \n00000000004024\n9\nd\n \nsp\n \n00007ff\nf7674cd80\n \nerror\n \n6\n\n\n[\n \n2476.201442\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n21\n\n\n[\n \n2476.254590\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2476.308670\n]\n \nsend\n \nrequest\n \nfailed\n \nat\n \nconnection\n \n4\n \nas\n \n12\n\n\n[\n \n2476.368991\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2476.423073\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2476.477153\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2476.531236\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1051\n\n\n[\n \n2476.598837\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1052\n\n\n[\n \n2476.666438\n]\n \n__clflush_one\n()\n:\n \nEPERM\n:\nOperation\n \nnot\n \npermitted\n \ntsk\n:\n \n32\n \nuser_va\n:\n \n0x7fff90745000\n\n\n[\n \n2476.765240\n]\n \nclient_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n \n2476.840122\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1053\n\n\n[\n \n2476.907724\n]\n \nclient_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n \n2476.982605\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1054\n\n\n[\n \n2477.050207\n]\n \nclient_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n \n2477.125089\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.179169\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.233251\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.287332\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.341414\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1055\n\n\n[\n \n2477.409016\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1056\n\n\n..\n\n\n..\n\n\n[\n \n2477.761583\n]\n \nclient_poll_cq\n:\n \nconnection\n \n4\n \nRecv\n \nweird\n \nevent\n \nas\n \n-\n30704\n\n\n[\n \n2477.836464\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.890545\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.944626\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2477.998707\n]\n \nmlx4_ib_handle_error_cqe\n \nsyndrome\n \n5\n\n\n[\n \n2478.052789\n]\n \nclient_poll_cq\n:\n \nfailed\n \nstatus\n \n(\n5\n)\n \nfor\n \nwr_id\n \n1059\n\n\n[\n \n2478.120392\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n           \n(\nnull\n)\n\n\n[\n \n2478.213992\n]\n \nIP\n:\n \n[\nffffffff81064894\n]\n \nclient_poll_cq\n+\n0x1f4\n/\n0x6c0\n\n\n[\n \n2478.284714\n]\n \nPGD\n \n0\n\n\n[\n \n2478.308635\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n \n2478.356476\n]\n \nCPU\n:\n \n2\n \nPID\n:\n \n29\n \nComm\n:\n \nrecvpollcq\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n473\n\n\n[\n \n2478.427197\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff81064894\n]\n  \n[\nffffffff81064894\n]\n \nclient_poll_cq\n+\n0x1f4\n/\n0x6c0\n\n\n[\n \n2478.527040\n]\n \nRSP\n:\n \n0000\n:\nffff88107e143d90\n  \nEFLAGS\n:\n \n00010246\n\n\n[\n \n2478.590481\n]\n \nRAX\n:\n \n0000000000000000\n \nRBX\n:\n \nffff88207fc6e000\n \nRCX\n:\n \n0000000000000000\n\n\n[\n \n2478.675762\n]\n \nRDX\n:\n \n000000000000100\n8\n \nRSI\n:\n \nffffffff811d36e0\n \nRDI\n:\n \nffffffff811dab08\n\n\n[\n \n2478.761044\n]\n \nRBP\n:\n \nffff88107e143eb0\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \n0000000000000000\n\n\n[\n \n2478.846327\n]\n \nR10\n:\n \n0000000000000002\n \nR11\n:\n \n0000000000000004\n \nR12\n:\n \nffff88207fd4f000\n\n\n[\n \n2478.931609\n]\n \nR13\n:\n \n0000000000000004\n \nR14\n:\n \nffff88107e143da8\n \nR15\n:\n \n0000000000000000\n\n\n[\n \n2479.016890\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc20000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n2479.113613\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n2479.182254\n]\n \nCR2\n:\n \n0000000000000000\n \nCR3\n:\n \n000000000113\nd000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n \n2479.267536\n]\n \nStack\n:\n\n\n[\n \n2479.291457\n]\n \nffff88107e143da0\n \n001012\n9\nc81019794\n \n0000000000000001\n \n0000000000000423\n\n\n[\n \n2479.378818\n]\n \n000000\n8100000005\n \n00001008000000f\n9\n \nffff88207fd39000\n \n0000000040000000\n\n\n[\n \n2479.466180\n]\n \n000f\n004000000002\n \nffff88107e140000\n \n0000000000000424\n \nffff881000000005\n\n\n[\n \n2479.553542\n]\n \n00000000000000f\n9\n \nffff88207fd39000\n \nffff88107e143e38\n \nffffffff81019e44\n\n\n[\n \n2479.640904\n]\n \n0000000000000001\n \n0000000000000425\n \nffff881000000005\n \nffffffff000000f9\n\n\n[\n \n2479.728266\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n2479.757386\n]\n \nTSK\n\n\n[\n \n2479.780268\n]\n \n[\nffffffff81019e44\n]\n \n?\n \ntry_to_wake_up\n+\n0xe4\n/\n0x1f0\n\n\n[\n \n2479.847869\n]\n \n[\nffffffff81066d78\n]\n \n?\n \n__schedule\n+\n0xf8\n/\n0x1e0\n\n\n[\n \n2479.911311\n]\n \n[\nffffffff81064d60\n]\n \n?\n \nclient_poll_cq\n+\n0x6c0\n/\n0x6c0\n\n\n[\n \n2479.979952\n]\n \n[\nffffffff81064d70\n]\n \nclient_poll_cq_pass\n+\n0x10\n/\n0x20\n\n\n[\n \n2480.049634\n]\n \n[\nffffffff81020336\n]\n \nkthread\n+\n0xf6\n/\n0x110\n\n\n[\n \n2480.107875\n]\n \n[\nffffffff81020240\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n \n2480.176516\n]\n \n[\nffffffff8100e732\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n\n\n\n\nA classical SMP bug. Lucky to find this one. Let me try to describe this. There are two CPU1. CPU0 and CPU1. CPU0 is doing eviction while CPU1 is doing munmap-\npcache_zap_pte. The CPU0 slected a pcm, while this pcm happen to be zapped at the same time by CPU1. There are not enough actions to either 1) prevent CPU0 from selecting this pcm, 2) prevent CPU1 from using this pcm. Both solutions might be work. But we need as least one.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nwuklab13\n \n0313\n-\n12\n\n\n[\n \n1073.616269\n]\n \npcache\n:\nffff880180777a80\n \nmapcount\n:\n0\n \nrefcount\n:\n3\n \nflags\n:(\nlocked\n|\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff88011ddea000\n\n\n[\n \n1073.734941\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n \ntsk\n:\n \n32\n \nuser_va\n:\n \n0x7fff4ddea000\n\n\n[\n \n1073.822304\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict\n/\nref\n \nbug\n\n\n\n[\n \n1073.987667\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nevict\n.\nc\n:\n301\n/\npcache_evict_line\n()\n!\n\n\n\n[\n \n1074.082308\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n763\n/\npcache_zap_pte\n()\n!\n\n\n[\n \n1074.172789\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1074.223751\n]\n \nCPU\n:\n \n23\n \nPID\n:\n \n50\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n476\n\n\n\n\n\n\n\n03/11 Mon\n\n\nDebug victim cache\n\n\nMorning. Today I will continue debugging victim and clflush, running with MT phoenix+2GB, seq+4GB. Sounds good.\n\n\nDigging into yesterday\ns 21th run log. The warning comes from \nvictim_alloc_slowpath\n. The allocation abort after 10 seconds timeout. And interestingly, a lot threads abort. (The case is, \npset\n is full, so \npcache_alloc\n will try to evict one to victim cache. But victim cache is full as well. So it needs to evict one victim cache line too. Somehow this does not proceed as planned.) I guess somewhere deadlock happens.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n[\n \n1682.040428\n]\n \nWARNING\n:\n \nCPU\n:\n \n7\n \nPID\n:\n \n34\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1682.161063\n]\n \nWARNING\n:\n \nCPU\n:\n \n19\n \nPID\n:\n \n46\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1686.602779\n]\n \nWARNING\n:\n \nCPU\n:\n \n10\n \nPID\n:\n \n37\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1687.384837\n]\n \nWARNING\n:\n \nCPU\n:\n \n3\n \nPID\n:\n \n53\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1687.505474\n]\n \nWARNING\n:\n \nCPU\n:\n \n21\n \nPID\n:\n \n48\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1687.737386\n]\n \nWARNING\n:\n \nCPU\n:\n \n16\n \nPID\n:\n \n43\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1687.859063\n]\n \nWARNING\n:\n \nCPU\n:\n \n4\n \nPID\n:\n \n54\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1688.034819\n]\n \nWARNING\n:\n \nCPU\n:\n \n6\n \nPID\n:\n \n56\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1688.210574\n]\n \nWARNING\n:\n \nCPU\n:\n \n14\n \nPID\n:\n \n41\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1688.488246\n]\n \nWARNING\n:\n \nCPU\n:\n \n5\n \nPID\n:\n \n55\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1689.598935\n]\n \nWARNING\n:\n \nCPU\n:\n \n22\n \nPID\n:\n \n49\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1689.953565\n]\n \nWARNING\n:\n \nCPU\n:\n \n0\n \nPID\n:\n \n51\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1691.740234\n]\n \nWARNING\n:\n \nCPU\n:\n \n13\n \nPID\n:\n \n40\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1691.861911\n]\n \nWARNING\n:\n \nCPU\n:\n \n1\n \nPID\n:\n \n52\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n[\n \n1791.554552\n]\n \nWARNING\n:\n \nCPU\n:\n \n11\n \nPID\n:\n \n38\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nvictim\n.\nc\n:\n447\n \nvictim_prepare_insert\n+\n0x322\n/\n0x4b0\n\n\n\n\n\n\n1\nst\n run. MT+2GB. Victim allocation as predicted. Somehow I already forgot how the code is designed. I need to take a detailed reread.\n\n\nAlong the testing, fixed a bug in eviction code: handle failed evict_line properly. If eviction mechanism failed, we need to clear what the algorithm part has done. This is also related to yesterday\ns big idea: always do proper cleanup. Many thanks go to pcache free checking, help me to find this bug.\n\n\nLess is more. I printed too much useless info when pcache_alloc or victim_alloc fail. I removed all the dump_pset from the failing path. It can give me a much more clean message to debug.\n\n\nHmm, it is really weird. I dump all victims once alloc timeout. You can see that all victim are not Flushed, that means none of them can be evicted. Take a look at the stat. Hmm, I probabaly should not do this per-cpu counter??\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n...\n\n\n[\n \n4751.460819\n]\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n     \n--\n\n\n[\n \n4751.518022\n]\n   \n--\n   \nCPU19\n \n[\nword_count\n-\npthr\n][\npid\n=\n46\n,\n \ntgid\n=\n32\n]\n \n--\n\n\n[\n \n4751.587706\n]\n \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800440\n\n\n[\n \n4751.747872\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff20011000\n\n\n[\n \n4751.827955\n]\n     \npset\n:\nffff88207f800440\n \nset_idx\n:\n \n17\n \nnr_lru\n:\n8\n\n\n[\n \n4751.893478\n]\n\n\n[\n \n4751.911159\n]\n \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f8003c0\n\n\n[\n \n4752.071326\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff2000f000\n\n\n[\n \n4752.428060\n]\n     \npset\n:\nffff88207f8003c0\n \nset_idx\n:\n \n15\n \nnr_lru\n:\n8\n\n\n[\n \n4752.630868\n]\n\n\n[\n \n4752.931441\n]\n \nvictim\n:\nffff88207ff71090\n \nindex\n:\n2\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800540\n\n\n[\n \n4753.370339\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff20015000\n\n\n[\n \n4753.450422\n]\n     \npset\n:\nffff88207f800540\n \nset_idx\n:\n \n21\n \nnr_lru\n:\n8\n\n\n[\n \n4753.515945\n]\n\n\n[\n \n4753.533627\n]\n \nvictim\n:\nffff88207ff710d8\n \nindex\n:\n3\n \nrefcount\n:\n3\n \nnr_fill\n:\n1\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207fbdff40\n\n\n[\n \n4753.693792\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fffbf7fd000\n\n\n[\n \n4753.773875\n]\n     \npset\n:\nffff88207fbdff40\n \nset_idx\n:\n \n63485\n \nnr_lru\n:\n7\n\n\n[\n \n4753.842518\n]\n\n\n[\n \n4753.860199\n]\n \nvictim\n:\nffff88207ff71120\n \nindex\n:\n4\n \nrefcount\n:\n3\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800500\n\n\n[\n \n4754.020367\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff20014000\n\n\n[\n \n4754.100449\n]\n     \npset\n:\nffff88207f800500\n \nset_idx\n:\n \n20\n \nnr_lru\n:\n8\n\n\n[\n \n4754.165971\n]\n\n\n[\n \n4754.183653\n]\n \nvictim\n:\nffff88207ff71168\n \nindex\n:\n5\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800480\n\n\n[\n \n4754.343819\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff30012000\n\n\n[\n \n4754.423902\n]\n     \npset\n:\nffff88207f800480\n \nset_idx\n:\n \n18\n \nnr_lru\n:\n8\n\n\n[\n \n4754.489426\n]\n\n\n[\n \n4754.507106\n]\n \nvictim\n:\nffff88207ff711b0\n \nindex\n:\n6\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f8004c0\n\n\n[\n \n4754.808718\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff30013000\n\n\n[\n \n4754.888802\n]\n     \npset\n:\nffff88207f8004c0\n \nset_idx\n:\n \n19\n \nnr_lru\n:\n8\n\n\n[\n \n4754.954325\n]\n\n\n[\n \n4754.972006\n]\n \nvictim\n:\nffff88207ff711f8\n \nindex\n:\n7\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800400\n\n\n[\n \n4755.132172\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff20010000\n\n\n[\n \n4755.212255\n]\n     \npset\n:\nffff88207f800400\n \nset_idx\n:\n \n16\n \nnr_lru\n:\n8\n\n\n[\n \n4755.277778\n]\n\n\n[\n \n4755.295458\n]\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n       \n--\n\n\n\n...\n\n\n\n[\n \n4757.948641\n]\n \nnr_pgfault\n:\n \n313898\n\n\n[\n \n4757.983067\n]\n \nnr_clflush\n:\n \n488\n\n\n[\n \n4758.016347\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n \n4758.050669\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n \n4758.089151\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n \n4758.129713\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n \n4758.188995\n]\n \nnr_pcache_fill_from_memory\n:\n \n313833\n\n\n[\n \n4758.242038\n]\n \nnr_pcache_fill_from_victim\n:\n \n54\n\n\n[\n \n4758.290919\n]\n \nnr_pcache_eviction_triggered\n:\n \n243280263\n\n\n[\n \n4758.349161\n]\n \nnr_pcache_eviction_eagain\n:\n \n243279763\n\n\n[\n \n4758.404283\n]\n \nnr_pcache_eviction_succeed\n:\n \n488\n\n\n[\n \n4758.454207\n]\n \nnr_victim_eviction\n:\n \n426\n\n\n[\n \n4758.495807\n]\n \nnr_victim_prepare_insert\n:\n \n500\n\n\n[\n \n4758.543649\n]\n \nnr_victim_finish_insert\n:\n \n488\n\n\n[\n \n4758.590451\n]\n \nnr_victim_flush_submitted\n:\n \n488\n\n\n[\n \n4758.639333\n]\n \nnr_victim_flush_finished\n:\n \n488\n\n\n\n\n\n\nI counted it wrong. Below is the log. Since \nnr_victim_flushd_run * 8 = nr_victim_flush_finished\n, it basically means for every run, victim_flushd needs to flush all 8 victims, which implies eviction rate is much higher than the flushd running rate. \nnr_pcache_fill_from_victim\n:\n \n21\n, which means there are some succeed refills, but I don\nt know how it can improve performance.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n[\n  \n475.468489\n]\n \nCPU4\n \nPID\n:\n54\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207f800000\n,\n \npset_idx\n:\n0\n,\n \nnr_lru\n:\n7\n\n\n[\n  \n475.602752\n]\n \nCPU3\n \nPID\n:\n53\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207f900a00\n,\n \npset_idx\n:\n16424\n,\n \nnr_lru\n:\n7\n\n\n[\n  \n476.029145\n]\n \nCPU5\n \nPID\n:\n55\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207fbdff40\n,\n \npset_idx\n:\n63485\n,\n \nnr_lru\n:\n7\n\n\n[\n  \n476.169542\n]\n \nCPU9\n \nPID\n:\n36\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207f900000\n,\n \npset_idx\n:\n16384\n,\n \nnr_lru\n:\n7\n\n\n[\n  \n477.360322\n]\n \nCPU1\n \nPID\n:\n52\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207fbfff80\n,\n \npset_idx\n:\n65534\n,\n \nnr_lru\n:\n7\n\n\n[\n  \n479.206291\n]\n \nCPU18\n \nPID\n:\n45\n \nAbort\n \nvictim\n \nalloc\n \n(\n10010\nms\n)\n \nnr_usable_victims\n:\n \n8\n \nreq\n \nfrom\n \npset\n:\nffff88207fb00000\n,\n \npset_idx\n:\n49152\n,\n \nnr_lru\n:\n7\n\n\n\n[\n  \n475.743150\n]\n   \n--\n   \nStart\n \nDump\n \nVictim\n \nCache\n     \n--\n\n\n[\n  \n475.800350\n]\n   \n--\n   \nCPU4\n \n[\nword_count\n-\npthr\n][\npid\n=\n54\n,\n \ntgid\n=\n32\n]\n \n--\n\n\n[\n  \n475.868989\n]\n \nvictim\n:\nffff88207ff71000\n \nindex\n:\n0\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800a80\n\n\n[\n  \n476.309940\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff3002a000\n\n\n[\n  \n476.390020\n]\n     \npset\n:\nffff88207f800a80\n \nset_idx\n:\n \n42\n \nnr_lru\n:\n8\n\n\n[\n  \n476.455538\n]\n\n\n[\n  \n476.473218\n]\n \nvictim\n:\nffff88207ff71048\n \nindex\n:\n1\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800bc0\n\n\n[\n  \n476.633376\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff4002f000\n\n\n[\n  \n476.713453\n]\n     \npset\n:\nffff88207f800bc0\n \nset_idx\n:\n \n47\n \nnr_lru\n:\n8\n\n\n[\n  \n476.778972\n]\n\n\n[\n  \n476.796652\n]\n \nvictim\n:\nffff88207ff71090\n \nindex\n:\n2\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800b80\n\n\n[\n  \n476.956809\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff3002e000\n\n\n[\n  \n477.036889\n]\n     \npset\n:\nffff88207f800b80\n \nset_idx\n:\n \n46\n \nnr_lru\n:\n8\n\n\n[\n  \n477.102406\n]\n\n\n[\n  \n477.120086\n]\n \nvictim\n:\nffff88207ff710d8\n \nindex\n:\n3\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800a00\n\n\n[\n  \n477.280245\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff30028000\n\n\n[\n  \n477.500721\n]\n     \npset\n:\nffff88207f800a00\n \nset_idx\n:\n \n40\n \nnr_lru\n:\n8\n\n\n[\n  \n477.566239\n]\n\n\n[\n  \n477.583918\n]\n \nvictim\n:\nffff88207ff71120\n \nindex\n:\n4\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800b40\n\n\n[\n  \n477.744077\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff3002d000\n\n\n[\n  \n477.824155\n]\n     \npset\n:\nffff88207f800b40\n \nset_idx\n:\n \n45\n \nnr_lru\n:\n8\n\n\n[\n  \n477.889673\n]\n\n\n[\n  \n477.907353\n]\n \nvictim\n:\nffff88207ff71168\n \nindex\n:\n5\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800b00\n\n\n[\n  \n478.067511\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff3002c000\n\n\n[\n  \n478.147590\n]\n     \npset\n:\nffff88207f800b00\n \nset_idx\n:\n \n44\n \nnr_lru\n:\n8\n\n\n[\n  \n478.213109\n]\n\n\n[\n  \n478.230788\n]\n \nvictim\n:\nffff88207ff711b0\n \nindex\n:\n6\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800a40\n\n\n[\n  \n478.390946\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff30029000\n\n\n[\n  \n478.471024\n]\n     \npset\n:\nffff88207f800a40\n \nset_idx\n:\n \n41\n \nnr_lru\n:\n8\n\n\n[\n  \n478.536542\n]\n\n\n[\n  \n478.554222\n]\n \nvictim\n:\nffff88207ff711f8\n \nindex\n:\n7\n \nrefcount\n:\n2\n \nnr_fill\n:\n0\n \nlocked\n:\n0\n \nflags\n:(\nallocated\n|\nusable\n|\nhasdata\n)\n \npcm\n:\n          \n(\nnull\n)\n \npset\n:\nffff88207f800ac0\n\n\n[\n  \n478.714380\n]\n     \nhit\n[\n0\n]\n \nowner\n:\n \n[\nword_count\n-\npthr\n][\n32\n]\n \naddr\n:\n \n0x7fff3002b000\n\n\n[\n  \n478.794458\n]\n     \npset\n:\nffff88207f800ac0\n \nset_idx\n:\n \n43\n \nnr_lru\n:\n8\n\n\n[\n  \n478.859977\n]\n\n\n[\n  \n478.877657\n]\n   \n--\n   \nEnd\n \nDump\n \nVictim\n \nCache\n       \n--\n\n\n\n[\n  \n480.324070\n]\n \nnr_pgfault\n:\n \n372353\n\n\n[\n  \n480.358494\n]\n \nnr_clflush\n:\n \n336\n\n\n[\n  \n480.391774\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n480.426093\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n480.464573\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n480.505132\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n480.564410\n]\n \nnr_pcache_fill_from_memory\n:\n \n372326\n\n\n[\n  \n480.617450\n]\n \nnr_pcache_fill_from_victim\n:\n \n21\n\n\n[\n  \n480.666330\n]\n \nnr_pcache_eviction_triggered\n:\n \n178320088\n\n\n[\n  \n480.724569\n]\n \nnr_pcache_eviction_eagain\n:\n \n178319746\n\n\n[\n  \n480.779687\n]\n \nnr_pcache_eviction_succeed\n:\n \n336\n\n\n[\n  \n480.829606\n]\n \nnr_victim_eviction_triggered\n:\n \n20589049\n\n\n[\n  \n480.886805\n]\n \nnr_victim_eviction_eagain\n:\n \n20588741\n\n\n[\n  \n480.940885\n]\n \nnr_victim_eviction_succeed\n:\n \n308\n\n\n[\n  \n480.990804\n]\n \nnr_victim_prepare_insert\n:\n \n342\n\n\n[\n  \n481.038643\n]\n \nnr_victim_finish_insert\n:\n \n336\n\n\n[\n  \n481.085442\n]\n \nnr_victim_flush_submitted\n:\n \n336\n\n\n[\n  \n481.134321\n]\n \nnr_victim_flush_finished\n:\n \n336\n\n\n[\n  \n481.182161\n]\n \nnr_victim_flushd_run\n:\n \n42\n\n\n\n\n\n\n\n03/10 Sun\n\n\nFix bug from \n__unhash_procees()\n\n\n[Summary]: a bug cause by laziness. When fork happens, the new thread is added into parent\ns thread_group list. However, we forgot to remove it when the new thread exit. Thus, the field in parent\ns thread_group will point to a freed page. To make it worse, the freed page got allocated again. In our case, the page was used by pgtable. So, when the parent tries to use that field, it simply corrupts pgtable. This bug is fixed by this commit: 64d43fc.\n\n\nGot something going on. Huh.\n\n\nAnyway, pick up what left last night.\n\n\n8\nth\n run,\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n[  426.595911] SYSC_mmap(cpu5): ret_addr:0x7ffefbeac000\n\npte page got allocated\n[  426.653216]     pmd is none index 0x1e3 line 567 from_addr 0x7ffefc6acd90\n[  426.734334] __pte_alloc(): for addr: 0x7ffefc6acd90 pte_index: ac\n[  426.807132]     pte is none index 0x38 line 574 from_addr 0x7ffefc6acd90\n[  427.304148]     pte is none index 0x38 line 576 from_addr 0x7ffefc6acd90\n\nthis addr seems fine\n[  427.382251]     pte is none index 0x38 line 567 from_addr 0x7ffefc6abe78\n[  427.462329]     pte is none index 0x38 line 574 from_addr 0x7ffefc6abe78\n[  427.644439]     pte is none index 0x38 line 576 from_addr 0x7ffefc6abe78\n\nSomething happen in between corrupted pgtable\n[  427.722547] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000)\n[  427.858779] line: 567 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0\n\n[  427.938858] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000)\n[  428.075095] line: 574 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0\n\n\n\n\n\n9\nth\n run, found actually it created another thread. And it exit. And it corrupted aftet the pid33 exit. Bang, it should be something wrong in exit().\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nwuklab13\n \n0311\n-\n4\n\n\n[\n  \n813.127325\n]\n \nCPU6\n \npid\n:\n33\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n586\n \nfrom_addr\n \n0x4b0db0\n\n\n[\n  \n813.214683\n]\n \nCPU5\n \npid\n:\n32\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n593\n \nfrom_addr\n \n0x6f4768\n\n\n[\n  \n813.302042\n]\n \nCPU6\n \npid\n:\n33\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n593\n \nfrom_addr\n \n0x4b0db0\n\n\n[\n  \n813.397836\n]\n \nCPU5\n \npid\n:\n32\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n595\n \nfrom_addr\n \n0x6f4768\n\n\n[\n  \n813.593364\n]\n \nCPU6\n \npid\n:\n33\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n595\n \nfrom_addr\n \n0x4b0db0\n\n\n\n\n[\n  \n813.678751\n]\n \ndo_exit\n()\n \npid\n:\n33\n,\ntgid\n:\n32\n \ncode\n:\n0x0\n\n\n\n\n[\n  \n814.474321\n]\n \nCPU5\n \npid\n:\n32\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n567\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n814.567918\n]\n \nCPU5\n \npid\n:\n32\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n575\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n814.661516\n]\n \nCPU5\n \npid\n:\n32\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n583\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n814.755115\n]\n \nCPU5\n \npid\n:\n32\n     \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n586\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n814.848714\n]\n \n__pte_alloc\n()\n:\n \nfor\n \naddr\n:\n \n0x7ffefc6acd90\n \npte_index\n:\n \nac\n\n\n[\n  \n814.921511\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n593\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n815.125249\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n595\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n815.215833\n]\n \nAfter\n \npcache_handle_fault\n\n\n[\n  \n815.259511\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n726\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n\n[\n  \n815.352071\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n567\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n815.444627\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n575\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n815.537186\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n583\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n815.629744\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n586\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n815.722303\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n593\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n815.916890\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n595\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n816.007471\n]\n \nAfter\n \npcache_handle_fault\n\n\n[\n  \n816.051151\n]\n \nCPU5\n \npid\n:\n32\n     \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n726\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n\n[\n  \n816.143715\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n816.279946\n]\n \ndo_exit\n()\n \npid\n:\n34\n,\ntgid\n:\n32\n \ncode\n:\n0x0\n\n\n[\n  \n816.331945\n]\n \nCPU5\n \npid\n:\n32\n \nline\n:\n \n567\n \nfrom_addr\n:\n \n0x6fc6d8\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n\n\n\n10\nth\n run, actually 2 threads are created. When pid 33 exit, everything stays okay. But after fork of pid 34. It went wrong:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\nwuklab13\n \n0311\n-\n8\n\n\n[\n  \n609.490893\n]\n \ndo_exit\n()\n \npid\n:\n33\n,\ntgid\n:\n32\n \ncode\n:\n0x0\n\n\n\n\n[\n  \n609.542894\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n401\n \nfrom_addr\n \n0x0\n\n\n[\n  \n609.640661\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n443\n \nfrom_addr\n \n0x0\n\n\n[\n  \n609.738429\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n465\n \nfrom_addr\n \n0x0\n\n\n[\n  \n609.836197\n]\n \nexit_mm\n:\n378\n \nmm\n-\nusers\n \n2\n \nmm\n-\ncount\n \n1\n\n\n[\n  \n609.891320\n]\n \nexit_mm\n:\n380\n \nmm\n-\nusers\n \n1\n \nmm\n-\ncount\n \n1\n\n\n[\n  \n609.946445\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n468\n \nfrom_addr\n \n0x0\n\n\n[\n  \n610.044212\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n471\n \nfrom_addr\n \n0x0\n\n\n[\n  \n610.141979\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n474\n \nfrom_addr\n \n0x0\n\n\n\n[\n  \n610.239747\n]\n \nSYSC_mmap\n(\ncpu5\n)\n:\n \nret_addr\n:\n0x7ffefbeac000\n\n\n[\n  \n610.299031\n]\n \nCPU6\n \npid\n:\n33\n \ncaller\n:\n \ndo_exit\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n482\n \nfrom_addr\n \n0x0\n\n\n[\n  \n610.396798\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n568\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n610.518489\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n576\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n610.640178\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n584\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n610.761866\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npmd\n \nis\n \nnone\n \nindex\n \n0x1e3\n \nline\n \n587\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n610.883557\n]\n \n__pte_alloc\n()\n:\n \nfor\n \naddr\n:\n \n0x7ffefc6acd90\n \npte_index\n:\n \nac\n\n\n[\n  \n610.956362\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n594\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n611.179051\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n596\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n611.297723\n]\n \nAfter\n \npcache_handle_fault\n\n\n[\n  \n611.341406\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ndo_page_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n726\n \nfrom_addr\n \n0x7ffefc6acd90\n\n\n[\n  \n611.455816\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n568\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n611.576464\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n576\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n611.697113\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n584\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n611.817762\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n587\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n611.938412\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n594\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n612.161103\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n596\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n[\n  \n612.279778\n]\n \nAfter\n \npcache_handle_fault\n\n\n[\n  \n612.323461\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ndo_page_fault\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n726\n \nfrom_addr\n \n0x7ffefc6abe78\n\n\n\n[\n  \n612.437875\n]\n \ndo_fork\n:\n \ncurrent\n:\n \n32\n \nnew\n:\n \n34\n\n\n\n\n[\n  \n612.484676\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n612.620924\n]\n \ndo_exit\n()\n \npid\n:\n34\n,\ntgid\n:\n32\n \ncode\n:\n0x0\n\n\n[\n  \n612.672928\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \npcache_handle_faultline\n:\n \n568\n \nfrom_addr\n:\n \n0x6fc6d8\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n\n[\n  \n612.793577\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n612.929828\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n613.066078\n]\n \nCPU7\n \npid\n:\n34\n \ncaller\n:\n \ndo_exitline\n:\n \n401\n \nfrom_addr\n:\n \n0x0\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n\n\n\n11\nth\n run, found it orignate from \ncopy_process()\n. Good.\n\n1\n2\n3\n4\n[\n  \n869.591729\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ndo_fork\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n886\n \nfrom_addr\n \n0x0\n\n\n\n[\n  \n869.688449\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n869.824681\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ndo_fork\n \nline\n:\n \n894\n \nfrom_addr\n:\n \n0x0\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n\n\n\n12\nth\n run, found the opeation that corrupt pgtable:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n[\n \n1099.974106\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ncopy_process\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n897\n \nfrom_addr\n \n0x0\n\n\n[\n \n1100.076032\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n \n1100.212282\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ncopy_process\n \nline\n:\n \n902\n \nfrom_addr\n:\n \n0x0\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n896\n         \nif\n \n(\ncurrent\n-\ntgid\n \n==\n \n32\n)\n\n\n897\n                 \njasmine\n(\n0\n,\n \n__LINE__\n,\n \n__func__\n);\n\n\n898\n\n\n899\n                         \nlist_add_tail\n(\np\n-\nthread_group\n,\n\n\n900\n                                           \np\n-\ngroup_leader\n-\nthread_group\n);\n\n\n901\n         \nif\n \n(\ncurrent\n-\ntgid\n \n==\n \n32\n)\n\n\n902\n                 \njasmine\n(\n0\n,\n \n__LINE__\n,\n \n__func__\n);\n\n\n\n\n\n\n13\nth\n run, interesting, the list_add_tail write to the pgtable. \npte.cont = 0xffff88207e8c31c0, p-\nthread_group: 0xffff88207e8c31c0\n.\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n916.269942\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ncopy_process\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n898\n \nfrom_addr\n \n0x0\n\n\n\n[\n  \n916.371863\n]\n \np\n:\n \nffff88207e8c3000\n \np\n-\ngroup_leader\n:\n \nffff88107e190000\n(\n32\n)\n \np\n-\nthread_group\n:\n \nffff88207e8c31c0\n \nleader\n-\nthread_grou\n:\n \nffff88107e1901c0\n\n\n\n[\n  \n916.523705\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n916.659947\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \ncopy_process\n \nline\n:\n \n906\n \nfrom_addr\n:\n \n0x0\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n[\n  \n916.769148\n]\n \np\n:\n \nffff88207e8c3000\n \np\n-\ngroup_leader\n:\n \nffff88107e190000\n(\n32\n)\n \np\n-\nthread_group\n:\n \nffff88207e8c31c0\n \nleader\n-\nthread_grou\n:\n \nffff88107e1901c0\n\n\n\n\n\n\n14\nth\n run, got an log like this. Clearly, the pte is written the value  of p-\nthread_group. But the leader\ns pointer is correct. Weird, going to dig deeper.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n                p: ffff88207e8c3000 p-\ngroup_leader: ffff88107e189000(32)\n\n                p-\nthread_group:        ffff88207e8c31c0\n                leader-\nthread_group:   ffff88107e1891c0\n\n                pte page:               ffff88207e8b5000\n                pte:                    ffff88207e8b51c0\n\n                pte.cont:               ffff88207e8c31c0\n\n\n\n\n\n15\nth\n run, found the bug.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\nwuklab13\n \n0311\n-\n15\n\n\n[\n \n1474.477687\n]\n \ndup_task_struct\n()\n:\n \ncurrent\n:\n \n32\n \nnew\n:\n \nffff88207e8b5000\n\n\n..\n\n\nwhile\n \npid\n \n33\n \nexit\n\n\nso\n \nthe\n \nffff88207e8b5000\n \nis\n \nfreed\n\n\n\nbut\n \nallocated\n \nagain\n \nby\n \npte_alloc\n\n\n[\n \n1481.420200\n]\n \n__pte_alloc\n()\n:\nCPU5\n \nfor\n \naddr\n:\n \n0x7ffefc6acd90\n \npte_index\n:\n \nac\n \nnew\n \npte\n \npage\n:\n \nffff88207e8b5000\n\n\n\n\nHowever\n,\n \nwe\n \nforgot\n \nto\n \nremove\n \nit\n \nfrom\n \ngroup_leader\ns\n \nthread_group\n\n\n\n[\n \n1485.895938\n]\n\n                \np\n:\n \nffff88207e8c3000\n\n                \np\n-\ngroup_leader\n:\n \nffff88107e19b000\n(\n32\n)\n\n\n                \np\n-\nthread_group\n:\n        \nffff88207e8c31c0\n\n                \nleader\n-\nthread_group\n:\n   \nffff88107e19b1c0\n\n\n\n[\n \n1486.047784\n]\n\n                \ntg\n-\nnext\n:\n               \nffff88207e8c31c8\n\n                \ntg\n-\nprev\n:\n               \nffff88207e8c31c0\n\n                \nleader\n-\ntg\n-\nnext\n        \nffff88107e19b1c8\n\n                \nleader\n-\ntg\n-\nprev\n        \nffff88107e19b1c0\n\n\n\n[\n \n1486.191311\n]\n  \nnext\n                    \nffff88107e19b1c0\n\n\n                \nprev\n                    \nffff88207e8b51c0\n\n\n                \nnext\n                    \nffff88107e19b1c0\n\n\n\n[\n \n1486.276594\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \n__list_add\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n61\n \nfrom_addr\n \n0x0\n \npage\n:\n \n0xffff88207e8b5000\n\n\n[\n \n1486.401399\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \n__list_add\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n65\n \nfrom_addr\n \n0x0\n \npage\n:\n \n0xffff88207e8b5000\n\n\n[\n \n1486.526203\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \n__list_add\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n69\n \nfrom_addr\n \n0x0\n \npage\n:\n \n0xffff88207e8b5000\n\n\n[\n \n1486.651010\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \n__list_add\n    \npte\n \nis\n \nnone\n \nindex\n \n0x38\n \nline\n \n73\n \nfrom_addr\n \n0x0\n \npage\n:\n \n0xffff88207e8b5000\n\n\n\n[\n \n1486.775814\n]\n \npte\n:\nffff88207e8b51c0\n \npfn\n:\n0x8207e8c3\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n \n1486.912060\n]\n \nCPU5\n \npid\n:\n32\n \ncaller\n:\n \n__list_add\n \nline\n:\n \n77\n \nfrom_addr\n:\n \n0x0\n \npte\n.\ncont\n:\n \n0xffff88207e8c31c0\n\n\n\n\n\n\n16\nth\n run, damn, after patching \n__unhash_process()\n, it finally works. Going to workout, see you tonight.\n\n\nvictim report error\n\n\n17\nth\n run. The phoenix program has bug itself, it is not able to run with 4GB dataset. So try it with 2GB dataset. Uuh, the log is too long. \n__put_vicim\n report a victim that has wrong flags. Going to disable the evict log and try again.\n\n\n18\nth\n run. Happen to run seq with 100MB\n It actually half finished. But the printf of phoenix has funny chars. I guess memory is corrupted. The log shows it is ib_mad_completion.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n[\n \n2244.018806\n]\n \nProcessor\n:\n \nProcessor\n \nmanager\n \nis\n \nrunning\n.\n\n\n[\n \n2246.394568\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nenvp\n[\n0\n]\n \nHOME\n=/\n\n\n\n]\n---\n\n\n[\n \n2246.447719\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nenvp\n[\n1\n]\n \nTERM\n=\nlinux\n\n\n\n]\n---\n\n\n[\n \n2246.507003\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nargv\n[\n0\n]\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\nseq\n\n\n\n]\n---\n\n\n[\n \n2246.618289\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nargv\n[\n1\n]\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_100MB\n.\ntxt\n\n\n\n]\n---\n\n\n[\n \n2258.805633\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nWord\n-\nCount\n-\nSeq\n:\n \nComputation\n \nCompleted\n \n12.46633\n \nsec\n\n\n\n]\n---\n\n\n[\n \n2258.923180\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nproc\n/\nmeminfo\n]\n\n\n[\n \n2258.995743\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nUse\n \nlen\n \nis\n \n123748\n\n\n[\n \n2263.484774\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nTHE\n:\n \n1115050\n\n\n]\n---\n\n\n[\n \n2263.666785\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nOF\n:\n \n615296\n\n\n]\n---\n\n\n[\n \n2266.103660\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nAND\n:\n \n545303\n \n(\na\n \nlot\n \nfunny\n \nchars\n,\n \ndeleted\n.)\n\n\n]\n---\n\n\n[\n \n2267.016837\n]\n \nCode\n:\n \n[\n \n2267.038680\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nTO\n:\n \n475179\n\n\n+\n\u00d5\u00fe\u00da\u00e9\u00d8\n^\nG\n\u00a7\n87\nk\n80\nz\n^\nT\n86\nruJ\n\u00b7\u00bf\u00bb\n9\ne\n\u00e9\u00de\u00ed\u00d1\nr\n\u00dc\u00d5\n^\nW\n\u00e5\n^\nW\n*^\n_\n{(\n\u00ca\n?\nR\n\u00f9\na\n\u00e9\u00f6\u00f7\n8\n\u00ed\n91\n\u00dc\u00e8\n8f\n\u00f2\u00bf\ni\n^?\n\u00e8\n4\n94\n\u00d7\u00b2\u00c9\u00b5\n^\nV\n\u00bf\u00ab\u00eb\nP\n]\n\u00ed\u00ef\nh\n^\nG\n\u00ca\u00eb\n98\n^\nT\n\u00d7\nQp\n\u00b9\nO\n\u00ae\u00ef\n^\n\\\u00da\n^?^\nA\n\u00ed\n91\n\u00d9\nv\n\u00dd\nBy\n^\n_\n\u00e9\niwP\n^\nr\n97\n\u00eb\u00f9\u00ef\u00df\n]\n\u00a3\u00df\u00ad\n98\n81\n\u00f8\n85\n\u00ce\nEy\n^\nY\n\u00e5\n^?\nV\n\u00f9\u00ba\n^\nY\n\u00de\u00f5\u00cb\n]\nr5\n\u00c9\u00f0\n^^\n92\n\u00c9\n]\n^\n]\nP\n^\n\u00c7\ni\n\u00bb\nz\n:\n\u00d4\n^\nS\n\n\n\u00ae\ne\n8\na\n+\n\\\u00e9\n8\na\n\u00ae\u00b1\u00e0\nE\n\u00d5\u00ce\n,\n\u00f0\u00d2\u00e2\n3\n\u00c1\n_\n^\nP_\n^\nH\n^\n[\n|\n\u00b8\u00ae\u00e1\ns\n\u00ed\nF\n\u00bf\nm\n95\n9\nd\n?\n82\n\u00f2\n:\n\u00be\u00de\u00f5\n3\n\u00ca\u00d7\nT\n\u00fc\u00ae\n\n\n]\n---\n\n\n[\n \n2263.339165\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \nffffffffffff8100\n\n\n[\n \n2263.422369\n]\n \nIP\n:\n \n[\nffffffffffff8100\n]\n \n0xffffffffffff8100\n\n\n[\n \n2263.570058\n]\n \nPGD\n \n1140067\n \nPUD\n \n1142067\n \nPMD\n \n0\n\n\n[\n \n2263.618942\n]\n \nOops\n:\n \n0010\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n \n2264.705811\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n27\n \nComm\n:\n \nib_mad_completi\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n408\n\n\n[\n \n2264.781736\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffffffff8100\n]\n  \n[\nffffffffffff8100\n]\n \n0xffffffffffff8100\n\n\n[\n \n2264.873262\n]\n \nRSP\n:\n \n0000\n:\nffff88107efabc90\n  \nEFLAGS\n:\n \n00010046\n\n\n[\n \n2264.936705\n]\n \nRAX\n:\n \n5636000000000098\n \nRBX\n:\n \ndb5affffffffffff\n \nRCX\n:\n \n0000000000000001\n\n\n[\n \n2265.021990\n]\n \nRDX\n:\n \nffff88107efabd38\n \nRSI\n:\n \n0000000000000000\n \nRDI\n:\n \n4460ff\nffffff8114\n\n\n[\n \n2265.107277\n]\n \nRBP\n:\n \nffff88107efabce0\n \nR08\n:\n \n000000000000001f\n \nR09\n:\n \nffff88107efa43c0\n\n\n[\n \n2265.192561\n]\n \nR10\n:\n \nffff88107efabe68\n \nR11\n:\n \n0000000000000001\n \nR12\n:\n \nac02000004ecbdbd\n\n\n[\n \n2265.277847\n]\n \nR13\n:\n \n0000000000000000\n \nR14\n:\n \nffff88107efa4228\n \nR15\n:\n \nffff88107e1ab000\n\n\n[\n \n2265.363133\n]\n \nFS\n:\n  \n0000000000000000\n(\n0000\n)\n \nGS\n:\nffff88107fc00000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n \n2265.459858\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n \n2265.528503\n]\n \nCR2\n:\n \nffffffffffff8100\n \nCR3\n:\n \n000000000113\nd000\n \nCR4\n:\n \n00000000000406\nb0\n\n\n[\n \n2265.613789\n]\n \nStack\n:\n\n\n[\n \n2265.637710\n]\n \nffffffff810151a7\n \n00000000000000\n82\n \nffff88107fc04980\n \n0000000000000000\n\n\n[\n \n2265.725075\n]\n \nffff88107efabcc8\n \nffff88107fc04980\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n2265.812441\n]\n \nffff88107efa4228\n \nffff88107e1ab000\n \nffff88107efabcf8\n \nffffffff81016e17\n\n\n[\n \n2265.899806\n]\n \n000000007\nefabe20\n \nffff88107efabd20\n \nffffffff810066f4\n \nffffffff81072f20\n\n\n[\n \n2265.987172\n]\n \nffff88107fc05e00\n \nffff88107efa4000\n \nffff88107efabe08\n \nffffffff8100e4aa\n\n\n[\n \n2266.074538\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n2266.206626\n]\n \nTSK\n\n\n[\n \n2266.229507\n]\n \n[\nffffffff810151a7\n]\n \n?\n \nupdate_wall_time\n+\n0x47\n/\n0x6b0\n\n\n[\n \n2266.299192\n]\n \n[\nffffffff81016e17\n]\n \ntick_handle_periodic\n+\n0x67\n/\n0x70\n\n\n[\n \n2266.369916\n]\n \n[\nffffffff810066f4\n]\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n2266.440641\n]\n \n[\nffffffff8100e4aa\n]\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n2266.516565\n]\n \n[\nffffffff810663b8\n]\n \n?\n \n__schedule\n+\n0xf8\n/\n0x1e0\n\n\n[\n \n2266.580010\n]\n \n[\nffffffff810664b3\n]\n \nschedule\n+\n0x13\n/\n0x30\n\n\n[\n \n2266.638254\n]\n \n[\nffffffff81058c97\n]\n \nib_mad_completion_handler\n+\n0x2b7\n/\n0x860\n\n\n[\n \n2266.716258\n]\n \n[\nffffffff810589e0\n]\n \n?\n \nib_mad_send_done_handler\n.\nisra\n.22\n+\n0x1d0\n/\n0x1d0\n\n\n[\n \n2266.803624\n]\n \n[\nffffffff81020376\n]\n \nkthread\n+\n0xf6\n/\n0x110\n\n\n[\n \n2266.861867\n]\n \n[\nffffffff81020280\n]\n \n?\n \n__kthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n \n2266.930512\n]\n \n[\nffffffff8100e732\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n[\n \n2266.993955\n]\n \nEOT\n\n\n\n\n\n\n19\nth\n, try seq+100MB again. Well succeed. I guess I start S too later. So that thread has issues. We run 12.3 sec, while linux run 9.7 sec.\n\n\n20\nth\n, try seq+4GB data. Linux runs \n314.4 sec\n. Lego runs \n403 sec\n. But Lego has some clflush error messages. I don\nt know why actually.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n[\n  \n794.604628\n]\n \nProcessor\n:\n \nProcessor\n \nmanager\n \nis\n \nrunning\n.\n\n\n[\n  \n796.884884\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nenvp\n[\n0\n]\n \nHOME\n=/\n\n\n\n]\n---\n\n\n[\n  \n796.938032\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nenvp\n[\n1\n]\n \nTERM\n=\nlinux\n\n\n\n]\n---\n\n\n[\n  \n796.997312\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nargv\n[\n0\n]\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\nseq\n\n\n\n]\n---\n\n\n[\n  \n797.108596\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nargv\n[\n1\n]\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_4GB\n.\ntxt\n\n\n\n]\n---\n\n\n[\n  \n980.640200\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n980.692315\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n980.746397\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n980.800478\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n980.854559\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n980.908642\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n980.962723\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.016804\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.070886\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.124968\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.179048\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.233129\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.287211\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.341293\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.395375\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.449456\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.503538\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.557619\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.611702\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.665782\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.719863\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.773945\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.828026\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.882108\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.936188\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n981.990271\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n982.044352\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n982.098434\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n982.152515\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n  \n982.206596\n]\n \n__clflush_one\n()\n:\n \nEFAULT\n:\nbad\n \naddress\n\n\n[\n \n1200.759741\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nWord\n-\nCount\n-\nSeq\n:\n \nComputation\n \nCompleted\n \n403.519401\n \nsec\n\n\n\n]\n---\n\n\n...\n\n\n[\n \n1200.989480\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nTHE\n:\n \n44602000\n\n\n...\n\n\n[\n \n1201.755779\n]\n \ndo_group_exit\n()\n \npid\n:\n32\n,\ntgid\n:\n32\n \nexit_code\n:\n0x0\n\n\n[\n \n1201.819136\n]\n \ndo_exit\n()\n \npid\n:\n32\n,\ntgid\n:\n32\n \ncode\n:\n0x0\n\n\n[\n \n1201.872451\n]\n \nnr_pgfault\n:\n \n1049525\n\n\n[\n \n1201.908579\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n \n1201.942899\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n \n1201.981380\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n \n1202.021941\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n \n1202.081223\n]\n \nnr_pcache_fill_from_memory\n:\n \n1045393\n\n\n[\n \n1202.135304\n]\n \nnr_pcache_fill_from_victim\n:\n \n4132\n\n\n[\n \n1202.186265\n]\n \nnr_pcache_eviction\n:\n \n525230\n\n\n[\n \n1202.230987\n]\n \nnr_victim_eviction\n:\n \n521090\n\n\n\n\n\n\n21th run. Do not have time and energy to debug the clflush issue. I just want to run MT+2GB again. Well victim has issues! Some warning are triggered. Log is \nwuklab13:~/ys/0311-22\n. Continue tomorrow! Good night world. (Such a lonly phd.)\n\n\n03/10 Sat\n\n\nRunning python hello world. Tried to make kmalloc use buddy directly.\n\n\nput_pcache in pcache_zap_pte\n\n\nSo this time, python keep running for a long time. But P crashed when the first time eviction was triggered.\n\n\nBelow is log from S side, those libraries do not exist, so these log are fine:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nS:\n[Mar10 10:39] handle_access_request /etc/ld.so.preload 4, -2\n[Mar10 10:44] local_file_open : Cannot open required file [/usr/lib64/python2.7/site.so].\n[  +0.352839] local_file_open : Cannot open required file [/usr/lib64/python2.7/sitemodule.so].\n[ +22.254465] local_file_open : Cannot open required file [/usr/lib64/python2.7/os.so].\n[  +0.350759] local_file_open : Cannot open required file [/usr/lib64/python2.7/osmodule.so].\n[Mar10 10:45] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpath.so].\n[  +0.358045] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpathmodule.so].\n[ +13.421033] local_file_open : Cannot open required file [/usr/lib64/python2.7/stat.so].\n[  +0.352838] local_file_open : Cannot open required file [/usr/lib64/python2.7/statmodule.so].\n[Mar10 10:46] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpath.so].\n[  +0.360126] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpathmodule.so].\n[ +11.582165] local_file_open : Cannot open required file [/usr/lib64/python2.7/warnings.so].\n[  +0.357003] local_file_open : Cannot open required file [/usr/lib64/python2.7/warningsmodule.so].\n[ +11.989828] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecache.so].\n[  +0.358043] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecachemodule.so].\n[Mar10 10:47] local_file_open : Cannot open required file [/usr/lib64/python2.7/types.so].\n[  +0.353879] local_file_open : Cannot open required file [/usr/lib64/python2.7/typesmodule.so].\n\n\n\n\n\nWeird P\ns bug, seems like the pcm returned by evict_find_line has issue. Well, I\nm trying to debug what is going with this set.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\nwuklab13\n \n0310\n-\n2\n\n\n[\n \n1046.880649\n]\n \nSYSC_read\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff6e117e0\n\n\n[\n \n1046.959692\n]\n     \nfd\n:\n \n8\n,\n \nbuf\n:\n \n00007ff\nff7ffb000\n,\n \ncount\n:\n \n4096\n\n\n[\n \n1048.726624\n]\n \npcache_evict_line\n()\n:\n \npset\n:\n \nffff88207f9ffec0\n,\n \nfor\n \nuva\n:\n \n0x7ffff7ffb000\n\n\n[\n \n1048.813053\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1048.868174\n]\n \nBUG\n:\n \nfailure\n \nat\n \n.\n/\ninclude\n/\nprocessor\n/\npcache\n.\nh\n:\n284\n/\npcache_meta_to_pcache_set\n()\n!\n\n\n[\n \n1048.965937\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1049.016898\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n347\n\n\n[\n \n1049.083460\n]\n \nStack\n:\n\n\n[\n \n1049.107380\n]\n \nffff88107e18fca8\n \nffffffff81026f1c\n \n000000000000000\n8\n \nffff88107e18fcb8\n\n\n[\n \n1049.194743\n]\n \nffff88107e18fc70\n \n0000000021475542\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1049.282107\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1049.369468\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1049.456832\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n \n1049.544193\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1049.573315\n]\n \nTSK\n\n\n[\n \n1049.596195\n]\n \n[\nffffffff81026f28\n]\n \npanic\n+\n0xc2\n/\n0xeb\n\n\n[\n \n1049.651318\n]\n \n[\nffffffff8101c3fc\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n \n1049.715799\n]\n \n[\nffffffff81019a65\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n \n1049.782360\n]\n \n[\nffffffff81017035\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n \n1049.855163\n]\n \n[\nffffffff81006764\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n1049.927966\n]\n \n[\nffffffff8101c3fc\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n \n1049.992447\n]\n \n[\nffffffff81019a65\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n \n1050.059009\n]\n \n[\nffffffff81017035\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n \n1050.131812\n]\n \n[\nffffffff8103c41a\n]\n \n?\n \nput_dec\n+\n0x1a\n/\n0x80\n\n\n[\n \n1050.191093\n]\n \n[\nffffffff81006764\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n \n1050.263895\n]\n \n[\nffffffff8100e56a\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n \n1050.341897\n]\n \n[\nffffffff81012ded\n]\n \n?\n \nprintk\n+\n0x11d\n/\n0x1b0\n\n\n[\n \n1050.402219\n]\n \n[\nffffffff810340c5\n]\n \ndump_pcache_meta\n+\n0xc5\n/\n0xd0\n\n\n[\n \n1050.468782\n]\n \n[\nffffffff81034588\n]\n \npcache_evict_line\n+\n0x158\n/\n0x220\n\n\n[\n \n1050.538463\n]\n \n[\nffffffff81030f5e\n]\n \npcache_alloc\n+\n0x22e\n/\n0x2f0\n\n\n[\n \n1050.602945\n]\n \n[\nffffffff8103015a\n]\n \ncommon_do_fill_page\n+\n0x2a\n/\n0x430\n\n\n[\n \n1050.673668\n]\n \n[\nffffffff8102fb20\n]\n \n?\n \npcache_meta_to_kva\n+\n0x30\n/\n0x30\n\n\n[\n \n1050.744389\n]\n \n[\nffffffff81030702\n]\n \npcache_handle_fault\n+\n0x1a2\n/\n0x6c0\n\n\n[\n \n1050.816152\n]\n \n[\nffffffff810103d2\n]\n \ndo_page_fault\n+\n0xa2\n/\n0x1a0\n\n\n[\n \n1050.880634\n]\n \n[\nffffffff8100db9f\n]\n \npage_fault\n+\n0x1f\n/\n0x30\n\n\n[\n \n1050.940955\n]\n \n[\nffffffff8103bb82\n]\n \n?\n \ncopy_user_enhanced_fast_string\n+\n0x2\n/\n0x10\n\n\n[\n \n1051.023118\n]\n \n[\nffffffff81038423\n]\n \n?\n \nnormal_p2m_read\n+\n0x233\n/\n0x330\n\n\n[\n \n1051.092800\n]\n \n[\nffffffff810363ce\n]\n \nsys_read\n+\n0x9e\n/\n0x160\n\n\n[\n \n1051.152081\n]\n \n[\nffffffff810268d0\n]\n \n?\n \nstrace_enter_default\n+\n0x30\n/\n0x40\n\n\n[\n \n1051.224884\n]\n \n[\nffffffff8100e935\n]\n \ndo_syscall_64\n+\n0x45\n/\n0xd0\n\n\n[\n \n1051.288326\n]\n \n[\nffffffff8100d82c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n\n\n\n\nInteresting, added several debug messages. The bug is I forgot to put_pcache when a rmap was zapped. One rmap counts one refcount (effectively one process), thus when a rmap was zapped, we should decrease the refcount. I found I\nve already done so for \npcache_remove_rmap\n, and \npcache_move_pte\n. But damn, forgot this one. I remember this code was written before fork+pcache. So.. I don\nt have a big picture at that time. \nMultithreaded system plus background reclaim really a very rigours design usage of refcount and lock\n.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n[\n \n1418.038411\n]\n \nCPU5\n \nPID32\n \nsys_read\n+\n0x0\n/\n0xa0\n\n\n[\n \n1418.085227\n]\n \npcache_evict_line\n()\n:\n \npset\n:\n \nffff88207f9ffec0\n,\n \nfor\n \nuva\n:\n \n0x7ffff7ffb000\n\n\n[\n \n1418.173617\n]\n \npset\n:\nffff88207f9ffec0\n \nset_idx\n:\n \n32763\n \nnr_lru\n:\n8\n\n\n[\n \n1418.238105\n]\n \npcache\n:\nffff8801801ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880107ffb000\n\n\n[\n \n1418.351476\n]\n \npcache\n:\nffff8801805ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880117ffb000\n\n\n[\n \n1418.464847\n]\n \npcache\n:\nffff8801809ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880127ffb000\n\n\n[\n \n1418.578220\n]\n \npcache\n:\nffff880180dffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880137ffb000\n\n\n[\n \n1418.691591\n]\n \npcache\n:\nffff8801811ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880147ffb000\n\n\n[\n \n1418.804963\n]\n \npcache\n:\nffff8801815ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880157ffb000\n\n\n[\n \n1418.918334\n]\n \npcache\n:\nffff8801819ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880167ffb000\n\n\n[\n \n1419.031706\n]\n \npcache\n:\nffff880181dffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880177ffb000\n\n\n[\n \n1419.145077\n]\n \nAfter\n \ndump\n \npset\n\n\n[\n \n1419.176280\n]\n \npcache\n:\nffff8801801ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880107ffb000\n\n\n[\n \n1419.289652\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1419.351018\n]\n \npcache\n:\nffff8801805ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880117ffb000\n\n\n[\n \n1419.464389\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1419.525757\n]\n \npcache\n:\nffff8801809ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880127ffb000\n\n\n[\n \n1419.639127\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1419.700494\n]\n \npcache\n:\nffff880180dffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880137ffb000\n\n\n[\n \n1419.813865\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1419.875231\n]\n \npcache\n:\nffff8801811ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880147ffb000\n\n\n[\n \n1419.988604\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1420.049969\n]\n \npcache\n:\nffff8801815ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880157ffb000\n\n\n[\n \n1420.163341\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1420.224708\n]\n \npcache\n:\nffff8801819ffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880167ffb000\n\n\n[\n \n1420.338079\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1420.399445\n]\n \npcache\n:\nffff880181dffec0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nkva\n:\n \nffff880177ffb000\n\n\n[\n \n1420.512817\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nevict_find_line_lru\n\n\n[\n \n1420.574183\n]\n \nevict_find_line_lru\n()\n:\n \npcm\n:\n \nffff88207f9ffea8\n\n\n[\n \n1420.637631\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n1420.692756\n]\n \nBUG\n:\n \nfailure\n \nat\n \n.\n/\ninclude\n/\nprocessor\n/\npcache\n.\nh\n:\n340\n/\npcache_meta_to_kva\n()\n!\n\n\n[\n \n1420.783245\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n1420.834210\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n349\n\n\n[\n \n1420.900777\n]\n \nStack\n:\n\n\n\n\n\n\npython hello world run to end\n\n\nGlad to say, python hello world finished, even with some missed syscalls. Especially the stdin stuff, so the string is actually not printed out. Log is wuklab13:~/ys/0310-4\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n[\n \n3149.540308\n]\n \nCPU5\n \nPID32\n \nsys_ioctl\n+\n0x0\n/\n0x10\n\n\n[\n \n3149.588144\n]\n \nCPU5\n \nPID32\n \nsys_ioctl\n+\n0x0\n/\n0x10\n\n\n[\n \n3149.635982\n]\n \nCPU5\n \nPID32\n \nsys_write\n+\n0x0\n/\n0xa0\n\n\n[\n \n3149.683818\n]\n \nSTDOUT\n:\n \n---\n[\n\n\n\n\n]\n---\n\n\n[\n \n3149.726456\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff6d9aeb0\n \nflags\n:\n0x150\n\n\n[\n \n3149.926247\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff6d9aeb0\n \nflags\n:\n0x150\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n \n3150.033464\n]\n \nCPU5\n \nPID32\n \nsys_newfstat\n+\n0x0\n/\n0x10\n\n\n[\n \n3150.084420\n]\n \nCPU5\n \nPID32\n \nsys_ioctl\n+\n0x0\n/\n0x10\n\n\n[\n \n3150.132256\n]\n \nstrace__mmap\n \ncpu5\n \naddr\n=\n0x0\n,\n \nlen\n=\n0x1000\n,\n \nprot\n(\n0x3\n)\n=\nPROT_READ\n|\nPROT_WRITE\n,\n \nflags\n(\n0x22\n)\n=\nMAP_PRIVATE\n|\nMAP_ANONYMOUS\n,\n \nfd\n=\n18446744073709551615\n(\n \n),\n \noff\n=\n0x0\n\n\n[\n \n3150.301772\n]\n \nCPU5\n \nPID32\n \nsys_read\n+\n0x0\n/\n0xa0\n\n\n[\n \n3150.348562\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n3150.403679\n]\n \nWARNING\n:\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nat\n \nmanagers\n/\nprocessor\n/\nfs\n/\nstdio\n.\nc\n:\n24\n \nstdio_file_read\n+\n0x30\n/\n0x50\n\n\n[\n \n3150.509751\n]\n \nProcess\n \nwants\n \nSTDIN\n!\n\n\n[\n \n3150.546149\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n352\n\n\n[\n \n3150.612705\n]\n \nStack\n:\n\n\n[\n \n3150.636624\n]\n \nffff88107e18fe90\n \nffffffff81012b15\n \nffffffff811464e0\n \n00007ff\nff7ffb000\n\n\n[\n \n3150.723977\n]\n \n0000000000000400\n \n00007ff\nff70e5640\n \nffff88107e18fef0\n \nffffffff81012bd2\n\n\n[\n \n3150.811331\n]\n \nffffffff81079d6b\n \nffff881000000018\n \nffff88107e18ff00\n \nffff88107e18fec0\n\n\n[\n \n3150.898687\n]\n \n0000000000000020\n \nffffffff810346b0\n \n0000000000000022\n \nffffffff811464f0\n\n\n[\n \n3150.986040\n]\n \n00007ff\nff7fdf740\n \n0000000000000000\n \nffff88107e18ff00\n \nffffffff81035ac0\n\n\n[\n \n3151.073394\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n3151.102514\n]\n \nTSK\n\n\n[\n \n3151.125392\n]\n \n[\nffffffff81012b21\n]\n \n__warn\n.\nconstprop\n.0\n+\n0x91\n/\n0xd0\n\n\n[\n \n3151.194028\n]\n \n[\nffffffff81012bd2\n]\n \nwarn_slowpath_fmt\n+\n0x42\n/\n0x50\n\n\n[\n \n3151.261623\n]\n \n[\nffffffff810346b0\n]\n \n?\n \nsweep_pset_lru\n+\n0x220\n/\n0x220\n\n\n[\n \n3151.330259\n]\n \n[\nffffffff81035ac0\n]\n \nstdio_file_read\n+\n0x30\n/\n0x50\n\n\n[\n \n3151.395775\n]\n \n[\nffffffff810346e3\n]\n \nsys_read\n+\n0x33\n/\n0xa0\n\n\n[\n \n3151.454010\n]\n \n[\nffffffff8100e875\n]\n \ndo_syscall_64\n+\n0x45\n/\n0xd0\n\n\n[\n \n3151.517446\n]\n \n[\nffffffff8100d76c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n \n3151.593362\n]\n \nEOT\n\n\n[\n \n3151.616240\n]\n \n---\n[\n \nend\n \ntrace\n \n0000000000000000\n \n]\n---\n\n\n[\n \n3151.671360\n]\n \nCPU5\n \nPID32\n \nsys_write\n+\n0x0\n/\n0xa0\n\n\n[\n \n3151.719194\n]\n \nSTDOUT\n:\n \n---\n[\n\n\n\n\n]\n---\n\n\n[\n \n3151.759756\n]\n \nCPU5\n \nPID32\n \nsys_close\n+\n0x0\n/\n0x140\n\n\n[\n \n3151.808628\n]\n \nSYSC_close\n()\n:\n \n[\n3\n]\n \n-\n \n[\n/\nroot\n/\nys\n/\npy_hello\n.\npy\n]\n\n\n[\n \n3151.871028\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a79380\n \nflags\n:\n0x150\n\n\n[\n \n3152.070817\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a79380\n \nflags\n:\n0x150\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n \n3152.178033\n]\n \nCPU5\n \nPID32\n \nsys_rt_sigaction\n+\n0x0\n/\n0xb0\n\n\n[\n \n3152.234151\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a77f60\n \nflags\n:\n0x150\n\n\n[\n \n3152.432941\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a77f60\n \nflags\n:\n0x150\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n \n3152.540242\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff73ee794\n \nflags\n:\n0x150\n\n\n[\n \n3152.739952\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff73ee794\n \nflags\n:\n0x150\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n \n3152.847171\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff715b278\n \nflags\n:\n0x150\n\n\n[\n \n3153.046958\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff715b278\n \nflags\n:\n0x150\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n \n3153.154179\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff6de74f0\n \nflags\n:\n0x150\n\n\n[\n \n3153.353965\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff6de74f0\n \nflags\n:\n0x150\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n \n3153.461180\n]\n \nCPU5\n \nPID32\n \nsys_exit_group\n+\n0x0\n/\n0x10\n\n\n\n\n\n\nTrying phoenix pthread again\n\n\n4GB pcache, 1GB dataset.\n\n\n1\nth\n run with CONFIG_STRACE on, 1GB dataset finished, result is correct.\n\n\n2\nth\n run without CONFIG_STRACE, 1GB dataset stuck. Two weird things:\n\n\n\n\nopen/close dev/cpu/online file too many times than a normal linux run\n\n\nIB stucked\nSo next I\nm going to try add a lock to ibapi, see if it is ib internal deadlock issue.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\nwuklab13\n \n0310\n-\n7\n\n\n[\n  \n702.895936\n]\n \nProcessor\n:\n \nProcessor\n \nmanager\n \nis\n \nrunning\n.\n\n\n[\n  \n722.400159\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nenvp\n[\n0\n]\n \nHOME\n=/\n\n\n\n]\n---\n\n\n[\n  \n722.453307\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nenvp\n[\n1\n]\n \nTERM\n=\nlinux\n\n\n\n]\n---\n\n\n[\n  \n722.512589\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nargv\n[\n0\n]\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\npthread\n\n\n\n]\n---\n\n\n[\n  \n722.628036\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nargv\n[\n1\n]\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_1GB\n.\ntxt\n\n\n\n]\n---\n\n\n[\n  \n722.759101\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nWordcount\n:\n \nRunning\n...\n\n\n]\n---\n\n\n[\n  \n722.819406\n]\n \nSTDOUT\n:\n \n---\n[\n\n\n\n\n]\n---\n\n\n[\n  \n722.860139\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n722.940653\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.011483\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.084287\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.157090\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.229894\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.302698\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.375502\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.448306\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.521111\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.593914\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.666718\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.739522\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.812326\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n723.885130\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n766.701260\n]\n \nibapi_send_reply\n()\n \npolling\n \ntimeout\n \n(\n30010\n \nms\n),\n \ncaller\n:\n \nnet_send_reply_timeout\n+\n0x11b\n/\n0x1ee\n\n\n[\n  \n766.809538\n]\n  \nnet_send_reply_timeout\n()\n \ncaller\n:\n \n__pcache_do_fill_page\n+\n0x82\n/\n0x140\n\n\n[\n  \n766.895863\n]\n \nword_count\n-\npthr\n[\n65\n]\n:\n \nsegfault\n \nat\n \n0x7fffb5eba000\n \nip\n \n00000000004024\n9\nd\n \nsp\n \n00007ff\nfb5e9ad80\n \nerror\n \n6\n\n\n[\n  \n767.012348\n]\n \nCPU\n:\n \n15\n \nPID\n:\n \n65\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n359\n\n\n[\n  \n767.089312\n]\n \nRIP\n:\n \n0033\n:\n[\n00000000004024\n9\nd\n]\n  \n[\n00000000004024\n9\nd\n]\n \n0x40249d\n\n\n[\n  \n767.170436\n]\n \nRSP\n:\n \n002\nb\n:\n00007ff\nfb5e9ad80\n  \nEFLAGS\n:\n \n00010216\n\n\n[\n  \n767.233879\n]\n \nRAX\n:\n \n00007ff\nfb5eba000\n \nRBX\n:\n \n00000000000013\n88\n \nRCX\n:\n \n000000000000004f\n\n\n[\n  \n767.319164\n]\n \nRDX\n:\n \n00007ff\nfe4ea92a4\n \nRSI\n:\n \n00007ff\nfe626fac9\n \nRDI\n:\n \n00007ff\nfe4ea92a4\n\n\n[\n  \n767.404449\n]\n \nRBP\n:\n \n00000000007540e0\n \nR08\n:\n \n0000000000000000\n \nR09\n:\n \n0000000000014f\na0\n\n\n[\n  \n767.489733\n]\n \nR10\n:\n \n0000000000427f\nb0\n \nR11\n:\n \n0000000000000202\n \nR12\n:\n \n0000000000012\nb12\n\n\n[\n  \n767.575018\n]\n \nR13\n:\n \n00007ff\nf496ab890\n \nR14\n:\n \n00007ff\nf48704fb0\n \nR15\n:\n \n00000000000013\n88\n\n\n[\n  \n767.660303\n]\n \nFS\n:\n  \n00007ff\nfb5e9b700\n(\n0000\n)\n \nGS\n:\nffff88207fce0000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n  \n767.757028\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n  \n767.825671\n]\n \nCR2\n:\n \n00007ff\nfb5eba000\n \nCR3\n:\n \n000000207f\ne3a000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n  \n767.910958\n]\n \nget_signal\n()\n:\n \ndequeue_signr\n:\n \n11\n,\n \nhandler\n:\n          \n(\nnull\n)\n\n\n[\n  \n767.987928\n]\n \nget_signal\n()\n:\n \ndequeue_signr\n:\n \n9\n,\n \nhandler\n:\n          \n(\nnull\n)\n\n\n\n\n\n\n\n3\nth\n run, without STRACE, with locked ibapi, it finished, result is correct. Runtime: \n18.692936 sec\n.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n[\n  \n555.423623\n]\n \nnr_pgfault\n:\n \n288100\n\n\n[\n  \n555.458042\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n555.492360\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n555.530838\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n555.571396\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n555.630673\n]\n \nnr_pcache_fill_from_memory\n:\n \n288081\n\n\n[\n  \n555.683710\n]\n \nnr_pcache_fill_from_victim\n:\n \n12\n\n\n[\n  \n555.732588\n]\n \nnr_pcache_eviction\n:\n \n494\n\n\n[\n  \n555.774187\n]\n \nnr_victim_eviction\n:\n \n474\n\n\n\n\n\n\n4\nth\n run, same setting with the 3\nth\n run, same result. But the nr_pgfault differs, I guess it is due to runtime things. Runtime: \n19.12861 sec\n.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n[\n  \n469.891700\n]\n \nnr_pgfault\n:\n \n288119\n\n\n[\n  \n469.926123\n]\n \nnr_pgfault_wp\n:\n \n0\n\n\n[\n  \n469.960444\n]\n \nnr_pgfault_wp_cow\n:\n \n0\n\n\n[\n  \n469.998924\n]\n \nnr_pgfault_wp_reuse\n:\n \n0\n\n\n[\n  \n470.039484\n]\n \nnr_pgfault_due_to_concurrent_eviction\n:\n \n0\n\n\n[\n  \n470.098764\n]\n \nnr_pcache_fill_from_memory\n:\n \n288093\n\n\n[\n  \n470.151805\n]\n \nnr_pcache_fill_from_victim\n:\n \n12\n\n\n[\n  \n470.200684\n]\n \nnr_pcache_eviction\n:\n \n513\n\n\n[\n  \n470.242285\n]\n \nnr_victim_eviction\n:\n \n493\n\n\n\n\n\n\n5\nth\n run, same with 4\nth\n, succeed, Runtime: \n18.653879 sec\n.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n[  313.202348] nr_pgfault: 288070\n[  313.236772] nr_pgfault_wp: 0\n[  313.271093] nr_pgfault_wp_cow: 0\n[  313.309575] nr_pgfault_wp_reuse: 0\n[  313.350139] nr_pgfault_due_to_concurrent_eviction: 0\n[  313.409421] nr_pcache_fill_from_memory: 288052\n[  313.462465] nr_pcache_fill_from_victim: 6\n[  313.510307] nr_pcache_eviction: 446\n[  313.551909] nr_victim_eviction: 432\n\n\n\n\n\n6\nth\n, setting is the same, but with 4GB dataset, crashed:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n[\n  \n512.028141\n]\n \nProcessor\n:\n \nProcessor\n \nmanager\n \nis\n \nrunning\n.\n\n\n[\n  \n529.375605\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nWordcount\n:\n \nRunning\n...\n\n\n]\n---\n\n\n[\n  \n529.435906\n]\n \nSTDOUT\n:\n \n---\n[\n\n\n\n\n]\n---\n\n\n[\n  \n529.476660\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n  \n529.555983\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n529.609128\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n735\n/\npcache_zap_pte\n()\n!\n\n\n[\n  \n529.699613\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n  \n529.750576\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n361\n\n\n[\n  \n529.826500\n]\n \nStack\n:\n\n\n[\n  \n529.850422\n]\n \nffff88107e1a3dd8\n \nffffffff810259b4\n \n000000000000000\n8\n \nffff88107e1a3de8\n\n\n[\n  \n529.937787\n]\n \nffff88107e1a3da0\n \n0000000021475542\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n530.025152\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n530.112517\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n530.199882\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n530.287247\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n530.316370\n]\n \nTSK\n\n\n[\n  \n530.339251\n]\n \n[\nffffffff810259c0\n]\n \npanic\n+\n0xc2\n/\n0xeb\n\n\n[\n  \n530.394374\n]\n \n[\nffffffff8106190a\n]\n \n?\n \nclient_internal_poll_sendcq\n+\n0x2a\n/\n0x80\n\n\n[\n  \n530.474458\n]\n \n[\nffffffff8101bfcc\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n  \n530.538943\n]\n \n[\nffffffff81019725\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n  \n530.605506\n]\n \n[\nffffffff81016df5\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n  \n530.678311\n]\n \n[\nffffffff8103768a\n]\n \n?\n \nput_dec\n+\n0x1a\n/\n0x80\n\n\n[\n  \n530.737595\n]\n \n[\nffffffff810066f4\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n  \n530.810398\n]\n \n[\nffffffff8100e4aa\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n530.888403\n]\n \n[\nffffffff81012ccd\n]\n \n?\n \nprintk\n+\n0x11d\n/\n0x1b0\n\n\n[\n  \n530.948726\n]\n \n[\nffffffff81030429\n]\n \npcache_zap_pte\n+\n0xf9\n/\n0x160\n\n\n[\n  \n531.014250\n]\n \n[\nffffffff8102f090\n]\n \n?\n \n__pcache_move_pte_fastpath\n+\n0x50\n/\n0x50\n\n\n[\n  \n531.093295\n]\n \n[\nffffffff8102c8dc\n]\n \nunmap_page_range\n+\n0x32c\n/\n0x3b0\n\n\n[\n  \n531.161940\n]\n \n[\nffffffff8102c97e\n]\n \nrelease_pgtable\n+\n0x1e\n/\n0x40\n\n\n[\n  \n531.227463\n]\n \n[\nffffffff8102bfb3\n]\n \nsys_munmap\n+\n0xc3\n/\n0x120\n\n\n[\n  \n531.288827\n]\n \n[\nffffffff8100e86d\n]\n \ndo_syscall_64\n+\n0x3d\n/\n0xc0\n\n\n[\n  \n531.352270\n]\n \n[\nffffffff8100d76c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n\n\n\n\n7\nth\n run, add debug info, does not seem that useful:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n]\n---\n\n\n[\n15755.579501\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n15755.672760\n]\n \npte\n:\nffff88107e1a3dd8\n \npfn\n:\n0x8207e80b\n \nflags\n:(\ndirty\n|\nlarge\n|\nglobal\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n15755.807015\n]\n \npte\n \ndumped\n \nbecause\n:\n \nInvalid\n \npte\n\n\n[\n15755.856932\n]\n \naddress\n:\n \n0x7ffefc638000\n\n\n[\n15755.899569\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n15755.954684\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nrmap\n.\nc\n:\n747\n/\npcache_zap_pte\n()\n!\n\n\n[\n15756.045159\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n15756.096114\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \nword_count\n-\npt\n\n\n\n\n\n\nTried several times, even with mmap/munmap debug option on, it crashed at the same point. Key is address \n0x7ffefc638000\n, and the mmap() related to it.\n\n\nClose to find the bug. Latest log in 0310-18.\n\n\n\n\n03/09 Fri\n\n\nFind bug in kmalloc\n\n\nTried to print pud in every syscall and catch the criminal:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\nwuklab13\n \n030\n9\n-\n1\n\n\n[\n  \n320.088684\n]\n \nCPU5\n \nPID32\n \nsys_close\n+\n0x0\n/\n0x1f0\n\n\n[\n  \n320.137567\n]\n \ndo_syscall_64\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fc6f000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fc6f000\n\n\n[\n  \n320.269657\n]\n \nSYSC_close\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff7df3c37\n\n\n[\n  \n320.349742\n]\n     \n3\n\n\n[\n  \n320.372624\n]\n \nSYSC_close\n()\n:\n \n[\n3\n]\n \n-\n \n[\n/\nlib64\n/\nlibpython2\n.7\n.\nso\n.1.0\n]\n\n\n[\n  \n320.441268\n]\n \nSYSC_close\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nret\n:\n \n0x0\n \n(\n0\n)\n\n\n[\n  \n320.510954\n]\n \ndo_syscall_64\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fc6f000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fc6f000\n\n\n\n[\n  \n320.643043\n]\n     \naddr\n:\n \n0x7ffff7a101f0\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n320.709607\n]\n     \naddr\n:\n \n0x7ffff7a101f0\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcaeff8\n\n\n[\n  \n320.798014\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a101f0\n \nflags\n:\n0x50\n\n\n[\n  \n320.995755\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a101f0\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n\n[\n  \n321.101944\n]\n     \naddr\n:\n \n0x7ffff7a21749\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n321.168509\n]\n     \naddr\n:\n \n0x7ffff7a21749\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcaeff8\n\n\n[\n  \n321.256914\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a21749\n \nflags\n:\n0x50\n\n\n[\n  \n321.454651\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a21749\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n\n[\n  \n321.560845\n]\n     \naddr\n:\n \n0x7ffff7ff2fda\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n321.627409\n]\n     \naddr\n:\n \n0x7ffff7ff2fda\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcaeff8\n\n\n[\n  \n321.715815\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7ff2fda\n \nflags\n:\n0x50\n\n\n[\n  \n321.913553\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7ff2fda\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n\n[\n  \n322.019745\n]\n \nCPU5\n \nPID32\n \nsys_open\n+\n0x0\n/\n0x10\n\n\n[\n  \n322.066548\n]\n \ndo_syscall_64\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff9001801ff000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff9001801ff000\n\n\n[\n  \n322.198638\n]\n \nSYSC_open\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff7df3b27\n\n\n[\n  \n322.277683\n]\n     \nf_name\n:\n \n/\nlib64\n/\nlibpthread\n.\nso\n.0\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \ne150\n\n\n[\n  \n322.357780\n]\n \nSYSC_open\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nret\n:\n \n0x3\n \n(\n3\n)\n\n\n[\n  \n322.426414\n]\n \ndo_syscall_64\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff9001801ff000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff9001801ff000\n\n\n\n\n\n\nAfter printing more in pcache_handle_fault, I found who corrupted pgtable:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\nwuklab13\n \n030\n9\n-\n5\n\n\n[\n  \n661.308584\n]\n \nCPU5\n \nPID32\n \nsys_close\n+\n0x0\n/\n0x1f0\n\n\n[\n  \n661.357466\n]\n \ndo_syscall_64\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n661.489557\n]\n \nSYSC_close\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff7df3c37\n\n\n[\n  \n661.569642\n]\n     \n3\n    \n\n[\n  \n661.592525\n]\n \nSYSC_close\n()\n:\n \n[\n3\n]\n \n-\n \n[\n/\nlib64\n/\nlibpython2\n.7\n.\nso\n.1.0\n]\n\n\n[\n  \n661.661170\n]\n \nSYSC_close\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nret\n:\n \n0x0\n \n(\n0\n)\n\n\n[\n  \n661.730854\n]\n \ndo_syscall_64\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n661.862944\n]\n \npcache_handle_fault\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n662.001275\n]\n     \naddr\n:\n \n0x7ffff7a101f0\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n662.067840\n]\n     \naddr\n:\n \n0x7ffff7a101f0\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcafff8\n\n\n[\n  \n662.156247\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a101f0\n \nflags\n:\n0x50\n\n\n[\n  \n662.353985\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a101f0\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n  \n662.460176\n]\n \npcache_handle_fault\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n\n[\n  \n662.600586\n]\n \npcache_handle_fault\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n662.738916\n]\n     \naddr\n:\n \n0x7ffff7a21749\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n662.805481\n]\n     \naddr\n:\n \n0x7ffff7a21749\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcafff8\n\n\n[\n  \n662.893888\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a21749\n \nflags\n:\n0x50\n\n\n[\n  \n663.091636\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a21749\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n  \n663.197831\n]\n \npcache_handle_fault\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n\n[\n  \n663.338242\n]\n \npcache_handle_fault\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n663.476572\n]\n     \naddr\n:\n \n0x7ffff7ff2fda\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n663.543135\n]\n     \naddr\n:\n \n0x7ffff7ff2fda\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcafff8\n\n\n[\n  \n663.631543\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7ff2fda\n \nflags\n:\n0x50\n\n\n[\n  \n663.829279\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7ff2fda\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n  \n663.935472\n]\n \npcache_handle_fault\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff9001801ff000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff9001801ff000\n\n\n\n\n[\n  \n664.075884\n]\n \nCPU5\n \nPID32\n \nsys_open\n+\n0x0\n/\n0x10\n\n\n[\n  \n664.122686\n]\n \ndo_syscall_64\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff9001801ff000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff9001801ff000\n\n\n[\n  \n664.254776\n]\n \nSYSC_open\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff7df3b27\n\n\n[\n  \n664.333821\n]\n     \nf_name\n:\n \n/\nlib64\n/\nlibpthread\n.\nso\n.0\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \ne150\n\n\n[\n  \n664.413918\n]\n \nSYSC_open\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nret\n:\n \n0x3\n \n(\n3\n)\n\n\n[\n  \n664.482552\n]\n \ndo_syscall_64\n()\n:\n \nleave\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff9001801ff000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff9001801ff000\n\n\n\n\n\n\nThen, try catching bug with address \n0x7ffff7ff2fda\n fault. Printing still being the most effective way to debug. :-)\n\n\nDig further, I found pgtable corrupted after \npcache_add_rmap()\n, namely after \nalloc_pcache_rmap()\n:\n\n1\n2\n3\n[\n \n5024.482570\n]\n \npcache_add_rmap\n()\n \n343\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n \n5024.613601\n]\n \nalloc_pcache_rmap\n()\n:\n \nsize\n:\n \n56\n,\n \nrmap\n:\n \nffff88207fccefd0\n\n\n[\n \n5024.686396\n]\n \npcache_add_rmap\n()\n \n358\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90207fcce000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90207fcce000\n\n\n\n\n\n\nWell, \nrmap\n:\n \nffff88207fccefd0\n \n \nffff90207fcce000\n, clearly\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n[\n  \n843.916517\n]\n \npcache_add_rmap\n()\n \n372\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n844.047557\n]\n \nalloc_pcache_rmap\n()\n \n60\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n844.179638\n]\n \nalloc_pcache_rmap\n()\n:\n \nsize\n:\n \n56\n,\n \nrmap\n:\n \nffff88207fccefd0\n\n\n[\n  \n844.252438\n]\n \nalloc_pcache_rmap\n()\n \n71\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n844.384517\n]\n \nalloc_pcache_rmap\n()\n:\n \nsize\n:\n \n56\n,\n \nrmap\n:\n \nffff88207fccefd0\n\n\n[\n  \n844.457317\n]\n \nalloc_pcache_rmap\n()\n \n85\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90207fcce000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90207fcce000\n\n\n[\n  \n844.589398\n]\n \npcache_add_rmap\n()\n \n387\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90207fcce000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90207fcce000\n\n\n\n46\n \nstatic\n \nstruct\n \npcache_rmap\n \n*\nalloc_pcache_rmap\n(\nvoid\n)\n\n\n47\n \n{\n\n\n48\n         \nstruct\n \npcache_rmap\n \n*\nrmap\n;\n\n\n49\n\n\n50\n         \npgd_t\n \n*\npgd\n;\n\n\n51\n         \npud_t\n \n*\npud\n;\n\n\n52\n         \nunsigned\n \nlong\n \naddr\n;\n\n\n53\n         \nstruct\n \nmm_struct\n \n*\nmm\n \n=\n \ncurrent\n-\nmm\n;\n\n\n54\n\n\n55\n         \nif\n \n(\npall\n)\n \n{\n\n\n56\n                 \naddr\n \n=\n \n0x601008\n;\n\n\n57\n                 \npgd\n \n=\n \npgd_offset\n(\nmm\n,\n \naddr\n);\n\n\n58\n                 \npud\n \n=\n \npud_alloc\n(\nmm\n,\n \npgd\n,\n \naddr\n);\n\n\n59\n                 \npr_info\n(\n%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p\n\\n\n,\n\n\n60\n                         \n__func__\n,\n \n__LINE__\n,\n \npgd\n,\n \npgd_page_vaddr\n(\n*\npgd\n),\n \npud_index\n(\naddr\n),\n \n(\nvoid\n \n*\n)\npud\n);\n\n\n61\n         \n}\n\n\n62\n\n\n63\n         \nrmap\n \n=\n \nkmalloc\n(\nsizeof\n(\n*\nrmap\n),\n \nGFP_KERNEL\n);\n\n\n64\n\n\n65\n         \nif\n \n(\npall\n)\n \n{\n\n\n66\n                 \naddr\n \n=\n \n0x601008\n;\n\n\n67\n                 \npgd\n \n=\n \npgd_offset\n(\nmm\n,\n \naddr\n);\n\n\n68\n                 \npud\n \n=\n \npud_alloc\n(\nmm\n,\n \npgd\n,\n \naddr\n);\n\n\n69\n                 \npr_info\n(\n%s(): size: %zu, rmap: %p\n\\n\n,\n \n__func__\n,\n \nsizeof\n(\n*\nrmap\n),\n \nrmap\n);\n\n\n70\n                 \npr_info\n(\n%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p\n\\n\n,\n\n\n71\n                         \n__func__\n,\n \n__LINE__\n,\n \npgd\n,\n \npgd_page_vaddr\n(\n*\npgd\n),\n \npud_index\n(\naddr\n),\n \n(\nvoid\n \n*\n)\npud\n);\n\n\n72\n         \n}\n\n\n73\n\n\n74\n         \nif\n \n(\nrmap\n)\n \n{\n\n\n75\n                 \nINIT_LIST_HEAD\n(\nrmap\n-\nnext\n);\n\n\n76\n                 \nrmap\n-\nflags\n \n=\n \n0\n;\n\n\n77\n         \n}\n\n\n78\n\n\n79\n         \nif\n \n(\npall\n)\n \n{\n\n\n80\n                 \naddr\n \n=\n \n0x601008\n;\n\n\n81\n                 \npgd\n \n=\n \npgd_offset\n(\nmm\n,\n \naddr\n);\n\n\n82\n                 \npud\n \n=\n \npud_alloc\n(\nmm\n,\n \npgd\n,\n \naddr\n);\n\n\n83\n                 \npr_info\n(\n%s(): size: %zu, rmap: %p\n\\n\n,\n \n__func__\n,\n \nsizeof\n(\n*\nrmap\n),\n \nrmap\n);\n\n\n84\n                 \npr_info\n(\n%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p\n\\n\n,\n\n\n85\n                         \n__func__\n,\n \n__LINE__\n,\n \npgd\n,\n \npgd_page_vaddr\n(\n*\npgd\n),\n \npud_index\n(\naddr\n),\n \n(\nvoid\n \n*\n)\npud\n);\n\n\n86\n         \n}\n\n\n87\n\n\n88\n         \nreturn\n \nrmap\n;\n\n\n89\n \n}\n\n\n\n\n\n\nNarrow it down to \nINIT_LIST_HEAD\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n[\n \n1334.548682\n]\n \nalloc_pcache_rmap\n()\n:\n \nsize\n:\n \n56\n,\n \nrmap\n:\n \nffff88207fccefd0\n\n\n[\n \n1334.621487\n]\n \nalloc_pcache_rmap\n()\n \n71\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n \n1334.753576\n]\n \nalloc_pcache_rmap\n()\n \n76\n \nrmap\n-\nnext\n \nffff88207fcceff8\n \nflags\n \nffff88207fccefd8\n\n\n[\n \n1334.922067\n]\n \nalloc_pcache_rmap\n()\n \n86\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90207fcce000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90207fcce000\n\n\n[\n \n1335.126962\n]\n \nalloc_pcache_rmap\n()\n \n98\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90207fcce000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90207fcce000\n\n\n\n74\n         \nif\n \n(\nrmap\n)\n \n{\n\n\n75\n         \npr_info\n(\n%s() %d \nrmap-\nnext %p \nflags %p\n\\n\n,\n\n\n76\n                 \n__func__\n,\n \n__LINE__\n,\n \nrmap\n-\nnext\n,\n \nrmap\n-\nflags\n);\n\n\n77\n\n\n78\n                 \nINIT_LIST_HEAD\n(\nrmap\n-\nnext\n);\n\n\n79\n\n\n80\n         \nif\n \n(\npall\n)\n \n{\n\n\n81\n                 \naddr\n \n=\n \n0x601008\n;\n\n\n82\n                 \npgd\n \n=\n \npgd_offset\n(\nmm\n,\n \naddr\n);\n\n\n83\n                 \npud\n \n=\n \npud_alloc\n(\nmm\n,\n \npgd\n,\n \naddr\n);\n\n\n84\n                 \npr_info\n(\n%s(): size: %zu, rmap: %p\n\\n\n,\n \n__func__\n,\n \nsizeof\n(\n*\nrmap\n),\n \nrmap\n);\n\n\n85\n                 \npr_info\n(\n%s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p\n\\n\n,\n\n\n86\n                         \n__func__\n,\n \n__LINE__\n,\n \npgd\n,\n \npgd_page_vaddr\n(\n*\npgd\n),\n \npud_index\n(\naddr\n),\n \n(\nvoid\n \n*\n)\npud\n);\n\n\n87\n         \n}\n\n\n88\n\n\n89\n                 \nrmap\n-\nflags\n \n=\n \n0\n;\n\n\n90\n         \n}\n\n\n\n\n\n\nSeriously, if this is running on user-level on VM, I would be able to find the bug maybe in 30min. But I spent several hours to find it out with physical machine. Damn you physical machine.\n\n\nHmm, this func is used A LOT. How can it fail at this point? Possible reasons:\n\n\n\n\nkmalloced area happen to intersect with pgtable?\n\n\none physical page is mapped twice? one to pgtable, one by this rmap.\n\n\ntty/serial code has bug? Really ancient code.\n\n\n\n\nAfter add a few printk, IB seems stuck. And this happens just with few more lines of code! Why? code size matters?\n\n1\n2\n3\n4\n5\n6\n7\n8\n[\n  \n722.381469\n]\n \npcache_handle_fault\n()\n:\n \nenter\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n  \n722.519778\n]\n     \naddr\n:\n \n0x7ffff7feffcc\n,\n \npgd\n:\n \nffff88207fccf7f8\n\n\n[\n  \n722.586334\n]\n     \naddr\n:\n \n0x7ffff7feffcc\n,\n \npgd\n:\n \nffff88207fccf7f8\n \npud\n \nffff88207fcafff8\n\n\n[\n  \n722.674727\n]\n \nBefore\n \nfill\n \naddress\n=\n0x7ffff7feffcc\n \nset_idx\n:\n0x7fef\n\n\n[\n  \n722.743362\n]\n \npcache\n:\nffff8801801ffbc0\n \nmapcount\n:\n0\n \nrefcount\n:\n1\n \nflags\n:(\nallocated\n|\nusable\n)\n \nset_idx\n=\n0x7fef\n \nkva\n:\n \nffff880107fef000\n\n\n[\n  \n722.872312\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7feffcc\n \nflags\n:\n0x50\n\n\n[\n  \n722.967985\n]\n \n__pcache_do_fill_page\n()\n:\n \nbefore\n \nnet\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\nlast\n \nline\n\n\n\n\n\n\nWell, the following finding finally find the bug line. And it kind of explains the above bug. Probably kmalloc\ned area has issues, so IB is touching wrong data. The following bug is related to kmalloc, the rmap is 56 bytes, and it should be within 1 single page, but it is not:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n[\n \n1862.307427\n]\n \npcache_add_rmap\n()\n \n413\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n \n1862.438477\n]\n \nalloc_pcache_rmap\n()\n \n86\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n \n1862.570568\n]\n \nsp\n-\nunits\n:\n \n50\n \nSLOB_UNITS\n:\n \n32\n\n\n[\n \n1862.617372\n]\n \nalloc_pcache_rmap\n()\n:\n \nsize\n:\n \n56\n,\n \nrmap\n:\n \nffff88207fccefd0\n\n\n[\n \n1862.690178\n]\n \nalloc_pcache_rmap\n()\n \n97\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n \n1862.822268\n]\n \nalloc_pcache_rmap\n()\n \n104\n \nrmap\n-\nnext\n \nffff88207fcceff8\n \nflags\n \nffff88207fccefd8\n\n\n[\n \n1862.918995\n]\n \n__INIT_LIST_HEAD\n()\n:\n \nnext\n \nffff88207fcceff8\n \nprev\n \nffff88207fccf000\n\n\n[\n \n1863.002202\n]\n \n__INIT_LIST_HEAD\n()\n \n63\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fcae000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fcae000\n\n\n[\n \n1863.133253\n]\n \n__INIT_LIST_HEAD\n()\n:\n \nnext\n \nffff88207fcceff8\n \nprev\n \nffff88207fccf000\n\n\n[\n \n1863.216459\n]\n \nalloc_pcache_rmap\n()\n:\n \nsize\n:\n \n56\n,\n \nrmap\n:\n \nffff88207fccefd0\n\n\n[\n \n1863.289265\n]\n \nalloc_pcache_rmap\n()\n \n114\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90207fcce000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90207fcce000\n\n\n\n\n\n\nAnalysis: The @prev field in line 7 has address \nffff88207fccf000\n, which happen to the pgd page (\npgd ffff88207fccf000\n). Thus when we do \nlist-\nprev = list\n, it writes to the first 8 bytes of pgd page, corrupts the original pgd entry. That is why we see a corrupted pgd entry (\nffff90207fcce000\n).\n\n\nThis roots from kmalloc, which should not allocate such an object that cross two pages.\n\n\n\n\n03/08 Thur\n\n\nTook several days off. This morning finished the porting of \nwait4\n and \nwaitid\n, which actually has a lot code change. The concept and mechanism is fairly simple, but the legacy UNIX tradition make the implementation quite complex.\n\n\nNow, look back to finish debugging the pcache issue. It must be fixed this week.\n\n\npython\n\n\nTried \npython hello_world.py\n, the program runs for a while and crashes at a deterministic point:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\nwuklab13\n \nand\n \nwuklab15\n,\n \n~/\nttyS1\n\n\n[\n419097.929969\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a4b008\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n419098.039145\n]\n \n__pcache_do_fill_page\n()\n:\n \nI\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a4c010\n \nflags\n:\n0x50\n\n\n[\n419098.306537\n]\n \n__pcache_do_fill_page\n()\n:\n \nO\n \npid\n:\n32\n \ntgid\n:\n32\n \naddress\n:\n0x7ffff7a4c010\n \nflags\n:\n0x50\n \nret\n:\n0\n(\nOKAY\n)\n\n\n[\n419098.413756\n]\n \nCPU5\n \nPID32\n \nsys_mprotect\n+\n0x0\n/\n0x90\n\n\n[\n419098.465753\n]\n \nSYSC_mprotect\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff7df3d27\n\n\n[\n419098.549990\n]\n     \nstart\n:\n0x7ffff7d8c000\n,\nlen\n:\n0x2000\n,\nprot\n:\n0x1\n\n\n[\n419098.614469\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \nffff9001801ff000\n\n\n[\n419098.698703\n]\n \nIP\n:\n \n[\nffffffff8102f7a9\n]\n \npcache_handle_fault\n+\n0x69\n/\n0x6c0\n\n\n[\n419098.774621\n]\n \nPGD\n \n0\n\n\n[\n419098.799579\n]\n \nOops\n:\n \n0000\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n419098.848457\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \npython\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n312\n\n\n[\n419098.916054\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8102f7a9\n]\n  \n[\nffffffff8102f7a9\n]\n \npcache_handle_fault\n+\n0x69\n/\n0x6c0\n\n\n[\n419099.021089\n]\n \nRSP\n:\n \n0000\n:\nffff88107e857ed8\n  \nEFLAGS\n:\n \n000102\n86\n\n\n[\n419099.085567\n]\n \nRAX\n:\n \nffff9001801ff000\n \nRBX\n:\n \nffff9001801ff000\n \nRCX\n:\n \n00003ff\nffffff000\n\n\n[\n419099.171884\n]\n \nRDX\n:\n \n00000801801ff\n000\n \nRSI\n:\n \n000000000060100\n8\n \nRDI\n:\n \nffff88107e83d648\n\n\n[\n419099.258199\n]\n \nRBP\n:\n \nffff88107e857f18\n \nR08\n:\n \n00007ff\nff7fe3000\n \nR09\n:\n \n00007ff\nff7fe3000\n\n\n[\n419099.344516\n]\n \nR10\n:\n \n0000000000000000\n \nR11\n:\n \n0000000000000206\n \nR12\n:\n \n000000000060100\n8\n\n\n[\n419099.430832\n]\n \nR13\n:\n \nffff88107e83d648\n \nR14\n:\n \n0000000000000050\n \nR15\n:\n \n00007ff\nff7ffe150\n\n\n[\n419099.517149\n]\n \nFS\n:\n  \n00007ff\nff7fdf740\n(\n0000\n)\n \nGS\n:\nffff88207fc40000\n(\n0000\n)\n \nknlGS\n:\n0000000000000000\n\n\n[\n419099.614905\n]\n \nCS\n:\n  \n0010\n \nDS\n:\n \n0000\n \nES\n:\n \n0000\n \nCR0\n:\n \n00000000\n80050033\n\n\n[\n419099.684582\n]\n \nCR2\n:\n \nffff9001801ff000\n \nCR3\n:\n \n000000207f\nccf000\n \nCR4\n:\n \n00000000000406\na0\n\n\n[\n419099.770899\n]\n \nStack\n:\n\n\n[\n419099.795858\n]\n \n00007ff\nff7d8c000\n \n0000000000002000\n \n0000000000000001\n \n0000000000000004\n\n\n[\n419099.884254\n]\n \n000000000060100\n8\n \nffff88107e857f58\n \n0000000000000000\n \n00007ff\nff7ffe150\n\n\n[\n419099.972650\n]\n \nffff88107e857f48\n \nffffffff81010082\n \n0000000000000000\n \n0000000000000001\n\n\n[\n419100.061047\n]\n \n0003\n92\nc29c720ba2\n \n0000000000000000\n \n00007ff\nfffffdc40\n \nffffffff8100d91f\n\n\n[\n419100.149442\n]\n \n00007ff\nff7ffe150\n \n0000000000000000\n \n0003\n92\nc29c720ba2\n \n0000000000000001\n\n\n[\n419100.237839\n]\n \nCall\n \nTrace\n:\n\n\n[\n419100.267998\n]\n \nTSK\n\n\n[\n419100.291917\n]\n \n[\nffffffff81010082\n]\n \ndo_page_fault\n+\n0xa2\n/\n0x1a0\n\n\n[\n419100.357434\n]\n \n[\nffffffff8100d91f\n]\n \npage_fault\n+\n0x1f\n/\n0x30\n\n\n[\n419100.418792\n]\n \nEOT\n\n\n\n\nM\n:\n\n\n...\n\n\n[\n419142.163396\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nI\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n50\n \nvaddr\n:\n0x7ffff7a4c010\n\n\n[\n419142.268460\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nO\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n50\n \nvaddr\n:\n0x7ffff7a4c010\n\n\n(\nLast\n \nMessage\n)\n\n\n\n\n\n\nDig deeper:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nint\n \npcache_handle_fault\n(\nstruct\n \nmm_struct\n \n*\nmm\n,\n\n                        \nunsigned\n \nlong\n \naddress\n,\n \nunsigned\n \nlong\n \nflags\n)\n\n\n{\n\n\n        \n..\n\n\n        \npgd\n \n=\n \npgd_offset\n(\nmm\n,\n \naddress\n);\n\n\n        \npr_info\n(\n    addr: %#lx, pgd: %p\n\\n\n,\n \naddress\n,\n \npgd\n);\n\n        \npud\n \n=\n \npud_alloc\n(\nmm\n,\n \npgd\n,\n \naddress\n);\n\n        \npr_info\n(\n    addr: %#lx, pgd: %p pud %p\n\\n\n,\n \naddress\n,\n \npgd\n,\n \npud\n);\n\n        \nif\n \n(\n!\npud\n)\n\n                \nreturn\n \nVM_FAULT_OOM\n;\n\n        \npmd\n \n=\n \npmd_alloc\n(\nmm\n,\n \npud\n,\n \naddress\n);\n\n        \nif\n \n(\n!\npmd\n)\n\n\n..\n\n\n}\n\n\n\n[\n21130.503314\n]\n \nstrace__mprotect\n \ncpu5\n \nstart\n=\n0x7ffff7d8c000\n,\n \nlen\n=\n0x2000\n,\n \nprot\n(\n0x1\n)\n=\nPROT_READ\n\n\n[\n21130.598994\n]\n \nSYSC_mprotect\n()\n \ncpu\n(\n5\n)\n \ntsk\n(\n32\n/\n32\n/\npython\n)\n \nuser\n-\nip\n:\n0x7ffff7df3d27\n\n\n[\n21130.682193\n]\n     \nstart\n:\n0x7ffff7d8c000\n,\nlen\n:\n0x2000\n,\nprot\n:\n0x1\n\n\n[\n21130.745635\n]\n     \naddr\n:\n \n0x601008\n,\n \npgd\n:\n \nffff88207fccf000\n\n\n[\n21130.805954\n]\n     \naddr\n:\n \n0x601008\n,\n \npgd\n:\n \nffff88207fccf000\n \npud\n \nffff9001801ff000\n\n\n[\n21130.888116\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \nffff9001801ff000\n\n\n[\n21130.971314\n]\n \nIP\n:\n \n[\nffffffff8102fa11\n]\n \npcache_handle_fault\n+\n0x91\n/\n0x6f0\n\n\n\n\n\n\nPrint pgd and pud info, these three messages are related and the last one leads to panic:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nwuklab13\n \n~/\nys\n/\n030\n8\n-\n6\n\n\n[\n  \n479.375498\n]\n \naddr\n:\n \n0x400040\n,\n \npgd\n:\n \nffff88207fccf000\n\n\n[\n  \n479.435819\n]\n \npud_alloc_one\n()\n:\n \naddr\n:\n \n0x400040\n,\n \npud\n:\n \nffff88207fc6f000\n\n\n[\n  \n479.511739\n]\n \npud_alloc\n()\n:\n \naddr\n:\n \n0x400040\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fc6f000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fc6f000\n\n\n[\n  \n479.649021\n]\n \naddr\n:\n \n0x400040\n,\n \npgd\n:\n \nffff88207fccf000\n \npud\n \nffff88207fc6f000\n\n\n\n[\n  \n480.016381\n]\n \naddr\n:\n \n0x600dd8\n,\n \npgd\n:\n \nffff88207fccf000\n\n\n[\n  \n480.076701\n]\n \npud_alloc\n()\n:\n \naddr\n:\n \n0x600dd8\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff88207fc6f000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff88207fc6f000\n\n\n[\n  \n480.213982\n]\n \naddr\n:\n \n0x600dd8\n,\n \npgd\n:\n \nffff88207fccf000\n \npud\n \nffff88207fc6f000\n\n\n\n[\n  \n680.072819\n]\n \naddr\n:\n \n0x601008\n,\n \npgd\n:\n \nffff88207fccf000\n\n\n[\n  \n680.133138\n]\n \npud_alloc\n()\n:\n \naddr\n:\n \n0x601008\n \npgd\n \nffff88207fccf000\n,\n \npgd\n.\ncont_va\n \nffff90107e834000\n,\n \npud_index\n=\n0x0\n \npud\n:\n \nffff90107e834000\n\n\n[\n  \n680.270422\n]\n \naddr\n:\n \n0x601008\n,\n \npgd\n:\n \nffff88207fccf000\n \npud\n \nffff90107e834000\n\n\n\n[\n  \n680.352583\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \npaging\n \nrequest\n \nat\n \nffff90107e834000\n\n\n[\n  \n680.435783\n]\n \nIP\n:\n \n[\nffffffff8102fc43\n]\n \npcache_handle_fault\n+\n0xb3\n/\n0x770\n\n\n[\n  \n680.510664\n]\n \nPGD\n \n0\n\n\n\n\n\n\nI need to check what happens between 480s to 680s. Something in between corrupted pgtable. I doubt it can be:\n\n\n\n\ncopy_to_user related syscalls\n\n\npcache establish mapping, mempcy\n\n\nall other memcpy strcpy etc stuff\n\n\n\n\n\n\n03/02 Fri\n\n\nTODO:\n\n\n\n\n-add vsyscall-\n\n\n-pcache_exit_process: free rmap, free cacheline, etc. When rmap is NULL, we clearly should free this pcache.-\n\n\npcache_exit_thread? I don\nt think we need this. All pcache related activities should relate to mm, or thread group leader, not one particular thread.\n\n\ncheck python bug\n\n\nuse omnigraffle to draw the whole workflow of pcache.\n\n\n\n\nPhoenix, word_count-seq, 4G dataset, 4GB pcache:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n[\n  \n273.268853\n]\n \nProcessor\n:\n \nProcessor\n \nmanager\n \nis\n \nrunning\n.\n\n\n[\n  \n573.272479\n]\n \npage\n:\nffffea0071bb9660\n \ncount\n:\n0\n \nmapcount\n:\n-\n128\n\n\n[\n  \n573.332903\n]\n \nflags\n:\n \n0x200000000000300\n(\nslab\n|\nslob_free\n)\n\n\n[\n  \n573.392182\n]\n \npage\n \ndumped\n \nbecause\n:\n \nVM_BUG_ON_PAGE\n(\npage_ref_count\n(\npage\n)\n \n==\n \n0\n)\n\n\n[\n  \n573.474340\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n573.529459\n]\n \nBUG\n:\n \nfailure\n \nat\n \n.\n/\ninclude\n/\nlego\n/\nmm\n.\nh\n:\n251\n/\nput_page_testzero\n()!\n\n\n[\n  \n573.609537\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n  \n573.660496\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n13\n \nComm\n:\n \nkvictim_flushd\n \n4.0.0\n-\nlego\n+\n \n#\n18\n\n\n[\n  \n573.731212\n]\n \nStack\n:\n\n\n[\n  \n573.755132\n]\n \nffff88207e4bfe10\n \nffffffff81023644\n \n0000000000000008\n \nffff88207e4bfe20\n\n\n[\n  \n573.842490\n]\n \nffff88207e4bfdd8\n \n0000000021475542\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n573.929848\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n574.017205\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n574.104563\n]\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n \n0000000000000000\n\n\n[\n  \n574.191921\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n574.221039\n]\n \nTSK\n\n\n[\n  \n574.243919\n]\n \n[\nffffffff81023650\n]\n \npanic\n+\n0xc2\n/\n0xeb\n\n\n[\n  \n574.299038\n]\n \n[\nffffffff8105a35a\n]\n \n?\n \nclient_internal_poll_sendcq\n+\n0x2a\n/\n0x80\n\n\n[\n  \n574.379115\n]\n \n[\nffffffff8105a4fd\n]\n \n?\n \nclient_send_message_with_rdma_write_with_imm_request\n+\n0x14d\n/\n0x360\n\n\n[\n  \n574.487273\n]\n \n[\nffffffff8101ac3c\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n  \n574.551751\n]\n \n[\nffffffff81018395\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n  \n574.618308\n]\n \n[\nffffffff81015a45\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n  \n574.691107\n]\n \n[\nffffffff810064c4\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n  \n574.763905\n]\n \n[\nffffffff8100dbaa\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n  \n574.841903\n]\n \n[\nffffffff8101198d\n]\n \n?\n \nprintk\n+\n0x11d\n/\n0x1b0\n\n\n[\n  \n574.902222\n]\n \n[\nffffffff81025c00\n]\n \n__\nfree_pages\n+\n0x2e0\n/\n0x3c0\n\n\n[\n  \n574.966699\n]\n \n[\nffffffff81028472\n]\n \nkfree\n+\n0x62\n/\n0x480\n\n\n[\n  \n575.022858\n]\n \n[\nffffffff8102e6be\n]\n \nvictim_flush_func\n+\n0x15e\n/\n0x1e0\n\n\n[\n  \n575.092536\n]\n \n[\nffffffff8102e560\n]\n \n?\n \nvictim_try_fill_pcache\n+\n0x390\n/\n0x390\n\n\n[\n  \n575.169494\n]\n \n[\nffffffff8101e446\n]\n \nkthread\n+\n0xf6\n/\n0x120\n\n\n[\n  \n575.227733\n]\n \n[\nffffffff8101e350\n]\n \n?\n \n__\nkthread_parkme\n+\n0x70\n/\n0x70\n\n\n[\n  \n575.296371\n]\n \n[\nffffffff8100de32\n]\n \nret_from_fork\n+\n0x22\n/\n0x30\n\n\n[\n  \n575.359810\n]\n \nEOT\n\n\n\n\n\n\n\n\n03/01 Thur\n\n\nWeird.\n\n1\n2\n3\n4\n5\n6\n7\n[43181.388400] p2m_fork(cpu5): I cur:24-word_count-seq new:25\n[43181.435341] p2m_fork(cpu5): O succeed cur:24-word_count-seq new:25\n[43181.436013] __pcache_do_fill_page(): I pid:24 tgid:24 address:0x4158d0 flags:0x150\n[43181.439246] __pcache_do_fill_page(): O pid:24 tgid:24 address:0x4158d0 flags:0x150 ret:0(OKAY) csum:0x9e8f028e\n\n[43181.510534] __pcache_do_fill_page(): I pid:25 tgid:25 address:0x415000 flags:0x150\n[43181.517729] __pcache_do_fill_page(): O pid:25 tgid:25 address:0x415000 flags:0x150 ret:0(OKAY) csum:0xffff88029e8f028e\n\n\n\n\n\nAfter all, it is TLB issue. I forgot to flush tlb after making the original pte read-only during fork. So the parent will be also to continue RW some pages, which should be process-private.\n\n\nLego\ns current TLB flush is very native, we do tlbflush after each pte changes. This will have worse performance compared to linux\ns batch flush.\n\n\nToday\ns case is flush tlb after making pte read-only. And this really has to be performed one by one", 
            "title": "March 2018"
        }, 
        {
            "location": "/lego/log/log-03-2018/#march-2018", 
            "text": "", 
            "title": "March 2018"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0331-sat", 
            "text": "Stay humble. Be real.", 
            "title": "03/31 Sat"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0330-fri", 
            "text": "Our scheduling, or IB do have issues. I must revisit this.  The case is: in P, we boot only 12 cores, and three of them are used by flush, sweep, and IB. So there are 9 cores left for user. Phoenix create 24 threads. During the run, a lot ib timeout will happen. If we have a good scheduling, this should never happen. I probably need to check more on this.  Anyway. Today I reorganized the opcode things. And now I m adding the final large piece of Lego: replication. It should be much simpler than the pcache part. I will first write down what code I need to add, e.g., opcode, handler, buffer mgmt etc.  End of day. Want to write down some simple thoughts on building system. Building system is fun, but you have to know that devil is in the details. And, you may end up debugging for many many hours on a very very little issue. But that is how it is. Building system does not mean you are always working on fantastic beautiful ideas. It is always about those little bugs, little things, trivial fixes, that make your system robust and usable. For example, the patch Yilun sent me today is about handling special cases of stat and lseek. The patch does not improve any performance or adding fancy features, it is a minor fix to make user progam run. But this enable us to run TF. I think it is a great patch and it stands for 90% of building systems in middle or late stage.  Of course, there are other trivial things on building systems: 1) initialize every possible used variables, can be local variables, malloced buffers. 2) have decent cleanup, which is a counterpart of your initialization, like dequeue list, decrease counter etc. 3) Clear coding style, write code for others, for yourself when you read the code two weeks later. This one is hard, need experience. But can be learned. I think Yilun and Yutong both improved a lot during this project. Me? I learned this from NVM emulator protect. It is a painful one, but also a valuable one. 4) Decent protect source file organization. 5) Remember, draw, the connections between subsystems. By adding this new feature to this subsystem A, will it broke subsystem B, which is using subsystem A. Something like this. 6) clear mind on lock usage, multithread issue. This is the most difficult one. I would say I learned this by coding pcache, or mm. I would say, mm is the most difficult multithread issue one can encounter.", 
            "title": "03/30 Fri"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0326-mon", 
            "text": "Spent several days on replication design. Now I m back on coding and debuging track.  Fixed a bug introduced by per-pte lock. A one hided by previous one big giant page table lock.  Also add an option to boot socket 0 only if Processor is configured. This is because pcache is normally registered at socket 0, if we schedule user threads to sockets other than socket 0, that will have bad performance.", 
            "title": "03/26 Mon"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0322-thur", 
            "text": "", 
            "title": "03/22 Thur"
        }, 
        {
            "location": "/lego/log/log-03-2018/#clear-registers-for-execve", 
            "text": "Want to figure out execve problem today.   Check if pcache is clean after process_exit.  Check if pgtable is clean.   Well. Checked, both are clean.  The bug looks like the return of main, evevntually does not go to library s exit. Is it because library pages are not loaded properly? Since the number of pgfault equals to normal setting, I guess it may originate from Memory side.  TLB is also flushed, so TLB should not be a hidden issue.  Going to check checsum. Well, checsum is okay too.  Syscall execve will change ip, sp, flags registers. So it will use  iretq  instead of  sysexit  to return to userspace.  Got an insteresting IB bug after execve. The CPU5 seems fail to return to userspace, and the CPU0 has the IB bug followed:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39 [   1201.940681 ]   CPU :   5   PID :   32   Comm :   seq . o   4.0.0 - lego - ys +   # 609  [   1202.006200 ]   RIP :   0033 : [ 0000000000401 d1d ]    [ 0000000000401 d1d ]   0x401d1d  [   1202.087320 ]   RSP :   002 b : 00007ff fffffedb0    EFLAGS :   00000200  [   1202.150760 ]   RAX :   0000000000000000   RBX :   00000000004002e0   RCX :   000000000043 b2c7  [   1202.236041 ]   RDX :   00007ff fffffedc8   RSI :   00007ff fffffeb40   RDI :   000000000048f9f 0  [   1202.321320 ]   RBP :   00007ff fffffeb60   R08 :   00000000006 ba4a0   R09 :   00000000006 bc880  [   1202.406601 ]   R10 :   000000000000000f   R11 :   0000000000000246   R12 :   0000000000000000  [   1202.491881 ]   R13 :   0000000000401 930   R14 :   0000000000401 9 c0   R15 :   0000000000000006  [   1202.577161 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88207fc40000 ( 0000 )   knlGS : 0000000000000000  [   1202.673880 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   1202.742521 ]   CR2 :   000000000042 c9a0   CR3 :   000000207f c2f000   CR4 :   00000000000406 a0  [   1220.465601 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000020  [   1220.557225 ]   IP :   [ ffffffff810591ef ]   ib_mad_completion_handler + 0x6f / 0x7c0  [   1220.638344 ]   PGD   0  [   1220.662265 ]   Oops :   0000   [ # 1 ]   SMP   PROCESSOR  [   1220.710105 ]   CPU :   0   PID :   27   Comm :   ib_mad_completi   4.0.0 - lego - ys +   # 609  [   1220.786025 ]   RIP :   0010 : [ ffffffff810591ef ]    [ ffffffff810591ef ]   ib_mad_completion_handler + 0x6f / 0x7c0  [   1220.896265 ]   RSP :   0000 : ffff88103eea7e30    EFLAGS :   00010246  [   1220.959704 ]   RAX :   0000000000000000   RBX :   ffff88103eeac728   RCX :   0000000000000001  [   1221.044985 ]   RDX :   000000002 8000000   RSI :   ffff88103ee8f000   RDI :   ffff88107ff841d8  [   1221.130265 ]   RBP :   ffff88103eea7ec0   R08 :   0000000000000000   R09 :   ffff88103eea03c0  [   1221.215545 ]   R10 :   ffff88103eea7ea0   R11 :   0000000000000001   R12 :   ffff88103ee8c3f0  [   1221.300825 ]   R13 :   ffff88103ee8c4e8   R14 :   ffff88103eeac620   R15 :   ffff88103eeac5f8  [   1221.386106 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc00000 ( 0000 )   knlGS : 0000000000000000  [   1221.482825 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   1221.551466 ]   CR2 :   0000000000000020   CR3 :   000000000113 d000   CR4 :   00000000000406 b0  [   1221.636746 ]   Stack :  [   1221.660666 ]   ffff88103eeaac10   ffff881000000001   ffff88103eeaac10   ffff88103eeaab50  [   1221.748026 ]   ffff88107fc05d80   ffff88103eea0000   ffff88103eeac728   000000 8000000000  [   1221.835386 ]   0000012 83 eea7ea8   ffff88103ee8c9a8   000000007f cf2000   ffff000000000000  [   1221.922746 ]   ffff88107fcf0000   ffff88207ff6cbd8   ffff88107fcf76e8   ffff88103ee8c3f0  [   1222.010106 ]   ffffffff81059180   0000000000000000   ffff88103eea7f48   ffffffff81020866  [   1222.097466 ]   Call   Trace :  [   1222.126586 ]   TSK  [   1222.149466 ]   [ ffffffff81059180 ]   ?   ib_mad_send_done_handler . isra .21 + 0x1d0 / 0x1d0  [   1222.236826 ]   [ ffffffff81020866 ]   kthread + 0xf6 / 0x120  [   1222.295066 ]   [ ffffffff81020770 ]   ?   __kthread_parkme + 0x70 / 0x70  [   1222.363707 ]   [ ffffffff8100e4b2 ]   ret_from_fork + 0x22 / 0x30    1\n2\n3\n4\n5 [ root @ wuklab12 :   LegoOS   git :( master )]   $   addr2line   - e   vmImage    - i   ffffffff810591ef  / root / ys / LegoOS / drivers / infiniband / core / mad . c : 1899  / root / ys / LegoOS / drivers / infiniband / core / mad . c : 2324  It   is   ib_mad_recv_done_handler ()    Well  Eventually, at 22:09, I figured out..  After I cleaned up all registers (except IP, SP, CS, SS, FLAGS) within start_thread, the execve ed program can run to end successfully.  I did not clear the registers because linux does not clear it. I thought this is fine. Glibc should clear it anyway, right?  But anyway, this works.", 
            "title": "Clear Registers for execve()"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0321-wed", 
            "text": "Task 1: add some checking in ib, flush, sweep thread. 1) If cpu changed, 2) if nr_threads on this core   1.   Had an issue while testing: execve(). I ran a exec.o first, then do execve to run seq.o:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33 wuklab13   0321 - 10  [    970.380252 ]   STDOUT :   --- [  uname () :  ---  [    970.431212 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x44605d   flags : 0x150  [   1101.862429 ]   mlx4_ib_handle_error_cqe   syndrome   21  [   1101.915570 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1101.969649 ]   send   request   failed   at   connection   4   as   12  [   1102.029968 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1102.084046 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1102.138125 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1102.192203 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1054  [   1102.256681 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1055  [   1102.321160 ]   csum :   442 a97c0 ,   reply - csum :   2 d352c33  [   1102.377319 ]   fit_poll_cq :   connection   4   Recv   weird   event   as   - 1  [   1102.444916 ]   pcache : ffff880180011180   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880100446000  [   1102.558273 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1056  [   1102.622751 ]   pcache   dumped   because :   csum   mismatch  [   1102.677871 ]   fit_poll_cq :   connection   4   Recv   weird   event   as   - 30704  [   1102.749627 ]   ------------ [   cut   here   ] ------------  [   1102.804746 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1057  [   1102.869225 ]   BUG :   failure   at   managers / processor / pcache / fault . c : 237 / __pcache_do_fill_page () !  [   1102.968022 ]   fit_poll_cq :   connection   4   Recv   weird   event   as   - 30704  [   1103.039780 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1103.090739 ]   CPU :   5   PID :   32   Comm :   seq . o   4.0.0 - lego - ys +   # 599  [   1103.156256 ]   Stack :  [   1103.180177 ]   ffff88103e85be18   ffffffff8102676c   ffffffff00000008   ffff88103e85be28  [   1103.267533 ]   ffff88103e85bde0   0000000021475542   00000000000002 96   ffff88103e85ba10  [   1103.354892 ]   ffffffff810195c5   ffff88207fc44980   0000000000000005   ffff88103e85ba28  [   1103.442249 ]   ffffffff81016c75   ffff88103e85ba40   ffff88103e85ba50   ffffffff810065d4  [   1103.529607 ]   ffffffff811d36e0   000000000000003 9   ffffffff81081718   ffff88103e85bb80  [   1103.616964 ]   Call   Trace :", 
            "title": "03/21 Wed"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0320-tue", 
            "text": "Task 1: calculate failure numbers  Task 2: read 0319-4 Log  Task 3: opt pte lock   Hmm, I finished the per-pte per-pmd lock patch. I think it works.\nBut I do found an issue. When I run MT+2GB, it will create 24 threads. Since I marked 3 CPUs inactive, so all new 24 threads will be scheduled to other cores (I may need to check this!). At some point, Lego P either stuck, or a lot ibapi_send_reply timeout.  When I change the cpu_online to may  0-6 , it finished. When I change it to  0-18 , also succeed. I really doubt if actually those pinned threads are not pinned. Need to check.", 
            "title": "03/20 Tue"
        }, 
        {
            "location": "/lego/log/log-03-2018/#ib-bug-again", 
            "text": "Running MT-phoenix, 2GB, somehow crashed in the middle:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65 [ 60095.857381 ]   SYSC_close ()   CPU6   PID : 33   [ fd :   4 ]   -   [ / proc / stat ]  [ 60286.127359 ]   mlx4_ib_handle_error_cqe   syndrome   21  [ 60286.180503 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.234582 ]   send   request   failed   at   connection   4   as   12  [ 60286.294903 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.348981 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.403062 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.457141 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.511221 ]   send   request   failed   at   connection   4   as   5  [ 60286.570500 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1056  [ 60286.634980 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.689059 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1057  [ 60286.753539 ]   send   request   failed   at   connection   4   as   5  [ 60286.812819 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1058  [ 60286.877298 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60286.931378 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1059  [ 60286.995857 ]   send   request   failed   at   connection   4   as   5  [ 60287.055138 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.109217 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.163297 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.217376 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.271456 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.325536 ]   send   request   failed   at   connection   4   as   5  [ 60287.384815 ]   fit_poll_cq :   failed   status   ( 5 )   for   wr_id   1060  [ 60287.449294 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.503375 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at             ( null )  [ 60287.596973 ]   IP :   [ ffffffff81063ffd ]   fit_poll_cq + 0x4dd / 0x530  [ 60287.664574 ]   send   request   failed   at   connection   4   as   5  [ 60287.723853 ]   PGD   0  [ 60287.747772 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60287.801852 ]   Oops :   0002   [ # 1 ]   PREEMPT   SMP   PROCESSOR  [ 60287.858013 ]   send   request   failed   at   connection   4   as   5  [ 60287.917292 ]   CPU :   2   PID :   29   Comm :   recvpollcq   4.0.0 - lego - ys +   # 569  [ 60287.988010 ]   RIP :   0010 : [ ffffffff81063ffd ]    [ ffffffff81063ffd ]   fit_poll_cq + 0x4dd / 0x530  [ 60288.084731 ]   RSP :   0000 : ffff88103e84fd88    EFLAGS :   00010206  [ 60288.148170 ]   RAX :   000000000000100 8   RBX :   ffff88103e848438   RCX :   0000000000000014  [ 60288.233450 ]   RDX :   0000000000000000   RSI :   ffffffff811d36e0   RDI :   ffffffff811dac08  [ 60288.318728 ]   RBP :   ffff88103e84fea8   R08 :   0000000000000000   R09 :   0000000000000000  [ 60288.404008 ]   R10 :   0000000000000002   R11 :   0000000000000004   R12 :   0000000000000000  [ 60288.489288 ]   R13 :   ffff88207fd6e008   R14 :   0000000000000004   R15 :   ffff88103e84fda0  [ 60288.574568 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60288.628647 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc20000 ( 0000 )   knlGS : 0000000000000000  [ 60288.725367 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [ 60288.794006 ]   CR2 :   0000000000000000   CR3 :   000000000113 d000   CR4 :   00000000000406 a0  [ 60288.879285 ]   send   request   failed   at   connection   4   as   5  [ 60288.938565 ]   Stack :  [ 60288.962484 ]   ffffffff810031d9   000 801 d43e84fda0   0000000000000007   0000000000000424  [ 60289.049844 ]   000000 8100000005   00001008000000f 9   ffff88103e848868   00616e6440000014  [ 60289.137204 ]   0020004000000002   ffff88207fc00000   0000000000000425   000000 8100000005  [ 60289.224563 ]   00001008000000f 9   ffff88103e848868   007370654000000 d   0010004000000002  [ 60289.311922 ]   ffffffff81010000   0000000000000426   000000 8100000005   00001008000000f 9  [ 60289.399282 ]   Call   Trace :  [ 60289.428402 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60289.482482 ]   TSK  [ 60289.505361 ]   [ ffffffff810031d9 ]   ?   native_smp_send_reschedule + 0x39 / 0x50  [ 60289.584400 ]   send   request   failed   at   connection   4   as   5  [ 60289.643680 ]   [ ffffffff81010000 ]   ?   __ioremap_caller + 0x170 / 0x570  [ 60289.714400 ]   [ ffffffff81060000 ]   ?   cm_work_handler + 0x270 / 0x1450  [ 60289.785119 ]   [ ffffffff81064050 ]   ?   fit_poll_cq + 0x530 / 0x530  [ 60289.850639 ]   [ ffffffff81064064 ]   fit_poll_cq_pass + 0x14 / 0x30  [ 60289.917198 ]   [ ffffffff81020c06 ]   kthread + 0xf6 / 0x120  [ 60289.975438 ]   mlx4_ib_handle_error_cqe   syndrome   5  [ 60290.029518 ]   [ ffffffff81020b10 ]   ?   __kthread_parkme + 0x70 / 0x70  [ 60290.098157 ]   [ ffffffff8100e722 ]   ret_from_fork + 0x22 / 0x30    Uuh:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 [   1002.803051 ]   mlx4_ib_handle_error_cqe   syndrome   1  [   1002.855153 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1002.909232 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1002.963310 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   1003.017390 ]   fit_poll_cq :   failed   status   ( 1 )   for   wr_id   512  [   1003.080829 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000200  [   1003.174425 ]   IP :   [ ffffffff8105d499 ]   fit_poll_cq + 0x179 / 0x510  [   1003.242024 ]   PGD   0  [   1003.265943 ]   Oops :   0000   [ # 1 ]   SMP   MEMORY  [   1003.310661 ]   CPU :   2   PID :   29   Comm :   recvpollcq   4.0.0 - lego - ys +   # 149  [   1003.381380 ]   RIP :   0010 : [ ffffffff8105d499 ]    [ ffffffff8105d499 ]   fit_poll_cq + 0x179 / 0x510  [   1003.478098 ]   RSP :   0000 : ffff88104e84fd88    EFLAGS :   00010246  [   1003.541537 ]   RAX :   ffff880000000000   RBX :   ffff88104e848008   RCX :   00000000000000 80  [   1003.626814 ]   RDX :   0000000000000200   RSI :   ffffffff811c76e0   RDI :   ffffffff811d0988  [   1003.712092 ]   RBP :   ffff88104e84fea8   R08 :   0000000000000000   R09 :   0000000000000000  [   1003.797369 ]   R10 :   0000000000000002   R11 :   0000000000000004   R12 :   0000000000000000  [   1003.882648 ]   R13 :   ffff88207ff75008   R14 :   0000000000000004   R15 :   ffff88104e84fda0  [   1003.967925 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc20000 ( 0000 )   knlGS : 0000000000000000  [   1004.064644 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   1004.133282 ]   CR2 :   0000000000000200   CR3 :   0000000001131000   CR4 :   00000000000406 a0  [   1004.218559 ]   Stack :  [   1004.242479 ]   ffffffff810031a9   ffff88104e84fda0   ffffffff81018ef4   0000000000000200  [   1004.329837 ]   000000 8000000001   0000004 8000000 d7   ffff88104e848c98   00000000 81019302  [   1004.417194 ]   0014000000000000   ffff88207fc00000   0000000000000201   ffffffff00000005  [   1004.504552 ]   ffff8810000000f9   ffff88104e848c98   0000000000000000   ffff88104e84fe38  [   1004.591910 ]   ffffffff810195a4   0000000000000202   ffff881000000005   ffff8810000000f9  [   1004.679268 ]   Call   Trace :  [   1004.708388 ]   TSK  [   1004.731267 ]   [ ffffffff810031a9 ]   ?   native_smp_send_reschedule + 0x39 / 0x50  [   1004.810305 ]   [ ffffffff81018ef4 ]   ?   resched_curr + 0x34 / 0x40  [   1004.874783 ]   [ ffffffff810195a4 ]   ?   try_to_wake_up + 0xe4 / 0x1f0  [   1004.942382 ]   [ ffffffff8105f458 ]   ?   __schedule + 0xf8 / 0x1e0  [   1005.005820 ]   [ ffffffff8105d830 ]   ?   fit_poll_cq + 0x510 / 0x510  [   1005.071338 ]   [ ffffffff8105d844 ]   fit_poll_cq_pass + 0x14 / 0x30  [   1005.137897 ]   [ ffffffff8101fdc6 ]   kthread + 0xf6 / 0x120  [   1005.196135 ]   [ ffffffff8101fcd0 ]   ?   __kthread_parkme + 0x70 / 0x70  [   1005.264773 ]   [ ffffffff8100e472 ]   ret_from_fork + 0x22 / 0x30", 
            "title": "IB Bug again"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0319-mon", 
            "text": "Not too many days left!!! Got to design full replication mechanism and algorithm today.  Merged pull request for  pipe ,  pipe2  and  /dev/null  from Yilun. Our simple file op mgmt concerns me. I left a note at kernel/fork.c.  Got a bug report from Yilun, syscall execv failed. To be honest, I ve never tried this syscall, always call it directly within kernel.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33 [    943.650712 ]   CPU6   PID17   sys_execve + 0x0 / 0x10  [    943.701899 ]   BUG :   unable   to   handle   kernel   paging   request   at   00000000004 90523  [    943.702776 ]   IP :   [ ffffffff8103db86 ]   strrchr + 0x6 / 0x20  [    943.711501 ]   PGD   0  [    943.711911 ]   Oops :   0000   [ # 1 ]   SMP   PROCESSOR  [    943.712433 ]   CPU :   6   PID :   17   Comm :   word_count - pthr   4.0.0 - lego +   # 64  [    943.713126 ]   RIP :   0010 : [ ffffffff8103db86 ]    [ ffffffff8103db86 ]   strrchr + 0x6 / 0x20  [    943.714090 ]   RSP :   001 8 : ffff88083e4bfe98    EFLAGS :   00010246  [    943.714724 ]   RAX :   0000000000000000   RBX :   ffff88083e4b3780   RCX :   0000000000000000  [    943.715511 ]   RDX :   00000000ff ffffff   RSI :   000000000000002f   RDI :   00000000004 90523  [    943.716297 ]   RBP :   ffff88083e4bfe98   R08 :   0000160000000000   R09 :   ffff88083e4b8400  [    943.717085 ]   R10 :   ffff880000000000   R11 :   6 db6db6db6db6db7   R12 :   ffff88083e4b8000  [    943.717871 ]   R13 :   ffff88083e4e6290   R14 :   00000000004 90523   R15 :   ffff88083e4b3920  [    943.718683 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88083fd80000 ( 0000 )   knlGS : 0000000000000000  [    943.719650 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    943.720319 ]   CR2 :   00000000004 90523   CR3 :   000000083e4 e7000   CR4 :   00000000000406 a0  [    943.721106 ]   Stack :  [    943.721459 ]   ffff88083e4bff18   ffffffff8102c6bf   ffff880800000000   0000000000000e10  [    943.722541 ]   00007ff fffffedb0   0000000000400 d0d   ffff88083e4c0000   00000000004 90523  [    943.723624 ]   ffff88083e4b9008   00007ff fffffed30   000000 8400000084   ffff88083e4bff58  [    943.724706 ]   000000000000003 b   0000000000401 9 d0   0000000000401 a60   0000000000000000  [    943.725789 ]   ffff88083e4bff28   ffffffff8102c989   ffff88083e4bff48   ffffffff8100e5f5  [    943.726870 ]   Call   Trace :  [    943.727260 ]   TSK  [    943.727619 ]   [ ffffffff8102c6bf ]   do_execve + 0x4af / 0x770  [    943.728236 ]   [ ffffffff8102c989 ]   sys_execve + 0x9 / 0x10  [    943.728868 ]   [ ffffffff8100e5f5 ]   do_syscall_64 + 0x45 / 0xd0  [    943.729499 ]   [ ffffffff8100d4ec ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [    943.730222 ]   EOT  [    943.730570 ]   Code :   d2   74   18   40   38   f2   89   f1   75   06   eb   0f   38   ca   74   0 b   48   83   c0   01   0f   b6   10   84   d2   75   f1   5 d   c3   0f   1f   84   00   00   00   00   00   55   31   c0   48   89   e5   0f   b6   17   40   38   f2   48   0f   44   c7   48   83   c7   01   84   d2   75   ee   5 d   c3   66  [    943.735455 ]   RIP    [ ffffffff8103db86 ]   strrchr + 0x6 / 0x20  [    943.736120 ]    RSP   ffff88083e4bfe98  [    943.736598 ]   CR2 :   00000000004 90523    It is  setup_new_exec() -  set_task_comm() . I passed the user pointer to  set_task_comm() , which I should pass a kernel pointer.  And I actually found we missed a function:  do_close_on_exec() . I also add a note above.", 
            "title": "03/19 Mon"
        }, 
        {
            "location": "/lego/log/log-03-2018/#random-ib-bug", 
            "text": "Another weird bug after pathing loader. Actually, I tried the same setting twice, the second time it works. I guess this is some random IB bug. (Setting: 1P, 1M, 1S. Running a simple exec.c testing program, this have not reach that point yet.)  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 wuklab13   031 9 - 2  [    496.288272 ]   p2m_fork ( cpu0 ) :   I   cur : 1 - kernel_init   new : 31  [    496.349624 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000004  [    496.443216 ]   IP :   [ ffffffff81064935 ]   fit_send_reply_with_rdma_write_with_imm + 0x65 / 0x3b0  [    496.538892 ]   PGD   0  [    496.562811 ]   Oops :   0002   [ # 1 ]   PREEMPT   SMP   PROCESSOR  [    496.618968 ]   CPU :   0   PID :   1   Comm :   kernel_init   4.0.0 - lego - ys +   # 559  [    496.689684 ]   RIP :   0010 : [ ffffffff81064935 ]    [ ffffffff81064935 ]   fit_send_reply_with_rdma_write_with_imm + 0x65 / 0x3b0  [    496.814478 ]   RSP :   0000 : ffff88107fcf7d00    EFLAGS :   00010202  [    496.877915 ]   RAX :   000000000000004 c   RBX :   0000000000000004   RCX :   000000000000002 c  [    496.963190 ]   RDX :   0000000000000004   RSI :   0000000000000001   RDI :   ffff88207ff6d008  [    497.048466 ]   RBP :   ffff88107fcf7d98   R08 :   ffff88107fcf7e3c   R09 :   0000000000000004  [    497.133742 ]   R10 :   ffffffff81145fe0   R11 :   000000000000001 c   R12 :   000000000000002 c  [    497.219018 ]   R13 :   0000000000000001   R14 :   ffff88107fcf7e40   R15 :   ffff88207ff6d008  [    497.304293 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc00000 ( 0000 )   knlGS : 0000000000000000  [    497.401009 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    497.469645 ]   CR2 :   0000000000000004   CR3 :   000000000113 d000   CR4 :   00000000000406 b0  [    497.554922 ]   Stack :  [    497.578840 ]   ffff88107fcf7d08   0000000000000000   00000000000002 82   ffffffff81077b10  [    497.666195 ]   000000000000003 a   000000047f cf7e18   ffff88107fcf7e3c   ffff88107fd5ed88  [    497.753552 ]   000000010000002 c   ffffff9b00000040   0000000000000034   ffffffff81145fe0  [    497.840906 ]   ffff88107fcf7db0   00000000000002 97   ffff88107fd5ed88   000000000000002 c  [    497.928263 ]   ffff88107fcf7e3c   ffff88107fcf7e40   000000000000003 9   ffff88107fcf7dc8  [    498.015618 ]   Call   Trace :  [    498.044736 ]   TSK  [    498.067615 ]   [ ffffffff810622ff ]   ibapi_send_reply_timeout + 0x3f / 0x50  [    498.142492 ]   [ ffffffff8103b0d4 ]   ?   net_send_reply_timeout + 0x94 / 0x132  [    498.218408 ]   [ ffffffff8103b0d4 ]   net_send_reply_timeout + 0x94 / 0x132  [    498.292244 ]   [ ffffffff8102c683 ]   p2m_fork + 0xd3 / 0x200  [    498.351521 ]   [ ffffffff8101f490 ]   do_fork + 0xf0 / 0x150  [    498.409758 ]   [ ffffffff8101f514 ]   kernel_thread + 0x24 / 0x30  [    498.473195 ]   [ ffffffff8115bf21 ]   processor_manager_init + 0x21 / 0x50  [    498.545991 ]   [ ffffffff81000354 ]   kernel_init + 0x94 / 0x120  [    498.608388 ]   [ ffffffff810002c0 ]   ?   0xffffffff810002c0  [    498.668706 ]   [ ffffffff81019b0a ]   ?   schedule_tail + 0xa / 0x40  [    498.733182 ]   [ ffffffff810002c0 ]   ?   0xffffffff810002c0  [    498.793499 ]   [ ffffffff8100e762 ]   ret_from_fork + 0x22 / 0x30  [    498.856936 ]   EOT", 
            "title": "Random IB Bug"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0318-sun", 
            "text": "Got a bug report after enable preempt and sweep thread  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 [    582.545444 ]   pcache : ffff8801812cb680   mapcount : 1   refcount : 2   flags :( locked | allocated | usable | valid | reclaim )   kva :   ffff88014b2da000  [    582.678677 ]   pcache   dumped   because :   PCACHE_BUG_ON_PCM ( pcache_mapped ( pcm ))  [    582.758760 ]   rmap : ffff88207e5e37e8   flags : 0x0   owner - tgid : 33   user_va : 0x7fff0b2da000   ptep : ffff88207e4a86d0  [    582.870046 ]   pte : ffff88207e4a86d0   pfn : 0x0   flags :()  [    582.926210 ]   ------------ [   cut   here   ] ------------  [    582.981333 ]   BUG :   failure   at   managers / processor / pcache / victim . c : 604 / victim_finish_insert () !  [    583.080137 ]   Kernel   Panic   -   not   syncing :   BUG !  ...  ...  [    588.847239 ]   nr_pgfault :   591101  [    588.883641 ]   nr_clflush :   66176  [    588.919003 ]   nr_pgfault_wp :   0  [    588.953325 ]   nr_pgfault_wp_cow :   0  [    588.991806 ]   nr_pgfault_wp_reuse :   0  [    589.032368 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    589.091651 ]   nr_pcache_fill_from_memory :   587057  [    589.144694 ]   nr_pcache_fill_from_victim :   4038  [    589.195656 ]   nr_pcache_eviction_triggered :   439562  [    589.250780 ]   nr_pcache_eviction_eagain_freeable :   373382  [    589.312143 ]   nr_pcache_eviction_eagain_concurrent :   0  [    589.370386 ]   nr_pcache_eviction_failure_find :   0  [    589.423429 ]   nr_pcache_eviction_failure_evict :   0  [    589.477512 ]   nr_pcache_eviction_succeed :   66176  [    589.529514 ]   nr_victim_eviction_triggered :   733361  [    589.584638 ]   nr_victim_eviction_eagain :   671227  [    589.636640 ]   nr_victim_eviction_succeed :   62134  [    589.688642 ]   nr_victim_prepare_insert :   66180  [    589.738566 ]   nr_victim_finish_insert :   66176  [    589.787447 ]   nr_victim_flush_submitted :   66176  [    589.838411 ]   nr_victim_flush_finished :   66176  [    589.888332 ]   nr_victim_flush_async_run :   0  [    589.935135 ]   nr_victim_flush_sync :   0  [    589.976738 ]   nr_sweep_run :   50580  [    590.014179 ]   nr_sweep_nr_pset :   116770383  [    590.059943 ]   nr_sweep_nr_moved_pcm :   100686435    This is an interesting bug. Two threads, one doing munmap or mremap, one doing eviction. They are using the same pcm. munmap and mremap will use  pte_get_and_clear()  to get the pcm.  While eviction will call  pcache_try_to_unamp , which will further call  rmap_get_locked_pte() , in which we check if the pte is none, if it is, then we know this is under munmap or mremap, then we skip. This is absolutely wrong. When  pcache_try_to_unamp  is called by eviction, it should always unmap ALL rmap. The above case is triggered because both two threads skip the final  __pcache_remove_rmap .  Hmm, looks like open/close filename is wrong. I need to check.  Last Log from MT+2GB, computation finished:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 wuklab13   031 8 - 10  [    627.280016 ]  ****      ERROR :  ***       current :   32 : kevict_sweepd   caller :             ( null )  ****      [ pte   ==   rmap - page_table ]     [ pcache_pfn   !=   pte_pfn ]  ****      rmap - owner_process :   word_count - pthr   uva :   0x7fff78f52000   ptep :   ffff88107e87fa90 ,   rmap - page_table :   ffff88107e87fa90  ****      pcache_pfn :   0x168f52 ,   pte_pfn :   0x178f52  [    627.624239 ]   rmap : ffff88107dc73740   flags : 0x0   owner - tgid : 33   user_va : 0x7fff78f52000   ptep : ffff88107e87fa90  [    627.735513 ]   pte : ffff88107e87fa90   pfn : 0x0   flags :()  [    627.791670 ]   pcache_rmap   dumped   because :   Corrupted   RMAP  [    627.853026 ]   pcache : ffff880181a3d480   mapcount : 1   refcount : 2   flags :( locked | allocated | usable | valid )   kva :   ffff880168f52000  [    627.979901 ]   pcache   dumped   because :   Corrupted   RMAP  [    628.036057 ]   ------------ [   cut   here   ] ------------  [    628.091175 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 109 / report_bad_rmap () !  [    628.182691 ]   Kernel   Panic   -   not   syncing :   BUG !  [    628.233647 ]   CPU :   5   PID :   32   Comm :   kevict_sweepd   4.0.0 - lego - ys +   # 543  [    628.307483 ]   Stack :  [    628.331401 ]   ffff88107e85bd00   ffffffff81026d24   000000000000000 8   ffff88107e85bd10  [    628.418756 ]   ffff88107e85bcc8   0000000021475542   0000000000000000   0000000000000000  [    628.506113 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    628.593468 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    628.680823 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    628.768179 ]   Call   Trace :  [    628.797299 ]   TSK  [    628.820176 ]   [ ffffffff81026d30 ]   panic + 0xc2 / 0x102  [    628.876334 ]   [ ffffffff8101c6ac ]   ?   task_tick_rt + 0x2c / 0xd0  [    628.940811 ]   [ ffffffff8101c6ac ]   ?   task_tick_rt + 0x2c / 0xd0  [    629.005288 ]   [ ffffffff81019bfc ]   ?   scheduler_tick + 0x5c / 0x70  [    629.071843 ]   [ ffffffff81017195 ]   ?   tick_handle_periodic + 0x45 / 0x70  [    629.144639 ]   [ ffffffff81006704 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [    629.217436 ]   [ ffffffff8100e4da ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [    629.295432 ]   [ ffffffff81012d94 ]   ?   printk + 0x124 / 0x1c0  [    629.355748 ]   [ ffffffff8103ad1f ]   report_bad_rmap + 0x144 / 0x144  [    629.423345 ]   [ ffffffff81031046 ]   pcache_referenced_trylock_one + 0x1c6 / 0x2c0  [    629.505500 ]   [ ffffffff8100e4da ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [    629.583497 ]   [ ffffffff810328a1 ]   rmap_walk + 0x71 / 0xe0  [    629.642774 ]   [ ffffffff81033329 ]   pcache_referenced_trylock + 0x59 / 0xd0", 
            "title": "03/18 Sun"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0317-sat", 
            "text": "I m too tired today.  Coding side, I will only optimize sweep. Besides, I will book tickets for Iceland trip.", 
            "title": "03/17 Sat"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0316-friday", 
            "text": "Task 1 : Add physical memory counter. It is a per-zone based counter, even though there is also some global counters. In Linux, per-cpu counter is first accumlated, global counter is updated only when per-cpu ones overflow. Lego s initial version save the trouble of per-cpu counter, I only port one global counter today, because I m not quite confident about our percpu_alloc  Anway, the info is reported in the format of  manager_sysinfo . Do note this is different from the oirginal  sysinfo  structure, which is used by sysinfo syscall.  Task 2 : Patch get_random_number and /dev/urandom /dev/random. Others wrote the code, but he did not stick to the tradition of format naming. So I have to rewrite some of them. Sigh.  Task 3 : optimize sweep", 
            "title": "03/16 Friday"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0315-thur", 
            "text": "Forgot to write the log yesterday. I actually solved the major bug, the refcount and eviction one. That is really nasty. I basically used pte lock, pcache_lock, and refcount to synchronize between eviction routine and other users such as munmap, mremap, write-protected-handler.  I m really not sure if this mode can be reproduced if I have any other similar systems. But I m glad that I find a way to do this.  Today I got few tasks going on. First merge storage syscall branch, then add sched_yield syscall, add zone/node counters, and probably patch get_random_number.  Task 1 : Merge Yilun s storage pull request, has bunch syscalls. I m reviewing now.   truncate  ftruncate  getdents  getcwd  mkdir  rmdir  creat  unlink  unlinkat  readlink  statfs  sync   Task 2 : Add  sched_yield() . Fairly simple.  Task 3 : Add physical memory counter. Fairly complex. The underlying is built long time ago. Need to pick up some. Well some facts:   pg_data_t (and zone) is allcoated by alloc_node_data if NUMA is configured.  all zones are built and initliazed in memory_init() in Lego  stats are reset to 0 when pg_data_t allocated (DUH?). Played directly in page_alloc.c   Have to continue tomorrow.  Task 4 : Patch get_random_number and /dev/urandom", 
            "title": "03/15 Thur"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0313-wed", 
            "text": "The slow victim flush issue is solved by pinning the thread to a core and remove that core from active_cpu mask.  Today I m going to solve the SMP object issue. I m hoping by solving this, we can have a complete working pcache and victim cache.  Continue yesterday s log:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 wuklab13   0313 - 12  [   1073.616269 ]   pcache : ffff880180777a80   mapcount : 0   refcount : 3   flags :( locked | allocated | usable )   kva :   ffff88011ddea000  [   1073.734941 ]   __clflush_one () :   EFAULT : bad   address   tsk :   32   user_va :   0x7fff4ddea000  [   1073.822304 ]   pcache   dumped   because :   evict / ref   bug  [   1073.987667 ]   BUG :   failure   at   managers / processor / pcache / evict . c : 301 / pcache_evict_line () !  [   1074.082308 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 763 / pcache_zap_pte () !  [   1074.172789 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1074.223751 ]   CPU :   23   PID :   50   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 476       Time  CPU0  CPU1      0  pcache_evict_line()  zap_pte_range()    1  find @pcm to evict  prepare to unmap pte which points to @pcm    2  lock_pcache()  ..    3  pcache_try_to_unmap()  pte_offset_lock()    4  try to lock pte  pcache_zap_pte()    5  ..spin..  trylock_pcache (failed)    6  ..spin..  unlock pte    7  lock pte  trylock pcache    8  clear pte  ..spin..    9  unlock pte  ..spin..    10  unlock pcache  ..spin..    11  ..  lock pcache    12  ..  lock pte    13  ..  HERE, should check if pte changed!     Huh, patched both eviction and other code. Use refcount, pcache lock, pte lock to synchronize between all users. Make sure a going-to-be-evicted pcm will not be used by others. And others will not have a chance to use such line.", 
            "title": "03/13 Wed"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0312-tue", 
            "text": "Continue victim cache. The current conclusion is victim has a unbalanced input and output rate. That is why some cores timeout and abort.  Got some more clean log. The log told us that the flushd_victim is too slow at flushing content. Next I going to print the current flush queue content. Make sure that they are really not flushed. If so, I want to add code to flush sync.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58 [    318.193591 ]   CPU4   PID : 54   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207f81d340 ,   pset_idx : 1869 ,   nr_lru : 7  [    318.330986 ]     --     Start   Dump   Victim   Cache       --  [    318.388190 ]     --     CPU4   [ word_count - pthr ][ pid = 54 ,   tgid = 32 ]   --  [    318.456835 ]   victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d200  [    318.627406 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90748000  [    318.707492 ]       pset : ffff88207f81d200   set_idx :   1864   nr_lru : 8  [    318.775096 ]  [    318.792778 ]   victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d240  [    318.963349 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90749000  [    319.043435 ]       pset : ffff88207f81d240   set_idx :   1865   nr_lru : 8  [    319.111040 ]  [    319.128721 ]   victim : ffff88207ff71090   index : 2   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d180  [    319.299292 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90746000  [    319.379378 ]       pset : ffff88207f81d180   set_idx :   1862   nr_lru : 8  [    319.446983 ]  [    319.464664 ]   victim : ffff88207ff710d8   index : 3   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d280  [    319.635237 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074a000  [    319.715321 ]       pset : ffff88207f81d280   set_idx :   1866   nr_lru : 8  [    319.782927 ]  [    319.800608 ]   victim : ffff88207ff71120   index : 4   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d140  [    319.971179 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90745000  [    320.051265 ]       pset : ffff88207f81d140   set_idx :   1861   nr_lru : 8  [    320.118870 ]  [    320.136551 ]   victim : ffff88207ff71168   index : 5   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d300  [    320.307123 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074c000  [    320.387208 ]       pset : ffff88207f81d300   set_idx :   1868   nr_lru : 8  [    320.454813 ]  [    320.472494 ]   victim : ffff88207ff711b0   index : 6   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d1c0  [    320.643066 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90747000  [    320.723152 ]       pset : ffff88207f81d1c0   set_idx :   1863   nr_lru : 8  [    320.790756 ]  [    320.808438 ]   victim : ffff88207ff711f8   index : 7   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d2c0  [    320.979009 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074b000  [    321.059096 ]       pset : ffff88207f81d2c0   set_idx :   1867   nr_lru : 8  [    321.126700 ]  [    321.144381 ]     --     End   Dump   Victim   Cache         --  [    321.200545 ]   CPU4   PID : 54   fail   to   allocate   pcache   or   victim   cache   lines .  [    321.278552 ]   word_count - pthr [ 54 ] :   segfault   at   0x74d000   ip   00000000004024 9 d   sp   00007ff f7674cd80   error   6  [    321.511925 ]   nr_pgfault :   551908  [    321.546357 ]   nr_clflush :   33449  [    321.581718 ]   nr_pgfault_wp :   0  [    321.616040 ]   nr_pgfault_wp_cow :   0  [    321.654523 ]   nr_pgfault_wp_reuse :   0  [    321.695087 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    321.754371 ]   nr_pcache_fill_from_memory :   546067  [    321.807414 ]   nr_pcache_fill_from_victim :   5750  [    321.858378 ]   nr_pcache_eviction_triggered :   38689  [    321.912461 ]   nr_pcache_eviction_eagain :   5239  [    321.962385 ]   nr_pcache_eviction_succeed :   33449  [    322.014389 ]   nr_victim_eviction_triggered :   41887455  [    322.071592 ]   nr_victim_eviction_eagain :   41859764  [    322.125676 ]   nr_victim_eviction_succeed :   27691  [    322.177680 ]   nr_victim_prepare_insert :   33450  [    322.227603 ]   nr_victim_finish_insert :   33449  [    322.276487 ]   nr_victim_flush_submitted :   33449  [    322.327451 ]   nr_victim_flush_finished :   33449  [    322.377374 ]   nr_victim_flush_async_run :   26989  [    322.428338 ]   nr_victim_flush_sync :   0    Yes, this victims are truly not being flushed. They are inside the flush_queue. No bug, hoo! Just some performance coding issues. But god why the flushd does not get a chance to run in 10 seconds? Hmm   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62 [   5520.236187 ]   __clflush_one () :   EFAULT : bad   address   tsk :   32   user_va :   0x7fff464fa000  [   5530.404269 ]   CPU4   PID : 54   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207f81d340 ,   pset_idx : 1869 ,   nr_lru : 7  [   5530.541664 ]   CPU4   PID54     --     Start   Dump   Victim   Cache   [ 0 ]  [   5530.606147 ]   CPU4   PID54    victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d1c0  [   5530.789194 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90747000  [   5530.880717 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d1c0   set_idx :   1863   nr_lru : 8  [   5530.968080 ]   CPU4   PID54    victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d280  [   5531.151128 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074a000  [   5531.242652 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d280   set_idx :   1866   nr_lru : 8  [   5531.330015 ]   CPU4   PID54    victim : ffff88207ff71090   index : 2   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d300  [   5531.513063 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074c000  [   5531.604586 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d300   set_idx :   1868   nr_lru : 8  [   5531.691950 ]   CPU4   PID54    victim : ffff88207ff710d8   index : 3   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d2c0  [   5531.874997 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074b000  [   5531.966521 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d2c0   set_idx :   1867   nr_lru : 8  [   5532.053885 ]   CPU4   PID54    victim : ffff88207ff71120   index : 4   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d200  [   5532.236932 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90748000  [   5532.328456 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d200   set_idx :   1864   nr_lru : 8  [   5532.415819 ]   CPU4   PID54    victim : ffff88207ff71168   index : 5   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d240  [   5532.598867 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90749000  [   5532.690390 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d240   set_idx :   1865   nr_lru : 8  [   5532.777753 ]   CPU4   PID54    victim : ffff88207ff711b0   index : 6   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d180  [   5532.960802 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90746000  [   5533.052325 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d180   set_idx :   1862   nr_lru : 8  [   5533.139689 ]   CPU4   PID54    victim : ffff88207ff711f8   index : 7   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d140  [   5533.322736 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff90745000  [   5533.414259 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d140   set_idx :   1861   nr_lru : 8  [   5533.501623 ]   CPU4   PID54     --     End   Dump   Victim   Cache   [ 0 ]  [   5533.566106 ]   CPU4   PID54     --    Start   Dump   Victim   Flush   Queue   [ 0 ]  [   5533.635789 ]   CPU4   PID54    victim : ffff88207ff711f8   index : 7   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d140  [   5533.818837 ]   CPU4   PID54    victim : ffff88207ff711b0   index : 6   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d180  [   5534.001884 ]   CPU4   PID54    victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d1c0  [   5534.184931 ]   CPU4   PID54    victim : ffff88207ff71120   index : 4   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d200  [   5534.367978 ]   CPU4   PID54    victim : ffff88207ff71168   index : 5   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d240  [   5534.551025 ]   CPU4   PID54    victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d280  [   5534.734074 ]   CPU4   PID54    victim : ffff88207ff710d8   index : 3   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d2c0  [   5534.917120 ]   CPU4   PID54    victim : ffff88207ff71090   index : 2   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d300  [   5535.100168 ]   CPU4   PID54     --    End   Dump   Victim   Flush   Queue   [ 0 ]  [   5535.169851 ]   CPU4   PID : 54   fail   to   allocate   pcache   or   victim   cache   lines .  [   5535.247854 ]   word_count - pthr [ 54 ] :   segfault   at   0x74d000   ip   00000000004024 9 d   sp   00007ff f7674cd80   error   6  [   5535.480513 ]   nr_pgfault :   549578  [   5535.514943 ]   nr_clflush :   31822  [   5535.550304 ]   nr_pgfault_wp :   0  [   5535.584625 ]   nr_pgfault_wp_cow :   0  [   5535.623107 ]   nr_pgfault_wp_reuse :   0  [   5535.663669 ]   nr_pgfault_due_to_concurrent_eviction :   0  [   5535.722952 ]   nr_pcache_fill_from_memory :   544279  [   5535.775993 ]   nr_pcache_fill_from_victim :   5201  [   5535.826955 ]   nr_pcache_eviction_triggered :   37437  [   5535.881038 ]   nr_pcache_eviction_eagain :   5614  [   5535.930960 ]   nr_pcache_eviction_succeed :   31822  [   5535.982963 ]   nr_victim_eviction_triggered :   42000029  [   5536.040165 ]   nr_victim_eviction_eagain :   41973416  [   5536.094247 ]   nr_victim_eviction_succeed :   26613  [   5536.146249 ]   nr_victim_prepare_insert :   31823  [   5536.196171 ]   nr_victim_finish_insert :   31822  [   5536.245052 ]   nr_victim_flush_submitted :   31822  [   5536.296015 ]   nr_victim_flush_finished :   31822  [   5536.345937 ]   nr_victim_flush_async_run :   26718  [   5536.396899 ]   nr_victim_flush_sync :   0    Hmm, got some interesting bug, which never happened before. We did a  unmap  before  finish_insert , so the mapcount must be zero. Since we have the  Reclaim  set for the candidate. But it looks like other code does not too much about the Reclaim bit. I need to check.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 [   1009.676839 ]   victim_flush_async   CPU4   jobs   1  [   1009.725830 ]   victim_flush_async   CPU4   jobs   1  [   1009.774423 ]   victim_flush_async   CPU4   jobs   1  [   1009.823147 ]   __clflush_one () :   EFAULT : bad   address   tsk :   32   user_va :   0x7fff465fc000  [   1009.910479 ]   pcache : ffff88018098d740   mapcount : 1   refcount : 3   flags :( locked | allocated | usable | valid | reclaim )   kva :   ffff88012635d000  [   1010.045652 ]   pcache   dumped   because :   PCACHE_BUG_ON_PCM ( pcache_mapped ( pcm ))  [   1010.125725 ]   victim_flush_async   CPU4   jobs   1  [   1010.174602 ]   ------------ [   cut   here   ] ------------  [   1010.229717 ]   BUG :   failure   at   managers / processor / pcache / victim . c : 601 / victim_finish_insert () !  [   1010.328509 ]   victim_flush_async   CPU4   jobs   1  [   1010.377385 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1010.428341 ]   CPU :   20   PID :   47   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 468  [   1010.505294 ]   Stack :  [   1010.529212 ]   ffff881f2040fe08   ffffffff810259f4   000000000000000 8   ffff881f2040fe18  [   1010.616565 ]   ffff881f2040fdd0   0000000021475542   0000000000000000   0000000000000000  [   1010.703918 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [   1010.791270 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [   1010.878623 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [   1010.965976 ]   Call   Trace :  [   1010.995095 ]   TSK  [   1011.017972 ]   [ ffffffff81025a00 ]   panic + 0xc2 / 0x102  [   1011.074127 ]   [ ffffffff81063a8a ]   ?   client_internal_poll_sendcq + 0x2a / 0x80  [   1011.154202 ]   [ ffffffff81063c2d ]   ?   client_send_message_with_rdma_write_with_imm_request + 0x14d / 0x360  [   1011.262351 ]   [ ffffffff8101bffc ]   ?   task_tick_rt + 0x2c / 0xd0  [   1011.326827 ]   [ ffffffff81019755 ]   ?   scheduler_tick + 0x55 / 0x60  [   1011.393382 ]   [ ffffffff81016e25 ]   ?   tick_handle_periodic + 0x45 / 0x70  [   1011.466175 ]   [ ffffffff810066e4 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [   1011.538969 ]   [ ffffffff8100e4aa ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [   1011.616964 ]   [ ffffffff81012cfd ]   ?   printk + 0x11d / 0x1b0  [   1011.677279 ]   [ ffffffff81032a19 ]   victim_finish_insert + 0x89 / 0x230  [   1011.749032 ]   [ ffffffff81031a99 ]   pcache_evict_line + 0x79 / 0x280  [   1011.817667 ]   [ ffffffff8102f00a ]   pcache_alloc + 0x23a / 0x340  [   1011.882141 ]   [ ffffffff8102e4da ]   common_do_fill_page + 0x2a / 0x1b0  [   1011.952856 ]   [ ffffffff8102e160 ]   ?   pcache_meta_to_kva + 0x30 / 0x30  [   1012.023570 ]   [ ffffffff8102e802 ]   pcache_handle_fault + 0x1a2 / 0x660  [   1012.095324 ]   [ ffffffff810102b2 ]   do_page_fault + 0xa2 / 0x1a0  [   1012.159799 ]   [ ffffffff8100dadf ]   page_fault + 0x1f / 0x30    Interesting. Memory consistency issue? Actually, I m not sure if it is the  v- flags = 0  issue. Others use atomic bit operations to play with this flag, while the reset is a simple store. I checked the list operations,  all of them are protected by spinlock. So the below should never happen in theory. I m changing the  v- flags = 0  to  smp_store_mb(v- flags, 0) , which is a  xchg  in x86. Same for pcache.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 [   1773.814490 ]   CPU17   PID44    victim : ffff88207ff71000   index : 0   refcount : 1   nr_fill : 0   locked : 0   flags :( allocated | usable )   pcm :            ( null )   pset :            ( null )  [   1773.979705 ]   CPU17   PID44       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff95b1c000  [   1774.072260 ]   CPU17   PID44       rmap   to   pset : ffff88207f96c700   set_idx :   23324   nr_lru : 8  [   1774.161694 ]   CPU17   PID44       victim   dumped   because :   PCACHE_BUG_ON_VICTIM ( ! VictimUsable ( v ))  [   1774.259451 ]   ------------ [   cut   here   ] ------------  [   1774.314567 ]   BUG :   failure   at   managers / processor / pcache / victim . c : 231 / find_victim_to_evict () !  [   1774.413363 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1774.464320 ]   CPU :   17   PID :   44   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 47  ...  [   1781.363348 ]   nr_pcache_fill_from_victim :   2    Did another run. I added an explicit  wake_up_victim_flushd  if victim failed to evict any line. But this fails with IB failure..  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81 [   2336.950087 ]   CPU4   PID : 54   Abort   victim   alloc   ( 20010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207f81d340 ,   pset_idx : 1869 ,   nr_lru : 7  [   2337.087474 ]   CPU4   PID54     --     Start   Dump   Victim   Cache   [ 0 ]  [   2337.151955 ]   CPU4   PID54    victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d280  [   2337.334999 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074a000  [   2337.426521 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d280   set_idx :   1866   nr_lru : 8  [   2337.513883 ]   CPU4   PID54    victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d2c0  [   2337.696927 ]   CPU4   PID54       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff9074b000  [   2337.788450 ]   CPU4   PID54       rmap   to   pset : ffff88207f81d2c0   set_idx :   1867   nr_lru : 8  ...  ...  [   2340.111861 ]   CPU4   PID54     --    Start   Dump   Victim   Flush   Queue   [ 0 ]  [   2340.181543 ]   CPU4   PID54    victim : ffff88207ff71090   index : 2   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d140  [   2340.364587 ]   CPU4   PID54    victim : ffff88207ff71120   index : 4   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d180  [   2340.547632 ]   CPU4   PID54    victim : ffff88207ff711f8   index : 7   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d1c0  [   2340.730675 ]   CPU4   PID54    victim : ffff88207ff71168   index : 5   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d200  [   2340.913720 ]   CPU4   PID54    victim : ffff88207ff711b0   index : 6   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d240  [   2341.096763 ]   CPU4   PID54    victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d280  [   2341.279808 ]   CPU4   PID54    victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d2c0  [   2341.462851 ]   CPU4   PID54    victim : ffff88207ff710d8   index : 3   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata | waitflush )   pcm :            ( null )   pset : ffff88207f81d300  [   2341.645895 ]   CPU4   PID54     --    End   Dump   Victim   Flush   Queue   [ 0 ]  [   2341.715577 ]   CPU4   PID : 54   fail   to   allocate   pcache   or   victim   cache   lines .  [   2341.793579 ]   word_count - pthr [ 54 ] :   segfault   at   0x74d000   ip   00000000004024 9 d   sp   00007ff f7674cd80   error   6  [   2476.201442 ]   mlx4_ib_handle_error_cqe   syndrome   21  [   2476.254590 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2476.308670 ]   send   request   failed   at   connection   4   as   12  [   2476.368991 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2476.423073 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2476.477153 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2476.531236 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1051  [   2476.598837 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1052  [   2476.666438 ]   __clflush_one () :   EPERM : Operation   not   permitted   tsk :   32   user_va :   0x7fff90745000  [   2476.765240 ]   client_poll_cq :   connection   4   Recv   weird   event   as   - 30704  [   2476.840122 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1053  [   2476.907724 ]   client_poll_cq :   connection   4   Recv   weird   event   as   - 30704  [   2476.982605 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1054  [   2477.050207 ]   client_poll_cq :   connection   4   Recv   weird   event   as   - 30704  [   2477.125089 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.179169 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.233251 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.287332 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.341414 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1055  [   2477.409016 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1056  ..  ..  [   2477.761583 ]   client_poll_cq :   connection   4   Recv   weird   event   as   - 30704  [   2477.836464 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.890545 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.944626 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2477.998707 ]   mlx4_ib_handle_error_cqe   syndrome   5  [   2478.052789 ]   client_poll_cq :   failed   status   ( 5 )   for   wr_id   1059  [   2478.120392 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at             ( null )  [   2478.213992 ]   IP :   [ ffffffff81064894 ]   client_poll_cq + 0x1f4 / 0x6c0  [   2478.284714 ]   PGD   0  [   2478.308635 ]   Oops :   0002   [ # 1 ]   SMP   PROCESSOR  [   2478.356476 ]   CPU :   2   PID :   29   Comm :   recvpollcq   4.0.0 - lego - ys +   # 473  [   2478.427197 ]   RIP :   0010 : [ ffffffff81064894 ]    [ ffffffff81064894 ]   client_poll_cq + 0x1f4 / 0x6c0  [   2478.527040 ]   RSP :   0000 : ffff88107e143d90    EFLAGS :   00010246  [   2478.590481 ]   RAX :   0000000000000000   RBX :   ffff88207fc6e000   RCX :   0000000000000000  [   2478.675762 ]   RDX :   000000000000100 8   RSI :   ffffffff811d36e0   RDI :   ffffffff811dab08  [   2478.761044 ]   RBP :   ffff88107e143eb0   R08 :   0000000000000000   R09 :   0000000000000000  [   2478.846327 ]   R10 :   0000000000000002   R11 :   0000000000000004   R12 :   ffff88207fd4f000  [   2478.931609 ]   R13 :   0000000000000004   R14 :   ffff88107e143da8   R15 :   0000000000000000  [   2479.016890 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc20000 ( 0000 )   knlGS : 0000000000000000  [   2479.113613 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   2479.182254 ]   CR2 :   0000000000000000   CR3 :   000000000113 d000   CR4 :   00000000000406 a0  [   2479.267536 ]   Stack :  [   2479.291457 ]   ffff88107e143da0   001012 9 c81019794   0000000000000001   0000000000000423  [   2479.378818 ]   000000 8100000005   00001008000000f 9   ffff88207fd39000   0000000040000000  [   2479.466180 ]   000f 004000000002   ffff88107e140000   0000000000000424   ffff881000000005  [   2479.553542 ]   00000000000000f 9   ffff88207fd39000   ffff88107e143e38   ffffffff81019e44  [   2479.640904 ]   0000000000000001   0000000000000425   ffff881000000005   ffffffff000000f9  [   2479.728266 ]   Call   Trace :  [   2479.757386 ]   TSK  [   2479.780268 ]   [ ffffffff81019e44 ]   ?   try_to_wake_up + 0xe4 / 0x1f0  [   2479.847869 ]   [ ffffffff81066d78 ]   ?   __schedule + 0xf8 / 0x1e0  [   2479.911311 ]   [ ffffffff81064d60 ]   ?   client_poll_cq + 0x6c0 / 0x6c0  [   2479.979952 ]   [ ffffffff81064d70 ]   client_poll_cq_pass + 0x10 / 0x20  [   2480.049634 ]   [ ffffffff81020336 ]   kthread + 0xf6 / 0x110  [   2480.107875 ]   [ ffffffff81020240 ]   ?   __kthread_parkme + 0x70 / 0x70  [   2480.176516 ]   [ ffffffff8100e732 ]   ret_from_fork + 0x22 / 0x30    A classical SMP bug. Lucky to find this one. Let me try to describe this. There are two CPU1. CPU0 and CPU1. CPU0 is doing eviction while CPU1 is doing munmap- pcache_zap_pte. The CPU0 slected a pcm, while this pcm happen to be zapped at the same time by CPU1. There are not enough actions to either 1) prevent CPU0 from selecting this pcm, 2) prevent CPU1 from using this pcm. Both solutions might be work. But we need as least one.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 wuklab13   0313 - 12  [   1073.616269 ]   pcache : ffff880180777a80   mapcount : 0   refcount : 3   flags :( locked | allocated | usable )   kva :   ffff88011ddea000  [   1073.734941 ]   __clflush_one () :   EFAULT : bad   address   tsk :   32   user_va :   0x7fff4ddea000  [   1073.822304 ]   pcache   dumped   because :   evict / ref   bug  [   1073.987667 ]   BUG :   failure   at   managers / processor / pcache / evict . c : 301 / pcache_evict_line () !  [   1074.082308 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 763 / pcache_zap_pte () !  [   1074.172789 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1074.223751 ]   CPU :   23   PID :   50   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 476", 
            "title": "03/12 Tue"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0311-mon", 
            "text": "", 
            "title": "03/11 Mon"
        }, 
        {
            "location": "/lego/log/log-03-2018/#debug-victim-cache", 
            "text": "Morning. Today I will continue debugging victim and clflush, running with MT phoenix+2GB, seq+4GB. Sounds good.  Digging into yesterday s 21th run log. The warning comes from  victim_alloc_slowpath . The allocation abort after 10 seconds timeout. And interestingly, a lot threads abort. (The case is,  pset  is full, so  pcache_alloc  will try to evict one to victim cache. But victim cache is full as well. So it needs to evict one victim cache line too. Somehow this does not proceed as planned.) I guess somewhere deadlock happens.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 [   1682.040428 ]   WARNING :   CPU :   7   PID :   34   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1682.161063 ]   WARNING :   CPU :   19   PID :   46   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1686.602779 ]   WARNING :   CPU :   10   PID :   37   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1687.384837 ]   WARNING :   CPU :   3   PID :   53   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1687.505474 ]   WARNING :   CPU :   21   PID :   48   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1687.737386 ]   WARNING :   CPU :   16   PID :   43   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1687.859063 ]   WARNING :   CPU :   4   PID :   54   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1688.034819 ]   WARNING :   CPU :   6   PID :   56   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1688.210574 ]   WARNING :   CPU :   14   PID :   41   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1688.488246 ]   WARNING :   CPU :   5   PID :   55   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1689.598935 ]   WARNING :   CPU :   22   PID :   49   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1689.953565 ]   WARNING :   CPU :   0   PID :   51   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1691.740234 ]   WARNING :   CPU :   13   PID :   40   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1691.861911 ]   WARNING :   CPU :   1   PID :   52   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0  [   1791.554552 ]   WARNING :   CPU :   11   PID :   38   at   managers / processor / pcache / victim . c : 447   victim_prepare_insert + 0x322 / 0x4b0    1 st  run. MT+2GB. Victim allocation as predicted. Somehow I already forgot how the code is designed. I need to take a detailed reread.  Along the testing, fixed a bug in eviction code: handle failed evict_line properly. If eviction mechanism failed, we need to clear what the algorithm part has done. This is also related to yesterday s big idea: always do proper cleanup. Many thanks go to pcache free checking, help me to find this bug.  Less is more. I printed too much useless info when pcache_alloc or victim_alloc fail. I removed all the dump_pset from the failing path. It can give me a much more clean message to debug.  Hmm, it is really weird. I dump all victims once alloc timeout. You can see that all victim are not Flushed, that means none of them can be evicted. Take a look at the stat. Hmm, I probabaly should not do this per-cpu counter??  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55 ...  [   4751.460819 ]     --     Start   Dump   Victim   Cache       --  [   4751.518022 ]     --     CPU19   [ word_count - pthr ][ pid = 46 ,   tgid = 32 ]   --  [   4751.587706 ]   victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800440  [   4751.747872 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff20011000  [   4751.827955 ]       pset : ffff88207f800440   set_idx :   17   nr_lru : 8  [   4751.893478 ]  [   4751.911159 ]   victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f8003c0  [   4752.071326 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff2000f000  [   4752.428060 ]       pset : ffff88207f8003c0   set_idx :   15   nr_lru : 8  [   4752.630868 ]  [   4752.931441 ]   victim : ffff88207ff71090   index : 2   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800540  [   4753.370339 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff20015000  [   4753.450422 ]       pset : ffff88207f800540   set_idx :   21   nr_lru : 8  [   4753.515945 ]  [   4753.533627 ]   victim : ffff88207ff710d8   index : 3   refcount : 3   nr_fill : 1   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207fbdff40  [   4753.693792 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fffbf7fd000  [   4753.773875 ]       pset : ffff88207fbdff40   set_idx :   63485   nr_lru : 7  [   4753.842518 ]  [   4753.860199 ]   victim : ffff88207ff71120   index : 4   refcount : 3   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800500  [   4754.020367 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff20014000  [   4754.100449 ]       pset : ffff88207f800500   set_idx :   20   nr_lru : 8  [   4754.165971 ]  [   4754.183653 ]   victim : ffff88207ff71168   index : 5   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800480  [   4754.343819 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff30012000  [   4754.423902 ]       pset : ffff88207f800480   set_idx :   18   nr_lru : 8  [   4754.489426 ]  [   4754.507106 ]   victim : ffff88207ff711b0   index : 6   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f8004c0  [   4754.808718 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff30013000  [   4754.888802 ]       pset : ffff88207f8004c0   set_idx :   19   nr_lru : 8  [   4754.954325 ]  [   4754.972006 ]   victim : ffff88207ff711f8   index : 7   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800400  [   4755.132172 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff20010000  [   4755.212255 ]       pset : ffff88207f800400   set_idx :   16   nr_lru : 8  [   4755.277778 ]  [   4755.295458 ]     --     End   Dump   Victim   Cache         --  ...  [   4757.948641 ]   nr_pgfault :   313898  [   4757.983067 ]   nr_clflush :   488  [   4758.016347 ]   nr_pgfault_wp :   0  [   4758.050669 ]   nr_pgfault_wp_cow :   0  [   4758.089151 ]   nr_pgfault_wp_reuse :   0  [   4758.129713 ]   nr_pgfault_due_to_concurrent_eviction :   0  [   4758.188995 ]   nr_pcache_fill_from_memory :   313833  [   4758.242038 ]   nr_pcache_fill_from_victim :   54  [   4758.290919 ]   nr_pcache_eviction_triggered :   243280263  [   4758.349161 ]   nr_pcache_eviction_eagain :   243279763  [   4758.404283 ]   nr_pcache_eviction_succeed :   488  [   4758.454207 ]   nr_victim_eviction :   426  [   4758.495807 ]   nr_victim_prepare_insert :   500  [   4758.543649 ]   nr_victim_finish_insert :   488  [   4758.590451 ]   nr_victim_flush_submitted :   488  [   4758.639333 ]   nr_victim_flush_finished :   488    I counted it wrong. Below is the log. Since  nr_victim_flushd_run * 8 = nr_victim_flush_finished , it basically means for every run, victim_flushd needs to flush all 8 victims, which implies eviction rate is much higher than the flushd running rate.  nr_pcache_fill_from_victim :   21 , which means there are some succeed refills, but I don t know how it can improve performance.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62 [    475.468489 ]   CPU4   PID : 54   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207f800000 ,   pset_idx : 0 ,   nr_lru : 7  [    475.602752 ]   CPU3   PID : 53   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207f900a00 ,   pset_idx : 16424 ,   nr_lru : 7  [    476.029145 ]   CPU5   PID : 55   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207fbdff40 ,   pset_idx : 63485 ,   nr_lru : 7  [    476.169542 ]   CPU9   PID : 36   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207f900000 ,   pset_idx : 16384 ,   nr_lru : 7  [    477.360322 ]   CPU1   PID : 52   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207fbfff80 ,   pset_idx : 65534 ,   nr_lru : 7  [    479.206291 ]   CPU18   PID : 45   Abort   victim   alloc   ( 10010 ms )   nr_usable_victims :   8   req   from   pset : ffff88207fb00000 ,   pset_idx : 49152 ,   nr_lru : 7  [    475.743150 ]     --     Start   Dump   Victim   Cache       --  [    475.800350 ]     --     CPU4   [ word_count - pthr ][ pid = 54 ,   tgid = 32 ]   --  [    475.868989 ]   victim : ffff88207ff71000   index : 0   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800a80  [    476.309940 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff3002a000  [    476.390020 ]       pset : ffff88207f800a80   set_idx :   42   nr_lru : 8  [    476.455538 ]  [    476.473218 ]   victim : ffff88207ff71048   index : 1   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800bc0  [    476.633376 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff4002f000  [    476.713453 ]       pset : ffff88207f800bc0   set_idx :   47   nr_lru : 8  [    476.778972 ]  [    476.796652 ]   victim : ffff88207ff71090   index : 2   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800b80  [    476.956809 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff3002e000  [    477.036889 ]       pset : ffff88207f800b80   set_idx :   46   nr_lru : 8  [    477.102406 ]  [    477.120086 ]   victim : ffff88207ff710d8   index : 3   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800a00  [    477.280245 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff30028000  [    477.500721 ]       pset : ffff88207f800a00   set_idx :   40   nr_lru : 8  [    477.566239 ]  [    477.583918 ]   victim : ffff88207ff71120   index : 4   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800b40  [    477.744077 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff3002d000  [    477.824155 ]       pset : ffff88207f800b40   set_idx :   45   nr_lru : 8  [    477.889673 ]  [    477.907353 ]   victim : ffff88207ff71168   index : 5   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800b00  [    478.067511 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff3002c000  [    478.147590 ]       pset : ffff88207f800b00   set_idx :   44   nr_lru : 8  [    478.213109 ]  [    478.230788 ]   victim : ffff88207ff711b0   index : 6   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800a40  [    478.390946 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff30029000  [    478.471024 ]       pset : ffff88207f800a40   set_idx :   41   nr_lru : 8  [    478.536542 ]  [    478.554222 ]   victim : ffff88207ff711f8   index : 7   refcount : 2   nr_fill : 0   locked : 0   flags :( allocated | usable | hasdata )   pcm :            ( null )   pset : ffff88207f800ac0  [    478.714380 ]       hit [ 0 ]   owner :   [ word_count - pthr ][ 32 ]   addr :   0x7fff3002b000  [    478.794458 ]       pset : ffff88207f800ac0   set_idx :   43   nr_lru : 8  [    478.859977 ]  [    478.877657 ]     --     End   Dump   Victim   Cache         --  [    480.324070 ]   nr_pgfault :   372353  [    480.358494 ]   nr_clflush :   336  [    480.391774 ]   nr_pgfault_wp :   0  [    480.426093 ]   nr_pgfault_wp_cow :   0  [    480.464573 ]   nr_pgfault_wp_reuse :   0  [    480.505132 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    480.564410 ]   nr_pcache_fill_from_memory :   372326  [    480.617450 ]   nr_pcache_fill_from_victim :   21  [    480.666330 ]   nr_pcache_eviction_triggered :   178320088  [    480.724569 ]   nr_pcache_eviction_eagain :   178319746  [    480.779687 ]   nr_pcache_eviction_succeed :   336  [    480.829606 ]   nr_victim_eviction_triggered :   20589049  [    480.886805 ]   nr_victim_eviction_eagain :   20588741  [    480.940885 ]   nr_victim_eviction_succeed :   308  [    480.990804 ]   nr_victim_prepare_insert :   342  [    481.038643 ]   nr_victim_finish_insert :   336  [    481.085442 ]   nr_victim_flush_submitted :   336  [    481.134321 ]   nr_victim_flush_finished :   336  [    481.182161 ]   nr_victim_flushd_run :   42", 
            "title": "Debug victim cache"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0310-sun", 
            "text": "", 
            "title": "03/10 Sun"
        }, 
        {
            "location": "/lego/log/log-03-2018/#fix-bug-from-__unhash_procees", 
            "text": "[Summary]: a bug cause by laziness. When fork happens, the new thread is added into parent s thread_group list. However, we forgot to remove it when the new thread exit. Thus, the field in parent s thread_group will point to a freed page. To make it worse, the freed page got allocated again. In our case, the page was used by pgtable. So, when the parent tries to use that field, it simply corrupts pgtable. This bug is fixed by this commit: 64d43fc.  Got something going on. Huh.  Anyway, pick up what left last night.  8 th  run,  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 [  426.595911] SYSC_mmap(cpu5): ret_addr:0x7ffefbeac000\n\npte page got allocated\n[  426.653216]     pmd is none index 0x1e3 line 567 from_addr 0x7ffefc6acd90\n[  426.734334] __pte_alloc(): for addr: 0x7ffefc6acd90 pte_index: ac\n[  426.807132]     pte is none index 0x38 line 574 from_addr 0x7ffefc6acd90\n[  427.304148]     pte is none index 0x38 line 576 from_addr 0x7ffefc6acd90\n\nthis addr seems fine\n[  427.382251]     pte is none index 0x38 line 567 from_addr 0x7ffefc6abe78\n[  427.462329]     pte is none index 0x38 line 574 from_addr 0x7ffefc6abe78\n[  427.644439]     pte is none index 0x38 line 576 from_addr 0x7ffefc6abe78\n\nSomething happen in between corrupted pgtable\n[  427.722547] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000)\n[  427.858779] line: 567 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0\n\n[  427.938858] pte:ffff88207e8b51c0 pfn:0x8207e8c3 flags:(dirty|large|global|softw4|pkey0|pkey1|pkey2|pkey3|nx|0x3ff800000000000)\n[  428.075095] line: 574 from_addr: 0x6fc6d8 pte.cont: 0xffff88207e8c31c0   9 th  run, found actually it created another thread. And it exit. And it corrupted aftet the pid33 exit. Bang, it should be something wrong in exit().  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31 wuklab13   0311 - 4  [    813.127325 ]   CPU6   pid : 33       pmd   is   none   index   0x1e3   line   586   from_addr   0x4b0db0  [    813.214683 ]   CPU5   pid : 32       pmd   is   none   index   0x1e3   line   593   from_addr   0x6f4768  [    813.302042 ]   CPU6   pid : 33       pmd   is   none   index   0x1e3   line   593   from_addr   0x4b0db0  [    813.397836 ]   CPU5   pid : 32       pmd   is   none   index   0x1e3   line   595   from_addr   0x6f4768  [    813.593364 ]   CPU6   pid : 33       pmd   is   none   index   0x1e3   line   595   from_addr   0x4b0db0   [    813.678751 ]   do_exit ()   pid : 33 , tgid : 32   code : 0x0   [    814.474321 ]   CPU5   pid : 32       pmd   is   none   index   0x1e3   line   567   from_addr   0x7ffefc6acd90  [    814.567918 ]   CPU5   pid : 32       pmd   is   none   index   0x1e3   line   575   from_addr   0x7ffefc6acd90  [    814.661516 ]   CPU5   pid : 32       pmd   is   none   index   0x1e3   line   583   from_addr   0x7ffefc6acd90  [    814.755115 ]   CPU5   pid : 32       pmd   is   none   index   0x1e3   line   586   from_addr   0x7ffefc6acd90  [    814.848714 ]   __pte_alloc () :   for   addr :   0x7ffefc6acd90   pte_index :   ac  [    814.921511 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   593   from_addr   0x7ffefc6acd90  [    815.125249 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   595   from_addr   0x7ffefc6acd90  [    815.215833 ]   After   pcache_handle_fault  [    815.259511 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   726   from_addr   0x7ffefc6acd90  [    815.352071 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   567   from_addr   0x7ffefc6abe78  [    815.444627 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   575   from_addr   0x7ffefc6abe78  [    815.537186 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   583   from_addr   0x7ffefc6abe78  [    815.629744 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   586   from_addr   0x7ffefc6abe78  [    815.722303 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   593   from_addr   0x7ffefc6abe78  [    815.916890 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   595   from_addr   0x7ffefc6abe78  [    816.007471 ]   After   pcache_handle_fault  [    816.051151 ]   CPU5   pid : 32       pte   is   none   index   0x38   line   726   from_addr   0x7ffefc6abe78  [    816.143715 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    816.279946 ]   do_exit ()   pid : 34 , tgid : 32   code : 0x0  [    816.331945 ]   CPU5   pid : 32   line :   567   from_addr :   0x6fc6d8   pte . cont :   0xffff88207e8c31c0    10 th  run, actually 2 threads are created. When pid 33 exit, everything stays okay. But after fork of pid 34. It went wrong:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 wuklab13   0311 - 8  [    609.490893 ]   do_exit ()   pid : 33 , tgid : 32   code : 0x0   [    609.542894 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   401   from_addr   0x0  [    609.640661 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   443   from_addr   0x0  [    609.738429 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   465   from_addr   0x0  [    609.836197 ]   exit_mm : 378   mm - users   2   mm - count   1  [    609.891320 ]   exit_mm : 380   mm - users   1   mm - count   1  [    609.946445 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   468   from_addr   0x0  [    610.044212 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   471   from_addr   0x0  [    610.141979 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   474   from_addr   0x0  [    610.239747 ]   SYSC_mmap ( cpu5 ) :   ret_addr : 0x7ffefbeac000  [    610.299031 ]   CPU6   pid : 33   caller :   do_exit      pmd   is   none   index   0x1e3   line   482   from_addr   0x0  [    610.396798 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pmd   is   none   index   0x1e3   line   568   from_addr   0x7ffefc6acd90  [    610.518489 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pmd   is   none   index   0x1e3   line   576   from_addr   0x7ffefc6acd90  [    610.640178 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pmd   is   none   index   0x1e3   line   584   from_addr   0x7ffefc6acd90  [    610.761866 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pmd   is   none   index   0x1e3   line   587   from_addr   0x7ffefc6acd90  [    610.883557 ]   __pte_alloc () :   for   addr :   0x7ffefc6acd90   pte_index :   ac  [    610.956362 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   594   from_addr   0x7ffefc6acd90  [    611.179051 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   596   from_addr   0x7ffefc6acd90  [    611.297723 ]   After   pcache_handle_fault  [    611.341406 ]   CPU5   pid : 32   caller :   do_page_fault      pte   is   none   index   0x38   line   726   from_addr   0x7ffefc6acd90  [    611.455816 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   568   from_addr   0x7ffefc6abe78  [    611.576464 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   576   from_addr   0x7ffefc6abe78  [    611.697113 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   584   from_addr   0x7ffefc6abe78  [    611.817762 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   587   from_addr   0x7ffefc6abe78  [    611.938412 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   594   from_addr   0x7ffefc6abe78  [    612.161103 ]   CPU5   pid : 32   caller :   pcache_handle_fault      pte   is   none   index   0x38   line   596   from_addr   0x7ffefc6abe78  [    612.279778 ]   After   pcache_handle_fault  [    612.323461 ]   CPU5   pid : 32   caller :   do_page_fault      pte   is   none   index   0x38   line   726   from_addr   0x7ffefc6abe78  [    612.437875 ]   do_fork :   current :   32   new :   34   [    612.484676 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    612.620924 ]   do_exit ()   pid : 34 , tgid : 32   code : 0x0  [    612.672928 ]   CPU5   pid : 32   caller :   pcache_handle_faultline :   568   from_addr :   0x6fc6d8   pte . cont :   0xffff88207e8c31c0   [    612.793577 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    612.929828 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    613.066078 ]   CPU7   pid : 34   caller :   do_exitline :   401   from_addr :   0x0   pte . cont :   0xffff88207e8c31c0    11 th  run, found it orignate from  copy_process() . Good. 1\n2\n3\n4 [    869.591729 ]   CPU5   pid : 32   caller :   do_fork      pte   is   none   index   0x38   line   886   from_addr   0x0  [    869.688449 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    869.824681 ]   CPU5   pid : 32   caller :   do_fork   line :   894   from_addr :   0x0   pte . cont :   0xffff88207e8c31c0    12 th  run, found the opeation that corrupt pgtable:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 [   1099.974106 ]   CPU5   pid : 32   caller :   copy_process      pte   is   none   index   0x38   line   897   from_addr   0x0  [   1100.076032 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [   1100.212282 ]   CPU5   pid : 32   caller :   copy_process   line :   902   from_addr :   0x0   pte . cont :   0xffff88207e8c31c0  896           if   ( current - tgid   ==   32 )  897                   jasmine ( 0 ,   __LINE__ ,   __func__ );  898  899                           list_add_tail ( p - thread_group ,  900                                             p - group_leader - thread_group );  901           if   ( current - tgid   ==   32 )  902                   jasmine ( 0 ,   __LINE__ ,   __func__ );    13 th  run, interesting, the list_add_tail write to the pgtable.  pte.cont = 0xffff88207e8c31c0, p- thread_group: 0xffff88207e8c31c0 . 1\n2\n3\n4\n5\n6\n7\n8 [    916.269942 ]   CPU5   pid : 32   caller :   copy_process      pte   is   none   index   0x38   line   898   from_addr   0x0  [    916.371863 ]   p :   ffff88207e8c3000   p - group_leader :   ffff88107e190000 ( 32 )   p - thread_group :   ffff88207e8c31c0   leader - thread_grou :   ffff88107e1901c0  [    916.523705 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    916.659947 ]   CPU5   pid : 32   caller :   copy_process   line :   906   from_addr :   0x0   pte . cont :   0xffff88207e8c31c0  [    916.769148 ]   p :   ffff88207e8c3000   p - group_leader :   ffff88107e190000 ( 32 )   p - thread_group :   ffff88207e8c31c0   leader - thread_grou :   ffff88107e1901c0    14 th  run, got an log like this. Clearly, the pte is written the value  of p- thread_group. But the leader s pointer is correct. Weird, going to dig deeper. 1\n2\n3\n4\n5\n6\n7\n8\n9                 p: ffff88207e8c3000 p- group_leader: ffff88107e189000(32)\n\n                p- thread_group:        ffff88207e8c31c0\n                leader- thread_group:   ffff88107e1891c0\n\n                pte page:               ffff88207e8b5000\n                pte:                    ffff88207e8b51c0\n\n                pte.cont:               ffff88207e8c31c0   15 th  run, found the bug.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 wuklab13   0311 - 15  [   1474.477687 ]   dup_task_struct () :   current :   32   new :   ffff88207e8b5000  ..  while   pid   33   exit  so   the   ffff88207e8b5000   is   freed  but   allocated   again   by   pte_alloc  [   1481.420200 ]   __pte_alloc () : CPU5   for   addr :   0x7ffefc6acd90   pte_index :   ac   new   pte   page :   ffff88207e8b5000   However ,   we   forgot   to   remove   it   from   group_leader s   thread_group  [   1485.895938 ] \n                 p :   ffff88207e8c3000 \n                 p - group_leader :   ffff88107e19b000 ( 32 ) \n\n                 p - thread_group :          ffff88207e8c31c0 \n                 leader - thread_group :     ffff88107e19b1c0  [   1486.047784 ] \n                 tg - next :                 ffff88207e8c31c8 \n                 tg - prev :                 ffff88207e8c31c0 \n                 leader - tg - next          ffff88107e19b1c8 \n                 leader - tg - prev          ffff88107e19b1c0  [   1486.191311 ]    next                      ffff88107e19b1c0                   prev                      ffff88207e8b51c0                   next                      ffff88107e19b1c0  [   1486.276594 ]   CPU5   pid : 32   caller :   __list_add      pte   is   none   index   0x38   line   61   from_addr   0x0   page :   0xffff88207e8b5000  [   1486.401399 ]   CPU5   pid : 32   caller :   __list_add      pte   is   none   index   0x38   line   65   from_addr   0x0   page :   0xffff88207e8b5000  [   1486.526203 ]   CPU5   pid : 32   caller :   __list_add      pte   is   none   index   0x38   line   69   from_addr   0x0   page :   0xffff88207e8b5000  [   1486.651010 ]   CPU5   pid : 32   caller :   __list_add      pte   is   none   index   0x38   line   73   from_addr   0x0   page :   0xffff88207e8b5000  [   1486.775814 ]   pte : ffff88207e8b51c0   pfn : 0x8207e8c3   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [   1486.912060 ]   CPU5   pid : 32   caller :   __list_add   line :   77   from_addr :   0x0   pte . cont :   0xffff88207e8c31c0    16 th  run, damn, after patching  __unhash_process() , it finally works. Going to workout, see you tonight.", 
            "title": "Fix bug from __unhash_procees()"
        }, 
        {
            "location": "/lego/log/log-03-2018/#victim-report-error", 
            "text": "17 th  run. The phoenix program has bug itself, it is not able to run with 4GB dataset. So try it with 2GB dataset. Uuh, the log is too long.  __put_vicim  report a victim that has wrong flags. Going to disable the evict log and try again.  18 th  run. Happen to run seq with 100MB  It actually half finished. But the printf of phoenix has funny chars. I guess memory is corrupted. The log shows it is ib_mad_completion.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73 [   2244.018806 ]   Processor :   Processor   manager   is   running .  [   2246.394568 ]   STDOUT :   --- [  envp [ 0 ]   HOME =/  ] ---  [   2246.447719 ]   STDOUT :   --- [  envp [ 1 ]   TERM = linux  ] ---  [   2246.507003 ]   STDOUT :   --- [  argv [ 0 ]   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - seq  ] ---  [   2246.618289 ]   STDOUT :   --- [  argv [ 1 ]   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_100MB . txt  ] ---  [   2258.805633 ]   STDOUT :   --- [  Word - Count - Seq :   Computation   Completed   12.46633   sec  ] ---  [   2258.923180 ]   SYSC_close () :   [ 4 ]   -   [ / proc / meminfo ]  [   2258.995743 ]   STDOUT :   --- [  Use   len   is   123748  [   2263.484774 ]   STDOUT :   --- [  THE :   1115050  ] ---  [   2263.666785 ]   STDOUT :   --- [  OF :   615296  ] ---  [   2266.103660 ]   STDOUT :   --- [  AND :   545303   ( a   lot   funny   chars ,   deleted .)  ] ---  [   2267.016837 ]   Code :   [   2267.038680 ]   STDOUT :   --- [  TO :   475179  + \u00d5\u00fe\u00da\u00e9\u00d8 ^ G \u00a7 87 k 80 z ^ T 86 ruJ \u00b7\u00bf\u00bb 9 e \u00e9\u00de\u00ed\u00d1 r \u00dc\u00d5 ^ W \u00e5 ^ W *^ _ {( \u00ca ? R \u00f9 a \u00e9\u00f6\u00f7 8 \u00ed 91 \u00dc\u00e8 8f \u00f2\u00bf i ^? \u00e8 4 94 \u00d7\u00b2\u00c9\u00b5 ^ V \u00bf\u00ab\u00eb P ] \u00ed\u00ef h ^ G \u00ca\u00eb 98 ^ T \u00d7 Qp \u00b9 O \u00ae\u00ef ^ \\\u00da ^?^ A \u00ed 91 \u00d9 v \u00dd By ^ _ \u00e9 iwP ^ r 97 \u00eb\u00f9\u00ef\u00df ] \u00a3\u00df\u00ad 98 81 \u00f8 85 \u00ce Ey ^ Y \u00e5 ^? V \u00f9\u00ba ^ Y \u00de\u00f5\u00cb ] r5 \u00c9\u00f0 ^^ 92 \u00c9 ] ^ ] P ^ \u00c7 i \u00bb z : \u00d4 ^ S  \u00ae e 8 a + \\\u00e9 8 a \u00ae\u00b1\u00e0 E \u00d5\u00ce , \u00f0\u00d2\u00e2 3 \u00c1 _ ^ P_ ^ H ^ [ | \u00b8\u00ae\u00e1 s \u00ed F \u00bf m 95 9 d ? 82 \u00f2 : \u00be\u00de\u00f5 3 \u00ca\u00d7 T \u00fc\u00ae  ] ---  [   2263.339165 ]   BUG :   unable   to   handle   kernel   paging   request   at   ffffffffffff8100  [   2263.422369 ]   IP :   [ ffffffffffff8100 ]   0xffffffffffff8100  [   2263.570058 ]   PGD   1140067   PUD   1142067   PMD   0  [   2263.618942 ]   Oops :   0010   [ # 1 ]   SMP   PROCESSOR  [   2264.705811 ]   CPU :   0   PID :   27   Comm :   ib_mad_completi   4.0.0 - lego - ys +   # 408  [   2264.781736 ]   RIP :   0010 : [ ffffffffffff8100 ]    [ ffffffffffff8100 ]   0xffffffffffff8100  [   2264.873262 ]   RSP :   0000 : ffff88107efabc90    EFLAGS :   00010046  [   2264.936705 ]   RAX :   5636000000000098   RBX :   db5affffffffffff   RCX :   0000000000000001  [   2265.021990 ]   RDX :   ffff88107efabd38   RSI :   0000000000000000   RDI :   4460ff ffffff8114  [   2265.107277 ]   RBP :   ffff88107efabce0   R08 :   000000000000001f   R09 :   ffff88107efa43c0  [   2265.192561 ]   R10 :   ffff88107efabe68   R11 :   0000000000000001   R12 :   ac02000004ecbdbd  [   2265.277847 ]   R13 :   0000000000000000   R14 :   ffff88107efa4228   R15 :   ffff88107e1ab000  [   2265.363133 ]   FS :    0000000000000000 ( 0000 )   GS : ffff88107fc00000 ( 0000 )   knlGS : 0000000000000000  [   2265.459858 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [   2265.528503 ]   CR2 :   ffffffffffff8100   CR3 :   000000000113 d000   CR4 :   00000000000406 b0  [   2265.613789 ]   Stack :  [   2265.637710 ]   ffffffff810151a7   00000000000000 82   ffff88107fc04980   0000000000000000  [   2265.725075 ]   ffff88107efabcc8   ffff88107fc04980   0000000000000000   0000000000000000  [   2265.812441 ]   ffff88107efa4228   ffff88107e1ab000   ffff88107efabcf8   ffffffff81016e17  [   2265.899806 ]   000000007 efabe20   ffff88107efabd20   ffffffff810066f4   ffffffff81072f20  [   2265.987172 ]   ffff88107fc05e00   ffff88107efa4000   ffff88107efabe08   ffffffff8100e4aa  [   2266.074538 ]   Call   Trace :  [   2266.206626 ]   TSK  [   2266.229507 ]   [ ffffffff810151a7 ]   ?   update_wall_time + 0x47 / 0x6b0  [   2266.299192 ]   [ ffffffff81016e17 ]   tick_handle_periodic + 0x67 / 0x70  [   2266.369916 ]   [ ffffffff810066f4 ]   apic_timer_interrupt + 0x54 / 0x90  [   2266.440641 ]   [ ffffffff8100e4aa ]   smp__apic_timer_interrupt + 0x6a / 0x70  [   2266.516565 ]   [ ffffffff810663b8 ]   ?   __schedule + 0xf8 / 0x1e0  [   2266.580010 ]   [ ffffffff810664b3 ]   schedule + 0x13 / 0x30  [   2266.638254 ]   [ ffffffff81058c97 ]   ib_mad_completion_handler + 0x2b7 / 0x860  [   2266.716258 ]   [ ffffffff810589e0 ]   ?   ib_mad_send_done_handler . isra .22 + 0x1d0 / 0x1d0  [   2266.803624 ]   [ ffffffff81020376 ]   kthread + 0xf6 / 0x110  [   2266.861867 ]   [ ffffffff81020280 ]   ?   __kthread_parkme + 0x70 / 0x70  [   2266.930512 ]   [ ffffffff8100e732 ]   ret_from_fork + 0x22 / 0x30  [   2266.993955 ]   EOT    19 th , try seq+100MB again. Well succeed. I guess I start S too later. So that thread has issues. We run 12.3 sec, while linux run 9.7 sec.  20 th , try seq+4GB data. Linux runs  314.4 sec . Lego runs  403 sec . But Lego has some clflush error messages. I don t know why actually.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66 [    794.604628 ]   Processor :   Processor   manager   is   running .  [    796.884884 ]   STDOUT :   --- [  envp [ 0 ]   HOME =/  ] ---  [    796.938032 ]   STDOUT :   --- [  envp [ 1 ]   TERM = linux  ] ---  [    796.997312 ]   STDOUT :   --- [  argv [ 0 ]   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - seq  ] ---  [    797.108596 ]   STDOUT :   --- [  argv [ 1 ]   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_4GB . txt  ] ---  [    980.640200 ]   __clflush_one () :   EFAULT : bad   address  [    980.692315 ]   __clflush_one () :   EFAULT : bad   address  [    980.746397 ]   __clflush_one () :   EFAULT : bad   address  [    980.800478 ]   __clflush_one () :   EFAULT : bad   address  [    980.854559 ]   __clflush_one () :   EFAULT : bad   address  [    980.908642 ]   __clflush_one () :   EFAULT : bad   address  [    980.962723 ]   __clflush_one () :   EFAULT : bad   address  [    981.016804 ]   __clflush_one () :   EFAULT : bad   address  [    981.070886 ]   __clflush_one () :   EFAULT : bad   address  [    981.124968 ]   __clflush_one () :   EFAULT : bad   address  [    981.179048 ]   __clflush_one () :   EFAULT : bad   address  [    981.233129 ]   __clflush_one () :   EFAULT : bad   address  [    981.287211 ]   __clflush_one () :   EFAULT : bad   address  [    981.341293 ]   __clflush_one () :   EFAULT : bad   address  [    981.395375 ]   __clflush_one () :   EFAULT : bad   address  [    981.449456 ]   __clflush_one () :   EFAULT : bad   address  [    981.503538 ]   __clflush_one () :   EFAULT : bad   address  [    981.557619 ]   __clflush_one () :   EFAULT : bad   address  [    981.611702 ]   __clflush_one () :   EFAULT : bad   address  [    981.665782 ]   __clflush_one () :   EFAULT : bad   address  [    981.719863 ]   __clflush_one () :   EFAULT : bad   address  [    981.773945 ]   __clflush_one () :   EFAULT : bad   address  [    981.828026 ]   __clflush_one () :   EFAULT : bad   address  [    981.882108 ]   __clflush_one () :   EFAULT : bad   address  [    981.936188 ]   __clflush_one () :   EFAULT : bad   address  [    981.990271 ]   __clflush_one () :   EFAULT : bad   address  [    982.044352 ]   __clflush_one () :   EFAULT : bad   address  [    982.098434 ]   __clflush_one () :   EFAULT : bad   address  [    982.152515 ]   __clflush_one () :   EFAULT : bad   address  [    982.206596 ]   __clflush_one () :   EFAULT : bad   address  [   1200.759741 ]   STDOUT :   --- [  Word - Count - Seq :   Computation   Completed   403.519401   sec  ] ---  ...  [   1200.989480 ]   STDOUT :   --- [  THE :   44602000  ...  [   1201.755779 ]   do_group_exit ()   pid : 32 , tgid : 32   exit_code : 0x0  [   1201.819136 ]   do_exit ()   pid : 32 , tgid : 32   code : 0x0  [   1201.872451 ]   nr_pgfault :   1049525  [   1201.908579 ]   nr_pgfault_wp :   0  [   1201.942899 ]   nr_pgfault_wp_cow :   0  [   1201.981380 ]   nr_pgfault_wp_reuse :   0  [   1202.021941 ]   nr_pgfault_due_to_concurrent_eviction :   0  [   1202.081223 ]   nr_pcache_fill_from_memory :   1045393  [   1202.135304 ]   nr_pcache_fill_from_victim :   4132  [   1202.186265 ]   nr_pcache_eviction :   525230  [   1202.230987 ]   nr_victim_eviction :   521090    21th run. Do not have time and energy to debug the clflush issue. I just want to run MT+2GB again. Well victim has issues! Some warning are triggered. Log is  wuklab13:~/ys/0311-22 . Continue tomorrow! Good night world. (Such a lonly phd.)", 
            "title": "victim report error"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0310-sat", 
            "text": "Running python hello world. Tried to make kmalloc use buddy directly.", 
            "title": "03/10 Sat"
        }, 
        {
            "location": "/lego/log/log-03-2018/#put_pcache-in-pcache_zap_pte", 
            "text": "So this time, python keep running for a long time. But P crashed when the first time eviction was triggered.  Below is log from S side, those libraries do not exist, so these log are fine:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 S:\n[Mar10 10:39] handle_access_request /etc/ld.so.preload 4, -2\n[Mar10 10:44] local_file_open : Cannot open required file [/usr/lib64/python2.7/site.so].\n[  +0.352839] local_file_open : Cannot open required file [/usr/lib64/python2.7/sitemodule.so].\n[ +22.254465] local_file_open : Cannot open required file [/usr/lib64/python2.7/os.so].\n[  +0.350759] local_file_open : Cannot open required file [/usr/lib64/python2.7/osmodule.so].\n[Mar10 10:45] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpath.so].\n[  +0.358045] local_file_open : Cannot open required file [/usr/lib64/python2.7/posixpathmodule.so].\n[ +13.421033] local_file_open : Cannot open required file [/usr/lib64/python2.7/stat.so].\n[  +0.352838] local_file_open : Cannot open required file [/usr/lib64/python2.7/statmodule.so].\n[Mar10 10:46] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpath.so].\n[  +0.360126] local_file_open : Cannot open required file [/usr/lib64/python2.7/genericpathmodule.so].\n[ +11.582165] local_file_open : Cannot open required file [/usr/lib64/python2.7/warnings.so].\n[  +0.357003] local_file_open : Cannot open required file [/usr/lib64/python2.7/warningsmodule.so].\n[ +11.989828] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecache.so].\n[  +0.358043] local_file_open : Cannot open required file [/usr/lib64/python2.7/linecachemodule.so].\n[Mar10 10:47] local_file_open : Cannot open required file [/usr/lib64/python2.7/types.so].\n[  +0.353879] local_file_open : Cannot open required file [/usr/lib64/python2.7/typesmodule.so].   Weird P s bug, seems like the pcm returned by evict_find_line has issue. Well, I m trying to debug what is going with this set.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42 wuklab13   0310 - 2  [   1046.880649 ]   SYSC_read ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff6e117e0  [   1046.959692 ]       fd :   8 ,   buf :   00007ff ff7ffb000 ,   count :   4096  [   1048.726624 ]   pcache_evict_line () :   pset :   ffff88207f9ffec0 ,   for   uva :   0x7ffff7ffb000  [   1048.813053 ]   ------------ [   cut   here   ] ------------  [   1048.868174 ]   BUG :   failure   at   . / include / processor / pcache . h : 284 / pcache_meta_to_pcache_set () !  [   1048.965937 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1049.016898 ]   CPU :   5   PID :   32   Comm :   python   4.0.0 - lego - ys +   # 347  [   1049.083460 ]   Stack :  [   1049.107380 ]   ffff88107e18fca8   ffffffff81026f1c   000000000000000 8   ffff88107e18fcb8  [   1049.194743 ]   ffff88107e18fc70   0000000021475542   0000000000000000   0000000000000000  [   1049.282107 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [   1049.369468 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [   1049.456832 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [   1049.544193 ]   Call   Trace :  [   1049.573315 ]   TSK  [   1049.596195 ]   [ ffffffff81026f28 ]   panic + 0xc2 / 0xeb  [   1049.651318 ]   [ ffffffff8101c3fc ]   ?   task_tick_rt + 0x2c / 0xd0  [   1049.715799 ]   [ ffffffff81019a65 ]   ?   scheduler_tick + 0x55 / 0x60  [   1049.782360 ]   [ ffffffff81017035 ]   ?   tick_handle_periodic + 0x45 / 0x70  [   1049.855163 ]   [ ffffffff81006764 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [   1049.927966 ]   [ ffffffff8101c3fc ]   ?   task_tick_rt + 0x2c / 0xd0  [   1049.992447 ]   [ ffffffff81019a65 ]   ?   scheduler_tick + 0x55 / 0x60  [   1050.059009 ]   [ ffffffff81017035 ]   ?   tick_handle_periodic + 0x45 / 0x70  [   1050.131812 ]   [ ffffffff8103c41a ]   ?   put_dec + 0x1a / 0x80  [   1050.191093 ]   [ ffffffff81006764 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [   1050.263895 ]   [ ffffffff8100e56a ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [   1050.341897 ]   [ ffffffff81012ded ]   ?   printk + 0x11d / 0x1b0  [   1050.402219 ]   [ ffffffff810340c5 ]   dump_pcache_meta + 0xc5 / 0xd0  [   1050.468782 ]   [ ffffffff81034588 ]   pcache_evict_line + 0x158 / 0x220  [   1050.538463 ]   [ ffffffff81030f5e ]   pcache_alloc + 0x22e / 0x2f0  [   1050.602945 ]   [ ffffffff8103015a ]   common_do_fill_page + 0x2a / 0x430  [   1050.673668 ]   [ ffffffff8102fb20 ]   ?   pcache_meta_to_kva + 0x30 / 0x30  [   1050.744389 ]   [ ffffffff81030702 ]   pcache_handle_fault + 0x1a2 / 0x6c0  [   1050.816152 ]   [ ffffffff810103d2 ]   do_page_fault + 0xa2 / 0x1a0  [   1050.880634 ]   [ ffffffff8100db9f ]   page_fault + 0x1f / 0x30  [   1050.940955 ]   [ ffffffff8103bb82 ]   ?   copy_user_enhanced_fast_string + 0x2 / 0x10  [   1051.023118 ]   [ ffffffff81038423 ]   ?   normal_p2m_read + 0x233 / 0x330  [   1051.092800 ]   [ ffffffff810363ce ]   sys_read + 0x9e / 0x160  [   1051.152081 ]   [ ffffffff810268d0 ]   ?   strace_enter_default + 0x30 / 0x40  [   1051.224884 ]   [ ffffffff8100e935 ]   do_syscall_64 + 0x45 / 0xd0  [   1051.288326 ]   [ ffffffff8100d82c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25    Interesting, added several debug messages. The bug is I forgot to put_pcache when a rmap was zapped. One rmap counts one refcount (effectively one process), thus when a rmap was zapped, we should decrease the refcount. I found I ve already done so for  pcache_remove_rmap , and  pcache_move_pte . But damn, forgot this one. I remember this code was written before fork+pcache. So.. I don t have a big picture at that time.  Multithreaded system plus background reclaim really a very rigours design usage of refcount and lock .  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 [   1418.038411 ]   CPU5   PID32   sys_read + 0x0 / 0xa0  [   1418.085227 ]   pcache_evict_line () :   pset :   ffff88207f9ffec0 ,   for   uva :   0x7ffff7ffb000  [   1418.173617 ]   pset : ffff88207f9ffec0   set_idx :   32763   nr_lru : 8  [   1418.238105 ]   pcache : ffff8801801ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880107ffb000  [   1418.351476 ]   pcache : ffff8801805ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880117ffb000  [   1418.464847 ]   pcache : ffff8801809ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880127ffb000  [   1418.578220 ]   pcache : ffff880180dffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880137ffb000  [   1418.691591 ]   pcache : ffff8801811ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880147ffb000  [   1418.804963 ]   pcache : ffff8801815ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880157ffb000  [   1418.918334 ]   pcache : ffff8801819ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880167ffb000  [   1419.031706 ]   pcache : ffff880181dffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880177ffb000  [   1419.145077 ]   After   dump   pset  [   1419.176280 ]   pcache : ffff8801801ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880107ffb000  [   1419.289652 ]   pcache   dumped   because :   evict_find_line_lru  [   1419.351018 ]   pcache : ffff8801805ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880117ffb000  [   1419.464389 ]   pcache   dumped   because :   evict_find_line_lru  [   1419.525757 ]   pcache : ffff8801809ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880127ffb000  [   1419.639127 ]   pcache   dumped   because :   evict_find_line_lru  [   1419.700494 ]   pcache : ffff880180dffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880137ffb000  [   1419.813865 ]   pcache   dumped   because :   evict_find_line_lru  [   1419.875231 ]   pcache : ffff8801811ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880147ffb000  [   1419.988604 ]   pcache   dumped   because :   evict_find_line_lru  [   1420.049969 ]   pcache : ffff8801815ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880157ffb000  [   1420.163341 ]   pcache   dumped   because :   evict_find_line_lru  [   1420.224708 ]   pcache : ffff8801819ffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880167ffb000  [   1420.338079 ]   pcache   dumped   because :   evict_find_line_lru  [   1420.399445 ]   pcache : ffff880181dffec0   mapcount : 0   refcount : 1   flags :( allocated | usable )   kva :   ffff880177ffb000  [   1420.512817 ]   pcache   dumped   because :   evict_find_line_lru  [   1420.574183 ]   evict_find_line_lru () :   pcm :   ffff88207f9ffea8  [   1420.637631 ]   ------------ [   cut   here   ] ------------  [   1420.692756 ]   BUG :   failure   at   . / include / processor / pcache . h : 340 / pcache_meta_to_kva () !  [   1420.783245 ]   Kernel   Panic   -   not   syncing :   BUG !  [   1420.834210 ]   CPU :   5   PID :   32   Comm :   python   4.0.0 - lego - ys +   # 349  [   1420.900777 ]   Stack :", 
            "title": "put_pcache in pcache_zap_pte"
        }, 
        {
            "location": "/lego/log/log-03-2018/#python-hello-world-run-to-end", 
            "text": "Glad to say, python hello world finished, even with some missed syscalls. Especially the stdin stuff, so the string is actually not printed out. Log is wuklab13:~/ys/0310-4  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52 [   3149.540308 ]   CPU5   PID32   sys_ioctl + 0x0 / 0x10  [   3149.588144 ]   CPU5   PID32   sys_ioctl + 0x0 / 0x10  [   3149.635982 ]   CPU5   PID32   sys_write + 0x0 / 0xa0  [   3149.683818 ]   STDOUT :   --- [   ] ---  [   3149.726456 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff6d9aeb0   flags : 0x150  [   3149.926247 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff6d9aeb0   flags : 0x150   ret : 0 ( OKAY )  [   3150.033464 ]   CPU5   PID32   sys_newfstat + 0x0 / 0x10  [   3150.084420 ]   CPU5   PID32   sys_ioctl + 0x0 / 0x10  [   3150.132256 ]   strace__mmap   cpu5   addr = 0x0 ,   len = 0x1000 ,   prot ( 0x3 ) = PROT_READ | PROT_WRITE ,   flags ( 0x22 ) = MAP_PRIVATE | MAP_ANONYMOUS ,   fd = 18446744073709551615 (   ),   off = 0x0  [   3150.301772 ]   CPU5   PID32   sys_read + 0x0 / 0xa0  [   3150.348562 ]   ------------ [   cut   here   ] ------------  [   3150.403679 ]   WARNING :   CPU :   5   PID :   32   at   managers / processor / fs / stdio . c : 24   stdio_file_read + 0x30 / 0x50  [   3150.509751 ]   Process   wants   STDIN !  [   3150.546149 ]   CPU :   5   PID :   32   Comm :   python   4.0.0 - lego - ys +   # 352  [   3150.612705 ]   Stack :  [   3150.636624 ]   ffff88107e18fe90   ffffffff81012b15   ffffffff811464e0   00007ff ff7ffb000  [   3150.723977 ]   0000000000000400   00007ff ff70e5640   ffff88107e18fef0   ffffffff81012bd2  [   3150.811331 ]   ffffffff81079d6b   ffff881000000018   ffff88107e18ff00   ffff88107e18fec0  [   3150.898687 ]   0000000000000020   ffffffff810346b0   0000000000000022   ffffffff811464f0  [   3150.986040 ]   00007ff ff7fdf740   0000000000000000   ffff88107e18ff00   ffffffff81035ac0  [   3151.073394 ]   Call   Trace :  [   3151.102514 ]   TSK  [   3151.125392 ]   [ ffffffff81012b21 ]   __warn . constprop .0 + 0x91 / 0xd0  [   3151.194028 ]   [ ffffffff81012bd2 ]   warn_slowpath_fmt + 0x42 / 0x50  [   3151.261623 ]   [ ffffffff810346b0 ]   ?   sweep_pset_lru + 0x220 / 0x220  [   3151.330259 ]   [ ffffffff81035ac0 ]   stdio_file_read + 0x30 / 0x50  [   3151.395775 ]   [ ffffffff810346e3 ]   sys_read + 0x33 / 0xa0  [   3151.454010 ]   [ ffffffff8100e875 ]   do_syscall_64 + 0x45 / 0xd0  [   3151.517446 ]   [ ffffffff8100d76c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [   3151.593362 ]   EOT  [   3151.616240 ]   --- [   end   trace   0000000000000000   ] ---  [   3151.671360 ]   CPU5   PID32   sys_write + 0x0 / 0xa0  [   3151.719194 ]   STDOUT :   --- [  ] ---  [   3151.759756 ]   CPU5   PID32   sys_close + 0x0 / 0x140  [   3151.808628 ]   SYSC_close () :   [ 3 ]   -   [ / root / ys / py_hello . py ]  [   3151.871028 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a79380   flags : 0x150  [   3152.070817 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a79380   flags : 0x150   ret : 0 ( OKAY )  [   3152.178033 ]   CPU5   PID32   sys_rt_sigaction + 0x0 / 0xb0  [   3152.234151 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a77f60   flags : 0x150  [   3152.432941 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a77f60   flags : 0x150   ret : 0 ( OKAY )  [   3152.540242 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff73ee794   flags : 0x150  [   3152.739952 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff73ee794   flags : 0x150   ret : 0 ( OKAY )  [   3152.847171 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff715b278   flags : 0x150  [   3153.046958 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff715b278   flags : 0x150   ret : 0 ( OKAY )  [   3153.154179 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff6de74f0   flags : 0x150  [   3153.353965 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff6de74f0   flags : 0x150   ret : 0 ( OKAY )  [   3153.461180 ]   CPU5   PID32   sys_exit_group + 0x0 / 0x10", 
            "title": "python hello world run to end"
        }, 
        {
            "location": "/lego/log/log-03-2018/#trying-phoenix-pthread-again", 
            "text": "4GB pcache, 1GB dataset.  1 th  run with CONFIG_STRACE on, 1GB dataset finished, result is correct.  2 th  run without CONFIG_STRACE, 1GB dataset stuck. Two weird things:   open/close dev/cpu/online file too many times than a normal linux run  IB stucked\nSo next I m going to try add a lock to ibapi, see if it is ib internal deadlock issue.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56 wuklab13   0310 - 7  [    702.895936 ]   Processor :   Processor   manager   is   running .  [    722.400159 ]   STDOUT :   --- [  envp [ 0 ]   HOME =/  ] ---  [    722.453307 ]   STDOUT :   --- [  envp [ 1 ]   TERM = linux  ] ---  [    722.512589 ]   STDOUT :   --- [  argv [ 0 ]   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread  ] ---  [    722.628036 ]   STDOUT :   --- [  argv [ 1 ]   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_1GB . txt  ] ---  [    722.759101 ]   STDOUT :   --- [  Wordcount :   Running ...  ] ---  [    722.819406 ]   STDOUT :   --- [  ] ---  [    722.860139 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    722.940653 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.011483 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.084287 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.157090 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.229894 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.302698 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.375502 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.448306 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.521111 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.593914 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.666718 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.739522 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.812326 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    723.885130 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    766.701260 ]   ibapi_send_reply ()   polling   timeout   ( 30010   ms ),   caller :   net_send_reply_timeout + 0x11b / 0x1ee  [    766.809538 ]    net_send_reply_timeout ()   caller :   __pcache_do_fill_page + 0x82 / 0x140  [    766.895863 ]   word_count - pthr [ 65 ] :   segfault   at   0x7fffb5eba000   ip   00000000004024 9 d   sp   00007ff fb5e9ad80   error   6  [    767.012348 ]   CPU :   15   PID :   65   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 359  [    767.089312 ]   RIP :   0033 : [ 00000000004024 9 d ]    [ 00000000004024 9 d ]   0x40249d  [    767.170436 ]   RSP :   002 b : 00007ff fb5e9ad80    EFLAGS :   00010216  [    767.233879 ]   RAX :   00007ff fb5eba000   RBX :   00000000000013 88   RCX :   000000000000004f  [    767.319164 ]   RDX :   00007ff fe4ea92a4   RSI :   00007ff fe626fac9   RDI :   00007ff fe4ea92a4  [    767.404449 ]   RBP :   00000000007540e0   R08 :   0000000000000000   R09 :   0000000000014f a0  [    767.489733 ]   R10 :   0000000000427f b0   R11 :   0000000000000202   R12 :   0000000000012 b12  [    767.575018 ]   R13 :   00007ff f496ab890   R14 :   00007ff f48704fb0   R15 :   00000000000013 88  [    767.660303 ]   FS :    00007ff fb5e9b700 ( 0000 )   GS : ffff88207fce0000 ( 0000 )   knlGS : 0000000000000000  [    767.757028 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [    767.825671 ]   CR2 :   00007ff fb5eba000   CR3 :   000000207f e3a000   CR4 :   00000000000406 a0  [    767.910958 ]   get_signal () :   dequeue_signr :   11 ,   handler :            ( null )  [    767.987928 ]   get_signal () :   dequeue_signr :   9 ,   handler :            ( null )    3 th  run, without STRACE, with locked ibapi, it finished, result is correct. Runtime:  18.692936 sec . 1\n2\n3\n4\n5\n6\n7\n8\n9 [    555.423623 ]   nr_pgfault :   288100  [    555.458042 ]   nr_pgfault_wp :   0  [    555.492360 ]   nr_pgfault_wp_cow :   0  [    555.530838 ]   nr_pgfault_wp_reuse :   0  [    555.571396 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    555.630673 ]   nr_pcache_fill_from_memory :   288081  [    555.683710 ]   nr_pcache_fill_from_victim :   12  [    555.732588 ]   nr_pcache_eviction :   494  [    555.774187 ]   nr_victim_eviction :   474    4 th  run, same setting with the 3 th  run, same result. But the nr_pgfault differs, I guess it is due to runtime things. Runtime:  19.12861 sec . 1\n2\n3\n4\n5\n6\n7\n8\n9 [    469.891700 ]   nr_pgfault :   288119  [    469.926123 ]   nr_pgfault_wp :   0  [    469.960444 ]   nr_pgfault_wp_cow :   0  [    469.998924 ]   nr_pgfault_wp_reuse :   0  [    470.039484 ]   nr_pgfault_due_to_concurrent_eviction :   0  [    470.098764 ]   nr_pcache_fill_from_memory :   288093  [    470.151805 ]   nr_pcache_fill_from_victim :   12  [    470.200684 ]   nr_pcache_eviction :   513  [    470.242285 ]   nr_victim_eviction :   493    5 th  run, same with 4 th , succeed, Runtime:  18.653879 sec . 1\n2\n3\n4\n5\n6\n7\n8\n9 [  313.202348] nr_pgfault: 288070\n[  313.236772] nr_pgfault_wp: 0\n[  313.271093] nr_pgfault_wp_cow: 0\n[  313.309575] nr_pgfault_wp_reuse: 0\n[  313.350139] nr_pgfault_due_to_concurrent_eviction: 0\n[  313.409421] nr_pcache_fill_from_memory: 288052\n[  313.462465] nr_pcache_fill_from_victim: 6\n[  313.510307] nr_pcache_eviction: 446\n[  313.551909] nr_victim_eviction: 432   6 th , setting is the same, but with 4GB dataset, crashed:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 [    512.028141 ]   Processor :   Processor   manager   is   running .  [    529.375605 ]   STDOUT :   --- [  Wordcount :   Running ...  ] ---  [    529.435906 ]   STDOUT :   --- [  ] ---  [    529.476660 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [    529.555983 ]   ------------ [   cut   here   ] ------------  [    529.609128 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 735 / pcache_zap_pte () !  [    529.699613 ]   Kernel   Panic   -   not   syncing :   BUG !  [    529.750576 ]   CPU :   5   PID :   32   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 361  [    529.826500 ]   Stack :  [    529.850422 ]   ffff88107e1a3dd8   ffffffff810259b4   000000000000000 8   ffff88107e1a3de8  [    529.937787 ]   ffff88107e1a3da0   0000000021475542   0000000000000000   0000000000000000  [    530.025152 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    530.112517 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    530.199882 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    530.287247 ]   Call   Trace :  [    530.316370 ]   TSK  [    530.339251 ]   [ ffffffff810259c0 ]   panic + 0xc2 / 0xeb  [    530.394374 ]   [ ffffffff8106190a ]   ?   client_internal_poll_sendcq + 0x2a / 0x80  [    530.474458 ]   [ ffffffff8101bfcc ]   ?   task_tick_rt + 0x2c / 0xd0  [    530.538943 ]   [ ffffffff81019725 ]   ?   scheduler_tick + 0x55 / 0x60  [    530.605506 ]   [ ffffffff81016df5 ]   ?   tick_handle_periodic + 0x45 / 0x70  [    530.678311 ]   [ ffffffff8103768a ]   ?   put_dec + 0x1a / 0x80  [    530.737595 ]   [ ffffffff810066f4 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [    530.810398 ]   [ ffffffff8100e4aa ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [    530.888403 ]   [ ffffffff81012ccd ]   ?   printk + 0x11d / 0x1b0  [    530.948726 ]   [ ffffffff81030429 ]   pcache_zap_pte + 0xf9 / 0x160  [    531.014250 ]   [ ffffffff8102f090 ]   ?   __pcache_move_pte_fastpath + 0x50 / 0x50  [    531.093295 ]   [ ffffffff8102c8dc ]   unmap_page_range + 0x32c / 0x3b0  [    531.161940 ]   [ ffffffff8102c97e ]   release_pgtable + 0x1e / 0x40  [    531.227463 ]   [ ffffffff8102bfb3 ]   sys_munmap + 0xc3 / 0x120  [    531.288827 ]   [ ffffffff8100e86d ]   do_syscall_64 + 0x3d / 0xc0  [    531.352270 ]   [ ffffffff8100d76c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25    7 th  run, add debug info, does not seem that useful: 1\n2\n3\n4\n5\n6\n7\n8\n9 ] ---  [ 15755.579501 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [ 15755.672760 ]   pte : ffff88107e1a3dd8   pfn : 0x8207e80b   flags :( dirty | large | global | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [ 15755.807015 ]   pte   dumped   because :   Invalid   pte  [ 15755.856932 ]   address :   0x7ffefc638000  [ 15755.899569 ]   ------------ [   cut   here   ] ------------  [ 15755.954684 ]   BUG :   failure   at   managers / processor / pcache / rmap . c : 747 / pcache_zap_pte () !  [ 15756.045159 ]   Kernel   Panic   -   not   syncing :   BUG !  [ 15756.096114 ]   CPU :   5   PID :   32   Comm :   word_count - pt    Tried several times, even with mmap/munmap debug option on, it crashed at the same point. Key is address  0x7ffefc638000 , and the mmap() related to it.  Close to find the bug. Latest log in 0310-18.", 
            "title": "Trying phoenix pthread again"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0309-fri", 
            "text": "", 
            "title": "03/09 Fri"
        }, 
        {
            "location": "/lego/log/log-03-2018/#find-bug-in-kmalloc", 
            "text": "Tried to print pud in every syscall and catch the criminal:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 wuklab13   030 9 - 1  [    320.088684 ]   CPU5   PID32   sys_close + 0x0 / 0x1f0  [    320.137567 ]   do_syscall_64 () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fc6f000 ,   pud_index = 0x0   pud :   ffff88207fc6f000  [    320.269657 ]   SYSC_close ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff7df3c37  [    320.349742 ]       3  [    320.372624 ]   SYSC_close () :   [ 3 ]   -   [ / lib64 / libpython2 .7 . so .1.0 ]  [    320.441268 ]   SYSC_close ()   cpu ( 5 )   tsk ( 32 / 32 / python )   ret :   0x0   ( 0 )  [    320.510954 ]   do_syscall_64 () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fc6f000 ,   pud_index = 0x0   pud :   ffff88207fc6f000  [    320.643043 ]       addr :   0x7ffff7a101f0 ,   pgd :   ffff88207fccf7f8  [    320.709607 ]       addr :   0x7ffff7a101f0 ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcaeff8  [    320.798014 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a101f0   flags : 0x50  [    320.995755 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a101f0   flags : 0x50   ret : 0 ( OKAY )  [    321.101944 ]       addr :   0x7ffff7a21749 ,   pgd :   ffff88207fccf7f8  [    321.168509 ]       addr :   0x7ffff7a21749 ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcaeff8  [    321.256914 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a21749   flags : 0x50  [    321.454651 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a21749   flags : 0x50   ret : 0 ( OKAY )  [    321.560845 ]       addr :   0x7ffff7ff2fda ,   pgd :   ffff88207fccf7f8  [    321.627409 ]       addr :   0x7ffff7ff2fda ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcaeff8  [    321.715815 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7ff2fda   flags : 0x50  [    321.913553 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7ff2fda   flags : 0x50   ret : 0 ( OKAY )  [    322.019745 ]   CPU5   PID32   sys_open + 0x0 / 0x10  [    322.066548 ]   do_syscall_64 () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff9001801ff000 ,   pud_index = 0x0   pud :   ffff9001801ff000  [    322.198638 ]   SYSC_open ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff7df3b27  [    322.277683 ]       f_name :   / lib64 / libpthread . so .0 ,   flags :   80000 ,   mode :   e150  [    322.357780 ]   SYSC_open ()   cpu ( 5 )   tsk ( 32 / 32 / python )   ret :   0x3   ( 3 )  [    322.426414 ]   do_syscall_64 () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff9001801ff000 ,   pud_index = 0x0   pud :   ffff9001801ff000    After printing more in pcache_handle_fault, I found who corrupted pgtable:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 wuklab13   030 9 - 5  [    661.308584 ]   CPU5   PID32   sys_close + 0x0 / 0x1f0  [    661.357466 ]   do_syscall_64 () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    661.489557 ]   SYSC_close ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff7df3c37  [    661.569642 ]       3      [    661.592525 ]   SYSC_close () :   [ 3 ]   -   [ / lib64 / libpython2 .7 . so .1.0 ]  [    661.661170 ]   SYSC_close ()   cpu ( 5 )   tsk ( 32 / 32 / python )   ret :   0x0   ( 0 )  [    661.730854 ]   do_syscall_64 () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    661.862944 ]   pcache_handle_fault () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    662.001275 ]       addr :   0x7ffff7a101f0 ,   pgd :   ffff88207fccf7f8  [    662.067840 ]       addr :   0x7ffff7a101f0 ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcafff8  [    662.156247 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a101f0   flags : 0x50  [    662.353985 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a101f0   flags : 0x50   ret : 0 ( OKAY )  [    662.460176 ]   pcache_handle_fault () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    662.600586 ]   pcache_handle_fault () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    662.738916 ]       addr :   0x7ffff7a21749 ,   pgd :   ffff88207fccf7f8  [    662.805481 ]       addr :   0x7ffff7a21749 ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcafff8  [    662.893888 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a21749   flags : 0x50  [    663.091636 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a21749   flags : 0x50   ret : 0 ( OKAY )  [    663.197831 ]   pcache_handle_fault () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    663.338242 ]   pcache_handle_fault () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    663.476572 ]       addr :   0x7ffff7ff2fda ,   pgd :   ffff88207fccf7f8  [    663.543135 ]       addr :   0x7ffff7ff2fda ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcafff8  [    663.631543 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7ff2fda   flags : 0x50  [    663.829279 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7ff2fda   flags : 0x50   ret : 0 ( OKAY )  [    663.935472 ]   pcache_handle_fault () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff9001801ff000 ,   pud_index = 0x0   pud :   ffff9001801ff000   [    664.075884 ]   CPU5   PID32   sys_open + 0x0 / 0x10  [    664.122686 ]   do_syscall_64 () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff9001801ff000 ,   pud_index = 0x0   pud :   ffff9001801ff000  [    664.254776 ]   SYSC_open ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff7df3b27  [    664.333821 ]       f_name :   / lib64 / libpthread . so .0 ,   flags :   80000 ,   mode :   e150  [    664.413918 ]   SYSC_open ()   cpu ( 5 )   tsk ( 32 / 32 / python )   ret :   0x3   ( 3 )  [    664.482552 ]   do_syscall_64 () :   leave   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff9001801ff000 ,   pud_index = 0x0   pud :   ffff9001801ff000    Then, try catching bug with address  0x7ffff7ff2fda  fault. Printing still being the most effective way to debug. :-)  Dig further, I found pgtable corrupted after  pcache_add_rmap() , namely after  alloc_pcache_rmap() : 1\n2\n3 [   5024.482570 ]   pcache_add_rmap ()   343   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [   5024.613601 ]   alloc_pcache_rmap () :   size :   56 ,   rmap :   ffff88207fccefd0  [   5024.686396 ]   pcache_add_rmap ()   358   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90207fcce000 ,   pud_index = 0x0   pud :   ffff90207fcce000    Well,  rmap :   ffff88207fccefd0     ffff90207fcce000 , clearly  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52 [    843.916517 ]   pcache_add_rmap ()   372   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    844.047557 ]   alloc_pcache_rmap ()   60   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    844.179638 ]   alloc_pcache_rmap () :   size :   56 ,   rmap :   ffff88207fccefd0  [    844.252438 ]   alloc_pcache_rmap ()   71   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    844.384517 ]   alloc_pcache_rmap () :   size :   56 ,   rmap :   ffff88207fccefd0  [    844.457317 ]   alloc_pcache_rmap ()   85   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90207fcce000 ,   pud_index = 0x0   pud :   ffff90207fcce000  [    844.589398 ]   pcache_add_rmap ()   387   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90207fcce000 ,   pud_index = 0x0   pud :   ffff90207fcce000  46   static   struct   pcache_rmap   * alloc_pcache_rmap ( void )  47   {  48           struct   pcache_rmap   * rmap ;  49  50           pgd_t   * pgd ;  51           pud_t   * pud ;  52           unsigned   long   addr ;  53           struct   mm_struct   * mm   =   current - mm ;  54  55           if   ( pall )   {  56                   addr   =   0x601008 ;  57                   pgd   =   pgd_offset ( mm ,   addr );  58                   pud   =   pud_alloc ( mm ,   pgd ,   addr );  59                   pr_info ( %s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n ,  60                           __func__ ,   __LINE__ ,   pgd ,   pgd_page_vaddr ( * pgd ),   pud_index ( addr ),   ( void   * ) pud );  61           }  62  63           rmap   =   kmalloc ( sizeof ( * rmap ),   GFP_KERNEL );  64  65           if   ( pall )   {  66                   addr   =   0x601008 ;  67                   pgd   =   pgd_offset ( mm ,   addr );  68                   pud   =   pud_alloc ( mm ,   pgd ,   addr );  69                   pr_info ( %s(): size: %zu, rmap: %p \\n ,   __func__ ,   sizeof ( * rmap ),   rmap );  70                   pr_info ( %s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n ,  71                           __func__ ,   __LINE__ ,   pgd ,   pgd_page_vaddr ( * pgd ),   pud_index ( addr ),   ( void   * ) pud );  72           }  73  74           if   ( rmap )   {  75                   INIT_LIST_HEAD ( rmap - next );  76                   rmap - flags   =   0 ;  77           }  78  79           if   ( pall )   {  80                   addr   =   0x601008 ;  81                   pgd   =   pgd_offset ( mm ,   addr );  82                   pud   =   pud_alloc ( mm ,   pgd ,   addr );  83                   pr_info ( %s(): size: %zu, rmap: %p \\n ,   __func__ ,   sizeof ( * rmap ),   rmap );  84                   pr_info ( %s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n ,  85                           __func__ ,   __LINE__ ,   pgd ,   pgd_page_vaddr ( * pgd ),   pud_index ( addr ),   ( void   * ) pud );  86           }  87  88           return   rmap ;  89   }    Narrow it down to  INIT_LIST_HEAD :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 [   1334.548682 ]   alloc_pcache_rmap () :   size :   56 ,   rmap :   ffff88207fccefd0  [   1334.621487 ]   alloc_pcache_rmap ()   71   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [   1334.753576 ]   alloc_pcache_rmap ()   76   rmap - next   ffff88207fcceff8   flags   ffff88207fccefd8  [   1334.922067 ]   alloc_pcache_rmap ()   86   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90207fcce000 ,   pud_index = 0x0   pud :   ffff90207fcce000  [   1335.126962 ]   alloc_pcache_rmap ()   98   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90207fcce000 ,   pud_index = 0x0   pud :   ffff90207fcce000  74           if   ( rmap )   {  75           pr_info ( %s() %d  rmap- next %p  flags %p \\n ,  76                   __func__ ,   __LINE__ ,   rmap - next ,   rmap - flags );  77  78                   INIT_LIST_HEAD ( rmap - next );  79  80           if   ( pall )   {  81                   addr   =   0x601008 ;  82                   pgd   =   pgd_offset ( mm ,   addr );  83                   pud   =   pud_alloc ( mm ,   pgd ,   addr );  84                   pr_info ( %s(): size: %zu, rmap: %p \\n ,   __func__ ,   sizeof ( * rmap ),   rmap );  85                   pr_info ( %s() %d pgd %p, pgd.cont_va %lx, pud_index=%#lx pud: %p \\n ,  86                           __func__ ,   __LINE__ ,   pgd ,   pgd_page_vaddr ( * pgd ),   pud_index ( addr ),   ( void   * ) pud );  87           }  88  89                   rmap - flags   =   0 ;  90           }    Seriously, if this is running on user-level on VM, I would be able to find the bug maybe in 30min. But I spent several hours to find it out with physical machine. Damn you physical machine.  Hmm, this func is used A LOT. How can it fail at this point? Possible reasons:   kmalloced area happen to intersect with pgtable?  one physical page is mapped twice? one to pgtable, one by this rmap.  tty/serial code has bug? Really ancient code.   After add a few printk, IB seems stuck. And this happens just with few more lines of code! Why? code size matters? 1\n2\n3\n4\n5\n6\n7\n8 [    722.381469 ]   pcache_handle_fault () :   enter   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [    722.519778 ]       addr :   0x7ffff7feffcc ,   pgd :   ffff88207fccf7f8  [    722.586334 ]       addr :   0x7ffff7feffcc ,   pgd :   ffff88207fccf7f8   pud   ffff88207fcafff8  [    722.674727 ]   Before   fill   address = 0x7ffff7feffcc   set_idx : 0x7fef  [    722.743362 ]   pcache : ffff8801801ffbc0   mapcount : 0   refcount : 1   flags :( allocated | usable )   set_idx = 0x7fef   kva :   ffff880107fef000  [    722.872312 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7feffcc   flags : 0x50  [    722.967985 ]   __pcache_do_fill_page () :   before   net   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  last   line    Well, the following finding finally find the bug line. And it kind of explains the above bug. Probably kmalloc ed area has issues, so IB is touching wrong data. The following bug is related to kmalloc, the rmap is 56 bytes, and it should be within 1 single page, but it is not:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 [   1862.307427 ]   pcache_add_rmap ()   413   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [   1862.438477 ]   alloc_pcache_rmap ()   86   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [   1862.570568 ]   sp - units :   50   SLOB_UNITS :   32  [   1862.617372 ]   alloc_pcache_rmap () :   size :   56 ,   rmap :   ffff88207fccefd0  [   1862.690178 ]   alloc_pcache_rmap ()   97   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [   1862.822268 ]   alloc_pcache_rmap ()   104   rmap - next   ffff88207fcceff8   flags   ffff88207fccefd8  [   1862.918995 ]   __INIT_LIST_HEAD () :   next   ffff88207fcceff8   prev   ffff88207fccf000  [   1863.002202 ]   __INIT_LIST_HEAD ()   63   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fcae000 ,   pud_index = 0x0   pud :   ffff88207fcae000  [   1863.133253 ]   __INIT_LIST_HEAD () :   next   ffff88207fcceff8   prev   ffff88207fccf000  [   1863.216459 ]   alloc_pcache_rmap () :   size :   56 ,   rmap :   ffff88207fccefd0  [   1863.289265 ]   alloc_pcache_rmap ()   114   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90207fcce000 ,   pud_index = 0x0   pud :   ffff90207fcce000    Analysis: The @prev field in line 7 has address  ffff88207fccf000 , which happen to the pgd page ( pgd ffff88207fccf000 ). Thus when we do  list- prev = list , it writes to the first 8 bytes of pgd page, corrupts the original pgd entry. That is why we see a corrupted pgd entry ( ffff90207fcce000 ).  This roots from kmalloc, which should not allocate such an object that cross two pages.", 
            "title": "Find bug in kmalloc"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0308-thur", 
            "text": "Took several days off. This morning finished the porting of  wait4  and  waitid , which actually has a lot code change. The concept and mechanism is fairly simple, but the legacy UNIX tradition make the implementation quite complex.  Now, look back to finish debugging the pcache issue. It must be fixed this week.", 
            "title": "03/08 Thur"
        }, 
        {
            "location": "/lego/log/log-03-2018/#python", 
            "text": "Tried  python hello_world.py , the program runs for a while and crashes at a deterministic point:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 wuklab13   and   wuklab15 ,   ~/ ttyS1  [ 419097.929969 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a4b008   flags : 0x50   ret : 0 ( OKAY )  [ 419098.039145 ]   __pcache_do_fill_page () :   I   pid : 32   tgid : 32   address : 0x7ffff7a4c010   flags : 0x50  [ 419098.306537 ]   __pcache_do_fill_page () :   O   pid : 32   tgid : 32   address : 0x7ffff7a4c010   flags : 0x50   ret : 0 ( OKAY )  [ 419098.413756 ]   CPU5   PID32   sys_mprotect + 0x0 / 0x90  [ 419098.465753 ]   SYSC_mprotect ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff7df3d27  [ 419098.549990 ]       start : 0x7ffff7d8c000 , len : 0x2000 , prot : 0x1  [ 419098.614469 ]   BUG :   unable   to   handle   kernel   paging   request   at   ffff9001801ff000  [ 419098.698703 ]   IP :   [ ffffffff8102f7a9 ]   pcache_handle_fault + 0x69 / 0x6c0  [ 419098.774621 ]   PGD   0  [ 419098.799579 ]   Oops :   0000   [ # 1 ]   SMP   PROCESSOR  [ 419098.848457 ]   CPU :   5   PID :   32   Comm :   python   4.0.0 - lego - ys +   # 312  [ 419098.916054 ]   RIP :   0010 : [ ffffffff8102f7a9 ]    [ ffffffff8102f7a9 ]   pcache_handle_fault + 0x69 / 0x6c0  [ 419099.021089 ]   RSP :   0000 : ffff88107e857ed8    EFLAGS :   000102 86  [ 419099.085567 ]   RAX :   ffff9001801ff000   RBX :   ffff9001801ff000   RCX :   00003ff ffffff000  [ 419099.171884 ]   RDX :   00000801801ff 000   RSI :   000000000060100 8   RDI :   ffff88107e83d648  [ 419099.258199 ]   RBP :   ffff88107e857f18   R08 :   00007ff ff7fe3000   R09 :   00007ff ff7fe3000  [ 419099.344516 ]   R10 :   0000000000000000   R11 :   0000000000000206   R12 :   000000000060100 8  [ 419099.430832 ]   R13 :   ffff88107e83d648   R14 :   0000000000000050   R15 :   00007ff ff7ffe150  [ 419099.517149 ]   FS :    00007ff ff7fdf740 ( 0000 )   GS : ffff88207fc40000 ( 0000 )   knlGS : 0000000000000000  [ 419099.614905 ]   CS :    0010   DS :   0000   ES :   0000   CR0 :   00000000 80050033  [ 419099.684582 ]   CR2 :   ffff9001801ff000   CR3 :   000000207f ccf000   CR4 :   00000000000406 a0  [ 419099.770899 ]   Stack :  [ 419099.795858 ]   00007ff ff7d8c000   0000000000002000   0000000000000001   0000000000000004  [ 419099.884254 ]   000000000060100 8   ffff88107e857f58   0000000000000000   00007ff ff7ffe150  [ 419099.972650 ]   ffff88107e857f48   ffffffff81010082   0000000000000000   0000000000000001  [ 419100.061047 ]   0003 92 c29c720ba2   0000000000000000   00007ff fffffdc40   ffffffff8100d91f  [ 419100.149442 ]   00007ff ff7ffe150   0000000000000000   0003 92 c29c720ba2   0000000000000001  [ 419100.237839 ]   Call   Trace :  [ 419100.267998 ]   TSK  [ 419100.291917 ]   [ ffffffff81010082 ]   do_page_fault + 0xa2 / 0x1a0  [ 419100.357434 ]   [ ffffffff8100d91f ]   page_fault + 0x1f / 0x30  [ 419100.418792 ]   EOT  M :  ...  [ 419142.163396 ]   handle_p2m_pcache_miss ()   cpu   4   I   nid : 0   pid : 32   tgid : 32   flags : 50   vaddr : 0x7ffff7a4c010  [ 419142.268460 ]   handle_p2m_pcache_miss ()   cpu   4   O   nid : 0   pid : 32   tgid : 32   flags : 50   vaddr : 0x7ffff7a4c010  ( Last   Message )    Dig deeper:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 int   pcache_handle_fault ( struct   mm_struct   * mm , \n                         unsigned   long   address ,   unsigned   long   flags )  {           ..           pgd   =   pgd_offset ( mm ,   address );           pr_info (     addr: %#lx, pgd: %p \\n ,   address ,   pgd ); \n         pud   =   pud_alloc ( mm ,   pgd ,   address ); \n         pr_info (     addr: %#lx, pgd: %p pud %p \\n ,   address ,   pgd ,   pud ); \n         if   ( ! pud ) \n                 return   VM_FAULT_OOM ; \n         pmd   =   pmd_alloc ( mm ,   pud ,   address ); \n         if   ( ! pmd )  ..  }  [ 21130.503314 ]   strace__mprotect   cpu5   start = 0x7ffff7d8c000 ,   len = 0x2000 ,   prot ( 0x1 ) = PROT_READ  [ 21130.598994 ]   SYSC_mprotect ()   cpu ( 5 )   tsk ( 32 / 32 / python )   user - ip : 0x7ffff7df3d27  [ 21130.682193 ]       start : 0x7ffff7d8c000 , len : 0x2000 , prot : 0x1  [ 21130.745635 ]       addr :   0x601008 ,   pgd :   ffff88207fccf000  [ 21130.805954 ]       addr :   0x601008 ,   pgd :   ffff88207fccf000   pud   ffff9001801ff000  [ 21130.888116 ]   BUG :   unable   to   handle   kernel   paging   request   at   ffff9001801ff000  [ 21130.971314 ]   IP :   [ ffffffff8102fa11 ]   pcache_handle_fault + 0x91 / 0x6f0    Print pgd and pud info, these three messages are related and the last one leads to panic:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 wuklab13   ~/ ys / 030 8 - 6  [    479.375498 ]   addr :   0x400040 ,   pgd :   ffff88207fccf000  [    479.435819 ]   pud_alloc_one () :   addr :   0x400040 ,   pud :   ffff88207fc6f000  [    479.511739 ]   pud_alloc () :   addr :   0x400040   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fc6f000 ,   pud_index = 0x0   pud :   ffff88207fc6f000  [    479.649021 ]   addr :   0x400040 ,   pgd :   ffff88207fccf000   pud   ffff88207fc6f000  [    480.016381 ]   addr :   0x600dd8 ,   pgd :   ffff88207fccf000  [    480.076701 ]   pud_alloc () :   addr :   0x600dd8   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff88207fc6f000 ,   pud_index = 0x0   pud :   ffff88207fc6f000  [    480.213982 ]   addr :   0x600dd8 ,   pgd :   ffff88207fccf000   pud   ffff88207fc6f000  [    680.072819 ]   addr :   0x601008 ,   pgd :   ffff88207fccf000  [    680.133138 ]   pud_alloc () :   addr :   0x601008   pgd   ffff88207fccf000 ,   pgd . cont_va   ffff90107e834000 ,   pud_index = 0x0   pud :   ffff90107e834000  [    680.270422 ]   addr :   0x601008 ,   pgd :   ffff88207fccf000   pud   ffff90107e834000  [    680.352583 ]   BUG :   unable   to   handle   kernel   paging   request   at   ffff90107e834000  [    680.435783 ]   IP :   [ ffffffff8102fc43 ]   pcache_handle_fault + 0xb3 / 0x770  [    680.510664 ]   PGD   0    I need to check what happens between 480s to 680s. Something in between corrupted pgtable. I doubt it can be:   copy_to_user related syscalls  pcache establish mapping, mempcy  all other memcpy strcpy etc stuff", 
            "title": "python"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0302-fri", 
            "text": "TODO:   -add vsyscall-  -pcache_exit_process: free rmap, free cacheline, etc. When rmap is NULL, we clearly should free this pcache.-  pcache_exit_thread? I don t think we need this. All pcache related activities should relate to mm, or thread group leader, not one particular thread.  check python bug  use omnigraffle to draw the whole workflow of pcache.   Phoenix, word_count-seq, 4G dataset, 4GB pcache:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33 [    273.268853 ]   Processor :   Processor   manager   is   running .  [    573.272479 ]   page : ffffea0071bb9660   count : 0   mapcount : - 128  [    573.332903 ]   flags :   0x200000000000300 ( slab | slob_free )  [    573.392182 ]   page   dumped   because :   VM_BUG_ON_PAGE ( page_ref_count ( page )   ==   0 )  [    573.474340 ]   ------------ [   cut   here   ] ------------  [    573.529459 ]   BUG :   failure   at   . / include / lego / mm . h : 251 / put_page_testzero ()!  [    573.609537 ]   Kernel   Panic   -   not   syncing :   BUG !  [    573.660496 ]   CPU :   4   PID :   13   Comm :   kvictim_flushd   4.0.0 - lego +   # 18  [    573.731212 ]   Stack :  [    573.755132 ]   ffff88207e4bfe10   ffffffff81023644   0000000000000008   ffff88207e4bfe20  [    573.842490 ]   ffff88207e4bfdd8   0000000021475542   0000000000000000   0000000000000000  [    573.929848 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    574.017205 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    574.104563 ]   0000000000000000   0000000000000000   0000000000000000   0000000000000000  [    574.191921 ]   Call   Trace :  [    574.221039 ]   TSK  [    574.243919 ]   [ ffffffff81023650 ]   panic + 0xc2 / 0xeb  [    574.299038 ]   [ ffffffff8105a35a ]   ?   client_internal_poll_sendcq + 0x2a / 0x80  [    574.379115 ]   [ ffffffff8105a4fd ]   ?   client_send_message_with_rdma_write_with_imm_request + 0x14d / 0x360  [    574.487273 ]   [ ffffffff8101ac3c ]   ?   task_tick_rt + 0x2c / 0xd0  [    574.551751 ]   [ ffffffff81018395 ]   ?   scheduler_tick + 0x55 / 0x60  [    574.618308 ]   [ ffffffff81015a45 ]   ?   tick_handle_periodic + 0x45 / 0x70  [    574.691107 ]   [ ffffffff810064c4 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [    574.763905 ]   [ ffffffff8100dbaa ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [    574.841903 ]   [ ffffffff8101198d ]   ?   printk + 0x11d / 0x1b0  [    574.902222 ]   [ ffffffff81025c00 ]   __ free_pages + 0x2e0 / 0x3c0  [    574.966699 ]   [ ffffffff81028472 ]   kfree + 0x62 / 0x480  [    575.022858 ]   [ ffffffff8102e6be ]   victim_flush_func + 0x15e / 0x1e0  [    575.092536 ]   [ ffffffff8102e560 ]   ?   victim_try_fill_pcache + 0x390 / 0x390  [    575.169494 ]   [ ffffffff8101e446 ]   kthread + 0xf6 / 0x120  [    575.227733 ]   [ ffffffff8101e350 ]   ?   __ kthread_parkme + 0x70 / 0x70  [    575.296371 ]   [ ffffffff8100de32 ]   ret_from_fork + 0x22 / 0x30  [    575.359810 ]   EOT", 
            "title": "03/02 Fri"
        }, 
        {
            "location": "/lego/log/log-03-2018/#0301-thur", 
            "text": "Weird. 1\n2\n3\n4\n5\n6\n7 [43181.388400] p2m_fork(cpu5): I cur:24-word_count-seq new:25\n[43181.435341] p2m_fork(cpu5): O succeed cur:24-word_count-seq new:25\n[43181.436013] __pcache_do_fill_page(): I pid:24 tgid:24 address:0x4158d0 flags:0x150\n[43181.439246] __pcache_do_fill_page(): O pid:24 tgid:24 address:0x4158d0 flags:0x150 ret:0(OKAY) csum:0x9e8f028e\n\n[43181.510534] __pcache_do_fill_page(): I pid:25 tgid:25 address:0x415000 flags:0x150\n[43181.517729] __pcache_do_fill_page(): O pid:25 tgid:25 address:0x415000 flags:0x150 ret:0(OKAY) csum:0xffff88029e8f028e   After all, it is TLB issue. I forgot to flush tlb after making the original pte read-only during fork. So the parent will be also to continue RW some pages, which should be process-private.  Lego s current TLB flush is very native, we do tlbflush after each pte changes. This will have worse performance compared to linux s batch flush.  Today s case is flush tlb after making pte read-only. And this really has to be performed one by one", 
            "title": "03/01 Thur"
        }, 
        {
            "location": "/lego/log/log-02-2018/", 
            "text": "Feb 2018\n\n\n\n\n02/28 Wed\n\n\n\n\npatch fork, and cow handler\n\n\ndebug pcache, while running python hello world\n\n\nadd vDSO, gettimeofday\n\n\n\n\nSo, it is end of the day. After adding wp handler, I now have the whole picture of pcache activities, and the interactions between them. The reclaim, zap, move, copy, add, operations needs to be carefully synchronized. Also the refcount etc. I feel the ground rule is we need to make sure a PCM that a function is currently using, can not suddenly become invalid due to other operations. This has to be synced by: refcount, lock, flags. Oh well, mm is hard with SMP, but also fun.\n\n\nWe are very close to have a fully working OS.\n\n\nI did not have time to look into the python hello world bug issue. It is a very serious one. It may also rule out some root bugs.\n\n\n\n\n02/27 Tue\n\n\nSpent two days on CS527 source project, implemented a small SSHD and SSD client. And we have to inject exactly five bugs, or vulnerabilities into the systems. Lol, it is really hard to intentionally plant BUGs!\n\n\nAnyway, back to Lego. Since others are having a hard time compile program statically, I will try to add dynamic loader today.\n\n\nThe interpreter: \n/lib64/ld-linux-x86-64.so.2\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nLinux seq.c maps (no randomization):\n00400000-00401000 r-xp 00000000 fd:00 18752683                           /root/ys/LegoOS/usr/a.out\n00600000-00601000 r--p 00000000 fd:00 18752683                           /root/ys/LegoOS/usr/a.out\n00601000-00602000 rw-p 00001000 fd:00 18752683                           /root/ys/LegoOS/usr/a.out\n00602000-00604000 rw-p 00000000 00:00 0                                  [heap]\n7ffff7a18000-7ffff7bd0000 r-xp 00000000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7bd0000-7ffff7dd0000 ---p 001b8000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7dd0000-7ffff7dd4000 r--p 001b8000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7dd4000-7ffff7dd6000 rw-p 001bc000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7dd6000-7ffff7ddb000 rw-p 00000000 00:00 0\n\n7ffff7ddb000-7ffff7dfc000 r-xp 00000000 fd:00 55051983                   /usr/lib64/ld-2.17.so\n\n7ffff7fde000-7ffff7fe1000 rw-p 00000000 00:00 0\n7ffff7ff9000-7ffff7ffa000 rw-p 00000000 00:00 0\n7ffff7ffa000-7ffff7ffc000 r-xp 00000000 00:00 0                          [vdso]\n\n7ffff7ffc000-7ffff7ffd000 r--p 00021000 fd:00 55051983                   /usr/lib64/ld-2.17.so\n\n7ffff7ffd000-7ffff7ffe000 rw-p 00022000 fd:00 55051983                   /usr/lib64/ld-2.17.so\n\n7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0\n\n7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0                          [stack]\n\nffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\nlego\n \nafter\n \nloading\n\n\n00400000\n-\n00401000\n \nr\n-\nxp\n \n00000000\n \n/\nroot\n/\nys\n/\nLegoOS\n/\nusr\n/\na\n.\nout\n\n\n00600000\n-\n00602000\n \nrw\n-\np\n \n00000000\n \n/\nroot\n/\nys\n/\nLegoOS\n/\nusr\n/\na\n.\nout\n\n\n00602000\n-\n00604000\n \nrw\n-\np\n \n00000000\n \n[\nheap\n]\n\n\n7ff\nff7ddb000\n-\n7ff\nff7dfc000\n \nr\n-\nxp\n \n00000000\n \n/\nlib64\n/\nld\n-\nlinux\n-\nx86\n-\n64.\nso\n.2\n\n\n7ff\nff7ffc000\n-\n7ff\nff7ffe000\n \nrw\n-\np\n \n00021000\n \n/\nlib64\n/\nld\n-\nlinux\n-\nx86\n-\n64.\nso\n.2\n\n\n7ff\nff7ffe000\n-\n7ff\nff7fff000\n \nrw\n-\np\n \n00000000\n\n\n7ff\nffffde000\n-\n7ff\nffffff000\n \nrw\n-\np\n \n00000000\n \n[\nstack\n]\n\n\n\n\n[\n \n2066.379224\n]\n \n****\n    \nFinish\n \ndump\n \nfinal\n \nmm\n\n\n[\n \n2066.426023\n]\n \nhandle_p2m_execve\n()\n:\n \nreply_status\n:\n \nOKAY\n,\n \nnew_ip\n:\n \n0x7ffff7ddc170\n,\n \nnew_sp\n:\n \n0x7fffffffede0\n\n\n[\n \n2066.628949\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nI\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n150\n \nvaddr\n:\n0x7ffff7ddc170\n\n\n[\n \n2066.732034\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nO\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n150\n \nvaddr\n:\n0x7ffff7ddc170\n\n\n[\n \n2066.934947\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nI\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n51\n \nvaddr\n:\n0x7fffffffedd8\n\n\n[\n \n2067.036978\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nO\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n51\n \nvaddr\n:\n0x7fffffffedd8\n\n\n[\n \n2067.238842\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nI\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n50\n \nvaddr\n:\n0x7ffff7ffce00\n\n\n[\n \n2067.340880\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nO\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n50\n \nvaddr\n:\n0x7ffff7ffce00\n\n\n[\n \n2067.542747\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nI\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n51\n \nvaddr\n:\n0x7ffff7ffd9a8\n\n\n[\n \n2067.644774\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nO\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n51\n \nvaddr\n:\n0x7ffff7ffd9a8\n\n\n[\n \n2067.846640\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nI\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n50\n \nvaddr\n:\n0x7ffff7ddb8e0\n\n\n[\n \n2067.948679\n]\n \nhandle_p2m_pcache_miss\n()\n \ncpu\n \n4\n \nO\n \nnid\n:\n0\n \npid\n:\n32\n \ntgid\n:\n32\n \nflags\n:\n50\n \nvaddr\n:\n0x7ffff7ddb8e0\n\n\n[\n \n2068.355424\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n2068.408568\n]\n \nWARNING\n:\n \nCPU\n:\n \n4\n \nPID\n:\n \n31\n \nat\n \nmanagers\n/\nmemory\n/\nhandle_pcache\n/\nfault\n.\nc\n:\n54\n \nhandle_p2m_pcache_miss\n+\n0x29d\n/\n0x380\n\n\n[\n \n2068.532327\n]\n \nsrc_nid\n:\n0\n,\npid\n:\n32\n,\nvaddr\n:\n0x7ffff7e0e000\n\n\n[\n \n2068.588487\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n31\n \nComm\n:\n \nmc\n-\nmanager\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n100\n\n\n[\n \n2068.659207\n]\n \nStack\n:\n\n\n\n\n\n\n\n1\n2\n3\n4\n[root@wuklab13: lib64] $ ll ld-*\n-rwxr-xr-x 1 root root 164112 Nov 30 13:53 ld-2.17.so\nlrwxrwxrwx 1 root root     10 Jan  8 12:34 ld-linux-x86-64.so.2 -\n ld-2.17.so\n[root@wuklab13: lib64]\n\n\n\n\n\n\nIt turns out there is a bug in mmap code: forgot to increment the file ref count when a file-backed vma is created. Some put_file in loader accidentally free the ld-linux file. Bug fixed, dyloader works like a charm.\n\n\n\n\n02/24 Sat\n\n\nWell. PhDs do not have weekends.\nAnyway, it is Saturday after all, relaxed a little bit. I was looking into the pcache issue.\nAlso added our own kernel version strace.\n\n\n\n\n02/23 Fri\n\n\nSolved FPU BUG\n\n\ncurrent\n is fine. I should not compare the old implementation with the new per-cpu current. I forgot that the kernel stack is switched in the \n__switch_to_asm\n. This means in \n__switch_to()\n, we are actually using the \nnext_p\ns kernel stack. So there is small time frame, where \ncurrent_thread_info()\n points to \nnext_p\n, while \ncurrent_task\n is still \nprev_p\n. Since interrupts are disabled during context switch, we are good with this mismatch.\n\n\nRule out current, the only thing left is \nfpu__copy\n warning, which happens during \ncopy_process()\n. One weird thing is this function has been called multiple times before it showed a warning. System itself use this function to create a lot background threads, which are fine. Only when it was triggered by \nsys_clone\n then we have the warning:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n[\n \n3213.055639\n]\n \nCPU\n:\n \n6\n \nPID\n:\n \n17\n \nsys_clone\n+\n0x0\n/\n0x30\n\n\n[\n \n3213.056584\n]\n \nnew\n \ntask_struct\n:\n \nffff88083e4c9838\n\n\n[\n \n3213.057530\n]\n \narch_dup_task_struct\n \ncpu6\n \ndst\n:\nffff88083e4c9838\n \n17\n \nword_count\n-\nseq\n \nsrc\n:\nffff88083e457838\n \n17\n \nword_count\n-\nseq\n\n\n[\n \n3213.059536\n]\n \nTRAP\n \ndo_general_protection\n \nin\n \nCPU6\n,\n \nerror_code\n:\n \n0\n \ncurrent\n:\nffff88083e457838\n \n17\n \nword_count\n-\nseq\n\n\n[\n \n3213.061289\n]\n \nfixup_exception\n \npid\n(\n17\n)\n \ncpu\n(\n6\n)\n \ninsn\n:\n0xffffffff81009a21\n(\nfpu__copy\n+\n0x81\n/\n0x260\n)\n \nfixup\n:\n0xffffffff8105d9b2\n(\n__fixup_text_start\n+\n0xc2\n/\n0x322\n)\n \nhandler\n:\nex_handler_default\n+\n0x0\n/\n0x20\n\n\n[\n \n3213.064114\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n3213.065040\n]\n \nWARNING\n:\n \nCPU\n:\n \n6\n \nPID\n:\n \n17\n \nat\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\nfpu\n/\ninternal\n.\nh\n:\n354\n \nfpu__copy\n+\n0xc3\n/\n0x260\n\n\n[\n \n3213.066760\n]\n \nCPU\n:\n \n6\n \nPID\n:\n \n17\n \nComm\n:\n \nword_count\n-\nseq\n \n4.0.0\n-\nlego\n+\n \n#\n6\n\n\n[\n \n3213.067855\n]\n \nStack\n:\n\n\n[\n \n3213.068424\n]\n \nffff88083e4c7dd0\n \nffffffff810124b5\n \nffff88083e4c9bf8\n \nffff88083e4c9c38\n\n\n[\n \n3213.070133\n]\n \nffff88083e4c9838\n \n00007ff\nff7ffd700\n \nffff88083e4c7de0\n \nffffffff8101258f\n\n\n[\n \n3213.071775\n]\n \nffff88083e4c7e08\n \nffffffff81009a63\n \nffff88083e457838\n \nffff88083e4c9838\n\n\n[\n \n3213.073419\n]\n \nffff88083e457838\n \nffff88083e4c7e40\n \nffffffff81000ebb\n \nffff88083e457838\n\n\n[\n \n3213.075057\n]\n \nffff880800000011\n \nffff88083e457a68\n \n00000000003\nd0f00\n \nffff88083e457838\n\n\n[\n \n3213.076703\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n3213.077295\n]\n \nTSK\n\n\n[\n \n3213.077828\n]\n \n[\nffffffff810124c1\n]\n \n__warn\n.\nconstprop\n.0\n+\n0x91\n/\n0xd0\n\n\n[\n \n3213.078855\n]\n \n[\nffffffff8101258f\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n \n3213.081653\n]\n \n[\nffffffff81009a63\n]\n \nfpu__copy\n+\n0xc3\n/\n0x260\n\n\n[\n \n3213.082543\n]\n \n[\nffffffff81000ebb\n]\n \narch_dup_task_struct\n+\n0x7b\n/\n0x90\n\n\n[\n \n3213.083667\n]\n \n[\nffffffff8101d32e\n]\n \ncopy_process\n+\n0x14e\n/\n0x10e0\n\n\n[\n \n3213.084618\n]\n \n[\nffffffff8103a3c6\n]\n \n?\n \nn_tty_write\n+\n0x166\n/\n0x3c0\n\n\n[\n \n3213.085564\n]\n \n[\nffffffff8101e2e6\n]\n \ndo_fork\n+\n0x26\n/\n0x140\n\n\n[\n \n3213.086439\n]\n \n[\nffffffff8101e4a0\n]\n \n?\n \nsys_vfork\n+\n0x40\n/\n0x40\n\n\n[\n \n3213.087333\n]\n \n[\nffffffff8101e4a0\n]\n \n?\n \nsys_vfork\n+\n0x40\n/\n0x40\n\n\n[\n \n3213.088232\n]\n \n[\nffffffff8101e4c9\n]\n \nsys_clone\n+\n0x29\n/\n0x30\n\n\n[\n \n3213.089109\n]\n \n[\nffffffff8100e719\n]\n \ndo_syscall_64\n+\n0x69\n/\n0xf0\n\n\n[\n \n3213.090030\n]\n \n[\nffffffff8100d5ec\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n \n3213.091078\n]\n \nEOT\n\n\n[\n \n3213.091580\n]\n \n---\n[\n \nend\n \ntrace\n \n0000000000000000\n \n]\n---\n\n\n[\n \n3213.093250\n]\n \nTRAP\n \ndo_general_protection\n \nin\n \nCPU7\n,\n \nerror_code\n:\n \n0\n \ncurrent\n:\nffff88083fd0f008\n \n0\n \nswapper\n/\n7\n\n\n[\n \n3213.096526\n]\n \nfixup_exception\n \npid\n(\n0\n)\n \ncpu\n(\n7\n)\n \ninsn\n:\n0xffffffff81000c62\n(\n__switch_to\n+\n0x452\n/\n0x630\n)\n \nfixup\n:\n0xffffffff8105d922\n(\n__fixup_text_start\n+\n0x32\n/\n0x322\n)\n \nhandler\n:\nex_handler_default\n+\n0x0\n/\n0x20\n\n\n[\n \n3213.101241\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n3213.103285\n]\n \nWARNING\n:\n \nCPU\n:\n \n7\n \nPID\n:\n \n0\n \nat\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\nfpu\n/\ninternal\n.\nh\n:\n369\n \n__switch_to\n+\n0x47e\n/\n0x630\n\n\n\n\n\n\nSo, dig into \nfpu__copy()\n, find out why it fails at this certain point. Glad I have something to dig into. \n\n\nThe instruction leads to GP is:\n\n1\nffffffff8100b0f5\n:\n       \n48\n \n0f\n \nae\n \n27\n             \nxsave64\n \n(\n%\nrdi\n)\n\n\n\n\n\n\nwhich is generated by:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n#define XSTATE_XSAVE(st, lmask, hmask, err)                             \\\n\n\n        asm volatile(ALTERNATIVE_2(XSAVE,                               \\\n\n\n                                   XSAVEOPT, X86_FEATURE_XSAVEOPT,      \\\n\n\n                                   XSAVES,   X86_FEATURE_XSAVES)        \\\n\n\n                     \n\\n\n                                               \\\n\n\n                     \nxor %[err], %[err]\\n\n                             \\\n\n\n                     \n3:\\n\n                                             \\\n\n\n                     \n.pushsection .fixup,\\\nax\\\n\\n\n                     \\\n\n\n                     \n4: movl $-2, %[err]\\n\n                            \\\n\n\n                     \njmp 3b\\n\n                                         \\\n\n\n                     \n.popsection\\n\n                                    \\\n\n\n                     _ASM_EXTABLE(661b, 4b)                             \\\n\n\n                     : [err] \n=r\n (err)                                 \\\n\n\n                     : \nD\n (st), \nm\n (*st), \na\n (lmask), \nd\n (hmask)    \\\n\n\n                     : \nmemory\n)\n\n\nstatic\n \ninline\n \nvoid\n \ncopy_xregs_to_kernel\n(\nstruct\n \nxregs_state\n \n*\nxstate\n)\n\n\n{\n\n        \nu64\n \nmask\n \n=\n \n-\n1\n;\n\n        \nu32\n \nlmask\n \n=\n \nmask\n;\n\n        \nu32\n \nhmask\n \n=\n \nmask\n \n \n32\n;\n\n        \nint\n \nerr\n;\n\n\n        \nWARN_ON\n(\n!\nalternatives_patched\n);\n\n\n        \nXSTATE_XSAVE\n(\nxstate\n,\n \nlmask\n,\n \nhmask\n,\n \nerr\n);\n\n\n        \n/* We should never fault when copying to a kernel buffer: */\n\n        \nWARN_ON_FPU\n(\nerr\n);\n\n\n}\n\n\n\n\n\n\nFrom SDM on \nXSAVE\n: \nUse of a destination operand not aligned to 64-byte boundary (in either 64-bit or 32-bit modes) results in a general-protection (#GP) exception. In 64-bit mode, the upper 32 bits of RDX and RAX are ignored.\n\n\n%rdi\n is \nstruct xregs_state *xstate\n in above code. Thus, check if \nxstate\n if 64-bytes aligned. Of course, it is not:\n\n1\n[10894.999997] copy_xregs_to_kernel CPU6 xstate: ffff88083e4c8c38\n\n\n\n\n\nHehe. Criminal identified. But why? The xstate structure is already marked as \n__attribute__(aliged 64)\n in the code. \nIt is the task_struct\n, which is \nNOT\n 0x40 aligned. But god why? Because we currently use \nkmalloc\n to allocate new task_struct, whose minimum alignment is \n8 bytes\n. Anyway, use \n__alloc_pages\n instead.\n\n\nSuch an deeply hidden bug. Took me almost a month to find out.\n\n\nIB\n\n\nSeen this during boot (at both P and M, although lego continue running correctly):\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n[\n54017.712533\n]\n \n***\n    \nNodeID\n    \nHostname\n    \nLID\n    \nQPN\n\n\n[\n54017.770776\n]\n \n***\n    \n-------------------------------------\n\n\n[\n54017.834220\n]\n \n***\n         \n0\n    \nwuklab12\n     \n13\n     \n72\n\n\n[\n54017.892462\n]\n \n***\n         \n1\n    \nwuklab14\n     \n16\n     \n72\n \n---\n\n\n[\n54017.955906\n]\n \n***\n         \n2\n    \nwuklab16\n     \n20\n     \n74\n\n\n[\n54018.014149\n]\n \n***\n\n\n[\n54074.552844\n]\n \n***\n  \nStart\n \nestablish\n \nconnection\n \n(\nmynodeid\n:\n \n1\n)\n\n\n[\n54102.554407\n]\n \nib_process_mad\n \nmad_ifc\n \nfails\n\n\n[\n54130.960691\n]\n \n***\n  \nrecvpollcq\n \nruns\n \non\n \nCPU2\n\n\n[\n54131.070918\n]\n \n***\n  \nSuccessfully\n \nbuilt\n \nQP\n \nfor\n \nnode\n  \n0\n \n[\nLID\n:\n \n13\n \nQPN\n:\n \n72\n]\n\n\n[\n54131.152936\n]\n \n***\n  \nSuccessfully\n \nbuilt\n \nQP\n \nfor\n \nnode\n  \n2\n \n[\nLID\n:\n \n20\n \nQPN\n:\n \n74\n]\n\n\n[\n54161.228245\n]\n \n***\n  \nFIT\n \nlayer\n \nready\n \nto\n \ngo\n!\n\n\n[\n54161.272034\n]\n \n***\n\n\n\n\n\nAnother one:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n[\n \n1966.930409\n]\n \n***\n\n\n[\n \n1966.951210\n]\n \n***\n  \nFIT_initial_timeout_s\n:\n   \n30\n\n\n[\n \n1967.002168\n]\n \n***\n  \nFIT_local_id\n:\n            \n0\n\n\n[\n \n1967.052087\n]\n \n***\n\n\n[\n \n1967.072887\n]\n \n***\n    \nNodeID\n    \nHostname\n    \nLID\n    \nQPN\n\n\n[\n \n1967.131126\n]\n \n***\n    \n-------------------------------------\n\n\n[\n \n1967.194567\n]\n \n***\n         \n0\n    \nwuklab12\n     \n13\n     \n72\n \n---\n\n\n[\n \n1967.258005\n]\n \n***\n         \n1\n    \nwuklab14\n     \n16\n     \n72\n\n\n[\n \n1967.316244\n]\n \n***\n         \n2\n    \nwuklab16\n     \n20\n     \n74\n\n\n[\n \n1967.374484\n]\n \n***\n\n\n[\n \n2032.926448\n]\n \n***\n  \nStart\n \nestablish\n \nconnection\n \n(\nmynodeid\n:\n \n0\n)\n\n\n[\n \n2032.996068\n]\n \nFail\n \nto\n \nmodify\n \nqp\n[\n6\n]\n\n\n[\n \n2033.032572\n]\n \nFail\n \nto\n \ndo\n \nclient_init_ctx\n\n\n[\n \n2033.077287\n]\n \nclient_establish_conn\n:\n \nctx\n           \n(\nnull\n)\n \nfail\n \nto\n \ninit_interface\n\n\n[\n \n2033.164646\n]\n \nibapi_establish_conn\n:\n \nctx\n           \n(\nnull\n)\n \nfail\n \nto\n \ninit_interface\n\n\n[\n \n2033.250967\n]\n \n***\n\n\n[\n \n2035.620167\n]\n \nBUG\n:\n \nunable\n \nto\n \nhandle\n \nkernel\n \nNULL\n \npointer\n \ndereference\n \nat\n \n0000000000000004\n\n\n[\n \n2035.713763\n]\n \nIP\n:\n \n[\nffffffff8105c589\n]\n \nclient_send_reply_with_rdma_write_with_imm\n+\n0x69\n/\n0x3b0\n\n\n[\n \n2035.812562\n]\n \nPGD\n \n0\n\n\n[\n \n2035.836482\n]\n \nOops\n:\n \n0002\n \n[\n#\n1\n]\n \nSMP\n \nPROCESSOR\n\n\n[\n \n2035.884321\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n1\n \nComm\n:\n \nkernel_init\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n253\n\n\n[\n \n2035.955041\n]\n \nRIP\n:\n \n0010\n:\n[\nffffffff8105c589\n]\n  \n[\nffffffff8105c589\n]\n \nclient_send_reply_with_rdma_write_with_imm\n+\n0x69\n/\n0x3b0\n\n\n...\n\n\n[\n \n2037.313267\n]\n \nTSK\n\n\n[\n \n2037.336146\n]\n \n[\nffffffff8105a377\n]\n \nibapi_send_reply_timeout\n+\n0x57\n/\n0x70\n\n\n[\n \n2037.411025\n]\n \n[\nffffffff81033d24\n]\n \n?\n \nnet_send_reply_timeout\n+\n0x94\n/\n0x132\n\n\n[\n \n2037.486944\n]\n \n[\nffffffff81033d24\n]\n \nnet_send_reply_timeout\n+\n0x94\n/\n0x132\n\n\n\n\n\n\npcache\n\n\nRunning word_count-pthread, with 100MB dataset, finally got some reasonable bug:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n[\n54211.243181\n]\n \npcache_evict_line\n()\n:\n \npset\n:\n \nffff88207f86e3c0\n,\n \nfor\n \nuva\n:\n \n0x7ffff1b8f000\n\n\n[\n54211.385654\n]\n \npcache\n:\nffff88207f86e3a8\n \nmapcount\n:\n8\n \nrefcount\n:\n0\n \nflags\n:()\n\n\n[\n54211.510447\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_PCM\n(\n!\nPcacheLocked\n(\npcm\n))\n\n\n[\n54212.080336\n]\n \nBUG\n:\n \nfailure\n \nat\n \nmanagers\n/\nprocessor\n/\npcache\n/\nevict\n.\nc\n:\n240\n/\npcache_evict_line\n()\n!\n\n\n[\n54212.664785\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n54212.715742\n]\n \nCPU\n:\n \n8\n \nPID\n:\n \n81\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n+\n \n#\n252\n\n\n...\n\n\n[\n54213.391706\n]\n \nTSK\n\n\n[\n54213.414584\n]\n \n[\nffffffff81024180\n]\n \npanic\n+\n0xc2\n/\n0xeb\n\n\n[\n54213.524818\n]\n \n[\nffffffff8101b81c\n]\n \n?\n \ntask_tick_rt\n+\n0x2c\n/\n0xd0\n\n\n[\n54213.589295\n]\n \n[\nffffffff81018f75\n]\n \n?\n \nscheduler_tick\n+\n0x55\n/\n0x60\n\n\n[\n54213.655850\n]\n \n[\nffffffff81016625\n]\n \n?\n \ntick_handle_periodic\n+\n0x45\n/\n0x70\n\n\n[\n54213.728647\n]\n \n[\nffffffff81006634\n]\n \n?\n \napic_timer_interrupt\n+\n0x54\n/\n0x90\n\n\n[\n54213.801443\n]\n \n[\nffffffff8100e22a\n]\n \n?\n \nsmp__apic_timer_interrupt\n+\n0x6a\n/\n0x70\n\n\n[\n54213.879439\n]\n \n[\nffffffff8101256d\n]\n \n?\n \nprintk\n+\n0x11d\n/\n0x1b0\n\n\n[\n54214.103027\n]\n \n[\nffffffff8102ecf4\n]\n \npcache_evict_line\n+\n0x134\n/\n0x220\n\n\n[\n54214.172703\n]\n \n[\nffffffff8102c6ae\n]\n \npcache_alloc\n+\n0x22e\n/\n0x2e0\n\n\n[\n54214.237179\n]\n \n[\nffffffff8102be0a\n]\n \ncommon_do_fill_page\n+\n0x2a\n/\n0x1f0\n\n\n[\n54214.307895\n]\n \n[\nffffffff8102baf0\n]\n \n?\n \nmove_page_tables\n+\n0x4c0\n/\n0x4c0\n\n\n[\n54214.378612\n]\n \n[\nffffffff8102c172\n]\n \npcache_handle_fault\n+\n0x1a2\n/\n0x3a0\n\n\n[\n54214.450367\n]\n \n[\nffffffff8100fc02\n]\n \ndo_page_fault\n+\n0xa2\n/\n0x1a0\n\n\n[\n54214.514843\n]\n \n[\nffffffff8100d85f\n]\n \npage_fault\n+\n0x1f\n/\n0x30\n\n\n[\n54214.575161\n]\n \n[\nffffffff81034842\n]\n \n?\n \ncopy_user_enhanced_fast_string\n+\n0x2\n/\n0x10\n\n\n[\n54214.657316\n]\n \n[\nffffffff81032368\n]\n \n?\n \nseq_read\n+\n0x248\n/\n0x360\n\n\n[\n54214.719714\n]\n \n[\nffffffff810307af\n]\n \nsys_read\n+\n0x3f\n/\n0xc0\n\n\n[\n54214.777949\n]\n \n[\nffffffff81030770\n]\n \n?\n \nsweep_pset_lru\n+\n0x220\n/\n0x220\n\n\n[\n54214.846587\n]\n \n[\nffffffff8100e619\n]\n \ndo_syscall_64\n+\n0x69\n/\n0xf0\n\n\n[\n54214.910022\n]\n \n[\nffffffff8100d4ec\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n54214.985939\n]\n \nEOT\n\n\n\n\n\n\nAnother one:\n\n1\n2\n3\n[\n  \n735.393244\n]\n \npcache_evict_line\n()\n:\n \npset\n:\n \nffff88207f86e3c0\n,\n \nfor\n \nuva\n:\n \n0x7ffff1b8fd90\n\n\n[\n  \n735.537804\n]\n \npcache\n:\nffff88207f86e3a8\n \nmapcount\n:\n8\n \nrefcount\n:\n0\n \nflags\n:()\n\n\n[\n  \n735.663642\n]\n \npcache\n \ndumped\n \nbecause\n:\n \nPCACHE_BUG_ON_PCM\n(\n!\nPcacheLocked\n(\npcm\n))\n\n\n\n\n\n\nDo note this happens after computation. This happens when phoenix create a lot threads to sort the results.\n\n\nBoth bug happen to the same set, same user page. The pcache is clearly corrupted: \nmapcount\n:\n8\n,\n \nrefcount\n:\n0\n,\n \nflags\n:().\n\n\nCome back after dinner.\nRemember to check altenative, cause the XSAVE above should be XSAVEOPT. Make sure it does not override other memory. Also, check linker script. Do not forget to link any sections.\n\n\nAnother several bug logs in wuklab13 and wuklab15: \n022318-*\n. I\nm really tired today after fixing the FPU bug. But I\nm also pretty confident pcache is something I\nm able to debug. Even thought it is hard in SMP case.\n\n\nAnyway, I gonna call for the day.\n\n\n\n\n02/22 Thur\n\n\n\n\ncontext switch fpu\n\n\nsignal compat check, all good.\n\n\n make \ncurrent\n use percpu current_task, so all code in Lego is consistent.\n\n\nchecked \nentry_SYSCALL-64\n again, which looks good to me.\n\n\nThe only concern is \nrsp_scratch\n and \ncurrent_top_of_stack\n, which are per-cpu variables. If these per-cpu is setup wrong, then we are doomed.\n\n\nAlso check if per-cpu is all cleared up?\n\n\ntry big syscall lock\n\n\ndoes x86 has to use different kernel stacks? Interrupt is using different stack in Linux, has to do so???\n\n\ncheck current is correct. compare with old implementation.\n\n\n\n\nFirst of all, FPU is definitely functional for now.\nSince I replaced the current macro today, I add some code to check if this current matches our old implementation:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nstatic __always_inline struct task_struct *get_current(void)                                                           \n{                                                                                                                      \n        return this_cpu_read_stable(current_task);                                                                     \n}\n\n//#define current get_current()\n\n#define current                                                 \\\n({                                                              \\\n        struct task_struct *old = current_thread_info()-\ntask;  \\\n        struct task_struct *new = get_current();                \\\n                                                                \\\n        if (old != new) {                                       \\\n                printk(\n%s:%d() cpu:%d old:%pS %d %s new:%pS %d %s\\n\n,  \\\n                        __func__, __LINE__, smp_processor_id(), old, old-\npid, old-\ncomm, \\\n                        new, new-\npid, new-\ncomm);              \\\n                BUG();                                          \\\n        }                                                       \\\n        get_current();                                          \\\n})\n\n\n\n\n\nCombined with some FPU warning, it is now like this:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n[\n \n3273.748819\n]\n \nCPU\n:\n5\n \nPID\n:\n32\n   \nsys_clone\n+\n0x0\n/\n0x30\n\n\n[\n \n3273.800808\n]\n \nalloc_task_struct_node\n:\n \nsize\n:\n740\n \nffff88107e831838\n\n\n[\n \n3273.869451\n]\n \narch_dup_task_struct\n()\n \nCPU5\n \ncurrent\n:\n32\n \nnew\n:\n \nffff88107e831838\n \nold\n:\n \nffff88107e827838\n \n32\n\n\n[\n \n3273.975533\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n3274.030651\n]\n \nWARNING\n:\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nat\n \n.\n/\narch\n/\nx86\n/\ninclude\n/\nasm\n/\nfpu\n/\ninternal\n.\nh\n:\n354\n \nfpu__copy\n+\n0xe2\n/\n0x310\n\n\n[\n \n3274.140895\n]\n \nCPU\n:\n \n5\n \nPID\n:\n \n32\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n-\nys\n-\ngdbe6dbe\n-\ndirty\n \n#\n249\n\n\n[\n \n3274.231377\n]\n \nStack\n:\n\n\n[\n \n3274.255298\n]\n \nffff88107e82fd68\n \nffffffff81016dbf\n \n00000000ff\nffffff\n \n0000000000000000\n\n\n[\n \n3274.342659\n]\n \n00000000ff\nffffff\n \n0000000000000000\n \nffff88107e831bf8\n \nffff88107e831c38\n\n\n[\n \n3274.430021\n]\n \nffff88107e831838\n \n000000207f\ne64000\n \nffff88107e82fd78\n \nffffffff810170af\n\n\n[\n \n3274.517382\n]\n \nffff88107e82fdc0\n \nffffffff8100b052\n \n0000000000000020\n \nffff88107e831838\n\n\n[\n \n3274.604745\n]\n \nffff88107e827838\n \nffff88107e827838\n \nffff88107e831838\n \nffff88107e827838\n\n\n[\n \n3274.692106\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n3274.721229\n]\n \nTSK\n\n\n[\n \n3274.744109\n]\n \n[\nffffffff81016dd8\n]\n \n__warn\n.\nconstprop\n.0\n+\n0xe8\n/\n0x3b0\n\n\n[\n \n3274.813790\n]\n \n[\nffffffff810170af\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n \n3274.881391\n]\n \n[\nffffffff8100b052\n]\n \nfpu__copy\n+\n0xe2\n/\n0x310\n\n\n[\n \n3274.941713\n]\n \n[\nffffffff810012e4\n]\n \narch_dup_task_struct\n+\n0x84\n/\n0x120\n\n\n[\n \n3275.013475\n]\n \n[\nffffffff81022c10\n]\n \ncopy_process\n+\n0x160\n/\n0x1e60\n\n\n[\n \n3275.078996\n]\n \n[\nffffffff81024936\n]\n \ndo_fork\n+\n0x26\n/\n0x140\n\n\n[\n \n3275.137238\n]\n \n[\nffffffff81024af0\n]\n \n?\n \nsys_vfork\n+\n0x40\n/\n0x40\n\n\n[\n \n3275.198599\n]\n \n[\nffffffff81024af0\n]\n \n?\n \nsys_vfork\n+\n0x40\n/\n0x40\n\n\n[\n \n3275.259960\n]\n \n[\nffffffff81024b19\n]\n \nsys_clone\n+\n0x29\n/\n0x30\n\n\n[\n \n3275.319242\n]\n \n[\nffffffff81012314\n]\n \ndo_syscall_64\n+\n0x84\n/\n0x240\n\n\n[\n \n3275.383723\n]\n \n[\nffffffff8101106c\n]\n \nentry_SYSCALL64_slow_path\n+\n0x25\n/\n0x25\n\n\n[\n \n3275.459645\n]\n \nEOT\n\n\n[\n \n3275.482526\n]\n \n---\n[\n \nend\n \ntrace\n \n0000000000000000\n \n]\n---\n\n\n[\n \n3275.537648\n]\n \nwake_up_new_task\n \nCPU5\n \ntask\n:\nffff88107e831838\n,\n \ndest_cpu\n:\n6\n \ncurrent\n:\n32\n\n\n[\n \n3275.623970\n]\n \nSMP\n \nIPI\n:\n \nreschedule_interrupt\n()\n \nCPU\n(\n6\n)\n \nPID\n(\n0\n)\n\n\n[\n \n3275.739412\n]\n \ndo_general_protection\n:\n186\n()\n \ncpu\n:\n6\n \nold\n:\n0xffff88107e831838\n \n33\n \nword_count\n-\npthr\n \nnew\n:\n0xffff88107fcaf008\n \n0\n \nswapper\n/\n6\n\n\n\n\n[\n \n3275.871493\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n \n3275.926614\n]\n \nBUG\n:\n \nfailure\n \nat\n \narch\n/\nx86\n/\nkernel\n/\ntraps\n.\nc\n:\n186\n/\ndo_general_protection\n()\n!\n\n\n[\n \n3276.015018\n]\n \nKernel\n \nPanic\n \n-\n \nnot\n \nsyncing\n:\n \nBUG\n!\n\n\n[\n \n3276.065978\n]\n \npanic\n:\n107\n()\n \ncpu\n:\n6\n \nold\n:\n0xffff88107e831838\n \n33\n \nword_count\n-\npthr\n \nnew\n:\n0xffff88107fcaf008\n \n0\n \nswapper\n/\n6\n\n\n\n\n\n\nBased on the switch code:\n\n1\n2\n3\n4\n5\n6\n7\n__switch_to\n(\nstruct\n \ntask_struct\n \n*\nprev_p\n,\n \nstruct\n \ntask_struct\n \n*\nnext_p\n)\n\n\n{\n\n        \nthis_cpu_write\n(\ncurrent_task\n,\n \nnext_p\n);\n\n\n        \n/* Reload sp0 This changes current_thread_info(). */\n\n        \nload_sp0\n(\ntss\n,\n \nnext\n);\n\n\n}\n\n\n\n\n\n\nBased on log line 30, \nload_sp0()\n already happened, which means \nthis_cpu_write(..)\n happened too. If \nthis_cpu_write(..)\n happened, then log line 30\ns new should have been updated to \n0xffff88107e831838\n. Something wrong with percpu?\n\n\n\n\n02/21 Wed\n\n\n\n\nirq_regs, old code, check\n\n\nsignal frame, and fpu hook together Done\n\n\nin_interrupt()\n, it is empty, TODO\n\n\ncheck arch/x86/Makefile, it introduce a lot FPU flags.\n\n\nadded more than 4K lines today. Damn FPU. Ugh go home sleep.\n\n\n\n\n\n\n02/20 Tue Cloudy\n\n\nNot too many Sunny days recently. Well, continue yesterday\ns work. I don\nt think I can easily find out why so many \n/proc/memoinfo\n open happened. Instead, I\nm trying to enable the \nflush_thread\n in P\ns exec code.\n\n\nDuring the way, I found some issue related to \n__ARCH_HAS_SA_RESTORER\n in signal code. I need to check if these x86 macros are defined, but lego does not port them.\n\n\nWell, it turns out flush_thread does not make too much difference. Next I\nm going to try to disable \nexit_thread\n, which uses \nfpu__drop()\n.\n\n\nHmm, disable \nexit_thread\n also does not work.\n\n\n\n\n02/19 Mon Rainy\n\n\nIt is another week. I can not deny I\nm a little tired about the bug. Tried so many possible solutions, but none of them work. Well, today I first need to test the vma changes (pgoff and anon_vma) thing. Especially the vma merge and split.\n\n\nThis morning I fixed a bug in kernel_init process: make kernel_init able to run all possible CPUs. Because the first user process is forked from kernel_init, it is quite important that it gets the right cpu affinity:\n\n1\n2\n3\n4\n5\n6\nstatic\n \nint\n \nkernel_init\n(\nvoid\n \n*\nunused\n)\n\n\n{\n\n        \n...\n\n        \nset_cpus_allowed_ptr\n(\ncurrent\n,\n \ncpu_possible_mask\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\nWell, interestingly, the unmodified word_count-pthread succeed with 50MB dataset\n with or without any DEBUG option! Amazing! I need to find out why the cpus_allowed becomes 0 at the beginning of kernel_init. Because \ninit_task\n actually has:\n\n1\n2\n    \n.\ncpus_allowed\n   \n=\n \nCPU_MASK_ALL\n,\n\n    \n.\nnr_cpus_allowed\n=\n \nNR_CPUS\n,\n\n\n\n\n\n\nThings to do next:\n\n\n\n\ncheck why the cpus_allowed changed\n\n\ncheck why word_count-pthread open \n/dev/../cpu\n so many times. Anything wrong with our \ncopy_files\n, or open, close?\n\n\nhere is an idea, to verify if FPU code is correct, run some scientific benchmarks.\n\n\n\n\nOkay, findings:\n\n\n\n\n\n\ncpus_allowd is fine, it is reset inside \nsched_init()\n, when it tries make the \ninit_task\n as the \nidle\n thread. Thus it is reasonable to set cpus_allowed again at \nkernel_init\n thread. And it should NOTHING to do with the bug.\n\n\n\n\n\n\nabout the second, check the following log:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n[\n11838.364543\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nWordcount\n:\n \nRunning\n...\n\n\n]\n---\n\n\n[\n11838.422886\n]\n \nSTDOUT\n:\n \n---\n[\n\n\n\n\n]\n---\n\n\n[\n11838.463445\n]\n \nSYSC_open\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nf_name\n:\n \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_50MB\n.\ntxt\n,\n \nflags\n:\n \n0\n,\n \nmode\n:\n \n900\n\n\n[\n11838.619460\n]\n \nSYSC_open\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nfd\n:\n \n3\n\n\n[\n11838.667406\n]\n \nSYSC_open\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nf_name\n:\n \n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n0\n\n\n[\n11838.773351\n]\n \nSYSC_open\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nfd\n:\n \n4\n\n\n[\n11838.821239\n]\n \nseq_file\n:\n\n  \ndest_uva\n:\n \n00007ff\nfffffc8d0\n,\n \nnr_chars\n:\n \n5\n\n  \nstring\n:\n \n[\n0\n-\n23\n\n\n]\n\n\n[\n11838.913791\n]\n \nSYSC_close\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nfd\n:\n \n4\n\n\n[\n11838.962622\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n11840.223255\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nWord\n \nCount\n:\n \nComputation\n \nCompleted\n \n1.555581\n \nsec\n\n\n\n]\n---\n\n\n[\n11840.309678\n]\n \nSYSC_open\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nf_name\n:\n \n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n0\n\n\n[\n11840.415754\n]\n \nSYSC_open\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nfd\n:\n \n4\n\n\n[\n11840.463593\n]\n \nseq_file\n:\n\n  \ndest_uva\n:\n \n00007ff\nfffffc8a0\n,\n \nnr_chars\n:\n \n5\n\n  \nstring\n:\n \n[\n0\n-\n23\n\n\n]\n\n\n[\n11840.556147\n]\n \nSYSC_close\n(\ncpu5\n \npid\n:\n32\n)\n:\n \nfd\n:\n \n4\n\n\n[\n11840.605024\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nsys\n/\ndevices\n/\nsystem\n/\ncpu\n/\nonline\n]\n\n\n[\n11840.677821\n]\n \nSTDOUT\n:\n \n---\n[\n\n\nTHe\n \nnumber\n \nof\n \nprocessors\n \nis\n \n24\n\n\n\n\u00f4\n\n\n]\n---\n\n\n[\n11840.753769\n]\n \nSYSC_open\n(\ncpu7\n \npid\n:\n80\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11840.844212\n]\n \nSYSC_open\n(\ncpu19\n \npid\n:\n92\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11840.935728\n]\n \nSYSC_open\n(\ncpu7\n \npid\n:\n80\n)\n:\n \nfd\n:\n \n4\n\n\n[\n11840.983567\n]\n \nSYSC_open\n(\ncpu19\n \npid\n:\n92\n)\n:\n \nfd\n:\n \n5\n\n\n[\n11841.032444\n]\n \nseq_file\n:\n\n  \ndest_uva\n:\n \n00007ff\nff444c000\n,\n \nnr_chars\n:\n \n172\n\n  \nstring\n:\n \n[\nMemTotal\n:\n       \n115355128\n \nkB\n\n\nMemFree\n:\n        \n115355128\n \nkB\n\n\nMemAvailable\n:\n   \n115355128\n \nkB\n\n\nDirectMap4k\n:\n        \n5812\n \nkB\n\n\nDirectMap2M\n:\n     \n1861632\n \nkB\n\n\nDirectMap1G\n:\n    \n134217728\n \nkB\n\n\n]\n\n\n[\n11841.305953\n]\n \nseq_file\n:\n\n  \ndest_uva\n:\n \n00007ff\nff444b000\n,\n \nnr_chars\n:\n \n172\n\n  \nstring\n:\n \n[\nMemTotal\n:\n       \n115355128\n \nkB\n\n\nMemFree\n:\n        \n115355128\n \nkB\n\n\nMemAvailable\n:\n   \n115355128\n \nkB\n\n\nDirectMap4k\n:\n        \n5812\n \nkB\n\n\nDirectMap2M\n:\n     \n1861632\n \nkB\n\n\nDirectMap1G\n:\n    \n134217728\n \nkB\n\n\n]\n\n\n[\n11841.579460\n]\n \nSYSC_close\n(\ncpu7\n \npid\n:\n80\n)\n:\n \nfd\n:\n \n4\n\n\n[\n11841.628339\n]\n \nSYSC_close\n(\ncpu19\n \npid\n:\n92\n)\n:\n \nfd\n:\n \n5\n\n\n[\n11841.678257\n]\n \nSYSC_close\n()\n:\n \n[\n4\n]\n \n-\n \n[\n/\nproc\n/\nmeminfo\n]\n\n\n[\n11841.733375\n]\n \nSYSC_close\n()\n:\n \n[\n5\n]\n \n-\n \n[\n/\nproc\n/\nmeminfo\n]\n\n\n[\n11841.788493\n]\n \nSYSC_open\n(\ncpu18\n \npid\n:\n91\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11841.880008\n]\n \nSYSC_open\n(\ncpu6\n \npid\n:\n102\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11841.971523\n]\n \nSYSC_open\n(\ncpu12\n \npid\n:\n85\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11842.063040\n]\n \nSYSC_open\n(\ncpu0\n \npid\n:\n97\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11842.153516\n]\n \nSYSC_open\n(\ncpu14\n \npid\n:\n87\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11842.245032\n]\n \nSYSC_open\n(\ncpu16\n \npid\n:\n89\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11842.336548\n]\n \nSYSC_open\n(\ncpu4\n \npid\n:\n100\n)\n:\n \nf_name\n:\n \n/\nproc\n/\nmeminfo\n,\n \nflags\n:\n \n80000\n,\n \nmode\n:\n \n1\nb6\n\n\n[\n11842.428064\n]\n \nSYSC_open\n(\ncpu16\n \npid\n:\n89\n)\n:\n \nfd\n:\n \n9\n\n\n[\n11842.476942\n]\n \nSYSC_open\n(\ncpu4\n \npid\n:\n100\n)\n:\n \nfd\n:\n \n10\n\n\n[\n11842.526860\n]\n \nseq_file\n:\n\n  \ndest_uva\n:\n \n00007ff\nff444c000\n,\n \nnr_chars\n:\n \n172\n\n  \nstring\n:\n \n[\nMemTotal\n:\n       \n115355128\n \nkB\n\n\nMemFree\n:\n        \n115355128\n \nkB\n\n\nMemAvailable\n:\n   \n115355128\n \nkB\n\n\nDirectMap4k\n:\n        \n5812\n \nkB\n\n\nDirectMap2M\n:\n     \n1861632\n \nkB\n\n\nDirectMap1G\n:\n    \n134217728\n \nkB\n\n\n]\n\n\n[\n11842.800368\n]\n \nseq_file\n:\n\n  \ndest_uva\n:\n \n00007ff\nff444b000\n,\n \nnr_chars\n:\n \n172\n\n  \nstring\n:\n \n[\nMemTotal\n:\n       \n115355128\n \nkB\n\n\nMemFree\n:\n        \n115355128\n \nkB\n\n\nMemAvailable\n:\n   \n115355128\n \nkB\n\n\nDirectMap4k\n:\n        \n5812\n \nkB\n\n\nDirectMap2M\n:\n     \n1861632\n \nkB\n\n\nDirectMap1G\n:\n    \n134217728\n \nkB\n\n\n]\n\n\n[\n11843.073877\n]\n \nSYSC_close\n(\ncpu16\n \npid\n:\n89\n)\n:\n \nfd\n:\n \n9\n\n\n\n\n\n\n\n\n\n\nHowever, in a normal Linux exeution:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nstrace\n \n-\nC\n \n-\no\n \nstrace_2\n \n.\n/\nword_count\n-\npthread\n \n.\n/\nword_count_datafiles\n/\nword_50MB\n.\ntxt\n\n\n\n%\n \ntime\n     \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n------\n \n-----------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n \n86.41\n    \n0.052074\n        \n1736\n        \n30\n           \nfutex\n\n  \n6.89\n    \n0.004151\n          \n67\n        \n62\n           \nmunmap\n\n  \n2.47\n    \n0.001490\n          \n17\n        \n88\n           \nmmap\n\n  \n2.12\n    \n0.001278\n          \n14\n        \n93\n           \nclone\n\n  \n1.51\n    \n0.000912\n          \n14\n        \n64\n           \nmprotect\n\n  \n0.19\n    \n0.000117\n           \n7\n        \n16\n           \nwrite\n\n\n  \n0.15\n    \n0.000092\n          \n46\n         \n2\n           \nopen\n\n\n\n\n$\n \ncat\n \nstrace_2\n \n|\n \ngrep\n \nopen\n\n\n  \nopen\n(\n./word_count_datafiles/word_50MB.txt\n,\n \nO_RDONLY\n)\n \n=\n \n3\n\n\n  \nopen\n(\n/sys/devices/system/cpu/online\n,\n \nO_RDONLY\n|\nO_CLOEXEC\n)\n \n=\n \n4\n\n\n\n\n\n\n\n\n\n\nIt opened the \n/proc/meminfo\n for way too many times. In the normal Linux execution, this should not happen. Is it because our meminfo is faked, so glibs is complaining? But why it does not open meminfo while running in Linux? Or does our entry assembly messed up some stuff in stack, so the return path changed?\n\n\n\n\n\n\noh, about the FPU. It reminds our \nflush_thread\n function actually has an issue before. When I enabled this function during loading in P, the P will crash. Within \nflush_thread\n, there is a \nfpu_clear\n!!! So, check this tomorrow! (12:00am, need to go home)\n\n\n\n\n\n\n\n\n02/18 Sun Sunny\n\n\nIt is a nice day. Yesterday I\nve changed one line of code in mmap code path: change anonymous vma\ns pgoff from some value to 0. The result is I got several succeed work-count-pthread(bind to one core) testing. However, it still fail with unmodified word-count-pthread.\n\n\nIt brings me to inspect pgoff manipulation code and all mmap.c code. We ported everything from linux without almost zero modification. That means we ported all those useless \nanon_vma\n and pgoff code, which is used a lot by vma_merge, vma_split code. The thing is: our memory manager, our vma code do not need such \nanon_vma\n structure, and do not maintain pgoff. Thus, I\nm a little bit worried linux code may doing some crazy behind our back: mess vma and pages, then pcache miss gets some wrong pages\n\n\nWell. Lego does not use \nanon_vma\n, and pgoff should only be used by file-backed vma. So, I decided to remove \nanon_vma\n from our code, and make sure pgoff is used properly. Of course, the goal is to make vma_merge, split,\ncopy, do the things we intended.\n\n\nLesson learned.\n\n\n\n\n02/17 Sat Snowy\n\n\nFixed the bss bug. It comes from loader. We did not implement the \nlego_clear_user\n function, so some part of bss is non-zero.\n\n\nBad news is word_count-pthread still fail at same fpu instruction. Have to look into memory code more.\n\n\nThis is actually a fun debugging story. We should always add TODO or XXX or some warnings to unfinished code, no matter what. Lesson learned.\n\n\n\n\n02/16 Fri Cloudy\n\n\nYilun found a major loader bug yesterday: the \n.bss\n section variables are not 0, in the \niozone\n benchmark. I did not encounter this issue before with simple test program. This is pretty serious.\n\n\n\n\n02/15 Thur Rainy\n\n\nToday is Chinese New Year.\n\n\nLine 7 and 8 show the uva belong to the same page. Need to revisit \nget_arg_pages\n etc functions.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n[  108.393991] handle_p2m_execve(): pid:22,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread\n[  108.395255]     argc[0] (len: 65):  /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread\n[  108.396329]     argc[1] (len: 82):  /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt\n[  108.397530]     envc[0] (len:  7):  HOME=/\n[  108.398069]     envc[1] (len: 11):  TERM=linux\n[  108.398640] __bprm_mm_init vma: ffff88083effe6b8\n\n[  108.399226] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffefed\n\n[  108.399949] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffef94\n\n\n\n\n\n\nWell, this is 100% fine. I wrote this loader code long time ago and need some time to pickup. So, after I read the loader code, especially the \ncopy_strings\n function, I found this is okay. Because copy_strings will be invoked three times, so the \nfaultin_page\n basically will be invoked at least three times. That is why it went to that pte fault handling code.\n\n\nAlthough actually I think \ncopy_strings\n should \nnot\n use \nfaultin_page\n, instead, it should use \nget_user_pages\n, which will walk through the pgtable first, then went to \nhandle_lego_mm_fault\n.\n\n\n\n\n02/14 Wed Rainy\n\n\nHmm, tried to make kmalloc behave as kzalloc, and bind all threads to one core, still gave the same old bug:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n  42731a:       f3 0f 6f 16             movdqu (%rsi),%xmm2\n\n\n  [93182.657376] word_count-pthr[85] general protection ip:42731a sp:7fffe3ffed28 error:0\n  [93182.747959] CPU: 8 PID: 85 Comm: word_count-pthr 4.0.0-lego+ #170\n  [93182.820758] RIP: 0033:[\n000000000042731a\n]  [\n000000000042731a\n] 0x42731a\n  [93182.901878] RSP: 002b:00007fffe3ffed28  EFLAGS: 00010283\n  [93182.965317] RAX: 000000000000001f RBX: 00007ffff001b010 RCX: 0000000000000005\n  [93183.050596] RDX: 0000000000000000 RSI: 5345485355420045 RDI: 00007ffff294791f\n  [93183.135876] RBP: 00007ffff294791f R08: 000000000000ffff R09: 0000000000000008\n  [93183.221156] R10: fffffffffffff048 R11: 00000000004acfc0 R12: 0000000000001cde\n  [93183.306435] R13: 00000000006e4a8c R14: 0000000000001cd7 R15: 0000000000001cda\n  [93183.391716] FS:  00007fffe3fff700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000\n  [93183.488434] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  [93183.557075] CR2: 00007ffff27a4000 CR3: 000000107e924000 CR4: 00000000000406a0\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n  427377:       66 0f 6f 17             movdqa (%rdi),%xmm2\n\n\n  [93180.527248] word_count-pthr[93]: segfault at 0x0 ip 0000000000427377 sp 00007fffdfff6d28 error 4\n  [93180.630314] CPU: 8 PID: 93 Comm: word_count-pthr 4.0.0-lego+ #170\n  [93180.703114] RIP: 0033:[\n0000000000427377\n]  [\n0000000000427377\n] 0x427377\n  [93180.784234] RSP: 002b:00007fffdfff6d28  EFLAGS: 00010297\n  [93180.847674] RAX: 0000000000000000 RBX: 000000000073c4c0 RCX: 000000000000000d\n  [93180.932953] RDX: 000000000000ffff RSI: 00007ffff4999070 RDI: 0000000000000000\n  [93181.018233] RBP: 00007ffff499907d R08: 000000000000ffff R09: 0000000000000000\n  [93181.103513] R10: 0000000000427760 R11: 00007ffff49982c0 R12: 0000000000000118\n  [93181.188791] R13: 00000000006e4aac R14: 0000000000000116 R15: 0000000000000117\n  [93181.274072] FS:  00007fffdfff7700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000\n  [93181.370790] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  [93181.439430] CR2: 0000000000000000 CR3: 000000107e924000 CR4: 00000000000406a0\n\n\n\n\n\n\nTried several ways to ensure memory safety. It still failed even if I enabled all of them. So, I guess the memory safety is ensured? Still some other things?\n\n\n\n\nforce \nalloc_pages\n to use \n__GFP_ZERO\n\n\nmake \nkmalloc\n behave as \nkzalloc\n\n\nmake \nkfree\n empty\n\n\n\n\nI also suspect \nmunmap\n may free extra wrong pgtable entries. Although I\nve went through all the code and checked, but in addition to the above things, I\nm going to:\n\n\n\n\nmake munmap dummy (no p2m_munmap, return 0 directly)\n\n\n\n\nFailed.\n\n\nNext, I\nm going to:\n\n\n\n\nadd checksum for every page transferred across network.\n\n\nadd warning for unnormal cases\n\n\n\n\nBang! I found something while running P+M:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n[\n  \n115.727597\n]\n \nMemory\n-\ncomponent\n \nmanager\n \nis\n \nup\n \nand\n \nrunning\n.\n\n\n[\n  \n116.691723\n]\n \nhandle_p2m_fork\n()\n:\n \nnid\n:\n0\n,\npid\n:\n22\n,\ntgid\n:\n22\n,\nparent_tgid\n:\n1\n\n\n[\n  \n116.697038\n]\n \nhandle_p2m_fork\n()\n:\n \nreply\n:\n \n0\n:\nOKAY\n\n\n[\n  \n116.791088\n]\n \nhandle_p2m_execve\n()\n:\n \npid\n:\n22\n,\nargc\n:\n2\n,\nenvc\n:\n2\n,\nfile\n:\n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\npthread\n\n\n[\n  \n116.792357\n]\n     \nargc\n[\n0\n]\n \n(\nlen\n:\n \n65\n)\n:\n  \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count\n-\npthread\n\n\n[\n  \n116.793439\n]\n     \nargc\n[\n1\n]\n \n(\nlen\n:\n \n82\n)\n:\n  \n/\nroot\n/\nys\n/\nphoenix\n/\nphoenix\n-\n2.0\n/\ntests\n/\nword_count\n/\nword_count_datafiles\n/\nword_100MB\n.\ntxt\n\n\n[\n  \n116.794653\n]\n     \nenvc\n[\n0\n]\n \n(\nlen\n:\n  \n7\n)\n:\n  \nHOME\n=/\n\n\n[\n  \n116.795196\n]\n     \nenvc\n[\n1\n]\n \n(\nlen\n:\n \n11\n)\n:\n  \nTERM\n=\nlinux\n\n\n[\n  \n116.795772\n]\n \n__bprm_mm_init\n \nvma\n:\n \nffff88083effe6b8\n\n\n[\n  \n116.796209\n]\n \nfaultin_page\n \nvma\n:\n \nffff88083effe6b8\n\n\n[\n  \n116.796729\n]\n \nfaultin_page\n \nvma\n:\n \nffff88083effe6b8\n\n\n[\n  \n116.797150\n]\n \nhandle_pte_fault\n \nvma\n:\n \nffff88083effe6b8\n \nentry\n:\n \n0xffff88083e8c1067\n\n\n[\n  \n116.798044\n]\n \npte\n:\nffff88083e8c0ff0\n \npfn\n:\n0x8083e8c1\n \nflags\n:(\npresent\n|\nwritable\n|\nuser\n|\naccessed\n|\ndirty\n|\nsoftw4\n|\npkey0\n|\npkey1\n|\npkey2\n|\npkey3\n|\nnx\n|\n0x3ff800000000000\n)\n\n\n[\n  \n116.799462\n]\n \n------------\n[\n \ncut\n \nhere\n \n]\n------------\n\n\n[\n  \n116.800049\n]\n \nWARNING\n:\n \nCPU\n:\n \n4\n \nPID\n:\n \n15\n \nat\n \nmanagers\n/\nmemory\n/\nvm\n/\nfault\n.\nc\n:\n148\n \nhandle_lego_mm_fault\n+\n0x4d8\n/\n0x550\n\n\n[\n  \n116.801148\n]\n \nCPU\n:\n \n4\n \nPID\n:\n \n15\n \nComm\n:\n \nmc\n-\nmanager\n \n4.0.0\n-\nlego\n+\n \n#\n78\n\n\n[\n  \n116.801818\n]\n \nStack\n:\n\n\n[\n  \n116.802179\n]\n \nffff88083e893c50\n \nffffffff8100e827\n \n00007ff\nfffffef94\n \nffff88083effe6b8\n\n\n[\n  \n116.803283\n]\n \nffff88083e894008\n \nffff88083e8c1067\n \nffff88083e893c60\n \nffffffff8100e91f\n\n\n[\n  \n116.804387\n]\n \nffff88083e893cf0\n \nffffffff8102b008\n \n0000000000000031\n \nffff88083e893cf0\n\n\n[\n  \n116.805488\n]\n \n00000000000002\n96\n \n00003ff\nfffe00000\n \nffff800000000067\n \nffff88083e893d50\n\n\n[\n  \n116.806590\n]\n \nffff880000000001\n \nffffffff81066798\n \nffff88083effe6b8\n \nffff88083e893d50\n\n\n[\n  \n116.807691\n]\n \nCall\n \nTrace\n:\n\n\n[\n  \n116.808087\n]\n \nTSK\n\n\n[\n  \n116.808448\n]\n \n[\nffffffff8100e836\n]\n \n__warn\n.\nconstprop\n.0\n+\n0xa6\n/\n0x100\n\n\n[\n  \n116.809126\n]\n \n[\nffffffff8100e91f\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n  \n116.809802\n]\n \n[\nffffffff8102b008\n]\n \nhandle_lego_mm_fault\n+\n0x4d8\n/\n0x550\n\n\n[\n  \n116.810505\n]\n \n[\nffffffff8102cfe3\n]\n \nfaultin_page\n+\n0x43\n/\n0xb0\n\n\n[\n  \n116.811131\n]\n \n[\nffffffff8102dab1\n]\n \ncopy_strings\n.\nisra\n.1\n+\n0xe1\n/\n0x130\n\n\n[\n  \n116.811819\n]\n \n[\nffffffff8102dd1e\n]\n \nexec_loader\n+\n0x21e\n/\n0x350\n\n\n[\n  \n116.812457\n]\n \n[\nffffffff8102680a\n]\n \nhandle_p2m_execve\n+\n0x1aa\n/\n0x290\n\n\n\n\n\n\nThis is a temporary stack vma that loader created for saving argv and envp. So, this vma was created here:\n\n\n1\n2\n3\n4\n5\n6\nstatic\n \nint\n \n__bprm_mm_init\n(\nstruct\n \nlego_binprm\n \n*\nbprm\n)\n\n\n{\n\n        \n...\n\n        \nbprm\n-\nvma\n \n=\n \nvma\n \n=\n \nkzalloc\n(\nsizeof\n(\n*\nvma\n),\n \nGFP_KERNEL\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nAnd then \ncopy_strings\n will call \nfaultin_page\n to populate a page for a specific user virtual adddress:\n\n\n1\n2\n3\n4\n5\n6\n7\nint\n \nfaultin_page\n(\nstruct\n \nvm_area_struct\n \n*\nvma\n,\n \nunsigned\n \nlong\n \nstart\n,\n\n                 \nunsigned\n \nlong\n \nflags\n,\n \nunsigned\n \nlong\n \n*\nkvaddr\n)\n\n\n{\n\n        \n...\n\n        \nret\n \n=\n \nhandle_lego_mm_fault\n(\nvma\n,\n \nstart\n,\n \nflags\n,\n \nkvaddr\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nEventually, the \nhandle_lego_mm_fault\n will call \nhandle_pte_fault\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nstatic\n \nint\n \nhandle_pte_fault\n(\nstruct\n \nvm_area_struct\n \n*\nvma\n,\n \nunsigned\n \nlong\n \naddress\n,\n\n                            \nunsigned\n \nint\n \nflags\n,\n \npte_t\n \n*\npte\n,\n \npmd_t\n \n*\npmd\n,\n\n                            \nunsigned\n \nlong\n \n*\nmapping_flags\n)\n\n\n{\n\n        \n...\n\n        \nif\n \n(\n!\npte_present\n(\nentry\n))\n \n{\n\n                \n...\n\n        \n}\n\n\n        \npr_info\n(\n%s vma: %p entry: %#lx\n\\n\n,\n \nFUNC\n,\n \nvma\n,\n \nentry\n.\npte\n);\n\n        \ndump_pte\n(\npte\n,\n \nNULL\n);\n\n        \nWARN_ON_ONCE\n(\n1\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nApparently, pte is wrong! But I don\nt have time today. Continue tomorrow.\nHmm forgot that we are saving kernel virtual addresses in the pte. Just take a quick look at the lego_pud_alloc things, seems will have some issues. I defenitly need to check all these stuff tomorrow. I\nve not touch this part for too long!\n\n\n\n\n02/13 Tue Sunny\n\n\nChecking our SLOB allocator today. So I found Yutong\ns code is using \nset_page_private\n when slob get a new page from buddy. This private field is only intended to be used by buddy to record the \norder\n. This mixed usage will confuse buddy and create bug.\n\n\nEven though I removed the \nset_page_private\n(\npage\n,\n \n0\n)\n after \nfree_page\n, word_count-pthread still fails. Damn.\n\n\n\n\n02/12 Mon Cloudy\n\n\nAdd this commit \n4cb3a8b6a943c90714fd9bb5e5465ee315f0aa30\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n    memory: Use kzalloc instead of kmalloc in __bprm_mm_init (loader)\n\n    This was an potentionl bug that was not triggered previously.\n    It is simply because kmalloc\ned vma contains some garbage area,\n    while later in the pgfault code, we use\n            if (vma-\nvm_ops \n vma-\nvm_ops-\nfault)\n                    ...\n    to check if it is an file-backed fault.\n\n    Fortunately the vma-\nvm_ops happens to have some leftover value.\n    So this bug was triggered.\n\n    This actually reminds me that this is a series of potential bugs!\n    Even though before I\nve added things like force GFP_ZERO in all\n    physical page allocation, I missed the kmalloc\ns case!\n\n\n\n\n\nThe story is:\n\n\nI patched the stop_machine code today, and tried to run code with P+M on VM, everything works fine. However, when I tried to run the new code with P+M+S on physical machine, M crashed at a very weird point:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n[ 7791.998168] handle_p2m_execve(): pid:81,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread\n[ 7792.129312] BUG: unable to handle kernel NULL pointer dereference at 0000000000000031\n[ 7792.222889] IP: [\nffffffff8102c180\n] handle_lego_mm_fault+0x160/0x4b0\n[ 7792.299842] PGD 0\n[ 7792.323760] Oops: 0000 [#1] PREEMPT SMP MEMORY\n[ 7792.376794] CPU: 4 PID: 79 Comm: mc-manager 4.0.0-lego+ #29\n\n[ 7792.443349] RIP: .. [\nffffffff8102c180\n] handle_lego_mm_fault+0x160/0x4b0\n\n......\n....\n[ 7793.750506] Call Trace:\n[ 7793.779623] \nTSK\n\n\n[ 7793.802501] [\nffffffff810053f4\n] ? apic_timer_interrupt+0x54/0x90\n\n[ 7793.875295] [\nffffffff8102e469\n] faultin_page+0x9/0x70\n\n[ 7793.936649] [\nffffffff8102ef01\n] copy_strings.isra.1+0xe1/0x130\n\n[ 7794.007362] [\nffffffff8102f11e\n] exec_loader+0x1ce/0x340\n\n[ 7794.070796] [\nffffffff81027def\n] handle_p2m_execve+0x12f/0x200\n\n[ 7794.140469] [\nffffffff810274fb\n] mc_manager+0x1ab/0x2b0\n[ 7794.202864] [\nffffffff81027350\n] ? bitmap_fill+0x33/0x33\n[ 7794.266298] [\nffffffff8101c6b7\n] kthread+0x107/0x130\n[ 7794.325572] [\nffffffff8101c5b0\n] ? __kthread_parkme+0x90/0x90\n[ 7794.394205] [\nffffffff8100b462\n] ret_from_fork+0x22/0x30\n\n\n\n\n\nSo faulting source code is:\n\n1\n2\n3\n4\n5\n6\n7\n8\nstatic\n \nint\n \nhandle_pte_fault\n(\nstruct\n \nvm_area_struct\n \n*\nvma\n,\n \nunsigned\n \nlong\n \naddress\n,\n\n                            \nunsigned\n \nint\n \nflags\n,\n \npte_t\n \n*\npte\n,\n \npmd_t\n \n*\npmd\n)\n\n\n{\n\n    \n....\n\n\n        \nif\n \n(\nvma\n-\nvm_ops\n \n \nvma\n-\nvm_ops\n-\nfault\n)\n\n\n                \nreturn\n \ndo_linear_fault\n(\nvma\n,\n \naddress\n,\n \nflags\n,\n\n\n                                       \npte\n,\n \npmd\n,\n \nentry\n)\n\n    \n....\n\n\n\n\n\n\nSomething wrong with \nvma\n? At this loader stage, this vma is a temporaty stack vma created for saving \nargv\n and \nenvp\n. So I look back into the code that created this vma:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nmanagers\n/\nmemory\n/\nloader\n/\ncore\n.\nc\n:\n\n\nstatic\n \nint\n \n__bprm_mm_init\n(\nstruct\n \nlego_binprm\n \n*\nbprm\n)\n\n\n{\n\n        \nint\n \nerr\n;\n\n        \nstruct\n \nvm_area_struct\n \n*\nvma\n \n=\n \nNULL\n;\n\n        \nstruct\n \nlego_mm_struct\n \n*\nmm\n \n=\n \nbprm\n-\nmm\n;\n\n\n\n        \nbprm\n-\nvma\n \n=\n \nvma\n \n=\n \nkmalloc\n(\nsizeof\n(\n*\nvma\n),\n \nGFP_KERNEL\n);\n\n\n        \nif\n \n(\n!\nvma\n)\n\n                \nreturn\n \n-\nENOMEM\n;\n\n\n\n\n\n\nThe code after this does NOT do necessary cleanup. The \nvm_ops\n happens to have some garbage value from last user. So it is not 0, so the above \nvma-\nvm_ops\n is true, and it will try to read \nvma-\nvm_ops-\nfault\n. And that, my friend, is where garbage turns into crash.\n\n\nThis presents a series of potential bugs. Ugh, \nmemory safety\n!\n\n\n\n\n02/09 Fri Cloudy\n\n\nTried to modify Phoneix code: replace \nrealloc\n with \nmalloc+mempcy\n. Thus the \nmremap\n syscall is avoided, but it still has general protection fault. Same with yesterday, corrupted at \n__strcmp_sse42\n, with corrupted \nRSI\n or \nRDI\n. So I guess it is not about \nmremap\n itself at all. I will follow yesterday\ns checking list.\n\n\n\n\n02/08 Thur Cloudy\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n00000000004272d0 \n__strcmp_sse42\n:\n\n  4272d0:       89 f1                   mov    %esi,%ecx\n  4272d2:       89 f8                   mov    %edi,%eax\n  4272d4:       48 83 e1 3f             and    $0x3f,%rcx\n  4272d8:       48 83 e0 3f             and    $0x3f,%rax\n  4272dc:       83 f9 30                cmp    $0x30,%ecx\n  4272df:       77 3f                   ja     427320 \n__strcmp_sse42+0x50\n\n  4272e1:       83 f8 30                cmp    $0x30,%eax\n  4272e4:       77 3a                   ja     427320 \n__strcmp_sse42+0x50\n\n  4272e6:       f3 0f 6f 0f             movdqu (%rdi),%xmm1\n\n* 4272ea:       f3 0f 6f 16             movdqu (%rsi),%xmm2\n\n  4272ee:       66 0f ef c0             pxor   %xmm0,%xmm0\n  4272f2:       66 0f 74 c1             pcmpeqb %xmm1,%xmm0\n  4272f6:       66 0f 74 ca             pcmpeqb %xmm2,%xmm1\n  4272fa:       66 0f f8 c8             psubb  %xmm0,%xmm1\n  4272fe:       66 0f d7 d1             pmovmskb %xmm1,%edx\n  427302:       81 ea ff ff 00 00       sub    $0xffff,%edx\n  427308:       0f 85 42 0d 00 00       jne    428050 \n__strcmp_sse42+0xd80\n\n  42730e:       48 83 c6 10             add    $0x10,%rsi\n  427312:       48 83 c7 10             add    $0x10,%rdi\n  427316:       66 2e 0f 1f 84 00 00    nopw   %cs:0x0(%rax,%rax,1)\n  42731d:       00 00 00  \n  427320:       48 83 e6 f0             and    $0xfffffffffffffff0,%rsi\n  427324:       48 83 e7 f0             and    $0xfffffffffffffff0,%rdi\n  427328:       ba ff ff 00 00          mov    $0xffff,%edx\n  42732d:       45 31 c0                xor    %r8d,%r8d\n  427330:       83 e1 0f                and    $0xf,%ecx\n  427333:       83 e0 0f                and    $0xf,%eax\n  427336:       66 0f ef c0             pxor   %xmm0,%xmm0\n  42733a:       39 c1                   cmp    %eax,%ecx\n  42733c:       74 32                   je     427370 \n__strcmp_sse42+0xa0\n\n  42733e:       77 07                   ja     427347 \n__strcmp_sse42+0x77\n\n  427340:       41 89 d0                mov    %edx,%r8d\n  427343:       91                      xchg   %eax,%ecx\n  427344:       48 87 f7                xchg   %rsi,%rdi\n\n* 427347:       66 0f 6f 17             movdqa (%rdi),%xmm2\n\n  (RDI: 0000000000000000)\n\n\n\n\n\n\nFrustrating! What is wrong with multithread program? Because of broken FPU-switch code? of inappropriate TLB flush? of IB corrupts memory? of what? ugh?\n\n\nI\nm done with this random guess and frustrated general protection or segfault, I need to first make sure underlying kernel is 100%  percent correct, this is a checking list:\n\n\n\n\nfpu save/restore\n\n\nalways fail at some XMM instruction\n\n\nalways with corrupted RDI or RSI\n\n\n\n\n\n\nswitch_to_asm\n\n\n%gs and %fs\n\n\nswitch_mm (pgd)\n\n\nstack frame\n\n\n\n\n\n\nset_arch_tls (%fs)\n\n\nglibc\ns way of using per thread data\n\n\n\n\n\n\nsome cpu may miss tlb flush\n\n\nkernel entry/exit assembly\n\n\ncurrent_task macro\n\n\nstack_stratch\n\n\nper-cpu data in entry.S\n\n\n\n\n\n\nfutex\n\n\nclear_tid\n\n\nset_tid\n\n\nshared mm\n\n\nrobust list\n\n\n\n\n\n\ninterrupts\n\n\nvector array\n\n\nAPIC setup\n\n\nIO-APIC\n\n\ntimer interrupt\n\n\n\n\n\n\ncpu_init and Trampoline\n\n\nfaked kernel version\n\n\nP side pgfault handling code (SMP)\n\n\nand M side pgfault handling (SMP)\n\n\nmremap, munmap\n\n\ncheck pgtable boundary\n\n\n\n\n\n\nIn all, check SMP implications\n\n\n\n\nIs there any code, that is solely used to test if the underlying kernel has appropriate behaviors? Like glibc test code?\n\n\nHow to protect kernel virtual memory? Any existing solutions in Linux?\n\n\nWhat is the implication of multiple CPU entering kernel at the same time? How can it corrupt user pages? Maybe: kernel entry code, per-cpu data in entry code, fpu code, switch_to, scheduler.\n\n\nWhy it always fail at those FPU code i.e. the strcmp function? I failed to compile without those sse, any solution? How it hurt performance?\n\n\n\n\n02/07 Wed Cloudy\n\n\n20\n:\n07\n\nPushed a small patch on mremap issue. Hope it will work. mremap really makes the whole thing very interesting, will be a very good research finding on combing virtual cache and operating system. Need to go gym with a friend, will be back on debugging late tonight.\n\n\n9\n:\n30\n\nHave two meetings to do today, and an security class, won\nt have too much time coding during daytime.\n\n\n\n\n02/06 Tue Sunny\n\n\nWell. We\nve ruled out both \nsmp_call_function\n and \nworkqueue\n yesterday with Yiying\ns help. But the multi-thread word-count still fails \n:-(\n Single thread word-count just finished 4GB dataset (with 8GB pcache). So what could be still wrong with multithread one????\n\n\n\n\nchill\n\n\ncheck exit code\n\n\n(Checked)\n check pcache\ns usage of task_struct, should always use the group_leader\n\n\ncheck cpu boot code and check the switch code again\n\n\nI believe pinpoint the issue in multithread word-count can solve a lot issues, it must be some thread creation, removal, schedule things.\n\n\nHow about adding a lock for ibapi, make it sequential? Sweet, I tried, finally it is \na bug that we are able to debug\n.\n\n\n\n\n22\n:\n39\n\nDone for today. I\nm trying to patch \nmove_pte\n and \npcache_move_pte\n. Although in theory we defenitly need to patch it, I keep thinking the code before should not trigger any serious bus or memory corruption. Ugh. Maybe it is concurrent \nmremap\n that one of them remap from A to B, while another one remap from C to A. It is possible. But my dead brain can not think of this anymore. I\nm going to hit the gym and do some squats.\n\n\n17\n:\n01\n\nCriminal found: \nmremap()\n and \nvirtual cache\n did the crime. Interesting, I have not seen any research paper, tech-reports, writeup, code about this, not even the OVC paper, which, by the way, I think they must consider this case. Otherwise, a mremap will simply crash its virtual cache. Many thanks went to my smoke-and-think time.\n\n\n15\n:\n14\n\nSomething new came up! After adding a spinlock for ibapi, this showed up (I tried one more time after this, which does not show up). We are lucky to catch this. At least I know where to look at. Also, this is defenitly triggered by \nmremap\n. It is seems it is overlapped \nmremap()\n. One thing I did not know is which thread trigger this bug, the sweep thread? Cause mremap related pcache rmap functions do not use \nrmap_get_locked_pte\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n[ 3826.048774] normal_p2s_open(): f_name: word_100MB.txt, mode: 04400, flags: 0\n[ 3827.891622] SYSC_mremap(cpu18): move: [0x7fffe5788000 - 0x7fffe5806000] -\n [0x7fffe531b000 - 0x7fffe5399000]\n[ 3828.178643] SYSC_mremap(cpu14): move: [0x7fffe5941000 - 0x7fffe5980000] -\n [0x7fffe57c7000 - 0x7fffe5806000]\n\n****    ERROR: mismatched PTE and rmap\n****    rmap-\nowner_process: word_count-pthr uva: 0x7fffe57c8000 ptep: ffff88107efe0e40, rmap-\npage_table: ffff88107efe0e40\n****    pcache_pfn: 0x1257c8, pte_pfn: 0x125942\n\n\n\n\n\n\n14\n:\n00\n \n\n\nword_count-pthread\n: 100MB dataset\n\npcache\n: 8GB, 8-way\n\nvictim\n: 8 entries\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n[ 1294.845313] STDOUT: ---[\nWordcount: Running...\n]---\n[ 1294.903661] STDOUT: ---[\n\no;\n]---\n[ 1294.946301] normal_p2s_open(): f_name: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt, mode: 04400, flags: 0\n[ 1295.100517] SYSC_close(): [4] -\n [/sys/devices/system/cpu/online]\n[ 1295.594658] word_count-pthr[59] general protection ip:4272ea sp:7ffff1b8ed28 error:0\n[ 1295.685236] CPU: 10 PID: 59 Comm: word_count-pthr 4.0.0-lego+ #113\n[ 1295.759070] RIP: 0033:[\n00000000004272ea\n]  [\n00000000004272ea\n] 0x4272ea\n[ 1295.840184] RSP: 002b:00007ffff1b8ed28  EFLAGS: 00010283\n[ 1295.903621] RAX: 000000000000000f RBX: 00007fffe5a3d010 RCX: 0000000000000001\n[ 1295.988893] RDX: 0000000000000000 RSI: 4854005942004441 RDI: 00007ffff1c1e80f\n[ 1296.074166] RBP: 00007ffff1c1e80f R08: 0000000000000000 R09: 0000000000000010\n[ 1296.211435] R10: 0000000000427ce0 R11: 00007ffff1bbb3ba R12: 0000000000001de4\n[ 1296.296711] R13: 00000000006e4a80 R14: 0000000000001d9e R15: 0000000000001dc1\n[ 1296.433978] FS:  00007ffff1b8f700(0000) GS:ffff88107fca0000(0000) knlGS:0000000000000000\n[ 1296.582686] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[ 1296.963297] CR2: 00007ffff1c1e000 CR3: 000000207fd8a000 CR4: 00000000000406a0\n\n\n\n\nSo what is this \nip\n:\n4272\nea\n, let us objdump the binary:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n0000000000425e60 \nstrcmp\n:\n  425e60:       48 8d 05 69 14 00 00    lea    0x1469(%rip),%rax        # 4272d0 \n__strcmp_sse42\n\n  425e67:       f7 05 5f b8 2b 00 00    testl  $0x100000,0x2bb85f(%rip)        # 6e16d0 \n_dl_x86_cpu_features+0x10\n\n  425e6e:       00 10 00\n  425e71:       75 1a                   jne    425e8d \nstrcmp+0x2d\n\n  425e73:       48 8d 05 46 b0 00 00    lea    0xb046(%rip),%rax        # 430ec0 \n__strcmp_ssse3\n\n  425e7a:       f7 05 4c b8 2b 00 00    testl  $0x200,0x2bb84c(%rip)        # 6e16d0 \n_dl_x86_cpu_features+0x10\n\n  425e81:       02 00 00\n  425e84:       75 07                   jne    425e8d \nstrcmp+0x2d\n\n  425e86:       48 8d 05 03 00 00 00    lea    0x3(%rip),%rax        # 425e90 \n__GI_strcmp\n\n  425e8d:       c3                      retq\n  425e8e:       66 90                   xchg   %ax,%ax\n .. ..\n .. ..\n00000000004272d0 \n__strcmp_sse42\n:\n  4272d0:       89 f1                   mov    %esi,%ecx\n  4272d2:       89 f8                   mov    %edi,%eax\n  4272d4:       48 83 e1 3f             and    $0x3f,%rcx\n  4272d8:       48 83 e0 3f             and    $0x3f,%rax\n  4272dc:       83 f9 30                cmp    $0x30,%ecx\n  4272df:       77 3f                   ja     427320 \n__strcmp_sse42+0x50\n\n  4272e1:       83 f8 30                cmp    $0x30,%eax\n  4272e4:       77 3a                   ja     427320 \n__strcmp_sse42+0x50\n\n  4272e6:       f3 0f 6f 0f             movdqu (%rdi),%xmm1\n* 4272ea:       f3 0f 6f 16             movdqu (%rsi),%xmm2\n  4272ee:       66 0f ef c0             pxor   %xmm0,%xmm0\n\n\n\n\nYou can see \n%rsi\n has some garbage value \nRSI\n:\n \n4854005942004441\n. Something went wrong. Will it be our FPU? I\nm not quite sure. If FPU code has error, why single-thread one succeed? Why it only shows up at multithread ones?\n\n\n\n\n02/05 Mon Sunny\n\n\nFrom yesterday\ns testing of Phoenix, it looks like something is wrong in \nsmp_call_functions()\n. They are invoked through \ntlb flush\n, which was further invoked by \nmremap\n, or \nmunmap\n. The warning from smp is:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n[\n \n1260.586696\n]\n \nWARNING\n:\n \nCPU\n:\n \n0\n \nPID\n:\n \n73\n \nat\n \nkernel\n/\nsmp\n.\nc\n:\n129\n \ngeneric_smp_call_function_single_interrupt\n+\n0xb8\n/\n0x160\n\n\n[\n \n1260.705251\n]\n \nCPU\n:\n \n0\n \nPID\n:\n \n73\n \nComm\n:\n \nword_count\n-\npthr\n \n4.0.0\n-\nlego\n+\n \n#\n99\n\n\n[\n \n1260.777008\n]\n \nStack\n:\n\n\n[\n \n1260.800927\n]\n \nffff88207fdffef8\n \nffffffff8100ec67\n \nffff88107fc00000\n \nffff88107fc00000\n\n\n[\n \n1260.888283\n]\n \nffffffff8100d410\n \nffff88207fe23df0\n \nffff88207fdfff08\n \nffffffff8100ed5f\n\n\n[\n \n1260.975639\n]\n \nffff88207fdfff38\n \nffffffff8100fe68\n \n00007ff\nfe58c3010\n \n0000000000000f\n96\n\n\n[\n \n1261.062995\n]\n \n000000000000f\n960\n \n0000000000000f\n95\n \nffff88207fdfff48\n \nffffffff810020dd\n\n\n[\n \n1261.150351\n]\n \n00007ff\nff58869c1\n \nffffffff8100b2e9\n \n0000000000000f\n96\n \n0000000000000f\n95\n\n\n[\n \n1261.237707\n]\n \nCall\n \nTrace\n:\n\n\n[\n \n1261.266825\n]\n \nTSK\n\n\n[\n \n1261.289704\n]\n \n[\nffffffff8100ec76\n]\n \n__warn\n.\nconstprop\n.0\n+\n0xa6\n/\n0x100\n\n\n[\n \n1261.359381\n]\n \n[\nffffffff8100d410\n]\n \n?\n \npgd_free\n+\n0x90\n/\n0x90\n\n\n[\n \n1261.419699\n]\n \n[\nffffffff8100ed5f\n]\n \nwarn_slowpath_null\n+\n0xf\n/\n0x20\n\n\n[\n \n1261.487295\n]\n \n[\nffffffff8100fe68\n]\n \ngeneric_smp_call_function_single_interrupt\n+\n0xb8\n/\n0x160\n\n\n[\n \n1261.581931\n]\n \n[\nffffffff810020dd\n]\n \ncall_function_interrupt\n+\n0x1d\n/\n0x20\n\n\n[\n \n1261.655767\n]\n \n[\nffffffff8100b2e9\n]\n \nsmp__call_function_interrupt\n+\n0x69\n/\n0x70\n\n\n\n\n\n\n\nSo I decided to look into smp.c a little bit to find out if there is something wrong (I wrote it long time ago). The warning itself is true, it means some inconsistent behavior.. I saw \nalloc_percpu\n stuff during \ncall_function_init\n, hence probably I also need to check percpu code a little code cause I\nm not sure if I port all the functionalities.\n\n\nIn all, today\ns task, check \npercpu\n and \nsmp_call_function\n code. Esp, \npercpu\n code, they are crucial and very hard to relate real bugs to it.\n\n\nWell\n things changed. I found a more serious bug: something about \ncpuhotplug\n, even though lego is not using it. \ncpuhotplug\n is a set of implict callbacks to all different subsystems who want to do some initialization work on each \noffline-\nonline\n cpu.\n\n\nLet us dig into how secondary cpu boots:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nTrampoline\n..\n \nsetup\n \n64\nbit\n \nmode\n\n\nstart_secondary\n()\n\n  \nsmp_callin\n()\n\n        \nnotify_cpu_starting\n()\n\n              \n...\n\n              \nwhile\n \n(\nst\n-\nstate\n \n \ntarget\n)\n \n{\n\n                      \nst\n-\nstate\n++\n;\n\n                      \ncpuhp_invoke_callback\n(\ncpu\n,\n \nst\n-\nstate\n,\n \ntrue\n,\n \nNULL\n);\n\n              \n}\n\n          \ncpuhp_invoke_callback\n()\n\n\n\n\n\n\nSee? There will be some callbacks! What are those callbacks exactly? Well, they are predefined at the \nkernel/cpu.c\n. To save the trouble of reading code, I just print what functions are executed, the log is:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n[    0.118235] cpuhp_invoke_callback(): 136  CPU:0  page_writeback_cpu_online+0x0/0x20\n\n[    0.368478] cpuhp_invoke_callback(): 136  CPU:1  smpboot_create_threads+0x0/0x90\n[    0.370196] cpuhp_invoke_callback(): 136  CPU:1  perf_event_init_cpu+0x0/0xa0\n[    0.370403] cpuhp_invoke_callback(): 136  CPU:1  workqueue_prepare_cpu+0x0/0x80\n[    0.371112] cpuhp_invoke_callback(): 136  CPU:1  hrtimers_prepare_cpu+0x0/0x60\n[    0.371339] cpuhp_invoke_callback(): 136  CPU:1  smpcfd_prepare_cpu+0x0/0x80\n[    0.371584] cpuhp_invoke_callback(): 136  CPU:1  relay_prepare_cpu+0x0/0xe0\n[    0.371794] cpuhp_invoke_callback(): 136  CPU:1  rcutree_prepare_cpu+0x0/0x170\n[    0.372333] cpuhp_invoke_callback(): 136  CPU:1  notify_prepare+0x0/0xa0\n[    0.372744] cpuhp_invoke_callback(): 136  CPU:1  bringup_cpu+0x0/0x100\n[    0.008000] cpuhp_invoke_callback(): 136  CPU:1  sched_cpu_starting+0x0/0x60\n[    0.926124] cpuhp_invoke_callback(): 136  CPU:1  smpboot_unpark_threads+0x0/0x90\n[    0.926124] cpuhp_invoke_callback(): 136  CPU:1  perf_event_init_cpu+0x0/0xa0\n[    0.927028] cpuhp_invoke_callback(): 136  CPU:1  workqueue_online_cpu+0x0/0x2a0\n[    0.927768] cpuhp_invoke_callback(): 136  CPU:1  rcutree_online_cpu+0x0/0x70\n[    0.928045] cpuhp_invoke_callback(): 136  CPU:1  notify_online+0x0/0x20\n[    0.928256] cpuhp_invoke_callback(): 136  CPU:1  page_writeback_cpu_online+0x0/0x20\n[    0.928527] cpuhp_invoke_callback(): 136  CPU:1  sched_cpu_activate+0x0/0x190\n\n[    0.929084] cpuhp_invoke_callback(): 136  CPU:2  smpboot_create_threads+0x0/0x90\n[    0.930240] cpuhp_invoke_callback(): 136  CPU:2  perf_event_init_cpu+0x0/0xa0\n[    0.930434] cpuhp_invoke_callback(): 136  CPU:2  workqueue_prepare_cpu+0x0/0x80\n[    0.931070] cpuhp_invoke_callback(): 136  CPU:2  hrtimers_prepare_cpu+0x0/0x60\n[    0.931264] cpuhp_invoke_callback(): 136  CPU:2  smpcfd_prepare_cpu+0x0/0x80\n[    0.931464] cpuhp_invoke_callback(): 136  CPU:2  relay_prepare_cpu+0x0/0xe0\n[    0.931649] cpuhp_invoke_callback(): 136  CPU:2  rcutree_prepare_cpu+0x0/0x170\n[    0.932245] cpuhp_invoke_callback(): 136  CPU:2  notify_prepare+0x0/0xa0\n[    0.932475] cpuhp_invoke_callback(): 136  CPU:2  bringup_cpu+0x0/0x100\n[    0.008000] cpuhp_invoke_callback(): 136  CPU:2  sched_cpu_starting+0x0/0x60\n[    1.005023] cpuhp_invoke_callback(): 136  CPU:2  smpboot_unpark_threads+0x0/0x90\n[    1.005065] cpuhp_invoke_callback(): 136  CPU:2  perf_event_init_cpu+0x0/0xa0\n[    1.005408] cpuhp_invoke_callback(): 136  CPU:2  workqueue_online_cpu+0x0/0x2a0\n[    1.005729] cpuhp_invoke_callback(): 136  CPU:2  rcutree_online_cpu+0x0/0x70\n[    1.006029] cpuhp_invoke_callback(): 136  CPU:2  notify_online+0x0/0x20\n[    1.006206] cpuhp_invoke_callback(): 136  CPU:2  page_writeback_cpu_online+0x0/0x20\n[    1.006549] cpuhp_invoke_callback(): 136  CPU:2  sched_cpu_activate+0x0/0x190\n\n\n\n\n\nInteresting! Currently, Lego need to add the \nsmpboot_create_threads()\n, \nworkqueue_prepare_cpu()\n, \nworkqueue_prepare_cpu()\n, \nbringup_cpu()\n, \nsmpboot_unpark_threads()\n, \nworkqueue_online_cpu()\n.\n\n\nThis hidden things is really hard to find and not easy to track during boot. Especially during boot, they should do something like \nfor_each_online_cpu\n and init one by one. But I guess, after adding support of cpu hotplug, code kind of merged. Some stuff will be executed whenever a cpu has been teardown or bought up. And bang, why not use the same set of hotplug during boot, right?\nWell.", 
            "title": "Feb 2018"
        }, 
        {
            "location": "/lego/log/log-02-2018/#feb-2018", 
            "text": "", 
            "title": "Feb 2018"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0228-wed", 
            "text": "patch fork, and cow handler  debug pcache, while running python hello world  add vDSO, gettimeofday   So, it is end of the day. After adding wp handler, I now have the whole picture of pcache activities, and the interactions between them. The reclaim, zap, move, copy, add, operations needs to be carefully synchronized. Also the refcount etc. I feel the ground rule is we need to make sure a PCM that a function is currently using, can not suddenly become invalid due to other operations. This has to be synced by: refcount, lock, flags. Oh well, mm is hard with SMP, but also fun.  We are very close to have a fully working OS.  I did not have time to look into the python hello world bug issue. It is a very serious one. It may also rule out some root bugs.", 
            "title": "02/28 Wed"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0227-tue", 
            "text": "Spent two days on CS527 source project, implemented a small SSHD and SSD client. And we have to inject exactly five bugs, or vulnerabilities into the systems. Lol, it is really hard to intentionally plant BUGs!  Anyway, back to Lego. Since others are having a hard time compile program statically, I will try to add dynamic loader today.  The interpreter:  /lib64/ld-linux-x86-64.so.2 .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 Linux seq.c maps (no randomization):\n00400000-00401000 r-xp 00000000 fd:00 18752683                           /root/ys/LegoOS/usr/a.out\n00600000-00601000 r--p 00000000 fd:00 18752683                           /root/ys/LegoOS/usr/a.out\n00601000-00602000 rw-p 00001000 fd:00 18752683                           /root/ys/LegoOS/usr/a.out\n00602000-00604000 rw-p 00000000 00:00 0                                  [heap]\n7ffff7a18000-7ffff7bd0000 r-xp 00000000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7bd0000-7ffff7dd0000 ---p 001b8000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7dd0000-7ffff7dd4000 r--p 001b8000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7dd4000-7ffff7dd6000 rw-p 001bc000 fd:00 55051990                   /usr/lib64/libc-2.17.so\n7ffff7dd6000-7ffff7ddb000 rw-p 00000000 00:00 0 7ffff7ddb000-7ffff7dfc000 r-xp 00000000 fd:00 55051983                   /usr/lib64/ld-2.17.so 7ffff7fde000-7ffff7fe1000 rw-p 00000000 00:00 0\n7ffff7ff9000-7ffff7ffa000 rw-p 00000000 00:00 0\n7ffff7ffa000-7ffff7ffc000 r-xp 00000000 00:00 0                          [vdso] 7ffff7ffc000-7ffff7ffd000 r--p 00021000 fd:00 55051983                   /usr/lib64/ld-2.17.so 7ffff7ffd000-7ffff7ffe000 rw-p 00022000 fd:00 55051983                   /usr/lib64/ld-2.17.so 7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0 7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0                          [stack] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 lego   after   loading  00400000 - 00401000   r - xp   00000000   / root / ys / LegoOS / usr / a . out  00600000 - 00602000   rw - p   00000000   / root / ys / LegoOS / usr / a . out  00602000 - 00604000   rw - p   00000000   [ heap ]  7ff ff7ddb000 - 7ff ff7dfc000   r - xp   00000000   / lib64 / ld - linux - x86 - 64. so .2  7ff ff7ffc000 - 7ff ff7ffe000   rw - p   00021000   / lib64 / ld - linux - x86 - 64. so .2  7ff ff7ffe000 - 7ff ff7fff000   rw - p   00000000  7ff ffffde000 - 7ff ffffff000   rw - p   00000000   [ stack ]  [   2066.379224 ]   ****      Finish   dump   final   mm  [   2066.426023 ]   handle_p2m_execve () :   reply_status :   OKAY ,   new_ip :   0x7ffff7ddc170 ,   new_sp :   0x7fffffffede0  [   2066.628949 ]   handle_p2m_pcache_miss ()   cpu   4   I   nid : 0   pid : 32   tgid : 32   flags : 150   vaddr : 0x7ffff7ddc170  [   2066.732034 ]   handle_p2m_pcache_miss ()   cpu   4   O   nid : 0   pid : 32   tgid : 32   flags : 150   vaddr : 0x7ffff7ddc170  [   2066.934947 ]   handle_p2m_pcache_miss ()   cpu   4   I   nid : 0   pid : 32   tgid : 32   flags : 51   vaddr : 0x7fffffffedd8  [   2067.036978 ]   handle_p2m_pcache_miss ()   cpu   4   O   nid : 0   pid : 32   tgid : 32   flags : 51   vaddr : 0x7fffffffedd8  [   2067.238842 ]   handle_p2m_pcache_miss ()   cpu   4   I   nid : 0   pid : 32   tgid : 32   flags : 50   vaddr : 0x7ffff7ffce00  [   2067.340880 ]   handle_p2m_pcache_miss ()   cpu   4   O   nid : 0   pid : 32   tgid : 32   flags : 50   vaddr : 0x7ffff7ffce00  [   2067.542747 ]   handle_p2m_pcache_miss ()   cpu   4   I   nid : 0   pid : 32   tgid : 32   flags : 51   vaddr : 0x7ffff7ffd9a8  [   2067.644774 ]   handle_p2m_pcache_miss ()   cpu   4   O   nid : 0   pid : 32   tgid : 32   flags : 51   vaddr : 0x7ffff7ffd9a8  [   2067.846640 ]   handle_p2m_pcache_miss ()   cpu   4   I   nid : 0   pid : 32   tgid : 32   flags : 50   vaddr : 0x7ffff7ddb8e0  [   2067.948679 ]   handle_p2m_pcache_miss ()   cpu   4   O   nid : 0   pid : 32   tgid : 32   flags : 50   vaddr : 0x7ffff7ddb8e0  [   2068.355424 ]   ------------ [   cut   here   ] ------------  [   2068.408568 ]   WARNING :   CPU :   4   PID :   31   at   managers / memory / handle_pcache / fault . c : 54   handle_p2m_pcache_miss + 0x29d / 0x380  [   2068.532327 ]   src_nid : 0 , pid : 32 , vaddr : 0x7ffff7e0e000  [   2068.588487 ]   CPU :   4   PID :   31   Comm :   mc - manager   4.0.0 - lego - ys +   # 100  [   2068.659207 ]   Stack :    1\n2\n3\n4 [root@wuklab13: lib64] $ ll ld-*\n-rwxr-xr-x 1 root root 164112 Nov 30 13:53 ld-2.17.so\nlrwxrwxrwx 1 root root     10 Jan  8 12:34 ld-linux-x86-64.so.2 -  ld-2.17.so\n[root@wuklab13: lib64]   It turns out there is a bug in mmap code: forgot to increment the file ref count when a file-backed vma is created. Some put_file in loader accidentally free the ld-linux file. Bug fixed, dyloader works like a charm.", 
            "title": "02/27 Tue"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0224-sat", 
            "text": "Well. PhDs do not have weekends.\nAnyway, it is Saturday after all, relaxed a little bit. I was looking into the pcache issue.\nAlso added our own kernel version strace.", 
            "title": "02/24 Sat"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0223-fri", 
            "text": "", 
            "title": "02/23 Fri"
        }, 
        {
            "location": "/lego/log/log-02-2018/#solved-fpu-bug", 
            "text": "current  is fine. I should not compare the old implementation with the new per-cpu current. I forgot that the kernel stack is switched in the  __switch_to_asm . This means in  __switch_to() , we are actually using the  next_p s kernel stack. So there is small time frame, where  current_thread_info()  points to  next_p , while  current_task  is still  prev_p . Since interrupts are disabled during context switch, we are good with this mismatch.  Rule out current, the only thing left is  fpu__copy  warning, which happens during  copy_process() . One weird thing is this function has been called multiple times before it showed a warning. System itself use this function to create a lot background threads, which are fine. Only when it was triggered by  sys_clone  then we have the warning:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 [   3213.055639 ]   CPU :   6   PID :   17   sys_clone + 0x0 / 0x30  [   3213.056584 ]   new   task_struct :   ffff88083e4c9838  [   3213.057530 ]   arch_dup_task_struct   cpu6   dst : ffff88083e4c9838   17   word_count - seq   src : ffff88083e457838   17   word_count - seq  [   3213.059536 ]   TRAP   do_general_protection   in   CPU6 ,   error_code :   0   current : ffff88083e457838   17   word_count - seq  [   3213.061289 ]   fixup_exception   pid ( 17 )   cpu ( 6 )   insn : 0xffffffff81009a21 ( fpu__copy + 0x81 / 0x260 )   fixup : 0xffffffff8105d9b2 ( __fixup_text_start + 0xc2 / 0x322 )   handler : ex_handler_default + 0x0 / 0x20  [   3213.064114 ]   ------------ [   cut   here   ] ------------  [   3213.065040 ]   WARNING :   CPU :   6   PID :   17   at   . / arch / x86 / include / asm / fpu / internal . h : 354   fpu__copy + 0xc3 / 0x260  [   3213.066760 ]   CPU :   6   PID :   17   Comm :   word_count - seq   4.0.0 - lego +   # 6  [   3213.067855 ]   Stack :  [   3213.068424 ]   ffff88083e4c7dd0   ffffffff810124b5   ffff88083e4c9bf8   ffff88083e4c9c38  [   3213.070133 ]   ffff88083e4c9838   00007ff ff7ffd700   ffff88083e4c7de0   ffffffff8101258f  [   3213.071775 ]   ffff88083e4c7e08   ffffffff81009a63   ffff88083e457838   ffff88083e4c9838  [   3213.073419 ]   ffff88083e457838   ffff88083e4c7e40   ffffffff81000ebb   ffff88083e457838  [   3213.075057 ]   ffff880800000011   ffff88083e457a68   00000000003 d0f00   ffff88083e457838  [   3213.076703 ]   Call   Trace :  [   3213.077295 ]   TSK  [   3213.077828 ]   [ ffffffff810124c1 ]   __warn . constprop .0 + 0x91 / 0xd0  [   3213.078855 ]   [ ffffffff8101258f ]   warn_slowpath_null + 0xf / 0x20  [   3213.081653 ]   [ ffffffff81009a63 ]   fpu__copy + 0xc3 / 0x260  [   3213.082543 ]   [ ffffffff81000ebb ]   arch_dup_task_struct + 0x7b / 0x90  [   3213.083667 ]   [ ffffffff8101d32e ]   copy_process + 0x14e / 0x10e0  [   3213.084618 ]   [ ffffffff8103a3c6 ]   ?   n_tty_write + 0x166 / 0x3c0  [   3213.085564 ]   [ ffffffff8101e2e6 ]   do_fork + 0x26 / 0x140  [   3213.086439 ]   [ ffffffff8101e4a0 ]   ?   sys_vfork + 0x40 / 0x40  [   3213.087333 ]   [ ffffffff8101e4a0 ]   ?   sys_vfork + 0x40 / 0x40  [   3213.088232 ]   [ ffffffff8101e4c9 ]   sys_clone + 0x29 / 0x30  [   3213.089109 ]   [ ffffffff8100e719 ]   do_syscall_64 + 0x69 / 0xf0  [   3213.090030 ]   [ ffffffff8100d5ec ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [   3213.091078 ]   EOT  [   3213.091580 ]   --- [   end   trace   0000000000000000   ] ---  [   3213.093250 ]   TRAP   do_general_protection   in   CPU7 ,   error_code :   0   current : ffff88083fd0f008   0   swapper / 7  [   3213.096526 ]   fixup_exception   pid ( 0 )   cpu ( 7 )   insn : 0xffffffff81000c62 ( __switch_to + 0x452 / 0x630 )   fixup : 0xffffffff8105d922 ( __fixup_text_start + 0x32 / 0x322 )   handler : ex_handler_default + 0x0 / 0x20  [   3213.101241 ]   ------------ [   cut   here   ] ------------  [   3213.103285 ]   WARNING :   CPU :   7   PID :   0   at   . / arch / x86 / include / asm / fpu / internal . h : 369   __switch_to + 0x47e / 0x630    So, dig into  fpu__copy() , find out why it fails at this certain point. Glad I have something to dig into.   The instruction leads to GP is: 1 ffffffff8100b0f5 :         48   0f   ae   27               xsave64   ( % rdi )    which is generated by:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 #define XSTATE_XSAVE(st, lmask, hmask, err)                             \\          asm volatile(ALTERNATIVE_2(XSAVE,                               \\                                     XSAVEOPT, X86_FEATURE_XSAVEOPT,      \\                                     XSAVES,   X86_FEATURE_XSAVES)        \\                        \\n                                                \\                        xor %[err], %[err]\\n                              \\                        3:\\n                                              \\                        .pushsection .fixup,\\ ax\\ \\n                      \\                        4: movl $-2, %[err]\\n                             \\                        jmp 3b\\n                                          \\                        .popsection\\n                                     \\                       _ASM_EXTABLE(661b, 4b)                             \\                       : [err]  =r  (err)                                 \\                       :  D  (st),  m  (*st),  a  (lmask),  d  (hmask)    \\                       :  memory )  static   inline   void   copy_xregs_to_kernel ( struct   xregs_state   * xstate )  { \n         u64   mask   =   - 1 ; \n         u32   lmask   =   mask ; \n         u32   hmask   =   mask     32 ; \n         int   err ; \n\n         WARN_ON ( ! alternatives_patched ); \n\n         XSTATE_XSAVE ( xstate ,   lmask ,   hmask ,   err ); \n\n         /* We should never fault when copying to a kernel buffer: */ \n         WARN_ON_FPU ( err );  }    From SDM on  XSAVE :  Use of a destination operand not aligned to 64-byte boundary (in either 64-bit or 32-bit modes) results in a general-protection (#GP) exception. In 64-bit mode, the upper 32 bits of RDX and RAX are ignored.  %rdi  is  struct xregs_state *xstate  in above code. Thus, check if  xstate  if 64-bytes aligned. Of course, it is not: 1 [10894.999997] copy_xregs_to_kernel CPU6 xstate: ffff88083e4c8c38   Hehe. Criminal identified. But why? The xstate structure is already marked as  __attribute__(aliged 64)  in the code.  It is the task_struct , which is  NOT  0x40 aligned. But god why? Because we currently use  kmalloc  to allocate new task_struct, whose minimum alignment is  8 bytes . Anyway, use  __alloc_pages  instead.  Such an deeply hidden bug. Took me almost a month to find out.", 
            "title": "Solved FPU BUG"
        }, 
        {
            "location": "/lego/log/log-02-2018/#ib", 
            "text": "Seen this during boot (at both P and M, although lego continue running correctly):  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 [ 54017.712533 ]   ***      NodeID      Hostname      LID      QPN  [ 54017.770776 ]   ***      -------------------------------------  [ 54017.834220 ]   ***           0      wuklab12       13       72  [ 54017.892462 ]   ***           1      wuklab14       16       72   ---  [ 54017.955906 ]   ***           2      wuklab16       20       74  [ 54018.014149 ]   ***  [ 54074.552844 ]   ***    Start   establish   connection   ( mynodeid :   1 )  [ 54102.554407 ]   ib_process_mad   mad_ifc   fails  [ 54130.960691 ]   ***    recvpollcq   runs   on   CPU2  [ 54131.070918 ]   ***    Successfully   built   QP   for   node    0   [ LID :   13   QPN :   72 ]  [ 54131.152936 ]   ***    Successfully   built   QP   for   node    2   [ LID :   20   QPN :   74 ]  [ 54161.228245 ]   ***    FIT   layer   ready   to   go !  [ 54161.272034 ]   ***   \nAnother one:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 [   1966.930409 ]   ***  [   1966.951210 ]   ***    FIT_initial_timeout_s :     30  [   1967.002168 ]   ***    FIT_local_id :              0  [   1967.052087 ]   ***  [   1967.072887 ]   ***      NodeID      Hostname      LID      QPN  [   1967.131126 ]   ***      -------------------------------------  [   1967.194567 ]   ***           0      wuklab12       13       72   ---  [   1967.258005 ]   ***           1      wuklab14       16       72  [   1967.316244 ]   ***           2      wuklab16       20       74  [   1967.374484 ]   ***  [   2032.926448 ]   ***    Start   establish   connection   ( mynodeid :   0 )  [   2032.996068 ]   Fail   to   modify   qp [ 6 ]  [   2033.032572 ]   Fail   to   do   client_init_ctx  [   2033.077287 ]   client_establish_conn :   ctx             ( null )   fail   to   init_interface  [   2033.164646 ]   ibapi_establish_conn :   ctx             ( null )   fail   to   init_interface  [   2033.250967 ]   ***  [   2035.620167 ]   BUG :   unable   to   handle   kernel   NULL   pointer   dereference   at   0000000000000004  [   2035.713763 ]   IP :   [ ffffffff8105c589 ]   client_send_reply_with_rdma_write_with_imm + 0x69 / 0x3b0  [   2035.812562 ]   PGD   0  [   2035.836482 ]   Oops :   0002   [ # 1 ]   SMP   PROCESSOR  [   2035.884321 ]   CPU :   0   PID :   1   Comm :   kernel_init   4.0.0 - lego - ys +   # 253  [   2035.955041 ]   RIP :   0010 : [ ffffffff8105c589 ]    [ ffffffff8105c589 ]   client_send_reply_with_rdma_write_with_imm + 0x69 / 0x3b0  ...  [   2037.313267 ]   TSK  [   2037.336146 ]   [ ffffffff8105a377 ]   ibapi_send_reply_timeout + 0x57 / 0x70  [   2037.411025 ]   [ ffffffff81033d24 ]   ?   net_send_reply_timeout + 0x94 / 0x132  [   2037.486944 ]   [ ffffffff81033d24 ]   net_send_reply_timeout + 0x94 / 0x132", 
            "title": "IB"
        }, 
        {
            "location": "/lego/log/log-02-2018/#pcache", 
            "text": "Running word_count-pthread, with 100MB dataset, finally got some reasonable bug:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 [ 54211.243181 ]   pcache_evict_line () :   pset :   ffff88207f86e3c0 ,   for   uva :   0x7ffff1b8f000  [ 54211.385654 ]   pcache : ffff88207f86e3a8   mapcount : 8   refcount : 0   flags :()  [ 54211.510447 ]   pcache   dumped   because :   PCACHE_BUG_ON_PCM ( ! PcacheLocked ( pcm ))  [ 54212.080336 ]   BUG :   failure   at   managers / processor / pcache / evict . c : 240 / pcache_evict_line () !  [ 54212.664785 ]   Kernel   Panic   -   not   syncing :   BUG !  [ 54212.715742 ]   CPU :   8   PID :   81   Comm :   word_count - pthr   4.0.0 - lego - ys +   # 252  ...  [ 54213.391706 ]   TSK  [ 54213.414584 ]   [ ffffffff81024180 ]   panic + 0xc2 / 0xeb  [ 54213.524818 ]   [ ffffffff8101b81c ]   ?   task_tick_rt + 0x2c / 0xd0  [ 54213.589295 ]   [ ffffffff81018f75 ]   ?   scheduler_tick + 0x55 / 0x60  [ 54213.655850 ]   [ ffffffff81016625 ]   ?   tick_handle_periodic + 0x45 / 0x70  [ 54213.728647 ]   [ ffffffff81006634 ]   ?   apic_timer_interrupt + 0x54 / 0x90  [ 54213.801443 ]   [ ffffffff8100e22a ]   ?   smp__apic_timer_interrupt + 0x6a / 0x70  [ 54213.879439 ]   [ ffffffff8101256d ]   ?   printk + 0x11d / 0x1b0  [ 54214.103027 ]   [ ffffffff8102ecf4 ]   pcache_evict_line + 0x134 / 0x220  [ 54214.172703 ]   [ ffffffff8102c6ae ]   pcache_alloc + 0x22e / 0x2e0  [ 54214.237179 ]   [ ffffffff8102be0a ]   common_do_fill_page + 0x2a / 0x1f0  [ 54214.307895 ]   [ ffffffff8102baf0 ]   ?   move_page_tables + 0x4c0 / 0x4c0  [ 54214.378612 ]   [ ffffffff8102c172 ]   pcache_handle_fault + 0x1a2 / 0x3a0  [ 54214.450367 ]   [ ffffffff8100fc02 ]   do_page_fault + 0xa2 / 0x1a0  [ 54214.514843 ]   [ ffffffff8100d85f ]   page_fault + 0x1f / 0x30  [ 54214.575161 ]   [ ffffffff81034842 ]   ?   copy_user_enhanced_fast_string + 0x2 / 0x10  [ 54214.657316 ]   [ ffffffff81032368 ]   ?   seq_read + 0x248 / 0x360  [ 54214.719714 ]   [ ffffffff810307af ]   sys_read + 0x3f / 0xc0  [ 54214.777949 ]   [ ffffffff81030770 ]   ?   sweep_pset_lru + 0x220 / 0x220  [ 54214.846587 ]   [ ffffffff8100e619 ]   do_syscall_64 + 0x69 / 0xf0  [ 54214.910022 ]   [ ffffffff8100d4ec ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [ 54214.985939 ]   EOT    Another one: 1\n2\n3 [    735.393244 ]   pcache_evict_line () :   pset :   ffff88207f86e3c0 ,   for   uva :   0x7ffff1b8fd90  [    735.537804 ]   pcache : ffff88207f86e3a8   mapcount : 8   refcount : 0   flags :()  [    735.663642 ]   pcache   dumped   because :   PCACHE_BUG_ON_PCM ( ! PcacheLocked ( pcm ))    Do note this happens after computation. This happens when phoenix create a lot threads to sort the results.  Both bug happen to the same set, same user page. The pcache is clearly corrupted:  mapcount : 8 ,   refcount : 0 ,   flags :().  Come back after dinner.\nRemember to check altenative, cause the XSAVE above should be XSAVEOPT. Make sure it does not override other memory. Also, check linker script. Do not forget to link any sections.  Another several bug logs in wuklab13 and wuklab15:  022318-* . I m really tired today after fixing the FPU bug. But I m also pretty confident pcache is something I m able to debug. Even thought it is hard in SMP case.  Anyway, I gonna call for the day.", 
            "title": "pcache"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0222-thur", 
            "text": "context switch fpu  signal compat check, all good.   make  current  use percpu current_task, so all code in Lego is consistent.  checked  entry_SYSCALL-64  again, which looks good to me.  The only concern is  rsp_scratch  and  current_top_of_stack , which are per-cpu variables. If these per-cpu is setup wrong, then we are doomed.  Also check if per-cpu is all cleared up?  try big syscall lock  does x86 has to use different kernel stacks? Interrupt is using different stack in Linux, has to do so???  check current is correct. compare with old implementation.   First of all, FPU is definitely functional for now.\nSince I replaced the current macro today, I add some code to check if this current matches our old implementation:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 static __always_inline struct task_struct *get_current(void)                                                           \n{                                                                                                                      \n        return this_cpu_read_stable(current_task);                                                                     \n}\n\n//#define current get_current()\n\n#define current                                                 \\\n({                                                              \\\n        struct task_struct *old = current_thread_info()- task;  \\\n        struct task_struct *new = get_current();                \\\n                                                                \\\n        if (old != new) {                                       \\\n                printk( %s:%d() cpu:%d old:%pS %d %s new:%pS %d %s\\n ,  \\\n                        __func__, __LINE__, smp_processor_id(), old, old- pid, old- comm, \\\n                        new, new- pid, new- comm);              \\\n                BUG();                                          \\\n        }                                                       \\\n        get_current();                                          \\\n})   Combined with some FPU warning, it is now like this:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 [   3273.748819 ]   CPU : 5   PID : 32     sys_clone + 0x0 / 0x30  [   3273.800808 ]   alloc_task_struct_node :   size : 740   ffff88107e831838  [   3273.869451 ]   arch_dup_task_struct ()   CPU5   current : 32   new :   ffff88107e831838   old :   ffff88107e827838   32  [   3273.975533 ]   ------------ [   cut   here   ] ------------  [   3274.030651 ]   WARNING :   CPU :   5   PID :   32   at   . / arch / x86 / include / asm / fpu / internal . h : 354   fpu__copy + 0xe2 / 0x310  [   3274.140895 ]   CPU :   5   PID :   32   Comm :   word_count - pthr   4.0.0 - lego - ys - gdbe6dbe - dirty   # 249  [   3274.231377 ]   Stack :  [   3274.255298 ]   ffff88107e82fd68   ffffffff81016dbf   00000000ff ffffff   0000000000000000  [   3274.342659 ]   00000000ff ffffff   0000000000000000   ffff88107e831bf8   ffff88107e831c38  [   3274.430021 ]   ffff88107e831838   000000207f e64000   ffff88107e82fd78   ffffffff810170af  [   3274.517382 ]   ffff88107e82fdc0   ffffffff8100b052   0000000000000020   ffff88107e831838  [   3274.604745 ]   ffff88107e827838   ffff88107e827838   ffff88107e831838   ffff88107e827838  [   3274.692106 ]   Call   Trace :  [   3274.721229 ]   TSK  [   3274.744109 ]   [ ffffffff81016dd8 ]   __warn . constprop .0 + 0xe8 / 0x3b0  [   3274.813790 ]   [ ffffffff810170af ]   warn_slowpath_null + 0xf / 0x20  [   3274.881391 ]   [ ffffffff8100b052 ]   fpu__copy + 0xe2 / 0x310  [   3274.941713 ]   [ ffffffff810012e4 ]   arch_dup_task_struct + 0x84 / 0x120  [   3275.013475 ]   [ ffffffff81022c10 ]   copy_process + 0x160 / 0x1e60  [   3275.078996 ]   [ ffffffff81024936 ]   do_fork + 0x26 / 0x140  [   3275.137238 ]   [ ffffffff81024af0 ]   ?   sys_vfork + 0x40 / 0x40  [   3275.198599 ]   [ ffffffff81024af0 ]   ?   sys_vfork + 0x40 / 0x40  [   3275.259960 ]   [ ffffffff81024b19 ]   sys_clone + 0x29 / 0x30  [   3275.319242 ]   [ ffffffff81012314 ]   do_syscall_64 + 0x84 / 0x240  [   3275.383723 ]   [ ffffffff8101106c ]   entry_SYSCALL64_slow_path + 0x25 / 0x25  [   3275.459645 ]   EOT  [   3275.482526 ]   --- [   end   trace   0000000000000000   ] ---  [   3275.537648 ]   wake_up_new_task   CPU5   task : ffff88107e831838 ,   dest_cpu : 6   current : 32  [   3275.623970 ]   SMP   IPI :   reschedule_interrupt ()   CPU ( 6 )   PID ( 0 )  [   3275.739412 ]   do_general_protection : 186 ()   cpu : 6   old : 0xffff88107e831838   33   word_count - pthr   new : 0xffff88107fcaf008   0   swapper / 6   [   3275.871493 ]   ------------ [   cut   here   ] ------------  [   3275.926614 ]   BUG :   failure   at   arch / x86 / kernel / traps . c : 186 / do_general_protection () !  [   3276.015018 ]   Kernel   Panic   -   not   syncing :   BUG !  [   3276.065978 ]   panic : 107 ()   cpu : 6   old : 0xffff88107e831838   33   word_count - pthr   new : 0xffff88107fcaf008   0   swapper / 6    Based on the switch code: 1\n2\n3\n4\n5\n6\n7 __switch_to ( struct   task_struct   * prev_p ,   struct   task_struct   * next_p )  { \n         this_cpu_write ( current_task ,   next_p ); \n\n         /* Reload sp0 This changes current_thread_info(). */ \n         load_sp0 ( tss ,   next );  }    Based on log line 30,  load_sp0()  already happened, which means  this_cpu_write(..)  happened too. If  this_cpu_write(..)  happened, then log line 30 s new should have been updated to  0xffff88107e831838 . Something wrong with percpu?", 
            "title": "02/22 Thur"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0221-wed", 
            "text": "irq_regs, old code, check  signal frame, and fpu hook together Done  in_interrupt() , it is empty, TODO  check arch/x86/Makefile, it introduce a lot FPU flags.  added more than 4K lines today. Damn FPU. Ugh go home sleep.", 
            "title": "02/21 Wed"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0220-tue-cloudy", 
            "text": "Not too many Sunny days recently. Well, continue yesterday s work. I don t think I can easily find out why so many  /proc/memoinfo  open happened. Instead, I m trying to enable the  flush_thread  in P s exec code.  During the way, I found some issue related to  __ARCH_HAS_SA_RESTORER  in signal code. I need to check if these x86 macros are defined, but lego does not port them.  Well, it turns out flush_thread does not make too much difference. Next I m going to try to disable  exit_thread , which uses  fpu__drop() .  Hmm, disable  exit_thread  also does not work.", 
            "title": "02/20 Tue Cloudy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0219-mon-rainy", 
            "text": "It is another week. I can not deny I m a little tired about the bug. Tried so many possible solutions, but none of them work. Well, today I first need to test the vma changes (pgoff and anon_vma) thing. Especially the vma merge and split.  This morning I fixed a bug in kernel_init process: make kernel_init able to run all possible CPUs. Because the first user process is forked from kernel_init, it is quite important that it gets the right cpu affinity: 1\n2\n3\n4\n5\n6 static   int   kernel_init ( void   * unused )  { \n         ... \n         set_cpus_allowed_ptr ( current ,   cpu_possible_mask ); \n         ...  }    Well, interestingly, the unmodified word_count-pthread succeed with 50MB dataset  with or without any DEBUG option! Amazing! I need to find out why the cpus_allowed becomes 0 at the beginning of kernel_init. Because  init_task  actually has: 1\n2      . cpus_allowed     =   CPU_MASK_ALL , \n     . nr_cpus_allowed =   NR_CPUS ,    Things to do next:   check why the cpus_allowed changed  check why word_count-pthread open  /dev/../cpu  so many times. Anything wrong with our  copy_files , or open, close?  here is an idea, to verify if FPU code is correct, run some scientific benchmarks.   Okay, findings:    cpus_allowd is fine, it is reset inside  sched_init() , when it tries make the  init_task  as the  idle  thread. Thus it is reasonable to set cpus_allowed again at  kernel_init  thread. And it should NOTHING to do with the bug.    about the second, check the following log:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88 [ 11838.364543 ]   STDOUT :   --- [  Wordcount :   Running ...  ] ---  [ 11838.422886 ]   STDOUT :   --- [  ] ---  [ 11838.463445 ]   SYSC_open ( cpu5   pid : 32 ) :   f_name :   / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_50MB . txt ,   flags :   0 ,   mode :   900  [ 11838.619460 ]   SYSC_open ( cpu5   pid : 32 ) :   fd :   3  [ 11838.667406 ]   SYSC_open ( cpu5   pid : 32 ) :   f_name :   / sys / devices / system / cpu / online ,   flags :   80000 ,   mode :   0  [ 11838.773351 ]   SYSC_open ( cpu5   pid : 32 ) :   fd :   4  [ 11838.821239 ]   seq_file : \n   dest_uva :   00007ff fffffc8d0 ,   nr_chars :   5 \n   string :   [ 0 - 23  ]  [ 11838.913791 ]   SYSC_close ( cpu5   pid : 32 ) :   fd :   4  [ 11838.962622 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [ 11840.223255 ]   STDOUT :   --- [  Word   Count :   Computation   Completed   1.555581   sec  ] ---  [ 11840.309678 ]   SYSC_open ( cpu5   pid : 32 ) :   f_name :   / sys / devices / system / cpu / online ,   flags :   80000 ,   mode :   0  [ 11840.415754 ]   SYSC_open ( cpu5   pid : 32 ) :   fd :   4  [ 11840.463593 ]   seq_file : \n   dest_uva :   00007ff fffffc8a0 ,   nr_chars :   5 \n   string :   [ 0 - 23  ]  [ 11840.556147 ]   SYSC_close ( cpu5   pid : 32 ) :   fd :   4  [ 11840.605024 ]   SYSC_close () :   [ 4 ]   -   [ / sys / devices / system / cpu / online ]  [ 11840.677821 ]   STDOUT :   --- [  THe   number   of   processors   is   24  \u00f4  ] ---  [ 11840.753769 ]   SYSC_open ( cpu7   pid : 80 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11840.844212 ]   SYSC_open ( cpu19   pid : 92 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11840.935728 ]   SYSC_open ( cpu7   pid : 80 ) :   fd :   4  [ 11840.983567 ]   SYSC_open ( cpu19   pid : 92 ) :   fd :   5  [ 11841.032444 ]   seq_file : \n   dest_uva :   00007ff ff444c000 ,   nr_chars :   172 \n   string :   [ MemTotal :         115355128   kB  MemFree :          115355128   kB  MemAvailable :     115355128   kB  DirectMap4k :          5812   kB  DirectMap2M :       1861632   kB  DirectMap1G :      134217728   kB  ]  [ 11841.305953 ]   seq_file : \n   dest_uva :   00007ff ff444b000 ,   nr_chars :   172 \n   string :   [ MemTotal :         115355128   kB  MemFree :          115355128   kB  MemAvailable :     115355128   kB  DirectMap4k :          5812   kB  DirectMap2M :       1861632   kB  DirectMap1G :      134217728   kB  ]  [ 11841.579460 ]   SYSC_close ( cpu7   pid : 80 ) :   fd :   4  [ 11841.628339 ]   SYSC_close ( cpu19   pid : 92 ) :   fd :   5  [ 11841.678257 ]   SYSC_close () :   [ 4 ]   -   [ / proc / meminfo ]  [ 11841.733375 ]   SYSC_close () :   [ 5 ]   -   [ / proc / meminfo ]  [ 11841.788493 ]   SYSC_open ( cpu18   pid : 91 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11841.880008 ]   SYSC_open ( cpu6   pid : 102 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11841.971523 ]   SYSC_open ( cpu12   pid : 85 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11842.063040 ]   SYSC_open ( cpu0   pid : 97 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11842.153516 ]   SYSC_open ( cpu14   pid : 87 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11842.245032 ]   SYSC_open ( cpu16   pid : 89 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11842.336548 ]   SYSC_open ( cpu4   pid : 100 ) :   f_name :   / proc / meminfo ,   flags :   80000 ,   mode :   1 b6  [ 11842.428064 ]   SYSC_open ( cpu16   pid : 89 ) :   fd :   9  [ 11842.476942 ]   SYSC_open ( cpu4   pid : 100 ) :   fd :   10  [ 11842.526860 ]   seq_file : \n   dest_uva :   00007ff ff444c000 ,   nr_chars :   172 \n   string :   [ MemTotal :         115355128   kB  MemFree :          115355128   kB  MemAvailable :     115355128   kB  DirectMap4k :          5812   kB  DirectMap2M :       1861632   kB  DirectMap1G :      134217728   kB  ]  [ 11842.800368 ]   seq_file : \n   dest_uva :   00007ff ff444b000 ,   nr_chars :   172 \n   string :   [ MemTotal :         115355128   kB  MemFree :          115355128   kB  MemAvailable :     115355128   kB  DirectMap4k :          5812   kB  DirectMap2M :       1861632   kB  DirectMap1G :      134217728   kB  ]  [ 11843.073877 ]   SYSC_close ( cpu16   pid : 89 ) :   fd :   9      However, in a normal Linux exeution:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 strace   - C   - o   strace_2   . / word_count - pthread   . / word_count_datafiles / word_50MB . txt  %   time       seconds    usecs / call       calls      errors   syscall  ------   -----------   -----------   ---------   ---------   ---------------- \n  86.41      0.052074          1736          30             futex \n   6.89      0.004151            67          62             munmap \n   2.47      0.001490            17          88             mmap \n   2.12      0.001278            14          93             clone \n   1.51      0.000912            14          64             mprotect \n   0.19      0.000117             7          16             write     0.15      0.000092            46           2             open   $   cat   strace_2   |   grep   open     open ( ./word_count_datafiles/word_50MB.txt ,   O_RDONLY )   =   3     open ( /sys/devices/system/cpu/online ,   O_RDONLY | O_CLOEXEC )   =   4      It opened the  /proc/meminfo  for way too many times. In the normal Linux execution, this should not happen. Is it because our meminfo is faked, so glibs is complaining? But why it does not open meminfo while running in Linux? Or does our entry assembly messed up some stuff in stack, so the return path changed?    oh, about the FPU. It reminds our  flush_thread  function actually has an issue before. When I enabled this function during loading in P, the P will crash. Within  flush_thread , there is a  fpu_clear !!! So, check this tomorrow! (12:00am, need to go home)", 
            "title": "02/19 Mon Rainy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0218-sun-sunny", 
            "text": "It is a nice day. Yesterday I ve changed one line of code in mmap code path: change anonymous vma s pgoff from some value to 0. The result is I got several succeed work-count-pthread(bind to one core) testing. However, it still fail with unmodified word-count-pthread.  It brings me to inspect pgoff manipulation code and all mmap.c code. We ported everything from linux without almost zero modification. That means we ported all those useless  anon_vma  and pgoff code, which is used a lot by vma_merge, vma_split code. The thing is: our memory manager, our vma code do not need such  anon_vma  structure, and do not maintain pgoff. Thus, I m a little bit worried linux code may doing some crazy behind our back: mess vma and pages, then pcache miss gets some wrong pages  Well. Lego does not use  anon_vma , and pgoff should only be used by file-backed vma. So, I decided to remove  anon_vma  from our code, and make sure pgoff is used properly. Of course, the goal is to make vma_merge, split,\ncopy, do the things we intended.  Lesson learned.", 
            "title": "02/18 Sun Sunny"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0217-sat-snowy", 
            "text": "Fixed the bss bug. It comes from loader. We did not implement the  lego_clear_user  function, so some part of bss is non-zero.  Bad news is word_count-pthread still fail at same fpu instruction. Have to look into memory code more.  This is actually a fun debugging story. We should always add TODO or XXX or some warnings to unfinished code, no matter what. Lesson learned.", 
            "title": "02/17 Sat Snowy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0216-fri-cloudy", 
            "text": "Yilun found a major loader bug yesterday: the  .bss  section variables are not 0, in the  iozone  benchmark. I did not encounter this issue before with simple test program. This is pretty serious.", 
            "title": "02/16 Fri Cloudy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0215-thur-rainy", 
            "text": "Today is Chinese New Year.  Line 7 and 8 show the uva belong to the same page. Need to revisit  get_arg_pages  etc functions.  1\n2\n3\n4\n5\n6\n7\n8 [  108.393991] handle_p2m_execve(): pid:22,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread\n[  108.395255]     argc[0] (len: 65):  /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread\n[  108.396329]     argc[1] (len: 82):  /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt\n[  108.397530]     envc[0] (len:  7):  HOME=/\n[  108.398069]     envc[1] (len: 11):  TERM=linux\n[  108.398640] __bprm_mm_init vma: ffff88083effe6b8 [  108.399226] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffefed [  108.399949] faultin_page vma: ffff88083effe6b8 uva: 0x7fffffffef94   Well, this is 100% fine. I wrote this loader code long time ago and need some time to pickup. So, after I read the loader code, especially the  copy_strings  function, I found this is okay. Because copy_strings will be invoked three times, so the  faultin_page  basically will be invoked at least three times. That is why it went to that pte fault handling code.  Although actually I think  copy_strings  should  not  use  faultin_page , instead, it should use  get_user_pages , which will walk through the pgtable first, then went to  handle_lego_mm_fault .", 
            "title": "02/15 Thur Rainy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0214-wed-rainy", 
            "text": "Hmm, tried to make kmalloc behave as kzalloc, and bind all threads to one core, still gave the same old bug:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14   42731a:       f3 0f 6f 16             movdqu (%rsi),%xmm2 \n  [93182.657376] word_count-pthr[85] general protection ip:42731a sp:7fffe3ffed28 error:0\n  [93182.747959] CPU: 8 PID: 85 Comm: word_count-pthr 4.0.0-lego+ #170\n  [93182.820758] RIP: 0033:[ 000000000042731a ]  [ 000000000042731a ] 0x42731a\n  [93182.901878] RSP: 002b:00007fffe3ffed28  EFLAGS: 00010283\n  [93182.965317] RAX: 000000000000001f RBX: 00007ffff001b010 RCX: 0000000000000005\n  [93183.050596] RDX: 0000000000000000 RSI: 5345485355420045 RDI: 00007ffff294791f\n  [93183.135876] RBP: 00007ffff294791f R08: 000000000000ffff R09: 0000000000000008\n  [93183.221156] R10: fffffffffffff048 R11: 00000000004acfc0 R12: 0000000000001cde\n  [93183.306435] R13: 00000000006e4a8c R14: 0000000000001cd7 R15: 0000000000001cda\n  [93183.391716] FS:  00007fffe3fff700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000\n  [93183.488434] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  [93183.557075] CR2: 00007ffff27a4000 CR3: 000000107e924000 CR4: 00000000000406a0    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14   427377:       66 0f 6f 17             movdqa (%rdi),%xmm2 \n  [93180.527248] word_count-pthr[93]: segfault at 0x0 ip 0000000000427377 sp 00007fffdfff6d28 error 4\n  [93180.630314] CPU: 8 PID: 93 Comm: word_count-pthr 4.0.0-lego+ #170\n  [93180.703114] RIP: 0033:[ 0000000000427377 ]  [ 0000000000427377 ] 0x427377\n  [93180.784234] RSP: 002b:00007fffdfff6d28  EFLAGS: 00010297\n  [93180.847674] RAX: 0000000000000000 RBX: 000000000073c4c0 RCX: 000000000000000d\n  [93180.932953] RDX: 000000000000ffff RSI: 00007ffff4999070 RDI: 0000000000000000\n  [93181.018233] RBP: 00007ffff499907d R08: 000000000000ffff R09: 0000000000000000\n  [93181.103513] R10: 0000000000427760 R11: 00007ffff49982c0 R12: 0000000000000118\n  [93181.188791] R13: 00000000006e4aac R14: 0000000000000116 R15: 0000000000000117\n  [93181.274072] FS:  00007fffdfff7700(0000) GS:ffff88107fc80000(0000) knlGS:0000000000000000\n  [93181.370790] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  [93181.439430] CR2: 0000000000000000 CR3: 000000107e924000 CR4: 00000000000406a0   Tried several ways to ensure memory safety. It still failed even if I enabled all of them. So, I guess the memory safety is ensured? Still some other things?   force  alloc_pages  to use  __GFP_ZERO  make  kmalloc  behave as  kzalloc  make  kfree  empty   I also suspect  munmap  may free extra wrong pgtable entries. Although I ve went through all the code and checked, but in addition to the above things, I m going to:   make munmap dummy (no p2m_munmap, return 0 directly)   Failed.  Next, I m going to:   add checksum for every page transferred across network.  add warning for unnormal cases   Bang! I found something while running P+M:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31 [    115.727597 ]   Memory - component   manager   is   up   and   running .  [    116.691723 ]   handle_p2m_fork () :   nid : 0 , pid : 22 , tgid : 22 , parent_tgid : 1  [    116.697038 ]   handle_p2m_fork () :   reply :   0 : OKAY  [    116.791088 ]   handle_p2m_execve () :   pid : 22 , argc : 2 , envc : 2 , file : / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread  [    116.792357 ]       argc [ 0 ]   ( len :   65 ) :    / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count - pthread  [    116.793439 ]       argc [ 1 ]   ( len :   82 ) :    / root / ys / phoenix / phoenix - 2.0 / tests / word_count / word_count_datafiles / word_100MB . txt  [    116.794653 ]       envc [ 0 ]   ( len :    7 ) :    HOME =/  [    116.795196 ]       envc [ 1 ]   ( len :   11 ) :    TERM = linux  [    116.795772 ]   __bprm_mm_init   vma :   ffff88083effe6b8  [    116.796209 ]   faultin_page   vma :   ffff88083effe6b8  [    116.796729 ]   faultin_page   vma :   ffff88083effe6b8  [    116.797150 ]   handle_pte_fault   vma :   ffff88083effe6b8   entry :   0xffff88083e8c1067  [    116.798044 ]   pte : ffff88083e8c0ff0   pfn : 0x8083e8c1   flags :( present | writable | user | accessed | dirty | softw4 | pkey0 | pkey1 | pkey2 | pkey3 | nx | 0x3ff800000000000 )  [    116.799462 ]   ------------ [   cut   here   ] ------------  [    116.800049 ]   WARNING :   CPU :   4   PID :   15   at   managers / memory / vm / fault . c : 148   handle_lego_mm_fault + 0x4d8 / 0x550  [    116.801148 ]   CPU :   4   PID :   15   Comm :   mc - manager   4.0.0 - lego +   # 78  [    116.801818 ]   Stack :  [    116.802179 ]   ffff88083e893c50   ffffffff8100e827   00007ff fffffef94   ffff88083effe6b8  [    116.803283 ]   ffff88083e894008   ffff88083e8c1067   ffff88083e893c60   ffffffff8100e91f  [    116.804387 ]   ffff88083e893cf0   ffffffff8102b008   0000000000000031   ffff88083e893cf0  [    116.805488 ]   00000000000002 96   00003ff fffe00000   ffff800000000067   ffff88083e893d50  [    116.806590 ]   ffff880000000001   ffffffff81066798   ffff88083effe6b8   ffff88083e893d50  [    116.807691 ]   Call   Trace :  [    116.808087 ]   TSK  [    116.808448 ]   [ ffffffff8100e836 ]   __warn . constprop .0 + 0xa6 / 0x100  [    116.809126 ]   [ ffffffff8100e91f ]   warn_slowpath_null + 0xf / 0x20  [    116.809802 ]   [ ffffffff8102b008 ]   handle_lego_mm_fault + 0x4d8 / 0x550  [    116.810505 ]   [ ffffffff8102cfe3 ]   faultin_page + 0x43 / 0xb0  [    116.811131 ]   [ ffffffff8102dab1 ]   copy_strings . isra .1 + 0xe1 / 0x130  [    116.811819 ]   [ ffffffff8102dd1e ]   exec_loader + 0x21e / 0x350  [    116.812457 ]   [ ffffffff8102680a ]   handle_p2m_execve + 0x1aa / 0x290    This is a temporary stack vma that loader created for saving argv and envp. So, this vma was created here:  1\n2\n3\n4\n5\n6 static   int   __bprm_mm_init ( struct   lego_binprm   * bprm )  { \n         ... \n         bprm - vma   =   vma   =   kzalloc ( sizeof ( * vma ),   GFP_KERNEL ); \n         ...  }    And then  copy_strings  will call  faultin_page  to populate a page for a specific user virtual adddress:  1\n2\n3\n4\n5\n6\n7 int   faultin_page ( struct   vm_area_struct   * vma ,   unsigned   long   start , \n                  unsigned   long   flags ,   unsigned   long   * kvaddr )  { \n         ... \n         ret   =   handle_lego_mm_fault ( vma ,   start ,   flags ,   kvaddr ); \n         ...  }    Eventually, the  handle_lego_mm_fault  will call  handle_pte_fault :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 static   int   handle_pte_fault ( struct   vm_area_struct   * vma ,   unsigned   long   address , \n                             unsigned   int   flags ,   pte_t   * pte ,   pmd_t   * pmd , \n                             unsigned   long   * mapping_flags )  { \n         ... \n         if   ( ! pte_present ( entry ))   { \n                 ... \n         } \n\n         pr_info ( %s vma: %p entry: %#lx \\n ,   FUNC ,   vma ,   entry . pte ); \n         dump_pte ( pte ,   NULL ); \n         WARN_ON_ONCE ( 1 ); \n         ...  }    Apparently, pte is wrong! But I don t have time today. Continue tomorrow.\nHmm forgot that we are saving kernel virtual addresses in the pte. Just take a quick look at the lego_pud_alloc things, seems will have some issues. I defenitly need to check all these stuff tomorrow. I ve not touch this part for too long!", 
            "title": "02/14 Wed Rainy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0213-tue-sunny", 
            "text": "Checking our SLOB allocator today. So I found Yutong s code is using  set_page_private  when slob get a new page from buddy. This private field is only intended to be used by buddy to record the  order . This mixed usage will confuse buddy and create bug.  Even though I removed the  set_page_private ( page ,   0 )  after  free_page , word_count-pthread still fails. Damn.", 
            "title": "02/13 Tue Sunny"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0212-mon-cloudy", 
            "text": "Add this commit  4cb3a8b6a943c90714fd9bb5e5465ee315f0aa30 :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15     memory: Use kzalloc instead of kmalloc in __bprm_mm_init (loader)\n\n    This was an potentionl bug that was not triggered previously.\n    It is simply because kmalloc ed vma contains some garbage area,\n    while later in the pgfault code, we use\n            if (vma- vm_ops   vma- vm_ops- fault)\n                    ...\n    to check if it is an file-backed fault.\n\n    Fortunately the vma- vm_ops happens to have some leftover value.\n    So this bug was triggered.\n\n    This actually reminds me that this is a series of potential bugs!\n    Even though before I ve added things like force GFP_ZERO in all\n    physical page allocation, I missed the kmalloc s case!   The story is:  I patched the stop_machine code today, and tried to run code with P+M on VM, everything works fine. However, when I tried to run the new code with P+M+S on physical machine, M crashed at a very weird point:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 [ 7791.998168] handle_p2m_execve(): pid:81,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread\n[ 7792.129312] BUG: unable to handle kernel NULL pointer dereference at 0000000000000031\n[ 7792.222889] IP: [ ffffffff8102c180 ] handle_lego_mm_fault+0x160/0x4b0\n[ 7792.299842] PGD 0\n[ 7792.323760] Oops: 0000 [#1] PREEMPT SMP MEMORY\n[ 7792.376794] CPU: 4 PID: 79 Comm: mc-manager 4.0.0-lego+ #29 [ 7792.443349] RIP: .. [ ffffffff8102c180 ] handle_lego_mm_fault+0x160/0x4b0 ......\n....\n[ 7793.750506] Call Trace:\n[ 7793.779623]  TSK  [ 7793.802501] [ ffffffff810053f4 ] ? apic_timer_interrupt+0x54/0x90 [ 7793.875295] [ ffffffff8102e469 ] faultin_page+0x9/0x70 [ 7793.936649] [ ffffffff8102ef01 ] copy_strings.isra.1+0xe1/0x130 [ 7794.007362] [ ffffffff8102f11e ] exec_loader+0x1ce/0x340 [ 7794.070796] [ ffffffff81027def ] handle_p2m_execve+0x12f/0x200 [ 7794.140469] [ ffffffff810274fb ] mc_manager+0x1ab/0x2b0\n[ 7794.202864] [ ffffffff81027350 ] ? bitmap_fill+0x33/0x33\n[ 7794.266298] [ ffffffff8101c6b7 ] kthread+0x107/0x130\n[ 7794.325572] [ ffffffff8101c5b0 ] ? __kthread_parkme+0x90/0x90\n[ 7794.394205] [ ffffffff8100b462 ] ret_from_fork+0x22/0x30   So faulting source code is: 1\n2\n3\n4\n5\n6\n7\n8 static   int   handle_pte_fault ( struct   vm_area_struct   * vma ,   unsigned   long   address , \n                             unsigned   int   flags ,   pte_t   * pte ,   pmd_t   * pmd )  { \n     ....           if   ( vma - vm_ops     vma - vm_ops - fault )                   return   do_linear_fault ( vma ,   address ,   flags ,                                          pte ,   pmd ,   entry ) \n     ....    Something wrong with  vma ? At this loader stage, this vma is a temporaty stack vma created for saving  argv  and  envp . So I look back into the code that created this vma:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 managers / memory / loader / core . c :  static   int   __bprm_mm_init ( struct   lego_binprm   * bprm )  { \n         int   err ; \n         struct   vm_area_struct   * vma   =   NULL ; \n         struct   lego_mm_struct   * mm   =   bprm - mm ;           bprm - vma   =   vma   =   kmalloc ( sizeof ( * vma ),   GFP_KERNEL );           if   ( ! vma ) \n                 return   - ENOMEM ;    The code after this does NOT do necessary cleanup. The  vm_ops  happens to have some garbage value from last user. So it is not 0, so the above  vma- vm_ops  is true, and it will try to read  vma- vm_ops- fault . And that, my friend, is where garbage turns into crash.  This presents a series of potential bugs. Ugh,  memory safety !", 
            "title": "02/12 Mon Cloudy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0209-fri-cloudy", 
            "text": "Tried to modify Phoneix code: replace  realloc  with  malloc+mempcy . Thus the  mremap  syscall is avoided, but it still has general protection fault. Same with yesterday, corrupted at  __strcmp_sse42 , with corrupted  RSI  or  RDI . So I guess it is not about  mremap  itself at all. I will follow yesterday s checking list.", 
            "title": "02/09 Fri Cloudy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0208-thur-cloudy", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38 00000000004272d0  __strcmp_sse42 :\n\n  4272d0:       89 f1                   mov    %esi,%ecx\n  4272d2:       89 f8                   mov    %edi,%eax\n  4272d4:       48 83 e1 3f             and    $0x3f,%rcx\n  4272d8:       48 83 e0 3f             and    $0x3f,%rax\n  4272dc:       83 f9 30                cmp    $0x30,%ecx\n  4272df:       77 3f                   ja     427320  __strcmp_sse42+0x50 \n  4272e1:       83 f8 30                cmp    $0x30,%eax\n  4272e4:       77 3a                   ja     427320  __strcmp_sse42+0x50 \n  4272e6:       f3 0f 6f 0f             movdqu (%rdi),%xmm1 * 4272ea:       f3 0f 6f 16             movdqu (%rsi),%xmm2   4272ee:       66 0f ef c0             pxor   %xmm0,%xmm0\n  4272f2:       66 0f 74 c1             pcmpeqb %xmm1,%xmm0\n  4272f6:       66 0f 74 ca             pcmpeqb %xmm2,%xmm1\n  4272fa:       66 0f f8 c8             psubb  %xmm0,%xmm1\n  4272fe:       66 0f d7 d1             pmovmskb %xmm1,%edx\n  427302:       81 ea ff ff 00 00       sub    $0xffff,%edx\n  427308:       0f 85 42 0d 00 00       jne    428050  __strcmp_sse42+0xd80 \n  42730e:       48 83 c6 10             add    $0x10,%rsi\n  427312:       48 83 c7 10             add    $0x10,%rdi\n  427316:       66 2e 0f 1f 84 00 00    nopw   %cs:0x0(%rax,%rax,1)\n  42731d:       00 00 00  \n  427320:       48 83 e6 f0             and    $0xfffffffffffffff0,%rsi\n  427324:       48 83 e7 f0             and    $0xfffffffffffffff0,%rdi\n  427328:       ba ff ff 00 00          mov    $0xffff,%edx\n  42732d:       45 31 c0                xor    %r8d,%r8d\n  427330:       83 e1 0f                and    $0xf,%ecx\n  427333:       83 e0 0f                and    $0xf,%eax\n  427336:       66 0f ef c0             pxor   %xmm0,%xmm0\n  42733a:       39 c1                   cmp    %eax,%ecx\n  42733c:       74 32                   je     427370  __strcmp_sse42+0xa0 \n  42733e:       77 07                   ja     427347  __strcmp_sse42+0x77 \n  427340:       41 89 d0                mov    %edx,%r8d\n  427343:       91                      xchg   %eax,%ecx\n  427344:       48 87 f7                xchg   %rsi,%rdi * 427347:       66 0f 6f 17             movdqa (%rdi),%xmm2   (RDI: 0000000000000000)   Frustrating! What is wrong with multithread program? Because of broken FPU-switch code? of inappropriate TLB flush? of IB corrupts memory? of what? ugh?  I m done with this random guess and frustrated general protection or segfault, I need to first make sure underlying kernel is 100%  percent correct, this is a checking list:   fpu save/restore  always fail at some XMM instruction  always with corrupted RDI or RSI    switch_to_asm  %gs and %fs  switch_mm (pgd)  stack frame    set_arch_tls (%fs)  glibc s way of using per thread data    some cpu may miss tlb flush  kernel entry/exit assembly  current_task macro  stack_stratch  per-cpu data in entry.S    futex  clear_tid  set_tid  shared mm  robust list    interrupts  vector array  APIC setup  IO-APIC  timer interrupt    cpu_init and Trampoline  faked kernel version  P side pgfault handling code (SMP)  and M side pgfault handling (SMP)  mremap, munmap  check pgtable boundary    In all, check SMP implications   Is there any code, that is solely used to test if the underlying kernel has appropriate behaviors? Like glibc test code?  How to protect kernel virtual memory? Any existing solutions in Linux?  What is the implication of multiple CPU entering kernel at the same time? How can it corrupt user pages? Maybe: kernel entry code, per-cpu data in entry code, fpu code, switch_to, scheduler.  Why it always fail at those FPU code i.e. the strcmp function? I failed to compile without those sse, any solution? How it hurt performance?", 
            "title": "02/08 Thur Cloudy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0207-wed-cloudy", 
            "text": "20 : 07 \nPushed a small patch on mremap issue. Hope it will work. mremap really makes the whole thing very interesting, will be a very good research finding on combing virtual cache and operating system. Need to go gym with a friend, will be back on debugging late tonight.  9 : 30 \nHave two meetings to do today, and an security class, won t have too much time coding during daytime.", 
            "title": "02/07 Wed Cloudy"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0206-tue-sunny", 
            "text": "Well. We ve ruled out both  smp_call_function  and  workqueue  yesterday with Yiying s help. But the multi-thread word-count still fails  :-(  Single thread word-count just finished 4GB dataset (with 8GB pcache). So what could be still wrong with multithread one????   chill  check exit code  (Checked)  check pcache s usage of task_struct, should always use the group_leader  check cpu boot code and check the switch code again  I believe pinpoint the issue in multithread word-count can solve a lot issues, it must be some thread creation, removal, schedule things.  How about adding a lock for ibapi, make it sequential? Sweet, I tried, finally it is  a bug that we are able to debug .   22 : 39 \nDone for today. I m trying to patch  move_pte  and  pcache_move_pte . Although in theory we defenitly need to patch it, I keep thinking the code before should not trigger any serious bus or memory corruption. Ugh. Maybe it is concurrent  mremap  that one of them remap from A to B, while another one remap from C to A. It is possible. But my dead brain can not think of this anymore. I m going to hit the gym and do some squats.  17 : 01 \nCriminal found:  mremap()  and  virtual cache  did the crime. Interesting, I have not seen any research paper, tech-reports, writeup, code about this, not even the OVC paper, which, by the way, I think they must consider this case. Otherwise, a mremap will simply crash its virtual cache. Many thanks went to my smoke-and-think time.  15 : 14 \nSomething new came up! After adding a spinlock for ibapi, this showed up (I tried one more time after this, which does not show up). We are lucky to catch this. At least I know where to look at. Also, this is defenitly triggered by  mremap . It is seems it is overlapped  mremap() . One thing I did not know is which thread trigger this bug, the sweep thread? Cause mremap related pcache rmap functions do not use  rmap_get_locked_pte .  1\n2\n3\n4\n5\n6\n7 [ 3826.048774] normal_p2s_open(): f_name: word_100MB.txt, mode: 04400, flags: 0\n[ 3827.891622] SYSC_mremap(cpu18): move: [0x7fffe5788000 - 0x7fffe5806000] -  [0x7fffe531b000 - 0x7fffe5399000]\n[ 3828.178643] SYSC_mremap(cpu14): move: [0x7fffe5941000 - 0x7fffe5980000] -  [0x7fffe57c7000 - 0x7fffe5806000]\n\n****    ERROR: mismatched PTE and rmap\n****    rmap- owner_process: word_count-pthr uva: 0x7fffe57c8000 ptep: ffff88107efe0e40, rmap- page_table: ffff88107efe0e40\n****    pcache_pfn: 0x1257c8, pte_pfn: 0x125942   14 : 00    word_count-pthread : 100MB dataset pcache : 8GB, 8-way victim : 8 entries  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 [ 1294.845313] STDOUT: ---[\nWordcount: Running...\n]---\n[ 1294.903661] STDOUT: ---[\n\no;\n]---\n[ 1294.946301] normal_p2s_open(): f_name: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt, mode: 04400, flags: 0\n[ 1295.100517] SYSC_close(): [4] -  [/sys/devices/system/cpu/online]\n[ 1295.594658] word_count-pthr[59] general protection ip:4272ea sp:7ffff1b8ed28 error:0\n[ 1295.685236] CPU: 10 PID: 59 Comm: word_count-pthr 4.0.0-lego+ #113\n[ 1295.759070] RIP: 0033:[ 00000000004272ea ]  [ 00000000004272ea ] 0x4272ea\n[ 1295.840184] RSP: 002b:00007ffff1b8ed28  EFLAGS: 00010283\n[ 1295.903621] RAX: 000000000000000f RBX: 00007fffe5a3d010 RCX: 0000000000000001\n[ 1295.988893] RDX: 0000000000000000 RSI: 4854005942004441 RDI: 00007ffff1c1e80f\n[ 1296.074166] RBP: 00007ffff1c1e80f R08: 0000000000000000 R09: 0000000000000010\n[ 1296.211435] R10: 0000000000427ce0 R11: 00007ffff1bbb3ba R12: 0000000000001de4\n[ 1296.296711] R13: 00000000006e4a80 R14: 0000000000001d9e R15: 0000000000001dc1\n[ 1296.433978] FS:  00007ffff1b8f700(0000) GS:ffff88107fca0000(0000) knlGS:0000000000000000\n[ 1296.582686] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[ 1296.963297] CR2: 00007ffff1c1e000 CR3: 000000207fd8a000 CR4: 00000000000406a0  \nSo what is this  ip : 4272 ea , let us objdump the binary:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 0000000000425e60  strcmp :\n  425e60:       48 8d 05 69 14 00 00    lea    0x1469(%rip),%rax        # 4272d0  __strcmp_sse42 \n  425e67:       f7 05 5f b8 2b 00 00    testl  $0x100000,0x2bb85f(%rip)        # 6e16d0  _dl_x86_cpu_features+0x10 \n  425e6e:       00 10 00\n  425e71:       75 1a                   jne    425e8d  strcmp+0x2d \n  425e73:       48 8d 05 46 b0 00 00    lea    0xb046(%rip),%rax        # 430ec0  __strcmp_ssse3 \n  425e7a:       f7 05 4c b8 2b 00 00    testl  $0x200,0x2bb84c(%rip)        # 6e16d0  _dl_x86_cpu_features+0x10 \n  425e81:       02 00 00\n  425e84:       75 07                   jne    425e8d  strcmp+0x2d \n  425e86:       48 8d 05 03 00 00 00    lea    0x3(%rip),%rax        # 425e90  __GI_strcmp \n  425e8d:       c3                      retq\n  425e8e:       66 90                   xchg   %ax,%ax\n .. ..\n .. ..\n00000000004272d0  __strcmp_sse42 :\n  4272d0:       89 f1                   mov    %esi,%ecx\n  4272d2:       89 f8                   mov    %edi,%eax\n  4272d4:       48 83 e1 3f             and    $0x3f,%rcx\n  4272d8:       48 83 e0 3f             and    $0x3f,%rax\n  4272dc:       83 f9 30                cmp    $0x30,%ecx\n  4272df:       77 3f                   ja     427320  __strcmp_sse42+0x50 \n  4272e1:       83 f8 30                cmp    $0x30,%eax\n  4272e4:       77 3a                   ja     427320  __strcmp_sse42+0x50 \n  4272e6:       f3 0f 6f 0f             movdqu (%rdi),%xmm1\n* 4272ea:       f3 0f 6f 16             movdqu (%rsi),%xmm2\n  4272ee:       66 0f ef c0             pxor   %xmm0,%xmm0  \nYou can see  %rsi  has some garbage value  RSI :   4854005942004441 . Something went wrong. Will it be our FPU? I m not quite sure. If FPU code has error, why single-thread one succeed? Why it only shows up at multithread ones?", 
            "title": "02/06 Tue Sunny"
        }, 
        {
            "location": "/lego/log/log-02-2018/#0205-mon-sunny", 
            "text": "From yesterday s testing of Phoenix, it looks like something is wrong in  smp_call_functions() . They are invoked through  tlb flush , which was further invoked by  mremap , or  munmap . The warning from smp is:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 [   1260.586696 ]   WARNING :   CPU :   0   PID :   73   at   kernel / smp . c : 129   generic_smp_call_function_single_interrupt + 0xb8 / 0x160  [   1260.705251 ]   CPU :   0   PID :   73   Comm :   word_count - pthr   4.0.0 - lego +   # 99  [   1260.777008 ]   Stack :  [   1260.800927 ]   ffff88207fdffef8   ffffffff8100ec67   ffff88107fc00000   ffff88107fc00000  [   1260.888283 ]   ffffffff8100d410   ffff88207fe23df0   ffff88207fdfff08   ffffffff8100ed5f  [   1260.975639 ]   ffff88207fdfff38   ffffffff8100fe68   00007ff fe58c3010   0000000000000f 96  [   1261.062995 ]   000000000000f 960   0000000000000f 95   ffff88207fdfff48   ffffffff810020dd  [   1261.150351 ]   00007ff ff58869c1   ffffffff8100b2e9   0000000000000f 96   0000000000000f 95  [   1261.237707 ]   Call   Trace :  [   1261.266825 ]   TSK  [   1261.289704 ]   [ ffffffff8100ec76 ]   __warn . constprop .0 + 0xa6 / 0x100  [   1261.359381 ]   [ ffffffff8100d410 ]   ?   pgd_free + 0x90 / 0x90  [   1261.419699 ]   [ ffffffff8100ed5f ]   warn_slowpath_null + 0xf / 0x20  [   1261.487295 ]   [ ffffffff8100fe68 ]   generic_smp_call_function_single_interrupt + 0xb8 / 0x160  [   1261.581931 ]   [ ffffffff810020dd ]   call_function_interrupt + 0x1d / 0x20  [   1261.655767 ]   [ ffffffff8100b2e9 ]   smp__call_function_interrupt + 0x69 / 0x70    So I decided to look into smp.c a little bit to find out if there is something wrong (I wrote it long time ago). The warning itself is true, it means some inconsistent behavior.. I saw  alloc_percpu  stuff during  call_function_init , hence probably I also need to check percpu code a little code cause I m not sure if I port all the functionalities.  In all, today s task, check  percpu  and  smp_call_function  code. Esp,  percpu  code, they are crucial and very hard to relate real bugs to it.  Well  things changed. I found a more serious bug: something about  cpuhotplug , even though lego is not using it.  cpuhotplug  is a set of implict callbacks to all different subsystems who want to do some initialization work on each  offline- online  cpu.  Let us dig into how secondary cpu boots:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 Trampoline ..   setup   64 bit   mode  start_secondary () \n   smp_callin () \n         notify_cpu_starting () \n               ... \n               while   ( st - state     target )   { \n                       st - state ++ ; \n                       cpuhp_invoke_callback ( cpu ,   st - state ,   true ,   NULL ); \n               } \n           cpuhp_invoke_callback ()    See? There will be some callbacks! What are those callbacks exactly? Well, they are predefined at the  kernel/cpu.c . To save the trouble of reading code, I just print what functions are executed, the log is:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 [    0.118235] cpuhp_invoke_callback(): 136  CPU:0  page_writeback_cpu_online+0x0/0x20\n\n[    0.368478] cpuhp_invoke_callback(): 136  CPU:1  smpboot_create_threads+0x0/0x90\n[    0.370196] cpuhp_invoke_callback(): 136  CPU:1  perf_event_init_cpu+0x0/0xa0\n[    0.370403] cpuhp_invoke_callback(): 136  CPU:1  workqueue_prepare_cpu+0x0/0x80\n[    0.371112] cpuhp_invoke_callback(): 136  CPU:1  hrtimers_prepare_cpu+0x0/0x60\n[    0.371339] cpuhp_invoke_callback(): 136  CPU:1  smpcfd_prepare_cpu+0x0/0x80\n[    0.371584] cpuhp_invoke_callback(): 136  CPU:1  relay_prepare_cpu+0x0/0xe0\n[    0.371794] cpuhp_invoke_callback(): 136  CPU:1  rcutree_prepare_cpu+0x0/0x170\n[    0.372333] cpuhp_invoke_callback(): 136  CPU:1  notify_prepare+0x0/0xa0\n[    0.372744] cpuhp_invoke_callback(): 136  CPU:1  bringup_cpu+0x0/0x100\n[    0.008000] cpuhp_invoke_callback(): 136  CPU:1  sched_cpu_starting+0x0/0x60\n[    0.926124] cpuhp_invoke_callback(): 136  CPU:1  smpboot_unpark_threads+0x0/0x90\n[    0.926124] cpuhp_invoke_callback(): 136  CPU:1  perf_event_init_cpu+0x0/0xa0\n[    0.927028] cpuhp_invoke_callback(): 136  CPU:1  workqueue_online_cpu+0x0/0x2a0\n[    0.927768] cpuhp_invoke_callback(): 136  CPU:1  rcutree_online_cpu+0x0/0x70\n[    0.928045] cpuhp_invoke_callback(): 136  CPU:1  notify_online+0x0/0x20\n[    0.928256] cpuhp_invoke_callback(): 136  CPU:1  page_writeback_cpu_online+0x0/0x20\n[    0.928527] cpuhp_invoke_callback(): 136  CPU:1  sched_cpu_activate+0x0/0x190\n\n[    0.929084] cpuhp_invoke_callback(): 136  CPU:2  smpboot_create_threads+0x0/0x90\n[    0.930240] cpuhp_invoke_callback(): 136  CPU:2  perf_event_init_cpu+0x0/0xa0\n[    0.930434] cpuhp_invoke_callback(): 136  CPU:2  workqueue_prepare_cpu+0x0/0x80\n[    0.931070] cpuhp_invoke_callback(): 136  CPU:2  hrtimers_prepare_cpu+0x0/0x60\n[    0.931264] cpuhp_invoke_callback(): 136  CPU:2  smpcfd_prepare_cpu+0x0/0x80\n[    0.931464] cpuhp_invoke_callback(): 136  CPU:2  relay_prepare_cpu+0x0/0xe0\n[    0.931649] cpuhp_invoke_callback(): 136  CPU:2  rcutree_prepare_cpu+0x0/0x170\n[    0.932245] cpuhp_invoke_callback(): 136  CPU:2  notify_prepare+0x0/0xa0\n[    0.932475] cpuhp_invoke_callback(): 136  CPU:2  bringup_cpu+0x0/0x100\n[    0.008000] cpuhp_invoke_callback(): 136  CPU:2  sched_cpu_starting+0x0/0x60\n[    1.005023] cpuhp_invoke_callback(): 136  CPU:2  smpboot_unpark_threads+0x0/0x90\n[    1.005065] cpuhp_invoke_callback(): 136  CPU:2  perf_event_init_cpu+0x0/0xa0\n[    1.005408] cpuhp_invoke_callback(): 136  CPU:2  workqueue_online_cpu+0x0/0x2a0\n[    1.005729] cpuhp_invoke_callback(): 136  CPU:2  rcutree_online_cpu+0x0/0x70\n[    1.006029] cpuhp_invoke_callback(): 136  CPU:2  notify_online+0x0/0x20\n[    1.006206] cpuhp_invoke_callback(): 136  CPU:2  page_writeback_cpu_online+0x0/0x20\n[    1.006549] cpuhp_invoke_callback(): 136  CPU:2  sched_cpu_activate+0x0/0x190   Interesting! Currently, Lego need to add the  smpboot_create_threads() ,  workqueue_prepare_cpu() ,  workqueue_prepare_cpu() ,  bringup_cpu() ,  smpboot_unpark_threads() ,  workqueue_online_cpu() .  This hidden things is really hard to find and not easy to track during boot. Especially during boot, they should do something like  for_each_online_cpu  and init one by one. But I guess, after adding support of cpu hotplug, code kind of merged. Some stuff will be executed whenever a cpu has been teardown or bought up. And bang, why not use the same set of hotplug during boot, right?\nWell.", 
            "title": "02/05 Mon Sunny"
        }, 
        {
            "location": "/lego/kernel/kconfig/", 
            "text": "Lego Kconfig\n\n\nNetwork\n\n\n\n\nEnable \nCONFIG_INFINIBAND\n\n\nEnable \nCONFIG_FIT\n\n\nSet \nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT\n: boot time connection timeout\n\n\nSet \nCONFIG_FIT_NR_NODES\n: number of Lego nodes in this run\n\n\nSet \nCONFIG_FIT_LOCAL_ID\n: current node id\n\n\n\n\nIn \nnet/lego/fit_machine.c\n, modify the \nlego_cluster_hostnames\n array to match the machines you are using.\n\n\n\n\n\n\nSet \nCONFIG_DEFAULT_MEM_NODE\n in processor manager\n\n\n\n\nSet \nCONFIG_DEFAULT_STORAGE_NODE\n if you are running with storage component.\n\n\n\n\nNetwork configuration is crucial, please make sure all Lego nodes have consistent configurations. Otherwise the system may panic or fail to connect.\n\n\nProcessor\n\n\n\n\nEnable \nCONFIG_COMP_PROCESSOR\n\n\nopen \n.config\n\n\nremove line \n# CONFIG_COMP_PROCESSOR is not set\n\n\nclose \n.config\n\n\ndo \nmake\n, you will see \nConfigure Lego as processor component (COMP_PROCESSOR) [N/y/?] (NEW)\n, select Y\n\n\nChoose default configuration for all new config options\n\n\n\n\n\n\nEnable \nCONFIG_USE_RAMFS\n if you are not using storage components\n\n\n\n\nMemory\n\n\n\n\nEnable \nCONFIG_COMP_MEMORY\n\n\nopen \n.config\n\n\nremove line \n# CONFIG_COMP_MEMORY is not set\n\n\nclose \n.config\n\n\ndo \nmake\n, you will see \nConfigure Lego as memory component manager (COMP_MEMORY) [N/y/?] (NEW)\n, select Y\n\n\nChoose default configuration for all new config options\n\n\n\n\n\n\nEnable \nCONFIG_USE_RAMFS\n if you are not using storage components\n\n\nSet \nCONFIG_RAMFS_OBJECT_FILE\n: points to \nstatic-linked\n ELF file that you want to execute.\n\n\ntips: you can put your test code under \nusr/\n directory, and a simple \nmake\n will compile everything under.\n\n\n\n\n\n\n\n\nRun without Storage Component\n\n\nTo run Lego just with one processor component and one memory component, you need to:\n\n\n\n\nEnable \nCONFIG_USE_RAMFS\n at both sides. And in memory side, you need to set the \nCONFIG_RAMFS_OBJECT_FILE\n, which points to the ELF binary you want to test.\n\n\nmake sure \nCONFIG_DEFAULT_MEM_NODE\n at processor component is pointing to memory component\ns node id.\n\n\n\n\nA typical code snippet and configuration would be:\n\n1\n2\n3\n4\nstatic\n \nconst\n \nchar\n \n*\nlego_cluster_hostnames\n[\nCONFIG_FIT_NR_NODES\n]\n \n=\n \n{\n\n        \n[\n0\n]\n     \n=\n       \nwuklab00\n,\n\n        \n[\n1\n]\n     \n=\n       \nwuklab01\n,\n\n\n};\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nwuklab00 Processor\n\n#\n# Lego Processor Component Configurations\n#\nCONFIG_COMP_PROCESSOR=y\nCONFIG_CHECKPOINT=y\nCONFIG_MEMMAP_MEMBLOCK_RESERVED=y\n# CONFIG_PCACHE_EVICT_RANDOM is not set\n# CONFIG_PCACHE_EVICT_FIFO is not set\nCONFIG_PCACHE_EVICT_LRU=y\nCONFIG_PCACHE_EVICT_GENERIC_SWEEP=y\n# CONFIG_PCACHE_EVICTION_WRITE_PROTECT is not set\n# CONFIG_PCACHE_EVICTION_PERSET_LIST is not set\nCONFIG_PCACHE_EVICTION_VICTIM=y\nCONFIG_PCACHE_EVICTION_VICTIM_NR_ENTRIES=8\nCONFIG_PCACHE_PREFETCH=y\n\n#\n# Processor DEBUG Options\n#\n\n#\n# Lego Memory Component Configurations\n#\n# CONFIG_COMP_MEMORY is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=0\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\nwuklab01 Memory\n\n#\n# Lego Memory Component Configurations\n#\nCONFIG_COMP_MEMORY=y\n\n#\n# Memory DEBUG Options\n#\n# CONFIG_MEM_PREFETCH is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\nCONFIG_RAMFS_OBJECT_FILE=\nusr/pcache_conflict.o\n\n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=1", 
            "title": "Kconfig"
        }, 
        {
            "location": "/lego/kernel/kconfig/#lego-kconfig", 
            "text": "", 
            "title": "Lego Kconfig"
        }, 
        {
            "location": "/lego/kernel/kconfig/#network", 
            "text": "Enable  CONFIG_INFINIBAND  Enable  CONFIG_FIT  Set  CONFIG_FIT_INITIAL_SLEEP_TIMEOUT : boot time connection timeout  Set  CONFIG_FIT_NR_NODES : number of Lego nodes in this run  Set  CONFIG_FIT_LOCAL_ID : current node id   In  net/lego/fit_machine.c , modify the  lego_cluster_hostnames  array to match the machines you are using.    Set  CONFIG_DEFAULT_MEM_NODE  in processor manager   Set  CONFIG_DEFAULT_STORAGE_NODE  if you are running with storage component.   Network configuration is crucial, please make sure all Lego nodes have consistent configurations. Otherwise the system may panic or fail to connect.", 
            "title": "Network"
        }, 
        {
            "location": "/lego/kernel/kconfig/#processor", 
            "text": "Enable  CONFIG_COMP_PROCESSOR  open  .config  remove line  # CONFIG_COMP_PROCESSOR is not set  close  .config  do  make , you will see  Configure Lego as processor component (COMP_PROCESSOR) [N/y/?] (NEW) , select Y  Choose default configuration for all new config options    Enable  CONFIG_USE_RAMFS  if you are not using storage components", 
            "title": "Processor"
        }, 
        {
            "location": "/lego/kernel/kconfig/#memory", 
            "text": "Enable  CONFIG_COMP_MEMORY  open  .config  remove line  # CONFIG_COMP_MEMORY is not set  close  .config  do  make , you will see  Configure Lego as memory component manager (COMP_MEMORY) [N/y/?] (NEW) , select Y  Choose default configuration for all new config options    Enable  CONFIG_USE_RAMFS  if you are not using storage components  Set  CONFIG_RAMFS_OBJECT_FILE : points to  static-linked  ELF file that you want to execute.  tips: you can put your test code under  usr/  directory, and a simple  make  will compile everything under.", 
            "title": "Memory"
        }, 
        {
            "location": "/lego/kernel/kconfig/#run-without-storage-component", 
            "text": "To run Lego just with one processor component and one memory component, you need to:   Enable  CONFIG_USE_RAMFS  at both sides. And in memory side, you need to set the  CONFIG_RAMFS_OBJECT_FILE , which points to the ELF binary you want to test.  make sure  CONFIG_DEFAULT_MEM_NODE  at processor component is pointing to memory component s node id.   A typical code snippet and configuration would be: 1\n2\n3\n4 static   const   char   * lego_cluster_hostnames [ CONFIG_FIT_NR_NODES ]   =   { \n         [ 0 ]       =         wuklab00 , \n         [ 1 ]       =         wuklab01 ,  };     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49 wuklab00 Processor\n\n#\n# Lego Processor Component Configurations\n#\nCONFIG_COMP_PROCESSOR=y\nCONFIG_CHECKPOINT=y\nCONFIG_MEMMAP_MEMBLOCK_RESERVED=y\n# CONFIG_PCACHE_EVICT_RANDOM is not set\n# CONFIG_PCACHE_EVICT_FIFO is not set\nCONFIG_PCACHE_EVICT_LRU=y\nCONFIG_PCACHE_EVICT_GENERIC_SWEEP=y\n# CONFIG_PCACHE_EVICTION_WRITE_PROTECT is not set\n# CONFIG_PCACHE_EVICTION_PERSET_LIST is not set\nCONFIG_PCACHE_EVICTION_VICTIM=y\nCONFIG_PCACHE_EVICTION_VICTIM_NR_ENTRIES=8\nCONFIG_PCACHE_PREFETCH=y\n\n#\n# Processor DEBUG Options\n#\n\n#\n# Lego Memory Component Configurations\n#\n# CONFIG_COMP_MEMORY is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=0    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 wuklab01 Memory\n\n#\n# Lego Memory Component Configurations\n#\nCONFIG_COMP_MEMORY=y\n\n#\n# Memory DEBUG Options\n#\n# CONFIG_MEM_PREFETCH is not set\n\n#\n# DRAM Cache Options\n#\nCONFIG_PCACHE_LINE_SIZE_SHIFT=12\nCONFIG_PCACHE_ASSOCIATIVITY_SHIFT=3\n\n#\n# General Manager Config/Debug Options\n#\nCONFIG_DEFAULT_MEM_NODE=1\nCONFIG_DEFAULT_STORAGE_NODE=2\nCONFIG_USE_RAMFS=y\nCONFIG_RAMFS_OBJECT_FILE= usr/pcache_conflict.o \n\n#\n# Networking\n#\n# CONFIG_LWIP is not set\nCONFIG_FIT=y\n# CONFIG_FIT_DEBUG is not set\nCONFIG_FIT_INITIAL_SLEEP_TIMEOUT=30\nCONFIG_FIT_NR_NODES=2\nCONFIG_FIT_LOCAL_ID=1", 
            "title": "Run without Storage Component"
        }, 
        {
            "location": "/lego/kernel/debug/", 
            "text": "Debug Facility in Lego\n\n\nLego provides several handy debug helpers to ease our coding pain. We category them by layers, namely \n1)\n \nCore Kernel\n, the lowest level of Lego, which is shared by all managers. \n2)\n \nProcessor Manager\n, which controls processor components. \n3)\n \nMemory Manager\n, which controls memory components.\n\n\nCore Kernel\n\n\n1\n2\nvoid\n \ndump_pte\n(\npte_t\n \n*\nptep\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_page\n(\nstruct\n \npage\n \n*\npage\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\n\n\n\nThese two helpers will dump a given pte entry or a page. Use this function if you are developing core related to physical memory allocation or pcache.\n\n\n\n\n1\nvoid\n \nptdump_walk_pgd_level\n(\npgd_t\n \n*\npgd\n);\n\n\n\n\n\nThis debug helper will dump the whole pgtable ranges. Contiguous page table entries that share the same property will be merged together and will be printed once. Use this function if you are developing code related to user page tables.\n\n\n\n\n1\n2\n3\nvoid\n \nshow_state_filter\n(\nunsigned\n \nlong\n \nstate_filter\n,\n \nbool\n \nprint_rq\n);\n\n\nvoid\n \nsched_show_task\n(\nstruct\n \ntask_struct\n \n*\np\n);\n\n\nvoid\n \nsysrq_sched_debug_show\n(\nvoid\n);\n\n\n\n\n\nThis set of functions are debug helpers for local scheduler. They will print all the tasks running in the system, and detailed information about percpu \nrunqueue\n. Use this set of functions if you are developing code related to scheduler.\n\n\nProcessor Manager\n\n\n1\n2\n3\n4\nvoid\n \ndump_pcache_meta\n(\nstruct\n \npcache_meta\n \n*\npcm\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_pcache_victim\n(\nstruct\n \npcache_victim_meta\n \n*\nvictim\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_pcache_rmap\n(\nstruct\n \npcache_rmap\n \n*\nrmap\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\nvoid\n \ndump_pcache_line\n(\nstruct\n \npcache_meta\n \n*\npcm\n,\n \nconst\n \nchar\n \n*\nreason\n);\n\n\n\n\n\nThese functions dump a given pcache line, a victim line, or a given reserve mapping. The last one will print the pcache line content, which generates a lot messages, you are warned. Use these functions if you are developing pcache or victim cache code.\n\n\nMemory Manager\n\n\n1\n2\nvoid\n \ndump_lego_mm\n(\nconst\n \nstruct\n \nlego_mm_struct\n \n*\nmm\n);\n\n\nvoid\n \ndump_vma\n(\nconst\n \nstruct\n \nvm_area_struct\n \n*\nvma\n);\n\n\n\n\n\nThese two functions are used to dump the virtual address space of a process. Use these functions if you developing process VM related things.", 
            "title": "Debug"
        }, 
        {
            "location": "/lego/kernel/debug/#debug-facility-in-lego", 
            "text": "Lego provides several handy debug helpers to ease our coding pain. We category them by layers, namely  1)   Core Kernel , the lowest level of Lego, which is shared by all managers.  2)   Processor Manager , which controls processor components.  3)   Memory Manager , which controls memory components.", 
            "title": "Debug Facility in Lego"
        }, 
        {
            "location": "/lego/kernel/debug/#core-kernel", 
            "text": "1\n2 void   dump_pte ( pte_t   * ptep ,   const   char   * reason );  void   dump_page ( struct   page   * page ,   const   char   * reason );   \nThese two helpers will dump a given pte entry or a page. Use this function if you are developing core related to physical memory allocation or pcache.   1 void   ptdump_walk_pgd_level ( pgd_t   * pgd );   \nThis debug helper will dump the whole pgtable ranges. Contiguous page table entries that share the same property will be merged together and will be printed once. Use this function if you are developing code related to user page tables.   1\n2\n3 void   show_state_filter ( unsigned   long   state_filter ,   bool   print_rq );  void   sched_show_task ( struct   task_struct   * p );  void   sysrq_sched_debug_show ( void );   \nThis set of functions are debug helpers for local scheduler. They will print all the tasks running in the system, and detailed information about percpu  runqueue . Use this set of functions if you are developing code related to scheduler.", 
            "title": "Core Kernel"
        }, 
        {
            "location": "/lego/kernel/debug/#processor-manager", 
            "text": "1\n2\n3\n4 void   dump_pcache_meta ( struct   pcache_meta   * pcm ,   const   char   * reason );  void   dump_pcache_victim ( struct   pcache_victim_meta   * victim ,   const   char   * reason );  void   dump_pcache_rmap ( struct   pcache_rmap   * rmap ,   const   char   * reason );  void   dump_pcache_line ( struct   pcache_meta   * pcm ,   const   char   * reason );   \nThese functions dump a given pcache line, a victim line, or a given reserve mapping. The last one will print the pcache line content, which generates a lot messages, you are warned. Use these functions if you are developing pcache or victim cache code.", 
            "title": "Processor Manager"
        }, 
        {
            "location": "/lego/kernel/debug/#memory-manager", 
            "text": "1\n2 void   dump_lego_mm ( const   struct   lego_mm_struct   * mm );  void   dump_vma ( const   struct   vm_area_struct   * vma );   \nThese two functions are used to dump the virtual address space of a process. Use these functions if you developing process VM related things.", 
            "title": "Memory Manager"
        }, 
        {
            "location": "/lego/kernel/grub/", 
            "text": "Use GRUB2 to boot Lego\n\n\nLast Updated: 02/02/2018\n\n\nThis document explains: \n1)\n how Lego itself is written to pretend as a Linux kernel, \n2)\n how to boot Lego kernel with GRUB2, \n3)\n GRUB2 configurations specific to Lego.\n\n\nHow Lego pretend as a Linux kernel\n\n\nasdsad\n\n\nHow to config GRUB2 for Lego\n\n\nasdsa", 
            "title": "GRUB"
        }, 
        {
            "location": "/lego/kernel/grub/#use-grub2-to-boot-lego", 
            "text": "Last Updated: 02/02/2018  This document explains:  1)  how Lego itself is written to pretend as a Linux kernel,  2)  how to boot Lego kernel with GRUB2,  3)  GRUB2 configurations specific to Lego.", 
            "title": "Use GRUB2 to boot Lego"
        }, 
        {
            "location": "/lego/kernel/grub/#how-lego-pretend-as-a-linux-kernel", 
            "text": "asdsad", 
            "title": "How Lego pretend as a Linux kernel"
        }, 
        {
            "location": "/lego/kernel/grub/#how-to-config-grub2-for-lego", 
            "text": "asdsa", 
            "title": "How to config GRUB2 for Lego"
        }, 
        {
            "location": "/lego/kernel/profile/", 
            "text": "Lego Profilers\n\n\nLego has three runtime profilers in kernel:\n\n\n\n\nstrace\n\n\nheatmap\n\n\nprofile points\n\n\n\n\nCombined together, they can provide the following information. Sweet, huh?\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n[\n \n1017.047366\n]\n \nKernel\n \nstrace\n\n\n[\n \n1017.050276\n]\n \nTask\n:\n \n20\n:\n20\n \nnr_accumulated_threads\n:\n \n46\n\n\n[\n \n1017.055837\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n1017.063213\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.071648\n]\n  \n98.16\n   \n33.839597842\n     \n1879978\n        \n18\n         \n0\n \nsys_futex\n\n\n[\n \n1017.079406\n]\n   \n0.26\n    \n0.260143997\n      \n260144\n         \n1\n         \n0\n \nsys_execve\n\n\n[\n \n1017.087260\n]\n   \n0.18\n    \n0.185456860\n        \n7133\n        \n26\n         \n0\n \nsys_write\n\n\n[\n \n1017.095017\n]\n   \n0.50\n    \n0.050189546\n         \n913\n        \n55\n         \n0\n \nsys_munmap\n\n\n[\n \n1017.102870\n]\n   \n0.25\n    \n0.025223661\n         \n255\n        \n99\n         \n0\n \nsys_mmap\n\n\n[\n \n1017.110531\n]\n   \n0.50\n    \n0.000505134\n          \n12\n        \n45\n         \n0\n \nsys_clone\n\n\n[\n \n1017.118288\n]\n   \n0.20\n    \n0.000202327\n          \n26\n         \n8\n         \n0\n \nsys_read\n\n\n[\n \n1017.125947\n]\n   \n0.14\n    \n0.000144065\n          \n17\n         \n9\n         \n0\n \nsys_open\n\n\n[\n \n1017.133608\n]\n   \n0.67\n    \n0.000067251\n           \n7\n        \n11\n         \n0\n \nsys_brk\n\n\n[\n \n1017.141171\n]\n   \n0.30\n    \n0.000030361\n           \n7\n         \n5\n         \n0\n \nsys_newfstat\n\n\n[\n \n1017.149219\n]\n   \n0.64\n    \n0.000006410\n           \n1\n         \n9\n         \n0\n \nsys_close\n\n\n[\n \n1017.156976\n]\n   \n0.48\n    \n0.000004842\n           \n1\n        \n45\n         \n0\n \nsys_madvise\n\n\n[\n \n1017.164927\n]\n   \n0.34\n    \n0.000003443\n           \n1\n        \n47\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n1017.173653\n]\n   \n0.21\n    \n0.000002137\n           \n1\n        \n52\n         \n0\n \nsys_mprotect\n\n\n[\n \n1017.181702\n]\n   \n0.71\n    \n0.000000717\n           \n1\n         \n4\n         \n0\n \nsys_gettimeofday\n\n\n[\n \n1017.190137\n]\n   \n0.60\n    \n0.000000608\n           \n1\n         \n3\n         \n0\n \nsys_time\n\n\n[\n \n1017.197797\n]\n   \n0.51\n    \n0.000000513\n           \n1\n         \n2\n         \n0\n \nsys_getrlimit\n\n\n[\n \n1017.205942\n]\n   \n0.49\n    \n0.000000498\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n1017.214572\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n4\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n1017.223008\n]\n   \n0.45\n    \n0.000000453\n           \n1\n         \n2\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n1017.231249\n]\n   \n0.27\n    \n0.000000272\n           \n1\n         \n2\n         \n0\n \nsys_newuname\n\n\n[\n \n1017.239298\n]\n   \n0.13\n    \n0.000000135\n           \n1\n         \n2\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n1017.248025\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.256460\n]\n \n100.00\n   \n34.361581541\n                   \n451\n         \n0\n \ntotal\n\n\n[\n \n1017.263830\n]\n\n\n[\n \n1017.308295\n]\n\n\n[\n \n1017.309754\n]\n \nKernel\n \nHeatmap\n \n(\ntop\n \n#\n10\n)\n\n\n[\n \n1017.313731\n]\n          \nAddress\n              \nFunction\n          \nNR\n          \n%\n\n\n[\n \n1017.321294\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.328858\n]\n \nffffffff8101a600\n              \ncpu_idle\n      \n112082\n      \n73.11\n\n\n[\n \n1017.336421\n]\n \nffffffff810666f0\n            \n__schedule\n       \n19192\n      \n12.95\n\n\n[\n \n1017.343983\n]\n \nffffffff8104f500\n       \nmlx4_ib_poll_cq\n        \n5551\n       \n3.99\n\n\n[\n \n1017.351546\n]\n \nffffffff8103bf50\n             \ndelay_tsc\n        \n5393\n       \n3.83\n\n\n[\n \n1017.359110\n]\n \nffffffff81034a10\n    \nvictim_flush_async\n        \n3766\n       \n2.72\n\n\n[\n \n1017.366673\n]\n \nffffffff8102b220\n   \nslob_alloc\n.\nconstpro\n        \n1992\n       \n1.47\n\n\n[\n \n1017.374235\n]\n \nffffffff810668d0\n              \nschedule\n        \n1519\n       \n0.15\n\n\n[\n \n1017.381800\n]\n \nffffffff810648f0\n   \nfit_send_reply_with\n         \n956\n       \n0.95\n\n\n[\n \n1017.389362\n]\n \nffffffff81062370\n   \nibapi_send_reply_ti\n         \n307\n       \n0.30\n\n\n[\n \n1017.396925\n]\n \nffffffff8105a0d0\n   \nib_mad_completion_h\n         \n232\n       \n0.23\n\n\n[\n \n1017.404487\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.412052\n]\n                                             \n151994\n     \n100.00\n\n\n[\n \n1017.419613\n]\n\n\n[\n \n1017.421267\n]\n\n\n[\n \n1017.422911\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n \n1017.426594\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n \n1017.436292\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n \n1017.445988\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000153470\n                \n55\n              \n2791\n\n\n[\n \n1017.455685\n]\n     \noff\n     \npcache_cache_miss\n      \n16.147020152\n            \n274698\n             \n58781\n\n\n[\n \n1017.465381\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------", 
            "title": "Profile"
        }, 
        {
            "location": "/lego/kernel/profile/#lego-profilers", 
            "text": "Lego has three runtime profilers in kernel:   strace  heatmap  profile points   Combined together, they can provide the following information. Sweet, huh?  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 [   1017.047366 ]   Kernel   strace  [   1017.050276 ]   Task :   20 : 20   nr_accumulated_threads :   46  [   1017.055837 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   1017.063213 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.071648 ]    98.16     33.839597842       1879978          18           0   sys_futex  [   1017.079406 ]     0.26      0.260143997        260144           1           0   sys_execve  [   1017.087260 ]     0.18      0.185456860          7133          26           0   sys_write  [   1017.095017 ]     0.50      0.050189546           913          55           0   sys_munmap  [   1017.102870 ]     0.25      0.025223661           255          99           0   sys_mmap  [   1017.110531 ]     0.50      0.000505134            12          45           0   sys_clone  [   1017.118288 ]     0.20      0.000202327            26           8           0   sys_read  [   1017.125947 ]     0.14      0.000144065            17           9           0   sys_open  [   1017.133608 ]     0.67      0.000067251             7          11           0   sys_brk  [   1017.141171 ]     0.30      0.000030361             7           5           0   sys_newfstat  [   1017.149219 ]     0.64      0.000006410             1           9           0   sys_close  [   1017.156976 ]     0.48      0.000004842             1          45           0   sys_madvise  [   1017.164927 ]     0.34      0.000003443             1          47           0   sys_set_robust_list  [   1017.173653 ]     0.21      0.000002137             1          52           0   sys_mprotect  [   1017.181702 ]     0.71      0.000000717             1           4           0   sys_gettimeofday  [   1017.190137 ]     0.60      0.000000608             1           3           0   sys_time  [   1017.197797 ]     0.51      0.000000513             1           2           0   sys_getrlimit  [   1017.205942 ]     0.49      0.000000498             1           2           0   sys_rt_sigprocmask  [   1017.214572 ]     0.46      0.000000469             1           4           0   sys_rt_sigaction  [   1017.223008 ]     0.45      0.000000453             1           2           0   sys_arch_prctl  [   1017.231249 ]     0.27      0.000000272             1           2           0   sys_newuname  [   1017.239298 ]     0.13      0.000000135             1           2           0   sys_set_tid_address  [   1017.248025 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.256460 ]   100.00     34.361581541                     451           0   total  [   1017.263830 ]  [   1017.308295 ]  [   1017.309754 ]   Kernel   Heatmap   ( top   # 10 )  [   1017.313731 ]            Address                Function            NR            %  [   1017.321294 ]   ----------------    --------------------    ----------    ---------  [   1017.328858 ]   ffffffff8101a600                cpu_idle        112082        73.11  [   1017.336421 ]   ffffffff810666f0              __schedule         19192        12.95  [   1017.343983 ]   ffffffff8104f500         mlx4_ib_poll_cq          5551         3.99  [   1017.351546 ]   ffffffff8103bf50               delay_tsc          5393         3.83  [   1017.359110 ]   ffffffff81034a10      victim_flush_async          3766         2.72  [   1017.366673 ]   ffffffff8102b220     slob_alloc . constpro          1992         1.47  [   1017.374235 ]   ffffffff810668d0                schedule          1519         0.15  [   1017.381800 ]   ffffffff810648f0     fit_send_reply_with           956         0.95  [   1017.389362 ]   ffffffff81062370     ibapi_send_reply_ti           307         0.30  [   1017.396925 ]   ffffffff8105a0d0     ib_mad_completion_h           232         0.23  [   1017.404487 ]   ----------------    --------------------    ----------    ---------  [   1017.412052 ]                                               151994       100.00  [   1017.419613 ]  [   1017.421267 ]  [   1017.422911 ]   Kernel   Profile   Points  [   1017.426594 ]    status                    name               total                  nr              avg . ns  [   1017.436292 ]   -------    --------------------    ----------------    ----------------    ----------------  [   1017.445988 ]       off        flush_tlb_others         0.000153470                  55                2791  [   1017.455685 ]       off       pcache_cache_miss        16.147020152              274698               58781  [   1017.465381 ]   -------    --------------------    ----------------    ----------------    ----------------", 
            "title": "Lego Profilers"
        }, 
        {
            "location": "/lego/kernel/profile_strace/", 
            "text": "Lego Profile strace\n\n\nLego has a built-in kernel-version syscall tracer, similar to \nstrace\n utility in the user space. Below we will just call our Lego\ns syscall tracer as strace for simplicity.\n\n\nDesign\n\n\nThere are essentially three important metrics to track for each syscall\n\n\n\n\nnumber of times invoked\n\n\nnumber of times error happened\n\n\ntotal execution, or per-call latency\n\n\n\n\nBesides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:\n\n\n\n\nPerformance: set of counters are \natomic_t\n, updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.\n\n\nSimplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.\n\n\n\n\nBelow is our simple design, where each thread has a \nstruct strace_info\n, which include a set of counters for each syscall. All \nstrace_info\n within a process are chained together by a doubly-linked list.\n\n\n\n\nWhen we want to look at the strace statistic numbers, we need to \naccumulate\n counters from all threads within a process, including those dead threads. We do the \naccumulate\n when the last thread of this process is going to exit.\n\n\nThe benefit of doubly-linked \nstrace_info\n is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if \ntask_struct\n is reaped, \nstrace_info\n is still there and linked.\n\n\nFor example, assume thread_3 has a SIGSEGV, and did a \nzap_other_threads\n. And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.\n\n\n\nDetails\n\n\nThere are essentially three hooks in core kernel:\n\n\n\n\nsyscall\n: before and after \nsys_call_table\n\n\nfork/clone\n: create \nstrace_info\n for each thread\n\n\ndo_exit()\n: when group_dead(signal-\nlive==1), accumulate\n\n\n\n\nExample Output\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n[\n \n1017.047366\n]\n \nKernel\n \nstrace\n\n\n[\n \n1017.050276\n]\n \nTask\n:\n \n20\n:\n20\n \nnr_accumulated_threads\n:\n \n46\n\n\n[\n \n1017.055837\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n1017.063213\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.071648\n]\n  \n98.16\n   \n33.839597842\n     \n1879978\n        \n18\n         \n0\n \nsys_futex\n\n\n[\n \n1017.079406\n]\n   \n0.26\n    \n0.260143997\n      \n260144\n         \n1\n         \n0\n \nsys_execve\n\n\n[\n \n1017.087260\n]\n   \n0.18\n    \n0.185456860\n        \n7133\n        \n26\n         \n0\n \nsys_write\n\n\n[\n \n1017.095017\n]\n   \n0.50\n    \n0.050189546\n         \n913\n        \n55\n         \n0\n \nsys_munmap\n\n\n[\n \n1017.102870\n]\n   \n0.25\n    \n0.025223661\n         \n255\n        \n99\n         \n0\n \nsys_mmap\n\n\n[\n \n1017.110531\n]\n   \n0.50\n    \n0.000505134\n          \n12\n        \n45\n         \n0\n \nsys_clone\n\n\n[\n \n1017.118288\n]\n   \n0.20\n    \n0.000202327\n          \n26\n         \n8\n         \n0\n \nsys_read\n\n\n[\n \n1017.125947\n]\n   \n0.14\n    \n0.000144065\n          \n17\n         \n9\n         \n0\n \nsys_open\n\n\n[\n \n1017.133608\n]\n   \n0.67\n    \n0.000067251\n           \n7\n        \n11\n         \n0\n \nsys_brk\n\n\n[\n \n1017.141171\n]\n   \n0.30\n    \n0.000030361\n           \n7\n         \n5\n         \n0\n \nsys_newfstat\n\n\n[\n \n1017.149219\n]\n   \n0.64\n    \n0.000006410\n           \n1\n         \n9\n         \n0\n \nsys_close\n\n\n[\n \n1017.156976\n]\n   \n0.48\n    \n0.000004842\n           \n1\n        \n45\n         \n0\n \nsys_madvise\n\n\n[\n \n1017.164927\n]\n   \n0.34\n    \n0.000003443\n           \n1\n        \n47\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n1017.173653\n]\n   \n0.21\n    \n0.000002137\n           \n1\n        \n52\n         \n0\n \nsys_mprotect\n\n\n[\n \n1017.181702\n]\n   \n0.71\n    \n0.000000717\n           \n1\n         \n4\n         \n0\n \nsys_gettimeofday\n\n\n[\n \n1017.190137\n]\n   \n0.60\n    \n0.000000608\n           \n1\n         \n3\n         \n0\n \nsys_time\n\n\n[\n \n1017.197797\n]\n   \n0.51\n    \n0.000000513\n           \n1\n         \n2\n         \n0\n \nsys_getrlimit\n\n\n[\n \n1017.205942\n]\n   \n0.49\n    \n0.000000498\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n1017.214572\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n4\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n1017.223008\n]\n   \n0.45\n    \n0.000000453\n           \n1\n         \n2\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n1017.231249\n]\n   \n0.27\n    \n0.000000272\n           \n1\n         \n2\n         \n0\n \nsys_newuname\n\n\n[\n \n1017.239298\n]\n   \n0.13\n    \n0.000000135\n           \n1\n         \n2\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n1017.248025\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.256460\n]\n \n100.00\n   \n34.361581541\n                   \n451\n         \n0\n \ntotal\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: April 05, 2018\n\nLast Updated: April 05, 2018", 
            "title": "Profile strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#lego-profile-strace", 
            "text": "Lego has a built-in kernel-version syscall tracer, similar to  strace  utility in the user space. Below we will just call our Lego s syscall tracer as strace for simplicity.", 
            "title": "Lego Profile strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#design", 
            "text": "There are essentially three important metrics to track for each syscall   number of times invoked  number of times error happened  total execution, or per-call latency   Besides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:   Performance: set of counters are  atomic_t , updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.  Simplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.   Below is our simple design, where each thread has a  struct strace_info , which include a set of counters for each syscall. All  strace_info  within a process are chained together by a doubly-linked list.   When we want to look at the strace statistic numbers, we need to  accumulate  counters from all threads within a process, including those dead threads. We do the  accumulate  when the last thread of this process is going to exit.  The benefit of doubly-linked  strace_info  is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if  task_struct  is reaped,  strace_info  is still there and linked.  For example, assume thread_3 has a SIGSEGV, and did a  zap_other_threads . And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.", 
            "title": "Design"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#details", 
            "text": "There are essentially three hooks in core kernel:   syscall : before and after  sys_call_table  fork/clone : create  strace_info  for each thread  do_exit() : when group_dead(signal- live==1), accumulate", 
            "title": "Details"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#example-output", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 [   1017.047366 ]   Kernel   strace  [   1017.050276 ]   Task :   20 : 20   nr_accumulated_threads :   46  [   1017.055837 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   1017.063213 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.071648 ]    98.16     33.839597842       1879978          18           0   sys_futex  [   1017.079406 ]     0.26      0.260143997        260144           1           0   sys_execve  [   1017.087260 ]     0.18      0.185456860          7133          26           0   sys_write  [   1017.095017 ]     0.50      0.050189546           913          55           0   sys_munmap  [   1017.102870 ]     0.25      0.025223661           255          99           0   sys_mmap  [   1017.110531 ]     0.50      0.000505134            12          45           0   sys_clone  [   1017.118288 ]     0.20      0.000202327            26           8           0   sys_read  [   1017.125947 ]     0.14      0.000144065            17           9           0   sys_open  [   1017.133608 ]     0.67      0.000067251             7          11           0   sys_brk  [   1017.141171 ]     0.30      0.000030361             7           5           0   sys_newfstat  [   1017.149219 ]     0.64      0.000006410             1           9           0   sys_close  [   1017.156976 ]     0.48      0.000004842             1          45           0   sys_madvise  [   1017.164927 ]     0.34      0.000003443             1          47           0   sys_set_robust_list  [   1017.173653 ]     0.21      0.000002137             1          52           0   sys_mprotect  [   1017.181702 ]     0.71      0.000000717             1           4           0   sys_gettimeofday  [   1017.190137 ]     0.60      0.000000608             1           3           0   sys_time  [   1017.197797 ]     0.51      0.000000513             1           2           0   sys_getrlimit  [   1017.205942 ]     0.49      0.000000498             1           2           0   sys_rt_sigprocmask  [   1017.214572 ]     0.46      0.000000469             1           4           0   sys_rt_sigaction  [   1017.223008 ]     0.45      0.000000453             1           2           0   sys_arch_prctl  [   1017.231249 ]     0.27      0.000000272             1           2           0   sys_newuname  [   1017.239298 ]     0.13      0.000000135             1           2           0   sys_set_tid_address  [   1017.248025 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.256460 ]   100.00     34.361581541                     451           0   total    \nYizhou Shan \nCreated: April 05, 2018 \nLast Updated: April 05, 2018", 
            "title": "Example Output"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/", 
            "text": "Lego Profile Kernel Heatmap\n\n\nTo get a sense of what is the hottest function within kernel, Lego adds a  counter based heatmap. It is the same with Linux\ns \n/proc/profile\n.\n\n\nMechanism\n\n\nGeneral idea: for each possible function/instruction byte in the kernel, we attach to a counter to it. Once we detect this function/instruction was executed, we increment its associated counter.\n\n\nHowever, fine granularity counting will need a lot extra memory, and it is not necessary to track each single instruction byte. Besides, it is hard to track down every time the function was executed. Furthermore, we only need an approximate heatmap.\n\n\nThus, kernel\ns solutions are:\n\n\n\n\nCoarse granularity\n: maintain a counter for each \n1\nprof_shift\n bytes.\n\n\nUpdate counter on timer interrupt tick\n, which is a constant stable entry.\n\n\n\n\nSupported Features\n\n\nCurrently, we only support \nCPU_PROFILING\n, which profile on each timer interrupt tick. We could also add \nSCHED_PROFILING\n, or \nSLEEP_PROFILING\n. But we are fine with current setting.\n\n\nOf course, we also have a simple dump function \nvoid print_profile_heatmap_nr(int nr)\n, which is similar to userspace tool \nreadprofile\n.\n\n\nExample Output\n\n\nWorkload is: MT-Phoenix word count, with 1GB data. (We probably want to rule out \ncpu_idle()\n)\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n[\n \n1017.309754\n]\n \nKernel\n \nHeatmap\n \n(\ntop\n \n#\n10\n)\n\n\n[\n \n1017.313731\n]\n          \nAddress\n              \nFunction\n          \nNR\n          \n%\n\n\n[\n \n1017.321294\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.328858\n]\n \nffffffff8101a600\n              \ncpu_idle\n      \n112082\n      \n73.11\n\n\n[\n \n1017.336421\n]\n \nffffffff810666f0\n            \n__schedule\n       \n19192\n      \n12.95\n\n\n[\n \n1017.343983\n]\n \nffffffff8104f500\n       \nmlx4_ib_poll_cq\n        \n5551\n       \n3.99\n\n\n[\n \n1017.351546\n]\n \nffffffff8103bf50\n             \ndelay_tsc\n        \n5393\n       \n3.83\n\n\n[\n \n1017.359110\n]\n \nffffffff81034a10\n    \nvictim_flush_async\n        \n3766\n       \n2.72\n\n\n[\n \n1017.366673\n]\n \nffffffff8102b220\n   \nslob_alloc\n.\nconstpro\n        \n1992\n       \n1.47\n\n\n[\n \n1017.374235\n]\n \nffffffff810668d0\n              \nschedule\n        \n1519\n       \n0.15\n\n\n[\n \n1017.381800\n]\n \nffffffff810648f0\n   \nfit_send_reply_with\n         \n956\n       \n0.95\n\n\n[\n \n1017.389362\n]\n \nffffffff81062370\n   \nibapi_send_reply_ti\n         \n307\n       \n0.30\n\n\n[\n \n1017.396925\n]\n \nffffffff8105a0d0\n   \nib_mad_completion_h\n         \n232\n       \n0.23\n\n\n[\n \n1017.404487\n]\n \n----------------\n  \n--------------------\n  \n----------\n  \n---------\n\n\n[\n \n1017.412052\n]\n                                             \n151994\n     \n100.00\n\n\n[\n \n1017.419613\n]\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: April 06, 2018\n\nLast Updated: April 06, 2018", 
            "title": "Profile heatmap"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#lego-profile-kernel-heatmap", 
            "text": "To get a sense of what is the hottest function within kernel, Lego adds a  counter based heatmap. It is the same with Linux s  /proc/profile .", 
            "title": "Lego Profile Kernel Heatmap"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#mechanism", 
            "text": "General idea: for each possible function/instruction byte in the kernel, we attach to a counter to it. Once we detect this function/instruction was executed, we increment its associated counter.  However, fine granularity counting will need a lot extra memory, and it is not necessary to track each single instruction byte. Besides, it is hard to track down every time the function was executed. Furthermore, we only need an approximate heatmap.  Thus, kernel s solutions are:   Coarse granularity : maintain a counter for each  1 prof_shift  bytes.  Update counter on timer interrupt tick , which is a constant stable entry.", 
            "title": "Mechanism"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#supported-features", 
            "text": "Currently, we only support  CPU_PROFILING , which profile on each timer interrupt tick. We could also add  SCHED_PROFILING , or  SLEEP_PROFILING . But we are fine with current setting.  Of course, we also have a simple dump function  void print_profile_heatmap_nr(int nr) , which is similar to userspace tool  readprofile .", 
            "title": "Supported Features"
        }, 
        {
            "location": "/lego/kernel/profile_heatmap/#example-output", 
            "text": "Workload is: MT-Phoenix word count, with 1GB data. (We probably want to rule out  cpu_idle() )  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 [   1017.309754 ]   Kernel   Heatmap   ( top   # 10 )  [   1017.313731 ]            Address                Function            NR            %  [   1017.321294 ]   ----------------    --------------------    ----------    ---------  [   1017.328858 ]   ffffffff8101a600                cpu_idle        112082        73.11  [   1017.336421 ]   ffffffff810666f0              __schedule         19192        12.95  [   1017.343983 ]   ffffffff8104f500         mlx4_ib_poll_cq          5551         3.99  [   1017.351546 ]   ffffffff8103bf50               delay_tsc          5393         3.83  [   1017.359110 ]   ffffffff81034a10      victim_flush_async          3766         2.72  [   1017.366673 ]   ffffffff8102b220     slob_alloc . constpro          1992         1.47  [   1017.374235 ]   ffffffff810668d0                schedule          1519         0.15  [   1017.381800 ]   ffffffff810648f0     fit_send_reply_with           956         0.95  [   1017.389362 ]   ffffffff81062370     ibapi_send_reply_ti           307         0.30  [   1017.396925 ]   ffffffff8105a0d0     ib_mad_completion_h           232         0.23  [   1017.404487 ]   ----------------    --------------------    ----------    ---------  [   1017.412052 ]                                               151994       100.00  [   1017.419613 ]    \nYizhou Shan \nCreated: April 06, 2018 \nLast Updated: April 06, 2018", 
            "title": "Example Output"
        }, 
        {
            "location": "/lego/kernel/profile_points/", 
            "text": "Lego Profile Points\n\n\nLego profile points facility is added to trace specific functions, or even a small piece of code. It is added in the hope that it can help to find performance bottleneck. It is added in the hope that it can reduce the redundant coding chore.\n\n\nExample\n\n\nTo trace TLB shootdown cost.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nDEFINE_PROFILE_POINT\n(\nflush_tlb_others\n)\n\n\n\n\nvoid\n \nflush_tlb_others\n(\nconst\n \nstruct\n \ncpumask\n \n*\ncpumask\n,\n \nstruct\n \nmm_struct\n \n*\nmm\n,\n\n                      \nunsigned\n \nlong\n \nstart\n,\n \nunsigned\n \nlong\n \nend\n)\n\n\n{\n       \n        \nstruct\n \nflush_tlb_info\n \ninfo\n;\n\n\n        \nPROFILE_POINT_TIME\n(\nflush_tlb_others\n)\n\n\n\n        \nif\n \n(\nend\n \n==\n \n0\n)\n\n                \nend\n \n=\n \nstart\n \n+\n \nPAGE_SIZE\n;\n\n        \ninfo\n.\nflush_mm\n \n=\n \nmm\n;\n\n        \ninfo\n.\nflush_start\n \n=\n \nstart\n;\n\n        \ninfo\n.\nflush_end\n \n=\n \nend\n;\n\n\n\n        \nprofile_point_start\n(\nflush_tlb_others\n);\n\n\n        \nsmp_call_function_many\n(\ncpumask\n,\n \nflush_tlb_func\n,\n \ninfo\n,\n \n1\n);\n\n\n        \nprofile_point_leave\n(\nflush_tlb_others\n);\n\n\n}\n\n\n\n\n\n\nExplanation: \nDEFINE_PROFILE_POINT()\n will define a local structure, that contains the profile point name, number of invoked times, and total execution time. \nPROFILE_POINT_TIME()\n will define a stack local variable, to save the starting time. \nprofile_point_start()\n will save the current time in nanosecond, while \nprofile_point_leave()\n will calculate the execution of this run, and update the global counters defined by \nDEFINE_PROFILE_POINT()\n.\n\n\nSystem-wide profile points will be printed together if you invoke \nprint_profile_points()\n:\n\n1\n2\n3\n4\n5\n6\n[\n \n1017.422911\n]\n \nKernel\n \nProfile\n \nPoints\n\n\n[\n \n1017.426594\n]\n  \nstatus\n                  \nname\n             \ntotal\n                \nnr\n            \navg\n.\nns\n\n\n[\n \n1017.436292\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n[\n \n1017.445988\n]\n     \noff\n      \nflush_tlb_others\n       \n0.000153470\n                \n55\n              \n2791\n\n\n[\n \n1017.455685\n]\n     \noff\n     \npcache_cache_miss\n      \n16.147020152\n            \n274698\n             \n58781\n\n\n[\n \n1017.465381\n]\n \n-------\n  \n--------------------\n  \n----------------\n  \n----------------\n  \n----------------\n\n\n\n\n\n\nMechanism\n\n\nOnce again, the profile points are aggregated by linker script. Each profile point will be in a special section \n.profile.point\n. The linker will merge them into one section, and export the starting and ending address of this section.\n\n\nPart I. Annotate.\n\n1\n2\n3\n4\n5\n6\n7\n#define __profile_point         __section(.profile.point)\n\n\n\n#define DEFINE_PROFILE_POINT(name)                                                      \\\n\n\n        struct profile_point _PP_NAME(name) __profile_point = {\n\n        \n...\n\n        \n...\n\n        \n};\n\n\n\n\n\n\nPart II. Link script merge.\n\n1\n2\n3\n4\n5\n6\n.\n \n=\n \nALIGN\n(\nL1_CACHE_BYTES\n);\n\n\n.\nprofile\n.\npoint\n \n:\n \nAT\n(\nADDR\n(.\nprofile\n.\npoint\n)\n \n-\n \nLOAD_OFFSET\n)\n \n{\n\n    \n__sprofilepoint\n \n=\n \n.;\n\n    \n*\n(.\nprofile\n.\npoint\n)\n\n    \n__eprofilepoint\n \n=\n \n.;\n\n\n}\n\n\n\n\n\n\nPart III. Walk through.\n\n1\n2\n3\n4\n5\n6\n7\n8\nvoid\n \nprint_profile_points\n(\nvoid\n)\n\n\n{\n\n        \nstruct\n \nprofile_point\n \n*\npp\n;\n\n\n        \nfor\n \n(\npp\n \n=\n \n__sprofilepoint\n;\n \npp\n \n \n__eprofilepoint\n;\n \npp\n++\n)\n \n{\n\n                \nprint_profile_point\n(\npp\n);\n\n        \n...\n\n    \n}\n  \n\n\n\n\n\nI really love the linker script. ;-)\n\n\n\nYizhou Shan\n\nCreated: April 06, 2018\n\nLast Updated: April 06, 2018", 
            "title": "Profile points"
        }, 
        {
            "location": "/lego/kernel/profile_points/#lego-profile-points", 
            "text": "Lego profile points facility is added to trace specific functions, or even a small piece of code. It is added in the hope that it can help to find performance bottleneck. It is added in the hope that it can reduce the redundant coding chore.", 
            "title": "Lego Profile Points"
        }, 
        {
            "location": "/lego/kernel/profile_points/#example", 
            "text": "To trace TLB shootdown cost.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 DEFINE_PROFILE_POINT ( flush_tlb_others )   void   flush_tlb_others ( const   struct   cpumask   * cpumask ,   struct   mm_struct   * mm , \n                       unsigned   long   start ,   unsigned   long   end )  {        \n         struct   flush_tlb_info   info ;           PROFILE_POINT_TIME ( flush_tlb_others )  \n         if   ( end   ==   0 ) \n                 end   =   start   +   PAGE_SIZE ; \n         info . flush_mm   =   mm ; \n         info . flush_start   =   start ; \n         info . flush_end   =   end ;           profile_point_start ( flush_tlb_others );           smp_call_function_many ( cpumask ,   flush_tlb_func ,   info ,   1 );           profile_point_leave ( flush_tlb_others );  }    Explanation:  DEFINE_PROFILE_POINT()  will define a local structure, that contains the profile point name, number of invoked times, and total execution time.  PROFILE_POINT_TIME()  will define a stack local variable, to save the starting time.  profile_point_start()  will save the current time in nanosecond, while  profile_point_leave()  will calculate the execution of this run, and update the global counters defined by  DEFINE_PROFILE_POINT() .  System-wide profile points will be printed together if you invoke  print_profile_points() : 1\n2\n3\n4\n5\n6 [   1017.422911 ]   Kernel   Profile   Points  [   1017.426594 ]    status                    name               total                  nr              avg . ns  [   1017.436292 ]   -------    --------------------    ----------------    ----------------    ----------------  [   1017.445988 ]       off        flush_tlb_others         0.000153470                  55                2791  [   1017.455685 ]       off       pcache_cache_miss        16.147020152              274698               58781  [   1017.465381 ]   -------    --------------------    ----------------    ----------------    ----------------", 
            "title": "Example"
        }, 
        {
            "location": "/lego/kernel/profile_points/#mechanism", 
            "text": "Once again, the profile points are aggregated by linker script. Each profile point will be in a special section  .profile.point . The linker will merge them into one section, and export the starting and ending address of this section.  Part I. Annotate. 1\n2\n3\n4\n5\n6\n7 #define __profile_point         __section(.profile.point)  #define DEFINE_PROFILE_POINT(name)                                                      \\          struct profile_point _PP_NAME(name) __profile_point = { \n         ... \n         ... \n         };    Part II. Link script merge. 1\n2\n3\n4\n5\n6 .   =   ALIGN ( L1_CACHE_BYTES );  . profile . point   :   AT ( ADDR (. profile . point )   -   LOAD_OFFSET )   { \n     __sprofilepoint   =   .; \n     * (. profile . point ) \n     __eprofilepoint   =   .;  }    Part III. Walk through. 1\n2\n3\n4\n5\n6\n7\n8 void   print_profile_points ( void )  { \n         struct   profile_point   * pp ; \n\n         for   ( pp   =   __sprofilepoint ;   pp     __eprofilepoint ;   pp ++ )   { \n                 print_profile_point ( pp ); \n         ... \n     }      I really love the linker script. ;-)  \nYizhou Shan \nCreated: April 06, 2018 \nLast Updated: April 06, 2018", 
            "title": "Mechanism"
        }, 
        {
            "location": "/lego/kernel/trampoline/", 
            "text": "How trampoline works in Lego\n\n\nWhat is trampoline code?\n\n\nTrampoline code is used by \nBSP\n to boot other secondary CPUs.\nAt startup, \nBSP\n wakeup secondary CPUs by sending a \nAPIC INIT\n\ncommand, which carry the \n[start_ip]\n where the secondary CPUs should\nstart to run.\n\n\nThe trampoline code is the code starting from \n[start_ip]\n. Used\nby the secondary CPU to jump from \n16-bit realmode\n to \n64-bit\n code\n(the first instruction of 64-bit code will be in \narch/x86/kernel/head_64.S\n).\n\n\nWhere is the trampoline source code?\n\n\nThe source files are all in \narch/x86/realmode/\n. There are two parts: \n1)\n \narch/x86/realmode/rm/trampoline.S\n: which is the code that will run. And it is a mix of 16-bit, 32-bit, 64-bit code (ugh..). \n2)\n \narch/x86/realmode/piggy.S\n: Since the trampoline code can not to linked\ninto kernel image directly. So we have to piggyback the trampoline.bin binary\ncode into a section, which is described by \ntrampoline_start\n and \ntrampoline_end\n. So the kernel can address the trampoline code via these two symbols.\n\n\nThe compile flow is:\n\n1\n2\n3\n4\n5\n6\n    arch/x86/realmode/rm/trmapoline.S\n    -\n CC__ arch/x86/realmode/rm/trmapoline.o\n       -\n LD arch/x86/realmode/rm/trampoline\n          -\n OBJCOPY arch/x86/realmode/rm/trampoline.bin\n             -\n This bin goes into piggy.o\n            -\n piggy.o goes into vmImage\n\n\n\n\n\nWhat happened at runtime?\n\n\nThe setup code was loaded by GRUB below 1MB. Inside \narch/x86/boot/main.c\n, we\nwill save the \ncs()\n into the \nboot_params\n and pass it to kernel. In \nsetup_arch()\n, we will copy the trampoline.bin code to the \ncs()\n address reported by \nboot_param\n. This means we will override setup code, which is okay.\n\n\nAt last, we wake up the secondary CPUs inside \nsmp_init()\n.\n\n\nCompare with Linux\n\n\nI vaguely remember how Linux implement this. The only thing I remember is that Linux use some sort of structure, which is filled by BSP and then passed, or used by secondary CPUs. The mechanism has no difference, though. Linux just has more robust debugging facilities.\n\n\n\nYizhou Shan\n\nMar 3, 2017", 
            "title": "Trampoline"
        }, 
        {
            "location": "/lego/kernel/trampoline/#how-trampoline-works-in-lego", 
            "text": "", 
            "title": "How trampoline works in Lego"
        }, 
        {
            "location": "/lego/kernel/trampoline/#what-is-trampoline-code", 
            "text": "Trampoline code is used by  BSP  to boot other secondary CPUs.\nAt startup,  BSP  wakeup secondary CPUs by sending a  APIC INIT \ncommand, which carry the  [start_ip]  where the secondary CPUs should\nstart to run.  The trampoline code is the code starting from  [start_ip] . Used\nby the secondary CPU to jump from  16-bit realmode  to  64-bit  code\n(the first instruction of 64-bit code will be in  arch/x86/kernel/head_64.S ).", 
            "title": "What is trampoline code?"
        }, 
        {
            "location": "/lego/kernel/trampoline/#where-is-the-trampoline-source-code", 
            "text": "The source files are all in  arch/x86/realmode/ . There are two parts:  1)   arch/x86/realmode/rm/trampoline.S : which is the code that will run. And it is a mix of 16-bit, 32-bit, 64-bit code (ugh..).  2)   arch/x86/realmode/piggy.S : Since the trampoline code can not to linked\ninto kernel image directly. So we have to piggyback the trampoline.bin binary\ncode into a section, which is described by  trampoline_start  and  trampoline_end . So the kernel can address the trampoline code via these two symbols.  The compile flow is: 1\n2\n3\n4\n5\n6     arch/x86/realmode/rm/trmapoline.S\n    -  CC__ arch/x86/realmode/rm/trmapoline.o\n       -  LD arch/x86/realmode/rm/trampoline\n          -  OBJCOPY arch/x86/realmode/rm/trampoline.bin\n             -  This bin goes into piggy.o\n            -  piggy.o goes into vmImage", 
            "title": "Where is the trampoline source code?"
        }, 
        {
            "location": "/lego/kernel/trampoline/#what-happened-at-runtime", 
            "text": "The setup code was loaded by GRUB below 1MB. Inside  arch/x86/boot/main.c , we\nwill save the  cs()  into the  boot_params  and pass it to kernel. In  setup_arch() , we will copy the trampoline.bin code to the  cs()  address reported by  boot_param . This means we will override setup code, which is okay.  At last, we wake up the secondary CPUs inside  smp_init() .", 
            "title": "What happened at runtime?"
        }, 
        {
            "location": "/lego/kernel/trampoline/#compare-with-linux", 
            "text": "I vaguely remember how Linux implement this. The only thing I remember is that Linux use some sort of structure, which is filled by BSP and then passed, or used by secondary CPUs. The mechanism has no difference, though. Linux just has more robust debugging facilities.  \nYizhou Shan \nMar 3, 2017", 
            "title": "Compare with Linux"
        }, 
        {
            "location": "/lego/kernel/pagefault_disable/", 
            "text": "The story of pagefault_disable/enable\n\n\npagefault_disable()\n is not really disabling the whole pgfault handling code. It is used to disable only the handling of pgfault that landed from \nuser virtual address\n. Please note the difference between \nuser virtual address\n and \nuser mode fault\n. The first means the faulting address belongs to user virtual address space, while it can come from either user mode (CPL3) or kernel mode (CPL0). The second is a fault come from user mode (CPL3).\n\n\nIf pgfault is disabled, then \ndo_page_fault()\n function will \nNOT\n try to solve the pgfault by calling into \npcache\n, instead, it will go straight to \nfixup\n code (in no_context()).\n\n\nThis function is not intended to be used standalone. Normally, we do \n1)\n \npagefault_disable()\n, \n2)\n then use some functions that have \nfixup\n code, \n3)\n then \npagefault_enable()\n. (The \nfixup\n code is another magic inside kernel. We will cover it in another document.)\n\n\nCurrently in Lego, this is only used by \nfutex\n, which needs something like \natomic_cmpxchg()\n with an user virtual address. If pgfault happens in the middle, then this will not be atomic since kernel need to do pcache operations, which further needs to through network.\n\n\nHowever, do note the difference with \nuaccess\n family functions. Most \nuaccess\n functions will not disable pgfault handling, which means pcache will be invoked. If pcache returns a \nSEGFAULT\n, pgfault code will go into \nfixup\n code. And that, my friend, is where \nuaccess\n returns \n-EFAULT\n to caller.\n\n\n\nYizhou Shan\n\nFeb 01, 2018", 
            "title": "Disable pgfault"
        }, 
        {
            "location": "/lego/kernel/pagefault_disable/#the-story-of-pagefault_disableenable", 
            "text": "pagefault_disable()  is not really disabling the whole pgfault handling code. It is used to disable only the handling of pgfault that landed from  user virtual address . Please note the difference between  user virtual address  and  user mode fault . The first means the faulting address belongs to user virtual address space, while it can come from either user mode (CPL3) or kernel mode (CPL0). The second is a fault come from user mode (CPL3).  If pgfault is disabled, then  do_page_fault()  function will  NOT  try to solve the pgfault by calling into  pcache , instead, it will go straight to  fixup  code (in no_context()).  This function is not intended to be used standalone. Normally, we do  1)   pagefault_disable() ,  2)  then use some functions that have  fixup  code,  3)  then  pagefault_enable() . (The  fixup  code is another magic inside kernel. We will cover it in another document.)  Currently in Lego, this is only used by  futex , which needs something like  atomic_cmpxchg()  with an user virtual address. If pgfault happens in the middle, then this will not be atomic since kernel need to do pcache operations, which further needs to through network.  However, do note the difference with  uaccess  family functions. Most  uaccess  functions will not disable pgfault handling, which means pcache will be invoked. If pcache returns a  SEGFAULT , pgfault code will go into  fixup  code. And that, my friend, is where  uaccess  returns  -EFAULT  to caller.  \nYizhou Shan \nFeb 01, 2018", 
            "title": "The story of pagefault_disable/enable"
        }, 
        {
            "location": "/lego/kernel/stop_machine/", 
            "text": "The highest priority thread in kernel\n\n\nThis document is about \nmigration/N\n kernel threads, \nstop_sched\n schdueling class, and the interesting source file \nkernel/stop_machine.c\n. Background on kernel scheduler design is recommended.\n\n\nScheduler uses the following code to pick the next runnable task:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nstatic\n \ninline\n \nstruct\n \ntask_struct\n \n*\n\n\npick_next_task\n(\nstruct\n \nrq\n \n*\nrq\n,\n \nstruct\n \ntask_struct\n \n*\nprev\n)\n\n\n{\n\n        \nstruct\n \ntask_struct\n \n*\np\n;\n\n        \nconst\n \nstruct\n \nsched_class\n \n*\nclass\n;\n\n\n\nagain\n:\n\n        \nfor_each_class\n(\nclass\n)\n \n{\n\n                \np\n \n=\n \nclass\n-\npick_next_task\n(\nrq\n,\n \nprev\n);\n\n                \nif\n \n(\np\n)\n \n{\n\n                        \nif\n \n(\nunlikely\n(\np\n \n==\n \nRETRY_TASK\n))\n\n                                \ngoto\n \nagain\n;\n\n                        \nreturn\n \np\n;\n\n                \n}\n    \n        \n}\n\n        \nBUG\n();\n\n\n}\n\n\n\n\n\n\nwhile the class is linked together as:\n\n1\n2\n3\n#define sched_class_highest     (\nstop_sched_class)                                                       \n\n\n#define for_each_class(class) \\                                                                           \n\n   \nfor\n \n(\nclass\n \n=\n \nsched_class_highest\n;\n \nclass\n;\n \nclass\n \n=\n \nclass\n-\nnext\n)\n\n\n\n\n\n\nClearly, the highest priority class is \nstop_sched_class\n. Whenever this scheduling has class runnable threads, scheduler will always run them first. So what kernel threads are using this scheduling class? Well, you must have seen something like \nmigration/0\n when you do \nps aux\n in Linux. And yes, these kernel threads are the only users.\n\n\nThese threads are sleeping most of their lifetime, they will be invoked to do some very urgent stuff. For example, when a user thread that is currently running on CPU0 calls \nsched_setaffinity()\n to bind to CPU1, kernel is not able to do this because this user thread is currently running (runqueue can not move a \nrunning\n task out, it can only move queued task out). Then, scheduler has to ask \nmigration/0\n for help. Once there is a job enqueued, \nmigration/0\n will be invoked. Since it has the highest-priority, it will start execution immediately. Thus the migration from CPU0 to CPU1 is performed safely and fast.\n\n\nmigration\n code is defined in \nkernel/stop_machine.c\n. They are created during early boot. They use the \nsmpboot_register_percpu_thread\n to create threads. They are written in this way because Linux supports cpu hotplug. To simplify we can also create them manually through \nkthread_create\n. Since Lego does not support cpu hotplug, and this \ncpu_stop_init\n is called after SMP is initialized, so Lego has slight different initialiaztion:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nvoid\n \n__init\n \ncpu_stop_init\n(\nvoid\n)\n\n\n{\n\n        \nunsigned\n \nint\n \ncpu\n;\n\n\n        \nfor_each_possible_cpu\n(\ncpu\n)\n \n{\n\n                \nstruct\n \ncpu_stopper\n \n*\nstopper\n \n=\n \nper_cpu\n(\ncpu_stopper\n,\n \ncpu\n);\n\n\n                \nspin_lock_init\n(\nstopper\n-\nlock\n);\n\n                \nINIT_LIST_HEAD\n(\nstopper\n-\nworks\n);\n\n        \n}\n\n\n        \nBUG_ON\n(\nsmpboot_register_percpu_thread\n(\ncpu_stop_threads\n));\n\n\n        \n/*\n\n\n         * smpboot_create_threads use kthread_create_on_cpu() to\n\n\n         * create new threads. And they are parked, too.\n\n\n         * Since we call this function after smp_init(), all CPUs\n\n\n         * are already online, thus we need to unpark them manually.\n\n\n         */\n\n        \nfor_each_online_cpu\n(\ncpu\n)\n\n                \nstop_machine_unpark\n(\ncpu\n);\n\n\n\n\n\n\nInternally, it also use a list to keep enqueued jobs. Once the thread is waken up, it tries to lookup this list and dequeue jobs (similar to kthread creation, kworker etc.):\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nstatic\n \nvoid\n \ncpu_stopper_thread\n(\nunsigned\n \nint\n \ncpu\n)\n\n\n{\n\n        \nstruct\n \ncpu_stopper\n \n*\nstopper\n \n=\n \nper_cpu\n(\ncpu_stopper\n,\n \ncpu\n);\n\n        \nstruct\n \ncpu_stop_work\n \n*\nwork\n;\n\n\n\nrepeat\n:\n\n        \nwork\n \n=\n \nNULL\n;\n\n        \nspin_lock_irq\n(\nstopper\n-\nlock\n);\n\n        \nif\n \n(\n!\nlist_empty\n(\nstopper\n-\nworks\n))\n \n{\n\n                \nwork\n \n=\n \nlist_first_entry\n(\nstopper\n-\nworks\n,\n\n                                        \nstruct\n \ncpu_stop_work\n,\n \nlist\n);\n\n                \nlist_del_init\n(\nwork\n-\nlist\n);\n\n        \n}\n   \n        \nspin_unlock_irq\n(\nstopper\n-\nlock\n);\n\n\n        \nif\n \n(\nwork\n)\n \n{\n\n                \n...\n\n                \nret\n \n=\n \nfn\n(\narg\n);\n\n                \n...\n\n                \ngoto\n \nrepeat\n;\n\n        \n}\n   \n\n}\n\n\n\n\n\n\nIt has several interesting public APIs that are quite similar to \nsmp_call_functions\n, but the difference is: this set of APIs provide a guaranteed time-to-execute waiting time, because it will simply preempt anything running on CPU.\n\n\n1\n2\n3\nint\n \nstop_one_cpu\n(\nunsigned\n \nint\n \ncpu\n,\n \ncpu_stop_fn_t\n \nfn\n,\n \nvoid\n \n*\narg\n);\n\n\nint\n \nstop_cpus\n(\nconst\n \nstruct\n \ncpumask\n \n*\ncpumask\n,\n \ncpu_stop_fn_t\n \nfn\n,\n \nvoid\n \n*\narg\n);\n\n\nint\n \ntry_stop_cpus\n(\nconst\n \nstruct\n \ncpumask\n \n*\ncpumask\n,\n \ncpu_stop_fn_t\n \nfn\n,\n \nvoid\n \n*\narg\n);\n\n\n\n\n\n\n\nThey are used only when there are some very urgent things to do. So, please use with caution.\n\n\n\nYizhou Shan\n\nCreated: Feb 12, 2018\n\nLast Updated: Feb 12, 2018", 
            "title": "Stop machine"
        }, 
        {
            "location": "/lego/kernel/stop_machine/#the-highest-priority-thread-in-kernel", 
            "text": "This document is about  migration/N  kernel threads,  stop_sched  schdueling class, and the interesting source file  kernel/stop_machine.c . Background on kernel scheduler design is recommended.  Scheduler uses the following code to pick the next runnable task:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 static   inline   struct   task_struct   *  pick_next_task ( struct   rq   * rq ,   struct   task_struct   * prev )  { \n         struct   task_struct   * p ; \n         const   struct   sched_class   * class ;  again : \n         for_each_class ( class )   { \n                 p   =   class - pick_next_task ( rq ,   prev ); \n                 if   ( p )   { \n                         if   ( unlikely ( p   ==   RETRY_TASK )) \n                                 goto   again ; \n                         return   p ; \n                 }     \n         } \n         BUG ();  }    while the class is linked together as: 1\n2\n3 #define sched_class_highest     ( stop_sched_class)                                                         #define for_each_class(class) \\                                                                            \n    for   ( class   =   sched_class_highest ;   class ;   class   =   class - next )    Clearly, the highest priority class is  stop_sched_class . Whenever this scheduling has class runnable threads, scheduler will always run them first. So what kernel threads are using this scheduling class? Well, you must have seen something like  migration/0  when you do  ps aux  in Linux. And yes, these kernel threads are the only users.  These threads are sleeping most of their lifetime, they will be invoked to do some very urgent stuff. For example, when a user thread that is currently running on CPU0 calls  sched_setaffinity()  to bind to CPU1, kernel is not able to do this because this user thread is currently running (runqueue can not move a  running  task out, it can only move queued task out). Then, scheduler has to ask  migration/0  for help. Once there is a job enqueued,  migration/0  will be invoked. Since it has the highest-priority, it will start execution immediately. Thus the migration from CPU0 to CPU1 is performed safely and fast.  migration  code is defined in  kernel/stop_machine.c . They are created during early boot. They use the  smpboot_register_percpu_thread  to create threads. They are written in this way because Linux supports cpu hotplug. To simplify we can also create them manually through  kthread_create . Since Lego does not support cpu hotplug, and this  cpu_stop_init  is called after SMP is initialized, so Lego has slight different initialiaztion:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 void   __init   cpu_stop_init ( void )  { \n         unsigned   int   cpu ; \n\n         for_each_possible_cpu ( cpu )   { \n                 struct   cpu_stopper   * stopper   =   per_cpu ( cpu_stopper ,   cpu ); \n\n                 spin_lock_init ( stopper - lock ); \n                 INIT_LIST_HEAD ( stopper - works ); \n         } \n\n         BUG_ON ( smpboot_register_percpu_thread ( cpu_stop_threads )); \n\n         /*           * smpboot_create_threads use kthread_create_on_cpu() to           * create new threads. And they are parked, too.           * Since we call this function after smp_init(), all CPUs           * are already online, thus we need to unpark them manually.           */ \n         for_each_online_cpu ( cpu ) \n                 stop_machine_unpark ( cpu );    Internally, it also use a list to keep enqueued jobs. Once the thread is waken up, it tries to lookup this list and dequeue jobs (similar to kthread creation, kworker etc.):  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 static   void   cpu_stopper_thread ( unsigned   int   cpu )  { \n         struct   cpu_stopper   * stopper   =   per_cpu ( cpu_stopper ,   cpu ); \n         struct   cpu_stop_work   * work ;  repeat : \n         work   =   NULL ; \n         spin_lock_irq ( stopper - lock ); \n         if   ( ! list_empty ( stopper - works ))   { \n                 work   =   list_first_entry ( stopper - works , \n                                         struct   cpu_stop_work ,   list ); \n                 list_del_init ( work - list ); \n         }    \n         spin_unlock_irq ( stopper - lock ); \n\n         if   ( work )   { \n                 ... \n                 ret   =   fn ( arg ); \n                 ... \n                 goto   repeat ; \n         }     }    It has several interesting public APIs that are quite similar to  smp_call_functions , but the difference is: this set of APIs provide a guaranteed time-to-execute waiting time, because it will simply preempt anything running on CPU.  1\n2\n3 int   stop_one_cpu ( unsigned   int   cpu ,   cpu_stop_fn_t   fn ,   void   * arg );  int   stop_cpus ( const   struct   cpumask   * cpumask ,   cpu_stop_fn_t   fn ,   void   * arg );  int   try_stop_cpus ( const   struct   cpumask   * cpumask ,   cpu_stop_fn_t   fn ,   void   * arg );    They are used only when there are some very urgent things to do. So, please use with caution.  \nYizhou Shan \nCreated: Feb 12, 2018 \nLast Updated: Feb 12, 2018", 
            "title": "The highest priority thread in kernel"
        }, 
        {
            "location": "/lego/kernel/loader/", 
            "text": "Lego Program Loader\n\n\nThis document explains the high-level workflow of Lego\ns program loader, and how we change the normal loader to fit the disaggregated operating system model. Background on linking and loading is recommended.\n\n\nStatus\n\n\n\n\n\n\n\n\nFormats\n\n\nSupported\n\n\n\n\n\n\n\n\n\n\nELF (static-linked)\n\n\n\n\n\n\n\n\nELF (dynamic-linked)\n\n\n\n\n\n\n\n\n\n\nOverall\n\n\nIn order to support different executable formats, Lego has a \nvirtual loader layer\n above all specific formats, which is quite similar to \nvirtual file system\n. In Lego, \nexecve()\n is divided into two parts: \n1)\n syscall hook at processor side, \n2)\n real loader at memory side. Combined together, they provide the same semantic of \nexecve()\n as described in Linux man page. Also for the code, we divide the Linux implementation into parts. But our emulation model introduces several interesting workarounds.\n\n\nLego\ns Loader\n\n\nLego basically divide the Linux loader into two parts, one in memory manager and other in processor manager. Most dirty work is done by memory manager. Processor manager only needs to make sure the new execution has a fresh environment to start.\n\n\nEntry Point\n\n\nSo the normal entry point is \ndo_execve()\n. Above that, it can be invoked by syscall from user space, or from kernel space by calling \ndo_execve()\n directly. There are not too many places that will call \ndo_execve\n within kernel. One notable case is how kernel starts the \npid 1\n user program. This happens after kernel finished all initialization. The code is:\n\n1\n2\n3\n4\n5\nstatic\n \nint\n \nrun_init_process\n(\nconst\n \nchar\n \n*\ninit_filename\n)\n                                                    \n\n{\n\n        \nargv_init\n[\n0\n]\n \n=\n \ninit_filename\n;\n\n        \nreturn\n \ndo_execve\n(\ninit_filename\n,\n \nargv_init\n,\n \nenvp_init\n);\n\n\n}\n\n\n\n\n\n\nMemory Manager\ns Job\n\n\nMemory manager side will do most of the dirty loading work. It will parse the ELF image, create new VMAs based on ELF information. After that, it only pass \nstart_ip\n and \nstart_stack\n back to processor manager. Once processor manager starts running this new execution, pages will be fetched from memory component on demand.\n\n\nLoad ld-linux\n\n\nFor dynamically-linked images, kernel ELF loader needs to load the \nld-linux.so\n as well. It will first try to map the \nld-linux.so\n into this process\ns virtual address space. Furthermore, the first user instruction that will run is no longer \n__libc_main_start\n, kernel will transfer the kernel to \nld-linux.so\n instead. Thus, for a normal user program, \nld-linux.so\n will load all the shared libraries before running glibc.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n\n        \n...\n\n        \n/* Dynamically-linked */\n\n        \nif\n \n(\nelf_interpreter\n)\n \n{\n\n                \nunsigned\n \nlong\n \ninterp_map_addr\n \n=\n \n0\n;\n\n\n\n                \nelf_entry\n \n=\n \nload_elf_interp\n(\ntsk\n,\n \nloc\n-\ninterp_elf_ex\n,\n\n\n                                            \ninterpreter\n,\n\n                                            \ninterp_map_addr\n,\n\n                                            \nload_bias\n,\n \ninterp_elf_phdata\n);\n\n                \nif\n \n(\n!\nIS_ERR\n((\nvoid\n \n*\n)\nelf_entry\n))\n \n{\n\n                        \n/*\n\n\n                         * load_elf_interp() returns relocation\n\n\n                         * adjustment\n\n\n                         */\n\n                        \ninterp_load_addr\n \n=\n \nelf_entry\n;\n\n                        \nelf_entry\n \n+=\n \nloc\n-\ninterp_elf_ex\n.\ne_entry\n;\n\n                \n}\n\n                \nif\n \n(\nBAD_ADDR\n(\nelf_entry\n))\n \n{\n\n                        \nretval\n \n=\n \nIS_ERR\n((\nvoid\n \n*\n)\nelf_entry\n)\n \n?\n\n                                        \n(\nint\n)\nelf_entry\n \n:\n \n-\nEINVAL\n;\n\n                        \ngoto\n \nout_free_dentry\n;\n\n                \n}\n\n                \nreloc_func_desc\n \n=\n \ninterp_load_addr\n;\n\n\n                \nput_lego_file\n(\ninterpreter\n);\n\n                \nkfree\n(\nelf_interpreter\n);\n\n        \n}\n \nelse\n \n{\n\n        \n/* Statically-linked */\n\n                \n/*\n\n\n                 * e_entry is the VA to which the system first transfers control\n\n\n                 * Not the start_code! Normally, it is the \n_start\n function.\n\n\n                 */\n\n\n                \nelf_entry\n \n=\n \nloc\n-\nelf_ex\n.\ne_entry\n;\n\n\n                \nif\n \n(\nBAD_ADDR\n(\nelf_entry\n))\n \n{\n\n                        \nretval\n \n=\n \n-\nEINVAL\n;\n\n                        \ngoto\n \nout_free_dentry\n;\n\n                \n}\n\n        \n}\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nProcessor Manager\ns Job\n\n\nIt needs to flush old execution environment, and setup the new execution environment, such as signal, FPU. Notably, processor manager need to run \nflush_old_exec()\n, and \nsetup_new_exec()\n.\n\n\nDestroy old context: flush_old_exec()\n\n\nZap other threads\n\n\nde_thread\n is used to kill other threads within the same thread group, thus make sure this process has its own signal table. Furthermore, A \nexec\n starts a new thread group with the same TGID of the previous thread group, so we probably also need to switch PID if calling thread is not a leader.\n\n\nSwitch to new address space\n\n\nWe also need to release the old mm, and allocate a new mm. The new mm only has the high address kernel mapping established. Do note that in Lego, pgtable is used to emulate the processor cache:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nstatic\n \nint\n \nexec_mmap\n(\nvoid\n)\n\n\n{\n\n        \nstruct\n \nmm_struct\n \n*\nnew_mm\n;\n\n        \nstruct\n \nmm_struct\n \n*\nold_mm\n;\n\n        \nstruct\n \ntask_struct\n \n*\ntsk\n;\n\n\n        \nnew_mm\n \n=\n \nmm_alloc\n();\n\n        \nif\n \n(\n!\nnew_mm\n)\n\n                \nreturn\n \n-\nENOMEM\n;\n\n\n        \ntsk\n \n=\n \ncurrent\n;\n\n        \nold_mm\n \n=\n \ncurrent\n-\nmm\n;\n\n        \nmm_release\n(\ntsk\n,\n \nold_mm\n);\n\n\n        \ntask_lock\n(\ntsk\n);\n\n        \ntsk\n-\nmm\n \n=\n \nnew_mm\n;\n\n        \ntsk\n-\nactive_mm\n \n=\n \nnew_mm\n;\n\n        \nactivate_mm\n(\nold_mm\n,\n \nnew_mm\n);\n\n        \ntask_unlock\n(\ntsk\n);\n\n\n        \nif\n \n(\nold_mm\n)\n\n                \nmmput\n(\nold_mm\n);\n\n        \nreturn\n \n0\n;\n\n\n}\n\n\n\n\n\n\nClear Architecture-Specific state\n\n\nThis is performed by \nflush_thread()\n, which is an architecture-specific callback. In x86, we need to clear FPU state, and reset TLS array:\n\n1\n2\n3\n4\n5\n6\n7\nvoid\n \nflush_thread\n(\nvoid\n)\n\n\n{\n\n        \nstruct\n \ntask_struct\n \n*\ntsk\n \n=\n \ncurrent\n;\n\n        \nmemset\n(\ntsk\n-\nthread\n.\ntls_array\n,\n \n0\n,\n \nsizeof\n(\ntsk\n-\nthread\n.\ntls_array\n));\n\n\n        \nfpu__clear\n(\ntsk\n-\nthread\n.\nfpu\n);\n\n\n}\n\n\n\n\n\n\nSetup new context: setup_new_exec()\n\n\nLego\ns \nsetup_new_exec()\n is quite different from Linux\ns default implementation. Lego moves several functions to memory component, like the \narch_pick_mmap_layout\n stuff. Thus, Lego only flush the signal handlers and reset the signal stack stuff:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nstatic\n \nvoid\n \nsetup_new_exec\n(\nconst\n \nchar\n \n*\nfilename\n)\n\n\n{\n\n        \n/* This is the point of no return */\n\n        \ncurrent\n-\nsas_ss_sp\n \n=\n \ncurrent\n-\nsas_ss_size\n \n=\n \n0\n;\n\n\n        \nset_task_comm\n(\ncurrent\n,\n \nkbasename\n(\nfilename\n));\n\n\n        \nflush_signal_handlers\n(\ncurrent\n,\n \n0\n);\n\n\n}\n\n\n\n\n\n\nChange return frame in stack\n\n\nWe do not return to user mode here, we simply replace the return IP of the regs frame. While the kernel thread returns, it will simply merge to syscall return path (check ret_from_fork() in entry.S for detail).\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n/**\n\n\n * start_thread - Starting a new user thread\n\n\n * @regs: pointer to pt_regs\n\n\n * @new_ip: the first instruction IP of user thread\n\n\n * @new_sp: the new stack pointer of user thread\n\n\n */\n\n\nvoid\n \nstart_thread\n(\nstruct\n \npt_regs\n \n*\nregs\n,\n \nunsigned\n \nlong\n \nnew_ip\n,\n\n                  \nunsigned\n \nlong\n \nnew_sp\n)\n\n\n{\n\n        \nloadsegment\n(\nfs\n,\n \n0\n);\n\n        \nloadsegment\n(\nes\n,\n \n0\n);\n\n        \nloadsegment\n(\nds\n,\n \n0\n);\n\n        \nload_gs_index\n(\n0\n);\n\n        \nregs\n-\nip\n                \n=\n \nnew_ip\n;\n\n        \nregs\n-\nsp\n                \n=\n \nnew_sp\n;\n\n        \nregs\n-\ncs\n                \n=\n \n__USER_CS\n;\n\n        \nregs\n-\nss\n                \n=\n \n__USER_DS\n;\n\n        \nregs\n-\nflags\n             \n=\n \nX86_EFLAGS_IF\n;\n\n\n}\n\n\n\n\n\n\nIf calling \nexecve()\n from userspace, the return frame is saved in the stack, we can simply do \nstart_thread\n above, and merge to syscall return path. However, if calling \nexecve()\n from a kernel thread, things changed. As you can see, all forked threads will run from \nret_from_fork\n when it wakes for the first time. If it is a kernel thread, it jumps to \nline 23\n, to execute the kernel function. Normally, the function should not return. If it does return, it normally has called an \nexecve()\n, and return frame has been changed by \nstart_thread()\n. So we jump to \nline 16\n to let it merge to syscall return path.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n/*\n\n \n*\n \nA\n \nnewly\n \nforked\n \nprocess\n \ndirectly\n \ncontext\n \nswitches\n \ninto\n \nthis\n \naddress.\n\n \n*\n\n \n*\n \nrax:\n \nprev\n \ntask\n \nwe\n \nswitched\n \nfrom\n\n \n*\n \nrbx:\n \nkernel\n \nthread\n \nfunc\n \n(\nNULL\n \nfor\n \nuser\n \nthread\n)\n\n \n*\n \nr12:\n \nkernel\n \nthread\n \narg\n\n \n*/\n\n\nENTRY\n(\nret_from_fork\n)\n\n        \nmovq\n    \n%rax\n,\n \n%rdi\n\n        \ncall\n    \nschedule_tail\n           \n/\n*\n \nrdi\n:\n \nprev\n \ntask\n \nparameter\n \n*\n/\n\n\n        \ntestq\n   \n%rbx\n,\n \n%rbx\n              \n/\n*\n \nfrom\n \nkernel_thread\n?\n \n*\n/\n\n        \njnz\n     \n1\nf\n                      \n/\n*\n \nkernel\n \nthreads\n \nare\n \nuncommon\n \n*\n/\n\n\n\n2:\n\n\n        \nmovq\n    \n%rsp\n,\n \n%rdi\n\n\n        \ncall\n    \nsyscall_return_slowpath\n \n/\n*\n \nreturn\n \nwith\n \nIRQs\n \ndisabled\n \n*\n/\n\n        \nSWAPGS\n                          \n/\n*\n \nswitch\n \nto\n \nuser\n \ngs.base\n \n*\n/\n\n        \njmp\n     \nrestore_regs_and_iret\n\n\n\n1:\n\n        \n/*\n \nkernel\n \nthread\n \n*\n/\n\n\n        \nmovq\n    \n%r12\n,\n \n%rdi\n\n\n        \ncall\n    \n*\n%rbx\n\n        \n/*\n  \n         \n*\n \nA\n \nkernel\n \nthread\n \nis\n \nallowed\n \nto\n \nreturn\n \nhere\n \nafter\n \nsuccessfully\n\n         \n*\n \ncalling\n \ndo_execve\n().\n  \nExit\n \nto\n \nuserspace\n \nto\n \ncomplete\n \nthe\n \nexecve\n()\n\n         \n*\n \nsyscall:\n\n         \n*/\n\n        \nmovq\n    \n$0\n,\n \nRAX\n(\n%rsp\n)\n\n        \njmp\n     \n2\nb\n  \n\nEND\n(\nret_from_fork\n)\n\n\n\n\n\n\n\nThis is such a typical control flow hijacking. :-)\n\n\nFeatures\n\n\nThis section lists various features, or behaviors and Lego\ns program loader.\n\n\n\n\nVirtual Address Space Range\n\n\nUser\ns virtual address falls into this range:\n\n1\n[sysctl_mmap_min_addr, TASK_SIZE)\n\n\n\n\n\nBy default,\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nunsigned\n \nlong\n \nsysctl_mmap_min_addr\n \n=\n \nPAGE_SIZE\n;\n\n\n\n/*\n\n\n * User space process size. 47bits minus one guard page.  The guard\n\n\n * page is necessary on Intel CPUs: if a SYSCALL instruction is at\n\n\n * the highest possible canonical userspace address, then that\n\n\n * syscall will enter the kernel with a non-canonical return\n\n\n * address, and SYSRET will explode dangerously.  We avoid this\n\n\n * particular problem by preventing anything from being mapped\n\n\n * at the maximum canonical address.\n\n\n */\n                                                                                                       \n\n#define TASK_SIZE       ((1UL \n 47) - PAGE_SIZE)\n\n\n\n\n\n\nEssentially:\n\n1\n[0x1000, 0x7ffffffff000)\n\n\n\n\n\n\n\nPre-Populated \n.bss\n and \n.brk\n\n\nThe heap vma created at loading time is a combination of \n.bss\n and \n.brk\n segments. Since brk usage is 0 (will it be non-zero?) at this moment, so the heap vma is essentially just \n.bss\n pages. Normally, Linux kernel does not populate pages for this vma during loading, but Lego does. It can save several page allocation cost for heap pcache miss. It is controlled by \nvm_brk()\n.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nint\n \nvm_brk\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n\n           \nunsigned\n \nlong\n \nstart\n,\n \nunsigned\n \nlong\n \nlen\n)\n\n\n{\n\n        \nint\n \nret\n;\n\n        \nstruct\n \nlego_mm_struct\n \n*\nmm\n \n=\n \ntsk\n-\nmm\n;\n\n\n        \nif\n \n(\ndown_write_killable\n(\nmm\n-\nmmap_sem\n))\n\n                \nreturn\n \n-\nEINTR\n;\n\n\n        \nret\n \n=\n \ndo_brk\n(\ntsk\n,\n \nstart\n,\n \nlen\n);\n\n        \nup_write\n(\nmm\n-\nmmap_sem\n);\n\n\n        \n/* Prepopulate brk pages */\n\n        \nif\n \n(\n!\nret\n)\n\n                \nlego_mm_populate\n(\nmm\n,\n \nstart\n,\n \nlen\n);\n\n\n        \nreturn\n \nret\n;\n\n\n}\n\n\n\n\n\n\n\n\nUn-Populated stack\n\n\nStack vma is manually expanded to \n32 pages + pages for argv info\n by loader to accommodate future usage. Only pages for argv are populated by default, the extra 32 pages are not. A typical program may need 1 page for saving argv info, plus the 32 extra, the layout will be:\n\n1\n7ffffffde000-7ffffffff000 rw-p 00000000 [stack]\n\n\n\n\n\nThe code to expand stack is done when ELF loader tries to finalize the stack vma, by calling \nsetup_arg_pages()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nint\n \nsetup_arg_pages\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                    \nunsigned\n \nlong\n \nstack_top\n,\n \nint\n \nexecutable_stack\n)\n\n\n{\n\n        \n...\n\n        \n/*\n\n\n         * 32*4k (or 2*64k) pages\n\n\n         */\n\n        \nstack_expand\n \n=\n \n131072UL\n;\n\n        \nstack_size\n \n=\n \nvma\n-\nvm_end\n \n-\n \nvma\n-\nvm_start\n;\n\n        \nstack_base\n \n=\n \nvma\n-\nvm_start\n \n-\n \nstack_expand\n;\n\n\n        \nmm\n-\nstart_stack\n \n=\n \nbprm\n-\np\n;\n\n        \nret\n \n=\n \nexpand_stack\n(\nvma\n,\n \nstack_base\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\n\nUn-Populated \n.text\n and \n.data\n\n\nIn essence, all PT_LOAD segments of ELF image are not pre-populated. They will be fetched from storage on demand. This is the traditional on-demand paging way. If we want to reduce the overhead of code and data\ns on-demand paging, we can prefault them in the future.\n\n\n\n\nDisabled Randomized Top of Stack\n\n\nLego currently does not randomize the stack top. The stack vma is allocated by \nbprm_mm_init()\n at early execve time. There is no randomization at the allocation time, and this applies to all exectuable formats. The end of vma is just \nTASK_SIZE\n:\n\n1\n2\n3\n4\n5\n6\n7\nstatic\n \nint\n \n__bprm_mm_init\n(\nstruct\n \nlego_binprm\n \n*\nbprm\n)\n\n\n{\n\n        \n...\n\n        \nvma\n-\nvm_end\n \n=\n \nTASK_SIZE\n;\n\n        \n...\n\n\n}\n\n\n(\nmanagers\n/\nmemory\n/\nloader\n/\nelf\n.\nc\n)\n\n\n\n\n\n\nTop of stack randomization happens within each specific format loader. They do this by calling back to virtual loader layer\ns \nsetup_arg_pages()\n function, which is used to finalize the top of stack:\n\n1\n2\nint\n \nsetup_arg_pages\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                    \nunsigned\n \nlong\n \nstack_top\n,\n \nint\n \nexecutable_stack\n);\n\n\n\n\n\n\nSo, to actually randomize the top of stack, you can simply do the following:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nstatic\n \nunsigned\n \nlong\n \nrandomize_stack_top\n(\nunsigned\n \nlong\n \nstack_top\n)\n\n\n{\n                                \n        \nunsigned\n \nlong\n \nrandom_variable\n \n=\n \n0\n;\n\n\n        \nif\n \n((\ncurrent\n-\nflags\n \n \nPF_RANDOMIZE\n)\n \n\n                \n!\n(\ncurrent\n-\npersonality\n \n \nADDR_NO_RANDOMIZE\n))\n \n{\n\n                \nrandom_variable\n \n=\n \nget_random_long\n();\n\n                \nrandom_variable\n \n=\n \nSTACK_RND_MASK\n;\n\n                \nrandom_variable\n \n=\n \nPAGE_SHIFT\n;\n\n        \n}\n\n\n#ifdef CONFIG_STACK_GROWSUP\n\n        \nreturn\n \nPAGE_ALIGN\n(\nstack_top\n)\n \n+\n \nrandom_variable\n;\n\n\n#else           \n\n        \nreturn\n \nPAGE_ALIGN\n(\nstack_top\n)\n \n-\n \nrandom_variable\n;\n\n\n#endif\n\n\n}\n\n\n\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n        \nretval\n \n=\n \nsetup_arg_pages\n(\nbprm\n,\n \nrandomize_stack_top\n(\nTASK_SIZE\n),\n\n                                 \nexecutable_stack\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\nHowever, current Lego disables randomization by passing \nTASK_SIZE\n:\n\n1\n2\n3\n4\n5\n6\n7\n8\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n        \nretval\n \n=\n \nsetup_arg_pages\n(\ntsk\n,\n \nbprm\n,\n \nTASK_SIZE\n,\n \nexecutable_stack\n);\n\n        \n...\n\n\n}\n\n\n(\nmanagers\n/\nmemory\n/\nloader\n/\nelf\n.\nc\n)\n\n\n\n\n\n\n\n\nNo vDSO\n\n\nCurrently, Lego does not have \nvDSO\n support. There are not too many syscalls mapped in the vDSO, for \nx86-64\n:\n\n\n\n\nclock_gettime\n\n\ngetcpu\n\n\ngettimeofday\n\n\ntime\n\n\n\n\nThe reason to add it back is simple: if those syscalls are used \na lot\n and hurt overall performance. Do note that when we add it back, it will be different from the common design: vDSO \nmust\n be mapped at processor side, mapped in our emulated pgtable.\n\n\nBelow is the original part where loader maps vDSO:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nstatic\n \nint\n \nload_elf_binary\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                           \nu64\n \n*\nnew_ip\n,\n \nu64\n \n*\nnew_sp\n,\n \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n\n#ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES\n\n        \n/*\n\n\n         * TODO: vdso\n\n\n         * x86 can map vdso vma here\n\n\n         */\n\n\n#endif\n\n        \n...\n\n\n}\n\n\nmanagers\n/\nmemory\n/\nloader\n/\nelf\n.\nc\n\n\n\n\n\n\nFor lego, we should move it to processor right before \nstart_thread()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nint\n \ndo_execve\n(\nconst\n \nchar\n \n*\nfilename\n,\n\n              \nconst\n \nchar\n \n*\n \nconst\n \n*\nargv\n,\n\n              \nconst\n \nchar\n \n*\n \nconst\n \n*\nenvp\n)\n\n\n{\n\n        \n...\n\n        \n/* Should be here */\n\n\n        \nstart_thread\n(\nregs\n,\n \nnew_ip\n,\n \nnew_sp\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\nBesides, don\nt forget to report the \nvDSO\n address in the aux vector:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nstatic\n \nint\n \ncreate_elf_tables\n(\nstruct\n \nlego_task_struct\n \n*\ntsk\n,\n \nstruct\n \nlego_binprm\n \n*\nbprm\n,\n\n                \nstruct\n \nelfhdr\n \n*\nexec\n,\n \nunsigned\n \nlong\n \nload_addr\n,\n \nunsigned\n \nlong\n \ninterp_load_addr\n,\n\n                \nunsigned\n \nlong\n \n*\nargv_len\n,\n \nunsigned\n \nlong\n \n*\nenvp_len\n)\n\n\n{\n\n        \n...\n\n\n#ifdef ARCH_DLINFO\n\n        \n/*\n\n\n         * ARCH_DLINFO must come first so PPC can do its special alignment of\n\n\n         * AUXV.\n\n\n         * update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in\n\n\n         * ARCH_DLINFO changes\n\n\n         */\n\n        \nARCH_DLINFO\n;\n\n\n#endif\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 16, 2018\n\nLast Updated: Feb 27, 2018", 
            "title": "Program Loader"
        }, 
        {
            "location": "/lego/kernel/loader/#lego-program-loader", 
            "text": "This document explains the high-level workflow of Lego s program loader, and how we change the normal loader to fit the disaggregated operating system model. Background on linking and loading is recommended.", 
            "title": "Lego Program Loader"
        }, 
        {
            "location": "/lego/kernel/loader/#status", 
            "text": "Formats  Supported      ELF (static-linked)     ELF (dynamic-linked)", 
            "title": "Status"
        }, 
        {
            "location": "/lego/kernel/loader/#overall", 
            "text": "In order to support different executable formats, Lego has a  virtual loader layer  above all specific formats, which is quite similar to  virtual file system . In Lego,  execve()  is divided into two parts:  1)  syscall hook at processor side,  2)  real loader at memory side. Combined together, they provide the same semantic of  execve()  as described in Linux man page. Also for the code, we divide the Linux implementation into parts. But our emulation model introduces several interesting workarounds.", 
            "title": "Overall"
        }, 
        {
            "location": "/lego/kernel/loader/#legos-loader", 
            "text": "Lego basically divide the Linux loader into two parts, one in memory manager and other in processor manager. Most dirty work is done by memory manager. Processor manager only needs to make sure the new execution has a fresh environment to start.", 
            "title": "Lego's Loader"
        }, 
        {
            "location": "/lego/kernel/loader/#entry-point", 
            "text": "So the normal entry point is  do_execve() . Above that, it can be invoked by syscall from user space, or from kernel space by calling  do_execve()  directly. There are not too many places that will call  do_execve  within kernel. One notable case is how kernel starts the  pid 1  user program. This happens after kernel finished all initialization. The code is: 1\n2\n3\n4\n5 static   int   run_init_process ( const   char   * init_filename )                                                      { \n         argv_init [ 0 ]   =   init_filename ; \n         return   do_execve ( init_filename ,   argv_init ,   envp_init );  }", 
            "title": "Entry Point"
        }, 
        {
            "location": "/lego/kernel/loader/#memory-managers-job", 
            "text": "Memory manager side will do most of the dirty loading work. It will parse the ELF image, create new VMAs based on ELF information. After that, it only pass  start_ip  and  start_stack  back to processor manager. Once processor manager starts running this new execution, pages will be fetched from memory component on demand.", 
            "title": "Memory Manager's Job"
        }, 
        {
            "location": "/lego/kernel/loader/#load-ld-linux", 
            "text": "For dynamically-linked images, kernel ELF loader needs to load the  ld-linux.so  as well. It will first try to map the  ld-linux.so  into this process s virtual address space. Furthermore, the first user instruction that will run is no longer  __libc_main_start , kernel will transfer the kernel to  ld-linux.so  instead. Thus, for a normal user program,  ld-linux.so  will load all the shared libraries before running glibc.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n\n         ... \n         /* Dynamically-linked */ \n         if   ( elf_interpreter )   { \n                 unsigned   long   interp_map_addr   =   0 ;                   elf_entry   =   load_elf_interp ( tsk ,   loc - interp_elf_ex ,                                               interpreter , \n                                             interp_map_addr , \n                                             load_bias ,   interp_elf_phdata ); \n                 if   ( ! IS_ERR (( void   * ) elf_entry ))   { \n                         /*                           * load_elf_interp() returns relocation                           * adjustment                           */ \n                         interp_load_addr   =   elf_entry ; \n                         elf_entry   +=   loc - interp_elf_ex . e_entry ; \n                 } \n                 if   ( BAD_ADDR ( elf_entry ))   { \n                         retval   =   IS_ERR (( void   * ) elf_entry )   ? \n                                         ( int ) elf_entry   :   - EINVAL ; \n                         goto   out_free_dentry ; \n                 } \n                 reloc_func_desc   =   interp_load_addr ; \n\n                 put_lego_file ( interpreter ); \n                 kfree ( elf_interpreter ); \n         }   else   { \n         /* Statically-linked */ \n                 /*                   * e_entry is the VA to which the system first transfers control                   * Not the start_code! Normally, it is the  _start  function.                   */                   elf_entry   =   loc - elf_ex . e_entry ;                   if   ( BAD_ADDR ( elf_entry ))   { \n                         retval   =   - EINVAL ; \n                         goto   out_free_dentry ; \n                 } \n         } \n         ...  }", 
            "title": "Load ld-linux"
        }, 
        {
            "location": "/lego/kernel/loader/#processor-managers-job", 
            "text": "It needs to flush old execution environment, and setup the new execution environment, such as signal, FPU. Notably, processor manager need to run  flush_old_exec() , and  setup_new_exec() .", 
            "title": "Processor Manager's Job"
        }, 
        {
            "location": "/lego/kernel/loader/#destroy-old-context-flush_old_exec", 
            "text": "", 
            "title": "Destroy old context: flush_old_exec()"
        }, 
        {
            "location": "/lego/kernel/loader/#zap-other-threads", 
            "text": "de_thread  is used to kill other threads within the same thread group, thus make sure this process has its own signal table. Furthermore, A  exec  starts a new thread group with the same TGID of the previous thread group, so we probably also need to switch PID if calling thread is not a leader.", 
            "title": "Zap other threads"
        }, 
        {
            "location": "/lego/kernel/loader/#switch-to-new-address-space", 
            "text": "We also need to release the old mm, and allocate a new mm. The new mm only has the high address kernel mapping established. Do note that in Lego, pgtable is used to emulate the processor cache:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 static   int   exec_mmap ( void )  { \n         struct   mm_struct   * new_mm ; \n         struct   mm_struct   * old_mm ; \n         struct   task_struct   * tsk ; \n\n         new_mm   =   mm_alloc (); \n         if   ( ! new_mm ) \n                 return   - ENOMEM ; \n\n         tsk   =   current ; \n         old_mm   =   current - mm ; \n         mm_release ( tsk ,   old_mm ); \n\n         task_lock ( tsk ); \n         tsk - mm   =   new_mm ; \n         tsk - active_mm   =   new_mm ; \n         activate_mm ( old_mm ,   new_mm ); \n         task_unlock ( tsk ); \n\n         if   ( old_mm ) \n                 mmput ( old_mm ); \n         return   0 ;  }", 
            "title": "Switch to new address space"
        }, 
        {
            "location": "/lego/kernel/loader/#clear-architecture-specific-state", 
            "text": "This is performed by  flush_thread() , which is an architecture-specific callback. In x86, we need to clear FPU state, and reset TLS array: 1\n2\n3\n4\n5\n6\n7 void   flush_thread ( void )  { \n         struct   task_struct   * tsk   =   current ; \n         memset ( tsk - thread . tls_array ,   0 ,   sizeof ( tsk - thread . tls_array )); \n\n         fpu__clear ( tsk - thread . fpu );  }", 
            "title": "Clear Architecture-Specific state"
        }, 
        {
            "location": "/lego/kernel/loader/#setup-new-context-setup_new_exec", 
            "text": "Lego s  setup_new_exec()  is quite different from Linux s default implementation. Lego moves several functions to memory component, like the  arch_pick_mmap_layout  stuff. Thus, Lego only flush the signal handlers and reset the signal stack stuff: 1\n2\n3\n4\n5\n6\n7\n8\n9 static   void   setup_new_exec ( const   char   * filename )  { \n         /* This is the point of no return */ \n         current - sas_ss_sp   =   current - sas_ss_size   =   0 ; \n\n         set_task_comm ( current ,   kbasename ( filename )); \n\n         flush_signal_handlers ( current ,   0 );  }", 
            "title": "Setup new context: setup_new_exec()"
        }, 
        {
            "location": "/lego/kernel/loader/#change-return-frame-in-stack", 
            "text": "We do not return to user mode here, we simply replace the return IP of the regs frame. While the kernel thread returns, it will simply merge to syscall return path (check ret_from_fork() in entry.S for detail).  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 /**   * start_thread - Starting a new user thread   * @regs: pointer to pt_regs   * @new_ip: the first instruction IP of user thread   * @new_sp: the new stack pointer of user thread   */  void   start_thread ( struct   pt_regs   * regs ,   unsigned   long   new_ip , \n                   unsigned   long   new_sp )  { \n         loadsegment ( fs ,   0 ); \n         loadsegment ( es ,   0 ); \n         loadsegment ( ds ,   0 ); \n         load_gs_index ( 0 ); \n         regs - ip                  =   new_ip ; \n         regs - sp                  =   new_sp ; \n         regs - cs                  =   __USER_CS ; \n         regs - ss                  =   __USER_DS ; \n         regs - flags               =   X86_EFLAGS_IF ;  }    If calling  execve()  from userspace, the return frame is saved in the stack, we can simply do  start_thread  above, and merge to syscall return path. However, if calling  execve()  from a kernel thread, things changed. As you can see, all forked threads will run from  ret_from_fork  when it wakes for the first time. If it is a kernel thread, it jumps to  line 23 , to execute the kernel function. Normally, the function should not return. If it does return, it normally has called an  execve() , and return frame has been changed by  start_thread() . So we jump to  line 16  to let it merge to syscall return path.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 /* \n  *   A   newly   forked   process   directly   context   switches   into   this   address. \n  * \n  *   rax:   prev   task   we   switched   from \n  *   rbx:   kernel   thread   func   ( NULL   for   user   thread ) \n  *   r12:   kernel   thread   arg \n  */  ENTRY ( ret_from_fork ) \n         movq      %rax ,   %rdi \n         call      schedule_tail             / *   rdi :   prev   task   parameter   * / \n\n         testq     %rbx ,   %rbx                / *   from   kernel_thread ?   * / \n         jnz       1 f                        / *   kernel   threads   are   uncommon   * /  2:           movq      %rsp ,   %rdi           call      syscall_return_slowpath   / *   return   with   IRQs   disabled   * / \n         SWAPGS                            / *   switch   to   user   gs.base   * / \n         jmp       restore_regs_and_iret  1: \n         /*   kernel   thread   * /           movq      %r12 ,   %rdi           call      * %rbx \n         /*   \n          *   A   kernel   thread   is   allowed   to   return   here   after   successfully \n          *   calling   do_execve ().    Exit   to   userspace   to   complete   the   execve () \n          *   syscall: \n          */ \n         movq      $0 ,   RAX ( %rsp ) \n         jmp       2 b    END ( ret_from_fork )    This is such a typical control flow hijacking. :-)", 
            "title": "Change return frame in stack"
        }, 
        {
            "location": "/lego/kernel/loader/#features", 
            "text": "This section lists various features, or behaviors and Lego s program loader.", 
            "title": "Features"
        }, 
        {
            "location": "/lego/kernel/loader/#virtual-address-space-range", 
            "text": "User s virtual address falls into this range: 1 [sysctl_mmap_min_addr, TASK_SIZE)   By default,  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 unsigned   long   sysctl_mmap_min_addr   =   PAGE_SIZE ;  /*   * User space process size. 47bits minus one guard page.  The guard   * page is necessary on Intel CPUs: if a SYSCALL instruction is at   * the highest possible canonical userspace address, then that   * syscall will enter the kernel with a non-canonical return   * address, and SYSRET will explode dangerously.  We avoid this   * particular problem by preventing anything from being mapped   * at the maximum canonical address.   */                                                                                                         #define TASK_SIZE       ((1UL   47) - PAGE_SIZE)    Essentially: 1 [0x1000, 0x7ffffffff000)", 
            "title": "Virtual Address Space Range"
        }, 
        {
            "location": "/lego/kernel/loader/#pre-populated-bss-and-brk", 
            "text": "The heap vma created at loading time is a combination of  .bss  and  .brk  segments. Since brk usage is 0 (will it be non-zero?) at this moment, so the heap vma is essentially just  .bss  pages. Normally, Linux kernel does not populate pages for this vma during loading, but Lego does. It can save several page allocation cost for heap pcache miss. It is controlled by  vm_brk() .  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 int   vm_brk ( struct   lego_task_struct   * tsk , \n            unsigned   long   start ,   unsigned   long   len )  { \n         int   ret ; \n         struct   lego_mm_struct   * mm   =   tsk - mm ; \n\n         if   ( down_write_killable ( mm - mmap_sem )) \n                 return   - EINTR ; \n\n         ret   =   do_brk ( tsk ,   start ,   len ); \n         up_write ( mm - mmap_sem ); \n\n         /* Prepopulate brk pages */ \n         if   ( ! ret ) \n                 lego_mm_populate ( mm ,   start ,   len ); \n\n         return   ret ;  }", 
            "title": "Pre-Populated .bss and .brk"
        }, 
        {
            "location": "/lego/kernel/loader/#un-populated-stack", 
            "text": "Stack vma is manually expanded to  32 pages + pages for argv info  by loader to accommodate future usage. Only pages for argv are populated by default, the extra 32 pages are not. A typical program may need 1 page for saving argv info, plus the 32 extra, the layout will be: 1 7ffffffde000-7ffffffff000 rw-p 00000000 [stack]   The code to expand stack is done when ELF loader tries to finalize the stack vma, by calling  setup_arg_pages() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 int   setup_arg_pages ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                     unsigned   long   stack_top ,   int   executable_stack )  { \n         ... \n         /*           * 32*4k (or 2*64k) pages           */ \n         stack_expand   =   131072UL ; \n         stack_size   =   vma - vm_end   -   vma - vm_start ; \n         stack_base   =   vma - vm_start   -   stack_expand ; \n\n         mm - start_stack   =   bprm - p ; \n         ret   =   expand_stack ( vma ,   stack_base ); \n         ...  }", 
            "title": "Un-Populated stack"
        }, 
        {
            "location": "/lego/kernel/loader/#un-populated-text-and-data", 
            "text": "In essence, all PT_LOAD segments of ELF image are not pre-populated. They will be fetched from storage on demand. This is the traditional on-demand paging way. If we want to reduce the overhead of code and data s on-demand paging, we can prefault them in the future.", 
            "title": "Un-Populated .text and .data"
        }, 
        {
            "location": "/lego/kernel/loader/#disabled-randomized-top-of-stack", 
            "text": "Lego currently does not randomize the stack top. The stack vma is allocated by  bprm_mm_init()  at early execve time. There is no randomization at the allocation time, and this applies to all exectuable formats. The end of vma is just  TASK_SIZE : 1\n2\n3\n4\n5\n6\n7 static   int   __bprm_mm_init ( struct   lego_binprm   * bprm )  { \n         ... \n         vma - vm_end   =   TASK_SIZE ; \n         ...  }  ( managers / memory / loader / elf . c )    Top of stack randomization happens within each specific format loader. They do this by calling back to virtual loader layer s  setup_arg_pages()  function, which is used to finalize the top of stack: 1\n2 int   setup_arg_pages ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                     unsigned   long   stack_top ,   int   executable_stack );    So, to actually randomize the top of stack, you can simply do the following:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 static   unsigned   long   randomize_stack_top ( unsigned   long   stack_top )  {                                 \n         unsigned   long   random_variable   =   0 ; \n\n         if   (( current - flags     PF_RANDOMIZE )   \n                 ! ( current - personality     ADDR_NO_RANDOMIZE ))   { \n                 random_variable   =   get_random_long (); \n                 random_variable   =   STACK_RND_MASK ; \n                 random_variable   =   PAGE_SHIFT ; \n         }  #ifdef CONFIG_STACK_GROWSUP \n         return   PAGE_ALIGN ( stack_top )   +   random_variable ;  #else            \n         return   PAGE_ALIGN ( stack_top )   -   random_variable ;  #endif  }  static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ... \n         retval   =   setup_arg_pages ( bprm ,   randomize_stack_top ( TASK_SIZE ), \n                                  executable_stack ); \n         ...  }    However, current Lego disables randomization by passing  TASK_SIZE : 1\n2\n3\n4\n5\n6\n7\n8 static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ... \n         retval   =   setup_arg_pages ( tsk ,   bprm ,   TASK_SIZE ,   executable_stack ); \n         ...  }  ( managers / memory / loader / elf . c )", 
            "title": "Disabled Randomized Top of Stack"
        }, 
        {
            "location": "/lego/kernel/loader/#no-vdso", 
            "text": "Currently, Lego does not have  vDSO  support. There are not too many syscalls mapped in the vDSO, for  x86-64 :   clock_gettime  getcpu  gettimeofday  time   The reason to add it back is simple: if those syscalls are used  a lot  and hurt overall performance. Do note that when we add it back, it will be different from the common design: vDSO  must  be mapped at processor side, mapped in our emulated pgtable.  Below is the original part where loader maps vDSO:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 static   int   load_elf_binary ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                            u64   * new_ip ,   u64   * new_sp ,   unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ...  #ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES \n         /*           * TODO: vdso           * x86 can map vdso vma here           */  #endif \n         ...  }  managers / memory / loader / elf . c    For lego, we should move it to processor right before  start_thread() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 int   do_execve ( const   char   * filename , \n               const   char   *   const   * argv , \n               const   char   *   const   * envp )  { \n         ... \n         /* Should be here */ \n\n         start_thread ( regs ,   new_ip ,   new_sp ); \n         ...  }    Besides, don t forget to report the  vDSO  address in the aux vector:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 static   int   create_elf_tables ( struct   lego_task_struct   * tsk ,   struct   lego_binprm   * bprm , \n                 struct   elfhdr   * exec ,   unsigned   long   load_addr ,   unsigned   long   interp_load_addr , \n                 unsigned   long   * argv_len ,   unsigned   long   * envp_len )  { \n         ...  #ifdef ARCH_DLINFO \n         /*           * ARCH_DLINFO must come first so PPC can do its special alignment of           * AUXV.           * update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT() in           * ARCH_DLINFO changes           */ \n         ARCH_DLINFO ;  #endif \n         ...  }    \nYizhou Shan \nCreated: Feb 16, 2018 \nLast Updated: Feb 27, 2018", 
            "title": "No vDSO"
        }, 
        {
            "location": "/lego/kernel/vDSO/", 
            "text": "vDSO and vsyscall\n\n\nWe have choice but port vDSO and vsyscall for Lego, because some dynamic-linked ELF images will use these features.\n\n\nReferences:\n\n\n\n\nhttps://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html\n\n\n\n\n\nYizhou Shan\n\nCreated: March 01, 2018\n\nLast Updated: March 01, 2018", 
            "title": "vDSO"
        }, 
        {
            "location": "/lego/kernel/vDSO/#vdso-and-vsyscall", 
            "text": "We have choice but port vDSO and vsyscall for Lego, because some dynamic-linked ELF images will use these features.  References:   https://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html   \nYizhou Shan \nCreated: March 01, 2018 \nLast Updated: March 01, 2018", 
            "title": "vDSO and vsyscall"
        }, 
        {
            "location": "/lego/kernel/vfs/", 
            "text": "Processor Manager\ns Virtual File System\n\n\nLego processor manager has an virtual file system layer to accommodate the famous legacy \nEverything is a file\n philosophy. But we implement this in a very dirty way.\n\n\nCover later.\n\n\n\nYizhou Shan\n\nCreated: Feb 20, 2018\n\nLast Updated: Feb 20, 2018", 
            "title": "Virtual File System"
        }, 
        {
            "location": "/lego/kernel/vfs/#processor-managers-virtual-file-system", 
            "text": "Lego processor manager has an virtual file system layer to accommodate the famous legacy  Everything is a file  philosophy. But we implement this in a very dirty way.  Cover later.  \nYizhou Shan \nCreated: Feb 20, 2018 \nLast Updated: Feb 20, 2018", 
            "title": "Processor Manager's Virtual File System"
        }, 
        {
            "location": "/lego/kernel/vm/", 
            "text": "Process Virtual Memory\n\n\nLimits\n\n\nMax Number of VMAs\n\n\nBy default, the maximum number of VMAs is: \n65530\n. It is defined by the following variable:\n\n1\n2\n3\n4\n#define MAPCOUNT_ELF_CORE_MARGIN        (5)\n\n\n#define DEFAULT_MAX_MAP_COUNT   (USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\n\n\n\nint\n \nsysctl_max_map_count\n \n__read_mostly\n \n=\n \nDEFAULT_MAX_MAP_COUNT\n;\n\n\n\n\n\n\nFacts\n\n\nmunmap\n can split vma\n\n\nmunmap\n can create a hole with an existing vma, thus divide one existing vma to two new vmas. Do note that, \nmunmap\n can create hole for both anonymous vma \nand\n file-backed vma.\n\n\nmsync()\n is not atomic\n\n\nDuring \nmsync()\n, pages are being written back to disk one by one (or batched). Consider the case where few pages have been flushed back, while some other few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.\u000b\u000b\n\n\nmsync()\n need concurrency control\n\n\nWith a multi-threaded application, does msync() provide the synchronization semantic? The answer is NO. Other threads within the same process are able to write to pages currently under \nmsync()\n. This implies that application need to handle concurrency by themselves, e.g., rwlocks.\n\n\n\nYizhou Shan\n\nCreated: Feb 19, 2018\n\nLast Updated: Feb 19, 2018", 
            "title": "Process Virtual Memory"
        }, 
        {
            "location": "/lego/kernel/vm/#process-virtual-memory", 
            "text": "", 
            "title": "Process Virtual Memory"
        }, 
        {
            "location": "/lego/kernel/vm/#limits", 
            "text": "", 
            "title": "Limits"
        }, 
        {
            "location": "/lego/kernel/vm/#max-number-of-vmas", 
            "text": "By default, the maximum number of VMAs is:  65530 . It is defined by the following variable: 1\n2\n3\n4 #define MAPCOUNT_ELF_CORE_MARGIN        (5)  #define DEFAULT_MAX_MAP_COUNT   (USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)  int   sysctl_max_map_count   __read_mostly   =   DEFAULT_MAX_MAP_COUNT ;", 
            "title": "Max Number of VMAs"
        }, 
        {
            "location": "/lego/kernel/vm/#facts", 
            "text": "", 
            "title": "Facts"
        }, 
        {
            "location": "/lego/kernel/vm/#munmap-can-split-vma", 
            "text": "munmap  can create a hole with an existing vma, thus divide one existing vma to two new vmas. Do note that,  munmap  can create hole for both anonymous vma  and  file-backed vma.", 
            "title": "munmap can split vma"
        }, 
        {
            "location": "/lego/kernel/vm/#msync-is-not-atomic", 
            "text": "During  msync() , pages are being written back to disk one by one (or batched). Consider the case where few pages have been flushed back, while some other few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.", 
            "title": "msync() is not atomic"
        }, 
        {
            "location": "/lego/kernel/vm/#msync-need-concurrency-control", 
            "text": "With a multi-threaded application, does msync() provide the synchronization semantic? The answer is NO. Other threads within the same process are able to write to pages currently under  msync() . This implies that application need to handle concurrency by themselves, e.g., rwlocks.  \nYizhou Shan \nCreated: Feb 19, 2018 \nLast Updated: Feb 19, 2018", 
            "title": "msync() need concurrency control"
        }, 
        {
            "location": "/lego/kernel/fpu/", 
            "text": "x86 Floating Point Unit\n\n\nThis is not a document about the FPU technology, this is just a simple note on FPU code and my debugging lesson.\n\n\nFPU is heavily used by user level code. You may not use it directly, but glibc library is using it a lot, e.g. the \nstrcmp\n function. x86 FPU is really another complex thing designed by Intel. Of course its performance is good and widely used, but the legacy compatible feature? Hmm.\n\n\nI would say, without Ingo Molnar\ns \nx86 FPU code rewrite\n, there is no way for me to easily understand it. The current x86 FPU code is well-written. Even though I don\nt quite understand what and why the code is, but I enjoy reading it. The naming convention, the code organization, the file organization, the header files, it is a nice piece of art.\n\n\nAnyway, Lego ported this low-level FPU code from Linux without any change. The porting is painful because it requires a lot other related features. And it also deals with compatible syscalls a little bit. Below I will just briefly list other subsystems that are using FPU, and talk about my thoughts.\n\n\nBoot\n\n\nFPU detection and init happen during early boot. You should know the \nstruct fpu\n is a dynamically-sized structure. The size of it depends on what features the underlying CPU support. Since \nstruct fpu\n is part of \ntask_struct\n, that implies \ntask_struct\n is dynamically-sized too. Apparently, \ncpu_init()\n will also callback to init its local FPU.\n\n\nContext Switch\n\n\nFPU consists a lot registers, and each thread has its own FPU context. However, CPU will not save the FPU registers for us, it is software\ns duty to save and restore FPU context properly. FPU context is saved in \nstruct fpu\n.\n\n\nThus whenever we switch task, we also need to switch FPU context:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n__visible\n \nstruct\n \ntask_struct\n \n*\n\n\n__switch_to\n(\nstruct\n \ntask_struct\n \n*\nprev_p\n,\n \nstruct\n \ntask_struct\n \n*\nnext_p\n)\n\n\n{\n\n        \n..\n\n        \nfpu_switch\n \n=\n \nswitch_fpu_prepare\n(\nprev_fpu\n,\n \nnext_fpu\n,\n \ncpu\n);\n\n        \n..\n\n        \nswitch_fpu_finish\n(\nnext_fpu\n,\n \nfpu_switch\n);\n\n        \n..\n\n\n}\n\n\n\n\n\n\nSYSCALL\n\n\n\n\n\n\nfork() and clone(): When a new thread or process is created, the FPU context is copied from the calling thread.\n\n\n\n\n\n\nexecve(): When \nexecve()\n is called, the FPU context will be cleared.\n\n\n\n\n\n\nexit(): When a thread exit,, FPU will do cleanup based on if \neagerfpu\n or \nlazyfpu\n is used.\n\n\n\n\n\n\nExceptions\n\n\nLike the \ndevice not available\n exception, which may be triggered if lazyfpu is used. Also, \ndo_simd_exception\n and \ndo_coprocessor_error\n, which are some math related exceptions.\n\n\nSignal\n\n\nKernel needs to setup a \nsigframe\n for user level signal handlers. \nsigframe\n is a contiguous stack memory consists the general purpose registers and FPU registers. So signal handling part will also call back to FPU to setup and copy the FPU registers to \nsigframe\n in stack.\n\n\nThoughts\n\n\nI\nve been debugging this FPU introduced bugs for over a month. And during this month, I\nm always not sure if it is FPU\ns bug, or some other code that corrupts memory. So I\nm lazy to re-port FPU again. But after rule out every other possibilities, I turned back to FPU. At first I did not port all FPU code, cause I don\nt think I need all of it.\n\n\nOne stupid thing is I forgot to turn on DEBUG_FPU, which should help me in the first place. I kind of lost myself in various engineering work during this debugging. I really need some big context switch in the middle to fresh my mind. Anyway, glad it is all done today (Feb 23), and I\nm able to move to next stage.\n\n\nCompatibility is a heavy thing to carry. But it is also a nice thing for marketing. No one can deny the success of Intel on its backward compatibility. Bad for programmers.\n\n\n\nYizhou Shan\n\nCreated: Feb 22, 2018\n\nLast Updated: Feb 23, 2018", 
            "title": "x86 FPU"
        }, 
        {
            "location": "/lego/kernel/fpu/#x86-floating-point-unit", 
            "text": "This is not a document about the FPU technology, this is just a simple note on FPU code and my debugging lesson.  FPU is heavily used by user level code. You may not use it directly, but glibc library is using it a lot, e.g. the  strcmp  function. x86 FPU is really another complex thing designed by Intel. Of course its performance is good and widely used, but the legacy compatible feature? Hmm.  I would say, without Ingo Molnar s  x86 FPU code rewrite , there is no way for me to easily understand it. The current x86 FPU code is well-written. Even though I don t quite understand what and why the code is, but I enjoy reading it. The naming convention, the code organization, the file organization, the header files, it is a nice piece of art.  Anyway, Lego ported this low-level FPU code from Linux without any change. The porting is painful because it requires a lot other related features. And it also deals with compatible syscalls a little bit. Below I will just briefly list other subsystems that are using FPU, and talk about my thoughts.", 
            "title": "x86 Floating Point Unit"
        }, 
        {
            "location": "/lego/kernel/fpu/#boot", 
            "text": "FPU detection and init happen during early boot. You should know the  struct fpu  is a dynamically-sized structure. The size of it depends on what features the underlying CPU support. Since  struct fpu  is part of  task_struct , that implies  task_struct  is dynamically-sized too. Apparently,  cpu_init()  will also callback to init its local FPU.", 
            "title": "Boot"
        }, 
        {
            "location": "/lego/kernel/fpu/#context-switch", 
            "text": "FPU consists a lot registers, and each thread has its own FPU context. However, CPU will not save the FPU registers for us, it is software s duty to save and restore FPU context properly. FPU context is saved in  struct fpu .  Thus whenever we switch task, we also need to switch FPU context: 1\n2\n3\n4\n5\n6\n7\n8\n9 __visible   struct   task_struct   *  __switch_to ( struct   task_struct   * prev_p ,   struct   task_struct   * next_p )  { \n         .. \n         fpu_switch   =   switch_fpu_prepare ( prev_fpu ,   next_fpu ,   cpu ); \n         .. \n         switch_fpu_finish ( next_fpu ,   fpu_switch ); \n         ..  }", 
            "title": "Context Switch"
        }, 
        {
            "location": "/lego/kernel/fpu/#syscall", 
            "text": "fork() and clone(): When a new thread or process is created, the FPU context is copied from the calling thread.    execve(): When  execve()  is called, the FPU context will be cleared.    exit(): When a thread exit,, FPU will do cleanup based on if  eagerfpu  or  lazyfpu  is used.", 
            "title": "SYSCALL"
        }, 
        {
            "location": "/lego/kernel/fpu/#exceptions", 
            "text": "Like the  device not available  exception, which may be triggered if lazyfpu is used. Also,  do_simd_exception  and  do_coprocessor_error , which are some math related exceptions.", 
            "title": "Exceptions"
        }, 
        {
            "location": "/lego/kernel/fpu/#signal", 
            "text": "Kernel needs to setup a  sigframe  for user level signal handlers.  sigframe  is a contiguous stack memory consists the general purpose registers and FPU registers. So signal handling part will also call back to FPU to setup and copy the FPU registers to  sigframe  in stack.", 
            "title": "Signal"
        }, 
        {
            "location": "/lego/kernel/fpu/#thoughts", 
            "text": "I ve been debugging this FPU introduced bugs for over a month. And during this month, I m always not sure if it is FPU s bug, or some other code that corrupts memory. So I m lazy to re-port FPU again. But after rule out every other possibilities, I turned back to FPU. At first I did not port all FPU code, cause I don t think I need all of it.  One stupid thing is I forgot to turn on DEBUG_FPU, which should help me in the first place. I kind of lost myself in various engineering work during this debugging. I really need some big context switch in the middle to fresh my mind. Anyway, glad it is all done today (Feb 23), and I m able to move to next stage.  Compatibility is a heavy thing to carry. But it is also a nice thing for marketing. No one can deny the success of Intel on its backward compatibility. Bad for programmers.  \nYizhou Shan \nCreated: Feb 22, 2018 \nLast Updated: Feb 23, 2018", 
            "title": "Thoughts"
        }, 
        {
            "location": "/lego/kernel/irq/", 
            "text": "IRQ\n\n\nIRQ is majorly ported based on \nlinux-4.4\n. The decision of porting of whole IRQ stack from linux was made at early stage of Lego, when I\nm not so familiar with this stuff. This technique decision has pros and cons.\n\n\nThe whole thing is made complicated by having IRQ domain. IRQ domain is introduced to address the multiple interrupt controller issue. And in x86, we kind of have mutiple as well: IO-APIC, REMAP, LAPIC. Although we are not supporting IRQ remap now.\n\n\nInit\n\n\n\n\nThe first part of initialization is \ntrap_init()\n at early \nsetup_arch()\n.\n\n\nThe second major entry point is \nirq_init()\n at \nstart_kernel()\n. This \nirq_init()\n is actually a combination of linux\ns:\n\n\nearly_irq_init()\n: 1) setup \nirq_desc[]\n array, and then call \narch_early_irq_init()\n, which will register two IRQ domains (x86_vector_domain, msi_domain).\n\n\ninit_IRQ()\n: is actually a callback to low-level x86 interrupt setup. It mainly setup the desc\ns data/chip etc, and register all different handlers.\n\n\nIn Lego, you will be able to find all the functionalitis are moved into \narch_irq_init()\n. And, to this point, we have a complete setup.\n\n\n\n\n\n\nThe third (and last) entry point is \nsmp_prepare_cpus()\n:\n\n1\n2\n3\n4\n5\nsmp_prepare_cpus()\n-\n apic_bsp_setup()\n   -\n setup_local_APIC()\n   -\n setup_IO_APIC()\n   -\n x86_init.timers.setup_percpu_clockev()\n\n\n\n\n\n\n\nIRQ Domain\n\n\nWe should have at least 2 or 3 IRQ domains:\n\n\n\n\nx86_vector\n\n\nx86_msi\n\n\nx86_ioapic-N (each ioapic has one)\n\n\n\n\nThe first two guys are created during \narch_irq_init()\n. While the latter ioapic ones are created during \nsetup_IO_APIC()\n. All of them are allocated eventually by \n__irq_domain_add()\n, and linked at \nLIST_HEAD(irq_domain_list)\n.\n\n\nSo....  Lego or Linux maintains its own IRQ numbers, starting from 0 to NR_IRQs.\nHowever, this IRQ number MAY not have a identical mapping to hardware\ns own IRQ number (let us call it hwirq). Given this, we want to know the mapping between IRQ and hwirq. That\ns the purpose of having \nlinear_revmap\n and \nrevmap_tree\n within each domain, it is used to translate hwirq to IRQ.\n\n\nWhy two different data structures? \nlinear_revmap\n is fairly simple, an array, which is indexed by hwirq. However, the hwirq maybe very large, we don\nt want to waste memory, that\ns how we want to use trees.\n\n\nThese two can be used together. If we fail to insert into \nlinear_revmap\n, we insert into tree. During search time, we need to look up both.\n\n\nBy default, \nx86_vector\n and \nx86_msi\n use radix tree only. \nx86_ioapic-N\n uses a mix of linear and radix tree.\n\n\nTo dump all IRQ domains, call \ndump_irq_domain_list()\n, which give you something like this:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n[\n  \n118.308544\n]\n  \nname\n              \nmapped\n  \nlinear\n-\nmax\n  \ndirect\n-\nmax\n  \ndevtree\n-\nnode\n\n\n[\n  \n118.316114\n]\n  \nx86_ioapic\n-\n2\n          \n24\n          \n24\n           \n0\n    \n\n[\n  \n118.322707\n]\n  \nx86_ioapic\n-\n1\n          \n24\n          \n24\n           \n0\n    \n\n[\n  \n118.329299\n]\n  \nx86_ioapic\n-\n0\n          \n24\n          \n24\n           \n0\n    \n\n[\n  \n118.335893\n]\n  \nx86_msi\n               \n25\n           \n0\n           \n0\n    \n\n[\n  \n118.342486\n]\n \n*\nx86_vector\n            \n40\n           \n0\n           \n0\n    \n\n[\n  \n118.349078\n]\n \nirq\n    \nhwirq\n    \nchip\n \nname\n        \nchip\n \ndata\n           \nactive\n  \ntype\n            \ndomain\n\n\n[\n  \n118.358775\n]\n     \n1\n  \n0x00001\n  \nIO\n-\nAPIC\n          \n0xffff88107fcae000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.368858\n]\n     \n3\n  \n0x00003\n  \nIO\n-\nAPIC\n          \n0xffff88107fc8f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.378940\n]\n     \n4\n  \n0x00004\n  \nIO\n-\nAPIC\n          \n0xffff88107fc6e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.389025\n]\n     \n5\n  \n0x00005\n  \nIO\n-\nAPIC\n          \n0xffff88107fc6f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.399109\n]\n     \n6\n  \n0x00006\n  \nIO\n-\nAPIC\n          \n0xffff88107fc4e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.409192\n]\n     \n7\n  \n0x00007\n  \nIO\n-\nAPIC\n          \n0xffff88107fc4f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.419276\n]\n     \n8\n  \n0x00008\n  \nIO\n-\nAPIC\n          \n0xffff88107fc2e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.429358\n]\n     \n9\n  \n0x00009\n  \nIO\n-\nAPIC\n          \n0xffff88107fc2f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.439442\n]\n    \n10\n  \n0x0000a\n  \nIO\n-\nAPIC\n          \n0xffff88107fc0e000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.449525\n]\n    \n11\n  \n0x0000b\n  \nIO\n-\nAPIC\n          \n0xffff88107fc0f000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.459609\n]\n    \n12\n  \n0x0000c\n  \nIO\n-\nAPIC\n          \n0xffff88107fff0000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.469692\n]\n    \n13\n  \n0x0000d\n  \nIO\n-\nAPIC\n          \n0xffff88107fff1000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.479776\n]\n    \n14\n  \n0x0000e\n  \nIO\n-\nAPIC\n          \n0xffff88107fff2000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.489860\n]\n    \n15\n  \n0x0000f\n  \nIO\n-\nAPIC\n          \n0xffff88107fff3000\n        \nLINEAR\n          \nx86_ioapic\n-\n0\n\n\n[\n  \n118.499943\n]\n    \n24\n  \n0x300000\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.509833\n]\n    \n25\n  \n0x300001\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.519722\n]\n    \n26\n  \n0x300002\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.529612\n]\n    \n27\n  \n0x300003\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n     \n*\n     \nRADIX\n          \nx86_msi\n\n\n[\n  \n118.539501\n]\n    \n28\n  \n0x300004\n  \nPCI\n-\nMSI\n                      \n(\nnull\n)\n           \nRADIX\n          \nx86_msi\n\n\n\n\n\n\nAug 20, 2018\n\n\nWell, I\nve ported the IRQ stuff at early days of Lego. At that time, I mainly ported the low-level APIC, IO-APIC, and ACPI stuff, along with the upper layer irqchip, irqdesc stuff.\n\n\nThese days, I was verifying our IB code and tried to add back mlx4en\ns interrupt handler, somehow, there is no interrupt after \nrequest_irq()\n.\n\n\nTwo possible reasons: 1) I missed something during PCI setup, 2) underlying APIC and IO-APIC need more work.\n\n\n\nLast Updated: Aug 28, 2018", 
            "title": "IRQ"
        }, 
        {
            "location": "/lego/kernel/irq/#irq", 
            "text": "IRQ is majorly ported based on  linux-4.4 . The decision of porting of whole IRQ stack from linux was made at early stage of Lego, when I m not so familiar with this stuff. This technique decision has pros and cons.  The whole thing is made complicated by having IRQ domain. IRQ domain is introduced to address the multiple interrupt controller issue. And in x86, we kind of have mutiple as well: IO-APIC, REMAP, LAPIC. Although we are not supporting IRQ remap now.", 
            "title": "IRQ"
        }, 
        {
            "location": "/lego/kernel/irq/#init", 
            "text": "The first part of initialization is  trap_init()  at early  setup_arch() .  The second major entry point is  irq_init()  at  start_kernel() . This  irq_init()  is actually a combination of linux s:  early_irq_init() : 1) setup  irq_desc[]  array, and then call  arch_early_irq_init() , which will register two IRQ domains (x86_vector_domain, msi_domain).  init_IRQ() : is actually a callback to low-level x86 interrupt setup. It mainly setup the desc s data/chip etc, and register all different handlers.  In Lego, you will be able to find all the functionalitis are moved into  arch_irq_init() . And, to this point, we have a complete setup.    The third (and last) entry point is  smp_prepare_cpus() : 1\n2\n3\n4\n5 smp_prepare_cpus()\n-  apic_bsp_setup()\n   -  setup_local_APIC()\n   -  setup_IO_APIC()\n   -  x86_init.timers.setup_percpu_clockev()", 
            "title": "Init"
        }, 
        {
            "location": "/lego/kernel/irq/#irq-domain", 
            "text": "We should have at least 2 or 3 IRQ domains:   x86_vector  x86_msi  x86_ioapic-N (each ioapic has one)   The first two guys are created during  arch_irq_init() . While the latter ioapic ones are created during  setup_IO_APIC() . All of them are allocated eventually by  __irq_domain_add() , and linked at  LIST_HEAD(irq_domain_list) .  So....  Lego or Linux maintains its own IRQ numbers, starting from 0 to NR_IRQs.\nHowever, this IRQ number MAY not have a identical mapping to hardware s own IRQ number (let us call it hwirq). Given this, we want to know the mapping between IRQ and hwirq. That s the purpose of having  linear_revmap  and  revmap_tree  within each domain, it is used to translate hwirq to IRQ.  Why two different data structures?  linear_revmap  is fairly simple, an array, which is indexed by hwirq. However, the hwirq maybe very large, we don t want to waste memory, that s how we want to use trees.  These two can be used together. If we fail to insert into  linear_revmap , we insert into tree. During search time, we need to look up both.  By default,  x86_vector  and  x86_msi  use radix tree only.  x86_ioapic-N  uses a mix of linear and radix tree.  To dump all IRQ domains, call  dump_irq_domain_list() , which give you something like this:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 [    118.308544 ]    name                mapped    linear - max    direct - max    devtree - node  [    118.316114 ]    x86_ioapic - 2            24            24             0      [    118.322707 ]    x86_ioapic - 1            24            24             0      [    118.329299 ]    x86_ioapic - 0            24            24             0      [    118.335893 ]    x86_msi                 25             0             0      [    118.342486 ]   * x86_vector              40             0             0      [    118.349078 ]   irq      hwirq      chip   name          chip   data             active    type              domain  [    118.358775 ]       1    0x00001    IO - APIC            0xffff88107fcae000          LINEAR            x86_ioapic - 0  [    118.368858 ]       3    0x00003    IO - APIC            0xffff88107fc8f000          LINEAR            x86_ioapic - 0  [    118.378940 ]       4    0x00004    IO - APIC            0xffff88107fc6e000          LINEAR            x86_ioapic - 0  [    118.389025 ]       5    0x00005    IO - APIC            0xffff88107fc6f000          LINEAR            x86_ioapic - 0  [    118.399109 ]       6    0x00006    IO - APIC            0xffff88107fc4e000          LINEAR            x86_ioapic - 0  [    118.409192 ]       7    0x00007    IO - APIC            0xffff88107fc4f000          LINEAR            x86_ioapic - 0  [    118.419276 ]       8    0x00008    IO - APIC            0xffff88107fc2e000          LINEAR            x86_ioapic - 0  [    118.429358 ]       9    0x00009    IO - APIC            0xffff88107fc2f000          LINEAR            x86_ioapic - 0  [    118.439442 ]      10    0x0000a    IO - APIC            0xffff88107fc0e000          LINEAR            x86_ioapic - 0  [    118.449525 ]      11    0x0000b    IO - APIC            0xffff88107fc0f000          LINEAR            x86_ioapic - 0  [    118.459609 ]      12    0x0000c    IO - APIC            0xffff88107fff0000          LINEAR            x86_ioapic - 0  [    118.469692 ]      13    0x0000d    IO - APIC            0xffff88107fff1000          LINEAR            x86_ioapic - 0  [    118.479776 ]      14    0x0000e    IO - APIC            0xffff88107fff2000          LINEAR            x86_ioapic - 0  [    118.489860 ]      15    0x0000f    IO - APIC            0xffff88107fff3000          LINEAR            x86_ioapic - 0  [    118.499943 ]      24    0x300000    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.509833 ]      25    0x300001    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.519722 ]      26    0x300002    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.529612 ]      27    0x300003    PCI - MSI                        ( null )       *       RADIX            x86_msi  [    118.539501 ]      28    0x300004    PCI - MSI                        ( null )             RADIX            x86_msi", 
            "title": "IRQ Domain"
        }, 
        {
            "location": "/lego/kernel/irq/#aug-20-2018", 
            "text": "Well, I ve ported the IRQ stuff at early days of Lego. At that time, I mainly ported the low-level APIC, IO-APIC, and ACPI stuff, along with the upper layer irqchip, irqdesc stuff.  These days, I was verifying our IB code and tried to add back mlx4en s interrupt handler, somehow, there is no interrupt after  request_irq() .  Two possible reasons: 1) I missed something during PCI setup, 2) underlying APIC and IO-APIC need more work.  \nLast Updated: Aug 28, 2018", 
            "title": "Aug 20, 2018"
        }, 
        {
            "location": "/lego/kernel/net_thpool/", 
            "text": "Thread Pool Model for Handling Network Requests\n\n\n\n\nPassive\n: whenever a network request comes in, callback to thpool.\n\n\nActive\n: thpool keep polling if there is new network requests queued.\n\n\n\n\nPreviously, our memory side use the Active mode to handle requests, which has very bad latency. Several days ago we changed to the Passive mode, which has a very good latency! One \nib_send_reply\n RRT drops from \n~20us\n to a normal \n~6us\n for a TensorFlow run.\n\n\nNever thought this could make such a big difference (~3x slowdown)! Dark network!\n\n\n\nYizhou Shan\n\nCreated: April 29, 2018\n\nLast Updated: April 29, 2018", 
            "title": "Network Thpool"
        }, 
        {
            "location": "/lego/kernel/net_thpool/#thread-pool-model-for-handling-network-requests", 
            "text": "Passive : whenever a network request comes in, callback to thpool.  Active : thpool keep polling if there is new network requests queued.   Previously, our memory side use the Active mode to handle requests, which has very bad latency. Several days ago we changed to the Passive mode, which has a very good latency! One  ib_send_reply  RRT drops from  ~20us  to a normal  ~6us  for a TensorFlow run.  Never thought this could make such a big difference (~3x slowdown)! Dark network!  \nYizhou Shan \nCreated: April 29, 2018 \nLast Updated: April 29, 2018", 
            "title": "Thread Pool Model for Handling Network Requests"
        }, 
        {
            "location": "/lego/syscall/facts/", 
            "text": "Lego SYSCALL Facts\n\n\nThis document is about the general concepts of Lego syscall implementation. If you are developing syscall, please read this document first.\n\n\nInterrupts Enabled\n\n\nEach syscall is invoked with interrupts enabled. Also, it must return with interrupts enabled as well. Any buggy syscall implementation will be catched by \nsyscall_return_slowpath()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nvoid\n \nsyscall_return_slowpath\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \nif\n \n(\nWARN\n(\nirqs_disabled\n(),\n \nsyscall %ld left IRQs disabled\n,\n \nregs\n-\norig_ax\n))\n\n                \nlocal_irq_enable\n();\n\n\n        \nlocal_irq_disable\n();\n\n        \nprepare_exit_to_usermode\n(\nregs\n);\n\n\n}\n\n\n\nvoid\n \ndo_syscall_64\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \n..\n\n        \nlocal_irq_enable\n();\n\n\n        \nif\n \n(\nlikely\n(\nnr\n \n \nNR_syscalls\n))\n \n{\n\n                \nregs\n-\nax\n \n=\n \nsys_call_table\n[\nnr\n](\n\n                        \nregs\n-\ndi\n,\n \nregs\n-\nsi\n,\n \nregs\n-\ndx\n,\n\n                        \nregs\n-\nr10\n,\n \nregs\n-\nr8\n,\n \nregs\n-\nr9\n);\n\n        \n}\n   \n\n        \nsyscall_return_slowpath\n(\nregs\n);\n\n        \n..\n\n\n}\n\n\n\n\n\n\nGet User Entry pt_regs\n\n\nThe macro \ntask_pt_regs()\n always return the \npt_regs\n, that saves the user context when it issued the syscall, no matter how many levels interrupts are nested when you call \ntask_pt_regs()\n. This is based on the fact that kernel stack is empty at syscall entry, thus this user \npt_regs\n was saved at the \ntop\n of kernel stack:\n\n1\n#define task_pt_regs(tsk)       ((struct pt_regs *)(tsk)-\nthread.sp0 - 1)\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\nENTRY\n(\nentry_SYSCALL_64\n)\n\n        \nSWAPGS\n\n\n        \n/*\n\n         \n*\n \nSYSCALL\n \ndoes\n \nnot\n \nchange\n \nrsp\n \nfor\n \nus\n!\n\n         \n*\n \nSave\n \nthe\n \nprevious\n \nrsp\n \nand\n \nload\n \nthe\n \ntop\n \nof\n \nkernel\n \nstack.\n\n         \n*\n \nIt\n \nmust\n \nbe\n \nthe\n \ntop\n \nof\n \nkernel\n \nstack\n,\n \nsince\n \nwe\n \ncame\n \nhere\n\n         \n*\n \nfrom\n \n*\nuserspace\n*.\n\n         \n*/\n\n        \nmovq\n    \n%rsp\n,\n \nPER_CPU_VAR\n(\nrsp_scratch\n)\n\n        \nmovq\n    \nPER_CPU_VAR\n(\ncpu_current_top_of_stack\n),\n \n%rsp\n\n\n        \n/*\n\n         \n*\n \nConstruct\n \nstruct\n \npt_regs\n \non\n \nstack\n\n         \n*\n\n         \n*\n \nIn\n \nany\n \nsyscall\n \nhandler\n,\n \nyou\n \ncan\n \nuse\n\n         \n*\n      \ncurrent_pt_regs\n()\n\n         \n*\n \nto\n \nget\n \nthese\n \nregisters.\n\n         \n*/\n\n        \npushq\n   \n$__USER_DS\n                      \n/\n*\n \npt_regs-\nss\n \n*\n/\n\n        \npushq\n   \nPER_CPU_VAR\n(\nrsp_scratch\n)\n        \n/\n*\n \npt_regs-\nsp\n \n*\n/\n\n        \npushq\n   \n%r11\n                            \n/\n*\n \npt_regs-\nflags\n \n*\n/\n\n        \npushq\n   \n$__USER_CS\n                      \n/\n*\n \npt_regs-\ncs\n \n*\n/\n\n        \npushq\n   \n%rcx\n                            \n/\n*\n \npt_regs-\nip\n \n*\n/\n\n        \npushq\n   \n%rax\n                            \n/\n*\n \npt_regs-\norig_ax\n \n*\n/\n\n        \npushq\n   \n%rdi\n                            \n/\n*\n \npt_regs-\ndi\n \n*\n/\n\n        \npushq\n   \n%rsi\n                            \n/\n*\n \npt_regs-\nsi\n \n*\n/\n\n        \npushq\n   \n%rdx\n                            \n/\n*\n \npt_regs-\ndx\n \n*\n/\n\n        \npushq\n   \n%rcx\n                            \n/\n*\n \npt_regs-\ncx\n \n*\n/\n\n        \npushq\n   \n$-ENOSYS\n                        \n/\n*\n \npt_regs-\nax\n \n*\n/\n\n        \npushq\n   \n%r8\n                             \n/\n*\n \npt_regs-\nr8\n \n*\n/\n\n        \npushq\n   \n%r9\n                             \n/\n*\n \npt_regs-\nr9\n \n*\n/\n\n        \npushq\n   \n%r10\n                            \n/\n*\n \npt_regs-\nr10\n \n*\n/\n\n        \npushq\n   \n%r11\n                            \n/\n*\n \npt_regs-\nr11\n \n*\n/\n\n        \nsub\n     \n$\n(\n6\n*\n8\n),\n \n%rsp\n                    \n/\n*\n \npt_regs-\nbp\n,\n \nbx\n,\n \nr12-15\n \n*\n/\n\n        \n....\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 22, 2018\n\nLast Updated: Feb 22, 2018", 
            "title": "Facts"
        }, 
        {
            "location": "/lego/syscall/facts/#lego-syscall-facts", 
            "text": "This document is about the general concepts of Lego syscall implementation. If you are developing syscall, please read this document first.", 
            "title": "Lego SYSCALL Facts"
        }, 
        {
            "location": "/lego/syscall/facts/#interrupts-enabled", 
            "text": "Each syscall is invoked with interrupts enabled. Also, it must return with interrupts enabled as well. Any buggy syscall implementation will be catched by  syscall_return_slowpath() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 void   syscall_return_slowpath ( struct   pt_regs   * regs )  { \n         if   ( WARN ( irqs_disabled (),   syscall %ld left IRQs disabled ,   regs - orig_ax )) \n                 local_irq_enable (); \n\n         local_irq_disable (); \n         prepare_exit_to_usermode ( regs );  }  void   do_syscall_64 ( struct   pt_regs   * regs )  { \n         .. \n         local_irq_enable (); \n\n         if   ( likely ( nr     NR_syscalls ))   { \n                 regs - ax   =   sys_call_table [ nr ]( \n                         regs - di ,   regs - si ,   regs - dx , \n                         regs - r10 ,   regs - r8 ,   regs - r9 ); \n         }    \n\n         syscall_return_slowpath ( regs ); \n         ..  }", 
            "title": "Interrupts Enabled"
        }, 
        {
            "location": "/lego/syscall/facts/#get-user-entry-pt_regs", 
            "text": "The macro  task_pt_regs()  always return the  pt_regs , that saves the user context when it issued the syscall, no matter how many levels interrupts are nested when you call  task_pt_regs() . This is based on the fact that kernel stack is empty at syscall entry, thus this user  pt_regs  was saved at the  top  of kernel stack: 1 #define task_pt_regs(tsk)       ((struct pt_regs *)(tsk)- thread.sp0 - 1)     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 ENTRY ( entry_SYSCALL_64 ) \n         SWAPGS \n\n         /* \n          *   SYSCALL   does   not   change   rsp   for   us ! \n          *   Save   the   previous   rsp   and   load   the   top   of   kernel   stack. \n          *   It   must   be   the   top   of   kernel   stack ,   since   we   came   here \n          *   from   * userspace *. \n          */ \n         movq      %rsp ,   PER_CPU_VAR ( rsp_scratch ) \n         movq      PER_CPU_VAR ( cpu_current_top_of_stack ),   %rsp \n\n         /* \n          *   Construct   struct   pt_regs   on   stack \n          * \n          *   In   any   syscall   handler ,   you   can   use \n          *        current_pt_regs () \n          *   to   get   these   registers. \n          */ \n         pushq     $__USER_DS                        / *   pt_regs- ss   * / \n         pushq     PER_CPU_VAR ( rsp_scratch )          / *   pt_regs- sp   * / \n         pushq     %r11                              / *   pt_regs- flags   * / \n         pushq     $__USER_CS                        / *   pt_regs- cs   * / \n         pushq     %rcx                              / *   pt_regs- ip   * / \n         pushq     %rax                              / *   pt_regs- orig_ax   * / \n         pushq     %rdi                              / *   pt_regs- di   * / \n         pushq     %rsi                              / *   pt_regs- si   * / \n         pushq     %rdx                              / *   pt_regs- dx   * / \n         pushq     %rcx                              / *   pt_regs- cx   * / \n         pushq     $-ENOSYS                          / *   pt_regs- ax   * / \n         pushq     %r8                               / *   pt_regs- r8   * / \n         pushq     %r9                               / *   pt_regs- r9   * / \n         pushq     %r10                              / *   pt_regs- r10   * / \n         pushq     %r11                              / *   pt_regs- r11   * / \n         sub       $ ( 6 * 8 ),   %rsp                      / *   pt_regs- bp ,   bx ,   r12-15   * / \n         ....    \nYizhou Shan \nCreated: Feb 22, 2018 \nLast Updated: Feb 22, 2018", 
            "title": "Get User Entry pt_regs"
        }, 
        {
            "location": "/lego/kernel/profile_strace/", 
            "text": "Lego Profile strace\n\n\nLego has a built-in kernel-version syscall tracer, similar to \nstrace\n utility in the user space. Below we will just call our Lego\ns syscall tracer as strace for simplicity.\n\n\nDesign\n\n\nThere are essentially three important metrics to track for each syscall\n\n\n\n\nnumber of times invoked\n\n\nnumber of times error happened\n\n\ntotal execution, or per-call latency\n\n\n\n\nBesides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:\n\n\n\n\nPerformance: set of counters are \natomic_t\n, updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.\n\n\nSimplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.\n\n\n\n\nBelow is our simple design, where each thread has a \nstruct strace_info\n, which include a set of counters for each syscall. All \nstrace_info\n within a process are chained together by a doubly-linked list.\n\n\n\n\nWhen we want to look at the strace statistic numbers, we need to \naccumulate\n counters from all threads within a process, including those dead threads. We do the \naccumulate\n when the last thread of this process is going to exit.\n\n\nThe benefit of doubly-linked \nstrace_info\n is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if \ntask_struct\n is reaped, \nstrace_info\n is still there and linked.\n\n\nFor example, assume thread_3 has a SIGSEGV, and did a \nzap_other_threads\n. And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.\n\n\n\nDetails\n\n\nThere are essentially three hooks in core kernel:\n\n\n\n\nsyscall\n: before and after \nsys_call_table\n\n\nfork/clone\n: create \nstrace_info\n for each thread\n\n\ndo_exit()\n: when group_dead(signal-\nlive==1), accumulate\n\n\n\n\nExample Output\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n[\n \n1017.047366\n]\n \nKernel\n \nstrace\n\n\n[\n \n1017.050276\n]\n \nTask\n:\n \n20\n:\n20\n \nnr_accumulated_threads\n:\n \n46\n\n\n[\n \n1017.055837\n]\n \n%\n \ntime\n        \nseconds\n  \nusecs\n/\ncall\n     \ncalls\n    \nerrors\n \nsyscall\n\n\n[\n \n1017.063213\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.071648\n]\n  \n98.16\n   \n33.839597842\n     \n1879978\n        \n18\n         \n0\n \nsys_futex\n\n\n[\n \n1017.079406\n]\n   \n0.26\n    \n0.260143997\n      \n260144\n         \n1\n         \n0\n \nsys_execve\n\n\n[\n \n1017.087260\n]\n   \n0.18\n    \n0.185456860\n        \n7133\n        \n26\n         \n0\n \nsys_write\n\n\n[\n \n1017.095017\n]\n   \n0.50\n    \n0.050189546\n         \n913\n        \n55\n         \n0\n \nsys_munmap\n\n\n[\n \n1017.102870\n]\n   \n0.25\n    \n0.025223661\n         \n255\n        \n99\n         \n0\n \nsys_mmap\n\n\n[\n \n1017.110531\n]\n   \n0.50\n    \n0.000505134\n          \n12\n        \n45\n         \n0\n \nsys_clone\n\n\n[\n \n1017.118288\n]\n   \n0.20\n    \n0.000202327\n          \n26\n         \n8\n         \n0\n \nsys_read\n\n\n[\n \n1017.125947\n]\n   \n0.14\n    \n0.000144065\n          \n17\n         \n9\n         \n0\n \nsys_open\n\n\n[\n \n1017.133608\n]\n   \n0.67\n    \n0.000067251\n           \n7\n        \n11\n         \n0\n \nsys_brk\n\n\n[\n \n1017.141171\n]\n   \n0.30\n    \n0.000030361\n           \n7\n         \n5\n         \n0\n \nsys_newfstat\n\n\n[\n \n1017.149219\n]\n   \n0.64\n    \n0.000006410\n           \n1\n         \n9\n         \n0\n \nsys_close\n\n\n[\n \n1017.156976\n]\n   \n0.48\n    \n0.000004842\n           \n1\n        \n45\n         \n0\n \nsys_madvise\n\n\n[\n \n1017.164927\n]\n   \n0.34\n    \n0.000003443\n           \n1\n        \n47\n         \n0\n \nsys_set_robust_list\n\n\n[\n \n1017.173653\n]\n   \n0.21\n    \n0.000002137\n           \n1\n        \n52\n         \n0\n \nsys_mprotect\n\n\n[\n \n1017.181702\n]\n   \n0.71\n    \n0.000000717\n           \n1\n         \n4\n         \n0\n \nsys_gettimeofday\n\n\n[\n \n1017.190137\n]\n   \n0.60\n    \n0.000000608\n           \n1\n         \n3\n         \n0\n \nsys_time\n\n\n[\n \n1017.197797\n]\n   \n0.51\n    \n0.000000513\n           \n1\n         \n2\n         \n0\n \nsys_getrlimit\n\n\n[\n \n1017.205942\n]\n   \n0.49\n    \n0.000000498\n           \n1\n         \n2\n         \n0\n \nsys_rt_sigprocmask\n\n\n[\n \n1017.214572\n]\n   \n0.46\n    \n0.000000469\n           \n1\n         \n4\n         \n0\n \nsys_rt_sigaction\n\n\n[\n \n1017.223008\n]\n   \n0.45\n    \n0.000000453\n           \n1\n         \n2\n         \n0\n \nsys_arch_prctl\n\n\n[\n \n1017.231249\n]\n   \n0.27\n    \n0.000000272\n           \n1\n         \n2\n         \n0\n \nsys_newuname\n\n\n[\n \n1017.239298\n]\n   \n0.13\n    \n0.000000135\n           \n1\n         \n2\n         \n0\n \nsys_set_tid_address\n\n\n[\n \n1017.248025\n]\n \n------\n \n--------------\n \n-----------\n \n---------\n \n---------\n \n----------------\n\n\n[\n \n1017.256460\n]\n \n100.00\n   \n34.361581541\n                   \n451\n         \n0\n \ntotal\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: April 05, 2018\n\nLast Updated: April 05, 2018", 
            "title": "strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#lego-profile-strace", 
            "text": "Lego has a built-in kernel-version syscall tracer, similar to  strace  utility in the user space. Below we will just call our Lego s syscall tracer as strace for simplicity.", 
            "title": "Lego Profile strace"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#design", 
            "text": "There are essentially three important metrics to track for each syscall   number of times invoked  number of times error happened  total execution, or per-call latency   Besides, there is another important design decision: 1) should all threads within a process share one copy of data to maintain bookkeeping, or 2) should each thread do its bookkeeping on its own set of data? Our answer is 2). For two reasons:   Performance: set of counters are  atomic_t , updating is performed by a locked instruction. The first solution will add huge overhead while tracing heavily multithreaded applications.  Simplicity: in order to track the latency of each syscall, we need to know when it enter and when it finish. As threads come and go, it is hard to maintain such information. To make it worse, a preemptable kernel, or schedule-related syscalls will move threads around cores.   Below is our simple design, where each thread has a  struct strace_info , which include a set of counters for each syscall. All  strace_info  within a process are chained together by a doubly-linked list.   When we want to look at the strace statistic numbers, we need to  accumulate  counters from all threads within a process, including those dead threads. We do the  accumulate  when the last thread of this process is going to exit.  The benefit of doubly-linked  strace_info  is we can walk through the list starting anywhere. There is really no list head here. In fact, everyone can be the head. See how we respect equality? Besides, even if  task_struct  is reaped,  strace_info  is still there and linked.  For example, assume thread_3 has a SIGSEGV, and did a  zap_other_threads . And he is the last standing live thread of this process. When it is going to exit, it will accumulate all the statistic and do the necessary printing.", 
            "title": "Design"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#details", 
            "text": "There are essentially three hooks in core kernel:   syscall : before and after  sys_call_table  fork/clone : create  strace_info  for each thread  do_exit() : when group_dead(signal- live==1), accumulate", 
            "title": "Details"
        }, 
        {
            "location": "/lego/kernel/profile_strace/#example-output", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 [   1017.047366 ]   Kernel   strace  [   1017.050276 ]   Task :   20 : 20   nr_accumulated_threads :   46  [   1017.055837 ]   %   time          seconds    usecs / call       calls      errors   syscall  [   1017.063213 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.071648 ]    98.16     33.839597842       1879978          18           0   sys_futex  [   1017.079406 ]     0.26      0.260143997        260144           1           0   sys_execve  [   1017.087260 ]     0.18      0.185456860          7133          26           0   sys_write  [   1017.095017 ]     0.50      0.050189546           913          55           0   sys_munmap  [   1017.102870 ]     0.25      0.025223661           255          99           0   sys_mmap  [   1017.110531 ]     0.50      0.000505134            12          45           0   sys_clone  [   1017.118288 ]     0.20      0.000202327            26           8           0   sys_read  [   1017.125947 ]     0.14      0.000144065            17           9           0   sys_open  [   1017.133608 ]     0.67      0.000067251             7          11           0   sys_brk  [   1017.141171 ]     0.30      0.000030361             7           5           0   sys_newfstat  [   1017.149219 ]     0.64      0.000006410             1           9           0   sys_close  [   1017.156976 ]     0.48      0.000004842             1          45           0   sys_madvise  [   1017.164927 ]     0.34      0.000003443             1          47           0   sys_set_robust_list  [   1017.173653 ]     0.21      0.000002137             1          52           0   sys_mprotect  [   1017.181702 ]     0.71      0.000000717             1           4           0   sys_gettimeofday  [   1017.190137 ]     0.60      0.000000608             1           3           0   sys_time  [   1017.197797 ]     0.51      0.000000513             1           2           0   sys_getrlimit  [   1017.205942 ]     0.49      0.000000498             1           2           0   sys_rt_sigprocmask  [   1017.214572 ]     0.46      0.000000469             1           4           0   sys_rt_sigaction  [   1017.223008 ]     0.45      0.000000453             1           2           0   sys_arch_prctl  [   1017.231249 ]     0.27      0.000000272             1           2           0   sys_newuname  [   1017.239298 ]     0.13      0.000000135             1           2           0   sys_set_tid_address  [   1017.248025 ]   ------   --------------   -----------   ---------   ---------   ----------------  [   1017.256460 ]   100.00     34.361581541                     451           0   total    \nYizhou Shan \nCreated: April 05, 2018 \nLast Updated: April 05, 2018", 
            "title": "Example Output"
        }, 
        {
            "location": "/lego/syscall/compat/", 
            "text": "Compat SYSCALL in Lego\n\n\nLego does \nnot\n support compatible syscalls, where one is able to run 32-bit image on 64-bit OS. However, the ugly FPU code and signal part in Linux is heavily hacked with the assumption that compat syscall is supported. We are no expert in this FPU thing, just to make sure we don\nt break this FPU evil, Lego adds the fake compat syscall support. Fake means whenever a 32-bit syscall is issued, Lego will just panic.\n\n\nKconfig\n\n\nIf one compiles a x86_64 Linux kernel, compat syscalls are supported by default. Everything related to compat syscalls are controlled by the following two Kconfig options. Lego may want to support compat syscalls in the future, thus we add these two Kconfigs to avoid future mess:\n\n\n\n\nCONFIG_COMPAT\n\n\nCONFIG_IA32_EMULATION\n\n\n\n\nInternal\n\n\nEntry Points\n\n\nThe assembly entry points are defined in \nentry/entry_64_compat.S\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nENTRY\n(\nentry_SYSENTER_compat\n)\n\n        \n...\n\n        \ncall\n    \ndo_fast_syscall_32\n\n\nGLOBAL\n(\n__end_entry_SYSENTER_compat\n)\n\n\nENDPROC\n(\nentry_SYSENTER_compat\n)\n\n\n\nENTRY\n(\nentry_SYSCALL_compat\n)\n\n        \n...\n\n        \ncall\n    \ndo_fast_syscall_32\n\n\nEND\n(\nentry_SYSCALL_compat\n)\n\n\n\nENTRY\n(\nentry_INT80_compat\n)\n\n        \n...\n\n        \ncall\n    \ndo_int80_syscall_32\n\n\nEND\n(\nentry_INT80_compat\n)\n\n\n\n\n\n\nEntry Points Setup\n\n\nThe assembly entry points are filled to system registers and IDT table. So users can \nactually\n issue those calls, Lego is able to catch them:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\nstatic\n \nvoid\n \nsyscall_init\n(\nvoid\n)\n\n\n{\n\n        \nwrmsr\n(\nMSR_STAR\n,\n \n0\n,\n \n(\n__USER32_CS\n \n \n16\n)\n \n|\n \n__KERNEL_CS\n);\n\n        \nwrmsrl\n(\nMSR_LSTAR\n,\n \n(\nunsigned\n \nlong\n)\nentry_SYSCALL_64\n);\n\n\n\n#ifdef CONFIG_IA32_EMULATION\n\n        \nwrmsrl\n(\nMSR_CSTAR\n,\n \n(\nunsigned\n \nlong\n)\nentry_SYSCALL_compat\n);\n\n        \n/*  \n\n\n         * This only works on Intel CPUs.\n\n\n         * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.\n\n\n         * This does not cause SYSENTER to jump to the wrong location, because\n\n\n         * AMD doesn\nt allow SYSENTER in long mode (either 32- or 64-bit).\n\n\n         */\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_CS\n,\n \n(\nu64\n)\n__KERNEL_CS\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_ESP\n,\n \n0ULL\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_EIP\n,\n \n(\nu64\n)\nentry_SYSENTER_compat\n);\n\n\n#else\n\n        \nwrmsrl\n(\nMSR_CSTAR\n,\n \n(\nunsigned\n \nlong\n)\nignore_sysret\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_CS\n,\n \n(\nu64\n)\nGDT_ENTRY_INVALID_SEG\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_ESP\n,\n \n0ULL\n);\n\n        \nwrmsrl_safe\n(\nMSR_IA32_SYSENTER_EIP\n,\n \n0ULL\n);\n\n\n#endif\n\n\n\n        \n/* Flags to clear on syscall */\n\n        \nwrmsrl\n(\nMSR_SYSCALL_MASK\n,\n\n               \nX86_EFLAGS_TF\n|\nX86_EFLAGS_DF\n|\nX86_EFLAGS_IF\n|\n\n               \nX86_EFLAGS_IOPL\n|\nX86_EFLAGS_AC\n|\nX86_EFLAGS_NT\n);\n\n\n}\n\n\narch\n/\nx86\n/\nkernel\n/\ncpu\n/\ncommon\n.\nc\n\n\n\nvoid\n \n__init\n \ntrap_init\n(\nvoid\n)\n\n\n{\n\n        \n...\n\n\n#ifdef CONFIG_IA32_EMULATION\n\n        \nset_system_intr_gate\n(\nIA32_SYSCALL_VECTOR\n,\n \nentry_INT80_compat\n);\n\n        \nset_bit\n(\nIA32_SYSCALL_VECTOR\n,\n \nused_vectors\n);\n\n\n#endif\n\n        \n...\n\n\n}\n\n\narch\n/\nx86\n/\nkernel\n/\ntraps\n.\nc\n\n\n\n\n\n\nC code\n\n\nThe actual C code is in \nentry/common.c\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)\n\n\nstatic\n \n__always_inline\n \nvoid\n \ndo_syscall_32_irqs_on\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n\n#ifdef CONFIG_IA32_EMULATION\n\n        \ncurrent\n-\nthread\n.\nstatus\n \n|=\n \nTS_COMPAT\n;\n\n\n#endif\n\n\n        \nBUG\n();\n\n\n}\n\n\n\n/* Handles int $0x80 */\n\n\n__visible\n \nvoid\n \ndo_int80_syscall_32\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \nBUG\n();\n\n\n}\n\n\n\n/* Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */\n\n\n__visible\n \nlong\n \ndo_fast_syscall_32\n(\nstruct\n \npt_regs\n \n*\nregs\n)\n\n\n{\n\n        \nBUG\n();\n\n\n}\n\n\n#endif\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 22, 2018\n\nLast Updated: Feb 22, 2018", 
            "title": "compat"
        }, 
        {
            "location": "/lego/syscall/compat/#compat-syscall-in-lego", 
            "text": "Lego does  not  support compatible syscalls, where one is able to run 32-bit image on 64-bit OS. However, the ugly FPU code and signal part in Linux is heavily hacked with the assumption that compat syscall is supported. We are no expert in this FPU thing, just to make sure we don t break this FPU evil, Lego adds the fake compat syscall support. Fake means whenever a 32-bit syscall is issued, Lego will just panic.", 
            "title": "Compat SYSCALL in Lego"
        }, 
        {
            "location": "/lego/syscall/compat/#kconfig", 
            "text": "If one compiles a x86_64 Linux kernel, compat syscalls are supported by default. Everything related to compat syscalls are controlled by the following two Kconfig options. Lego may want to support compat syscalls in the future, thus we add these two Kconfigs to avoid future mess:   CONFIG_COMPAT  CONFIG_IA32_EMULATION", 
            "title": "Kconfig"
        }, 
        {
            "location": "/lego/syscall/compat/#internal", 
            "text": "", 
            "title": "Internal"
        }, 
        {
            "location": "/lego/syscall/compat/#entry-points", 
            "text": "The assembly entry points are defined in  entry/entry_64_compat.S :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 ENTRY ( entry_SYSENTER_compat ) \n         ... \n         call      do_fast_syscall_32  GLOBAL ( __end_entry_SYSENTER_compat )  ENDPROC ( entry_SYSENTER_compat )  ENTRY ( entry_SYSCALL_compat ) \n         ... \n         call      do_fast_syscall_32  END ( entry_SYSCALL_compat )  ENTRY ( entry_INT80_compat ) \n         ... \n         call      do_int80_syscall_32  END ( entry_INT80_compat )", 
            "title": "Entry Points"
        }, 
        {
            "location": "/lego/syscall/compat/#entry-points-setup", 
            "text": "The assembly entry points are filled to system registers and IDT table. So users can  actually  issue those calls, Lego is able to catch them:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 static   void   syscall_init ( void )  { \n         wrmsr ( MSR_STAR ,   0 ,   ( __USER32_CS     16 )   |   __KERNEL_CS ); \n         wrmsrl ( MSR_LSTAR ,   ( unsigned   long ) entry_SYSCALL_64 );  #ifdef CONFIG_IA32_EMULATION \n         wrmsrl ( MSR_CSTAR ,   ( unsigned   long ) entry_SYSCALL_compat ); \n         /*             * This only works on Intel CPUs.           * On AMD CPUs these MSRs are 32-bit, CPU truncates MSR_IA32_SYSENTER_EIP.           * This does not cause SYSENTER to jump to the wrong location, because           * AMD doesn t allow SYSENTER in long mode (either 32- or 64-bit).           */ \n         wrmsrl_safe ( MSR_IA32_SYSENTER_CS ,   ( u64 ) __KERNEL_CS ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_ESP ,   0ULL ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_EIP ,   ( u64 ) entry_SYSENTER_compat );  #else \n         wrmsrl ( MSR_CSTAR ,   ( unsigned   long ) ignore_sysret ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_CS ,   ( u64 ) GDT_ENTRY_INVALID_SEG ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_ESP ,   0ULL ); \n         wrmsrl_safe ( MSR_IA32_SYSENTER_EIP ,   0ULL );  #endif \n\n\n         /* Flags to clear on syscall */ \n         wrmsrl ( MSR_SYSCALL_MASK , \n                X86_EFLAGS_TF | X86_EFLAGS_DF | X86_EFLAGS_IF | \n                X86_EFLAGS_IOPL | X86_EFLAGS_AC | X86_EFLAGS_NT );  }  arch / x86 / kernel / cpu / common . c  void   __init   trap_init ( void )  { \n         ...  #ifdef CONFIG_IA32_EMULATION \n         set_system_intr_gate ( IA32_SYSCALL_VECTOR ,   entry_INT80_compat ); \n         set_bit ( IA32_SYSCALL_VECTOR ,   used_vectors );  #endif \n         ...  }  arch / x86 / kernel / traps . c", 
            "title": "Entry Points Setup"
        }, 
        {
            "location": "/lego/syscall/compat/#c-code", 
            "text": "The actual C code is in  entry/common.c :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 #if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)  static   __always_inline   void   do_syscall_32_irqs_on ( struct   pt_regs   * regs )  {  #ifdef CONFIG_IA32_EMULATION \n         current - thread . status   |=   TS_COMPAT ;  #endif \n\n         BUG ();  }  /* Handles int $0x80 */  __visible   void   do_int80_syscall_32 ( struct   pt_regs   * regs )  { \n         BUG ();  }  /* Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */  __visible   long   do_fast_syscall_32 ( struct   pt_regs   * regs )  { \n         BUG ();  }  #endif    \nYizhou Shan \nCreated: Feb 22, 2018 \nLast Updated: Feb 22, 2018", 
            "title": "C code"
        }, 
        {
            "location": "/lego/syscall/msync/", 
            "text": "msync()\n\n\nThe document is a summary I wrote after reading \nFailure-atomic msync()\n paper, which help me understand several questions related to \nmsync()\n.\n\n\n\n\n\n\nmsync() is not atomic.\n During msync(), pages are being written back to disk one by one (or batched): few pages have been flushed back, but few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.\u000b\u000b\n\n\n\n\n\n\nmsync() need concurrency control\n. This actually is the issue I asked before. With a multi-threaded application, does msync() provide the synchronization semantic? The answer is no. Other threads within the same process are able to write to pages under msync(). This implies, application need to handle concurrency by themselves, e.g., rwlocks. \u000b\u000bAt the very beginning, I thought msync() provide this semantic. The only way to implement this should be: kernel make all pages\n PTE read-only, and then perform flush back. If any other threads does a write during flush, they will have a page fault. And in the pgfault function, we hold the threads until the pages are written back.\n\n\n\n\n\n\nProbably some nice reading. \nfsync, fdatasync\n1\n.\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 01, 2018\n\nLast Updated: Mar 23, 2018\n\n\n\n\n\n\n\n\n\n\nRFLUSH: Rethink the Flush", 
            "title": "msync()"
        }, 
        {
            "location": "/lego/syscall/msync/#msync", 
            "text": "The document is a summary I wrote after reading  Failure-atomic msync()  paper, which help me understand several questions related to  msync() .    msync() is not atomic.  During msync(), pages are being written back to disk one by one (or batched): few pages have been flushed back, but few pages are still in the memory. This premature writeback is not atomic and will be affected by failure.\u000b\u000b    msync() need concurrency control . This actually is the issue I asked before. With a multi-threaded application, does msync() provide the synchronization semantic? The answer is no. Other threads within the same process are able to write to pages under msync(). This implies, application need to handle concurrency by themselves, e.g., rwlocks. \u000b\u000bAt the very beginning, I thought msync() provide this semantic. The only way to implement this should be: kernel make all pages  PTE read-only, and then perform flush back. If any other threads does a write during flush, they will have a page fault. And in the pgfault function, we hold the threads until the pages are written back.    Probably some nice reading.  fsync, fdatasync 1 .    \nYizhou Shan \nCreated: Feb 01, 2018 \nLast Updated: Mar 23, 2018      RFLUSH: Rethink the Flush", 
            "title": "msync()"
        }, 
        {
            "location": "/lego/syscall/mremap/", 
            "text": "mremap()", 
            "title": "mremap()"
        }, 
        {
            "location": "/lego/syscall/mremap/#mremap", 
            "text": "", 
            "title": "mremap()"
        }, 
        {
            "location": "/lego/syscall/fork/", 
            "text": "fork()\n\n\nMemory Manager\n\n\nWe need to duplicate the address space in the memory manager side. Follow the traditional \nfork()\n semantic, both the existing and newly created address space will be write-protected.\n\n\nSince we have the flexibility to implement any VM organization, we should be careful while duplicating the address space. Currently, we are using page-based VM, thus the duplicating is basically creating a new \npgd\n and copy existing pgtables, and further downgrade permission to read-only. This is now performed by \nlego_copy_page_range()\n.\n\n\nThe final write-protect is performed by \nlego_copy_one_pte()\n:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nstatic\n \ninline\n \nint\n \nlego_copy_one_pte\n(..)\n\n\n{\n\n    \n..\n\n    \n/*\n\n\n     * If it\ns a COW mapping, write protect it both\n\n\n     * in the parent and the child\n\n\n     */\n\n    \nif\n \n(\nis_cow_mapping\n(\nvm_flags\n))\n \n{\n\n        \nptep_set_wrprotect\n(\nsrc_pte\n);\n   \n        \npte\n \n=\n \npte_wrprotect\n(\npte\n);\n      \n    \n}\n\n    \n...\n\n\n}\n\n\n\n\n\n\nDuplicate VM Free Pool\n\n\nTODO\n Yutong\n\n\nProcessor Manager\n\n\nBoring implementation details in the processor manager side.\n\n\nEntry Points\n\n\n\n\nfork()\n\n\nvfork()\n\n\nclone()\n\n\nkernel_thread()\n\n\n\n\nAll of them land on \ndo_fork()\n, which is Lego\ns main fork function.\n\n\ndo_fork()\n\n\nThere are mainly three parts within \ndo_fork()\n: \n1)\n \ncopy_process()\n, which duplicates a new task based on \ncurrent\n, including allocate new kernel stack, new task_struct, increase mm reference counter, etc. \n2)\n If we are creating a new process, then tell global monitor or memory manager to let them update bookkeeping and create corresponding data structures. \n3)\n \nwake_up_new_task()\n, which gives away the newly created task to local scheduler.\n\n\ncopy_process()\n\n\nThe routine is kind of boring. It do a lot dirty work to copy information from calling thread to new thread. The most important data structures of course are \ntask_struct\n, \nmm_sturct\n, \nsighand\n, and so on. This section only talks about few of them, and leave others to readers who are interested.\n\n\nSanity Checking\n\n\nMainly check if \nclone_flags\n are passed properly. For example, if user is creating a new thread, that implies certain data structures are shared, cause new thread belongs to the same process with the calling thread. If \nCLONE_THREAD\n is passed, then \nCLONE_SIGHAND\n, \nCLONE_VM\n, and so on must be set as well.\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n    \n/*\n\n\n     * Thread groups must share signals as well, and detached threads\n\n\n     * can only be started up within the thread group.\n\n\n     */\n\n    \nif\n \n((\nclone_flags\n \n \nCLONE_THREAD\n)\n \n \n!\n(\nclone_flags\n \n \nCLONE_SIGHAND\n))\n\n        \nreturn\n \nERR_PTR\n(\n-\nEINVAL\n);\n\n\n    \n/*\n\n\n     * Shared signal handlers imply shared VM. By way of the above,\n\n\n     * thread groups also imply shared VM. Blocking this case allows\n\n\n     * for various simplifications in other code.\n\n\n     */\n\n    \nif\n \n((\nclone_flags\n \n \nCLONE_SIGHAND\n)\n \n \n!\n(\nclone_flags\n \n \nCLONE_VM\n))\n\n        \nreturn\n \nERR_PTR\n(\n-\nEINVAL\n);\n\n\n\n\n\n\ndup_task_struct()\n\n\nTwo main things: 1) duplicate a new \ntask_struct\n, 2) duplicate a new kernel stack. x86 is just a weird architecture, the size of \ntask_struct\n depends on the size of fpu. So the allocation and duplication need to callback to x86-specific code to duplicate the task_struct and fpu info.\n\n1\n2\n3\n4\n5\n6\nint\n \narch_dup_task_struct\n(\nstruct\n \ntask_struct\n \n*\ndst\n,\n \nstruct\n \ntask_struct\n \n*\nsrc\n)\n\n\n{\n\n    \nmemcpy\n(\ndst\n,\n \nsrc\n,\n \narch_task_struct_size\n);\n\n\n    \nreturn\n \nfpu__copy\n(\ndst\n-\nthread\n.\nfpu\n,\n \nsrc\n-\nthread\n.\nfpu\n);\n\n\n}\n\n\n\n\n\nThe stack duplication is fairly simple, just copy everything from the old stack to new stack. Of course, it needs to setup the \nthread_info\n to points to this new thread, so the \ncurrent\n macro will work.\n\n1\n2\n3\n4\n5\n6\n7\n8\nstatic\n \nvoid\n \nsetup_thread_stack\n(\nstruct\n \ntask_struct\n \n*\np\n,\n \nstruct\n \ntask_struct\n \n*\norg\n)\n\n\n{\n\n        \n/* Duplicate whole stack! */\n\n        \n*\ntask_thread_info\n(\np\n)\n \n=\n \n*\ntask_thread_info\n(\norg\n);\n\n\n        \n/* Make the `current\n macro work */\n\n        \ntask_thread_info\n(\np\n)\n-\ntask\n \n=\n \np\n;\n\n\n}\n\n\n\n\n\n\ncopy_mm()\n\n\nThis is where threads within a process will share the virtual address space happens. If we are creating a new process, then this function will create a new \nmm_struct\n, and also a new \npgd\n:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n/*\n\n\n * pgd_alloc() will duplicate the identity kernel mapping\n\n\n * but leaves other entries empty:\n\n\n */\n\n\nmm\n-\npgd\n \n=\n \npgd_alloc\n(\nmm\n);\n\n\nif\n \n(\nunlikely\n(\n!\nmm\n-\npgd\n))\n \n{\n\n        \nkfree\n(\nmm\n);\n\n        \nreturn\n \nNULL\n;\n\n\n}\n\n\n\n\n\n\nDuplicate pcache data\n\n\nTODO\n\n\nTODO: hook with pcache\nWe need to duplicate the pcache vm_range array, once Yutong finished the code.\nsetup_sched_fork()\n\n\nCallback to scheduler to setup this new task. It may reset all scheduler related information. Here we also have a chance to change this task\ns scheduler class:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\nint\n \nsetup_sched_fork\n(\nunsigned\n \nlong\n \nclone_flags\n,\n \nstruct\n \ntask_struct\n \n*\np\n)\n\n\n{\n\n        \nint\n \ncpu\n \n=\n \nget_cpu\n();\n\n\n        \n__sched_fork\n(\nclone_flags\n,\n \np\n);\n\n\n        \np\n-\nstate\n \n=\n \nTASK_NEW\n;\n\n        \n...\n\n        \nif\n \n(\nunlikely\n(\np\n-\nsched_reset_on_fork\n))\n \n{\n\n                \nif\n \n(\ntask_has_rt_policy\n(\np\n))\n \n{\n\n                        \np\n-\npolicy\n \n=\n \nSCHED_NORMAL\n;\n\n                        \np\n-\nstatic_prio\n \n=\n \nNICE_TO_PRIO\n(\n0\n);\n\n                        \np\n-\nrt_priority\n \n=\n \n0\n;\n\n                \n}\n \nelse\n \nif\n \n(\nPRIO_TO_NICE\n(\np\n-\nstatic_prio\n)\n \n \n0\n)\n\n                        \np\n-\nstatic_prio\n \n=\n \nNICE_TO_PRIO\n(\n0\n);\n\n\n                \np\n-\nprio\n \n=\n \np\n-\nnormal_prio\n \n=\n \n__normal_prio\n(\np\n);\n\n                \nset_load_weight\n(\np\n);\n\n                \n...\n\n        \n}\n    \n\n        \nif\n \n(\nrt_prio\n(\np\n-\nprio\n))\n\n                \np\n-\nsched_class\n \n=\n \nrt_sched_class\n;\n\n        \nelse\n \n{\n\n                \np\n-\nsched_class\n \n=\n \nfair_sched_class\n;\n\n                \nset_load_weight\n(\np\n);\n\n        \n}\n    \n\n        \n__set_task_cpu\n(\np\n,\n \ncpu\n);\n\n        \nif\n \n(\np\n-\nsched_class\n-\ntask_fork\n)\n\n                \np\n-\nsched_class\n-\ntask_fork\n(\np\n);\n\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nAllocate new pid\n\n\nIn both Lego and Linux, we don\nt allocate new pid for a new thread, if that thread is an \nidle thread\n. So callers of \ndo_fork\n needs to pass something to let \ndo_fork\n know. In Linux, they use \nstruct pid, init_struct_pid\n to check. In Lego, we introduce an new clone_flag \nCLONE_IDLE_THREAD\n. If that flag is set, \ndo_fork()\n will try to allocate a new pid for the new thread. Otherwise, it will be 0:\n\n1\n2\n3\n4\n5\n6\n/* clone idle thread, whose pid is 0 */\n\n\nif\n \n(\n!\n(\nclone_flags\n \n \nCLONE_IDLE_THREAD\n))\n \n{\n\n        \npid\n \n=\n \nalloc_pid\n(\np\n);\n\n        \nif\n \n(\n!\npid\n)\n\n                \ngoto\n \nout_cleanup_thread\n;\n\n\n}\n\n\n\n\n\n\nSo, only the \ninit_idle()\n function can pass this \nCLONE_IDLE_THREAD\n down. All other usages are wrong and should be reported.\n\n\nIn order to avoid conflict with Linux clone_flag, we define it as:\n\n1\n#define CLONE_IDLE_THREAD       0x100000000\n\n\n\n\n\n\nSETTID/CLEARTID\n\n\nThese are some futex related stuff. I will cover these stuff in futex document:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\np\n-\nset_child_tid\n \n=\n \n(\nclone_flags\n \n \nCLONE_CHILD_SETTID\n)\n \n?\n \nchild_tidptr\n \n:\n \nNULL\n;\n\n\n/*  \n\n\n * Clear TID on mm_release()?\n\n\n */\n\n\np\n-\nclear_child_tid\n \n=\n \n(\nclone_flags\n \n \nCLONE_CHILD_CLEARTID\n)\n \n?\n \nchild_tidptr\n \n:\n \nNULL\n;\n\n\n\n#ifdef CONFIG_FUTEX\n\n\np\n-\nrobust_list\n \n=\n \nNULL\n;\n\n\n#endif\n\n\n\n\n\n\ncopy_thread_tls()\n\n\nThis is the most interesting function. Cover later.\n\n\np2m_fork()\n\n\nIn order to track user activities, we need to know when user are going to create new process. Fork is the best time and the only time we kernel know. So, Lego adds this special hook to tell remote global monitor or memory manager that there is a new process going to be created. Upon receiving this message, remote monitor will update its bookkeeping for this specific user/vNode.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n/* Tell remote memory component */\n\n\n#ifdef CONFIG_COMP_PROCESSOR\n\n\nif\n \n(\nclone_flags\n \n \nCLONE_GLOBAL_THREAD\n)\n \n{\n\n        \n...\n\n        \np2m_fork\n(\np\n,\n \nclone_flags\n);\n\n        \n...\n\n\n}\n   \n\n#endif\n\n\n\n\n\n\n\nThe \nCLONE_GLOBAL_THREAD\n should only be set, if the following cases happen:\n\n\n\n\nfork()\n\n\nvfork()\n\n\nclone(), without \nCLONE_THREAD\n being set\n\n\n\n\nIn order to avoid conflict with Linux clone_flag, we define it as:\n\n1\n#define CLONE_GLOBAL_THREAD     0x200000000\n\n\n\n\n\n\nwake_up_new_task()\n\n\nThe last step of \ndo_fork\n is waking up the new thread or process, which is performed by \nwake_up_new_task()\n function. The first question this function will ask is: \nwhich cpu to land?\n The answer comes from \nselect_task_rq()\n:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nstatic\n \ninline\n\n\nint\n \nselect_task_rq\n(\nstruct\n \ntask_struct\n \n*\np\n,\n \nint\n \ncpu\n,\n \nint\n \nsd_flags\n,\n \nint\n \nwake_flags\n)\n\n\n{\n\n        \nif\n \n(\np\n-\nnr_cpus_allowed\n \n \n1\n)\n\n                \ncpu\n \n=\n \np\n-\nsched_class\n-\nselect_task_rq\n(\np\n,\n \ncpu\n,\n \nsd_flags\n,\n \nwake_flags\n);\n\n        \nelse\n\n                \ncpu\n \n=\n \ncpumask_any\n(\np\n-\ncpus_allowed\n);\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\nClearly, this is determined by \ncpus_allowed\n, which is the same with its parent at this point. That being said, if the parent is only able to run on one specific CPU, then all its children will end up running on the same CPU when they wake up (they could change their affinity later). This is also the default on Linux: \nA child created via fork(2) inherits its parent\ns CPU affinity mask. The affinity mask is preserved across an execve(2).\n\n\nAfter landing CPU is selected, following operation is simple: just enqueue this task into landing CPU\ns runqueue, and we are done:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nvoid\n \nwake_up_new_task\n(\nstruct\n \ntask_struct\n \n*\np\n)\n\n\n{\n\n        \n...\n\n\n/* Select a CPU for new thread to run */\n\n\n#ifdef CONFIG_SMP\n\n        \n/*   \n\n\n         * Fork balancing, do it here and not earlier because:\n\n\n         *  - cpus_allowed can change in the fork path\n\n\n         *  - any previously selected cpu might disappear through hotplug\n\n\n         */\n\n        \nset_task_cpu\n(\np\n,\n \nselect_task_rq\n(\np\n,\n \ntask_cpu\n(\np\n),\n \nSD_BALANCE_FORK\n,\n \n0\n));\n\n\n#endif\n\n\n        \nrq\n \n=\n \n__task_rq_lock\n(\np\n);\n\n        \nactivate_task\n(\nrq\n,\n \np\n,\n \n0\n);\n\n        \np\n-\non_rq\n \n=\n \nTASK_ON_RQ_QUEUED\n;\n\n        \n...\n\n\n}\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Feb 11, 2018\n\nLast Updated: Feb 27, 2018", 
            "title": "fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#fork", 
            "text": "", 
            "title": "fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#memory-manager", 
            "text": "We need to duplicate the address space in the memory manager side. Follow the traditional  fork()  semantic, both the existing and newly created address space will be write-protected.  Since we have the flexibility to implement any VM organization, we should be careful while duplicating the address space. Currently, we are using page-based VM, thus the duplicating is basically creating a new  pgd  and copy existing pgtables, and further downgrade permission to read-only. This is now performed by  lego_copy_page_range() .  The final write-protect is performed by  lego_copy_one_pte() :  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 static   inline   int   lego_copy_one_pte (..)  { \n     .. \n     /*       * If it s a COW mapping, write protect it both       * in the parent and the child       */ \n     if   ( is_cow_mapping ( vm_flags ))   { \n         ptep_set_wrprotect ( src_pte );    \n         pte   =   pte_wrprotect ( pte );       \n     } \n     ...  }", 
            "title": "Memory Manager"
        }, 
        {
            "location": "/lego/syscall/fork/#duplicate-vm-free-pool", 
            "text": "TODO  Yutong", 
            "title": "Duplicate VM Free Pool"
        }, 
        {
            "location": "/lego/syscall/fork/#processor-manager", 
            "text": "Boring implementation details in the processor manager side.", 
            "title": "Processor Manager"
        }, 
        {
            "location": "/lego/syscall/fork/#entry-points", 
            "text": "fork()  vfork()  clone()  kernel_thread()   All of them land on  do_fork() , which is Lego s main fork function.", 
            "title": "Entry Points"
        }, 
        {
            "location": "/lego/syscall/fork/#do_fork", 
            "text": "There are mainly three parts within  do_fork() :  1)   copy_process() , which duplicates a new task based on  current , including allocate new kernel stack, new task_struct, increase mm reference counter, etc.  2)  If we are creating a new process, then tell global monitor or memory manager to let them update bookkeeping and create corresponding data structures.  3)   wake_up_new_task() , which gives away the newly created task to local scheduler.", 
            "title": "do_fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#copy_process", 
            "text": "The routine is kind of boring. It do a lot dirty work to copy information from calling thread to new thread. The most important data structures of course are  task_struct ,  mm_sturct ,  sighand , and so on. This section only talks about few of them, and leave others to readers who are interested.", 
            "title": "copy_process()"
        }, 
        {
            "location": "/lego/syscall/fork/#sanity-checking", 
            "text": "Mainly check if  clone_flags  are passed properly. For example, if user is creating a new thread, that implies certain data structures are shared, cause new thread belongs to the same process with the calling thread. If  CLONE_THREAD  is passed, then  CLONE_SIGHAND ,  CLONE_VM , and so on must be set as well.  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14      /*       * Thread groups must share signals as well, and detached threads       * can only be started up within the thread group.       */ \n     if   (( clone_flags     CLONE_THREAD )     ! ( clone_flags     CLONE_SIGHAND )) \n         return   ERR_PTR ( - EINVAL ); \n\n     /*       * Shared signal handlers imply shared VM. By way of the above,       * thread groups also imply shared VM. Blocking this case allows       * for various simplifications in other code.       */ \n     if   (( clone_flags     CLONE_SIGHAND )     ! ( clone_flags     CLONE_VM )) \n         return   ERR_PTR ( - EINVAL );", 
            "title": "Sanity Checking"
        }, 
        {
            "location": "/lego/syscall/fork/#dup_task_struct", 
            "text": "Two main things: 1) duplicate a new  task_struct , 2) duplicate a new kernel stack. x86 is just a weird architecture, the size of  task_struct  depends on the size of fpu. So the allocation and duplication need to callback to x86-specific code to duplicate the task_struct and fpu info. 1\n2\n3\n4\n5\n6 int   arch_dup_task_struct ( struct   task_struct   * dst ,   struct   task_struct   * src )  { \n     memcpy ( dst ,   src ,   arch_task_struct_size ); \n\n     return   fpu__copy ( dst - thread . fpu ,   src - thread . fpu );  }   \nThe stack duplication is fairly simple, just copy everything from the old stack to new stack. Of course, it needs to setup the  thread_info  to points to this new thread, so the  current  macro will work. 1\n2\n3\n4\n5\n6\n7\n8 static   void   setup_thread_stack ( struct   task_struct   * p ,   struct   task_struct   * org )  { \n         /* Duplicate whole stack! */ \n         * task_thread_info ( p )   =   * task_thread_info ( org ); \n\n         /* Make the `current  macro work */ \n         task_thread_info ( p ) - task   =   p ;  }", 
            "title": "dup_task_struct()"
        }, 
        {
            "location": "/lego/syscall/fork/#copy_mm", 
            "text": "This is where threads within a process will share the virtual address space happens. If we are creating a new process, then this function will create a new  mm_struct , and also a new  pgd : 1\n2\n3\n4\n5\n6\n7\n8\n9 /*   * pgd_alloc() will duplicate the identity kernel mapping   * but leaves other entries empty:   */  mm - pgd   =   pgd_alloc ( mm );  if   ( unlikely ( ! mm - pgd ))   { \n         kfree ( mm ); \n         return   NULL ;  }", 
            "title": "copy_mm()"
        }, 
        {
            "location": "/lego/syscall/fork/#duplicate-pcache-data", 
            "text": "TODO  TODO: hook with pcache We need to duplicate the pcache vm_range array, once Yutong finished the code.", 
            "title": "Duplicate pcache data"
        }, 
        {
            "location": "/lego/syscall/fork/#setup_sched_fork", 
            "text": "Callback to scheduler to setup this new task. It may reset all scheduler related information. Here we also have a chance to change this task s scheduler class:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 int   setup_sched_fork ( unsigned   long   clone_flags ,   struct   task_struct   * p )  { \n         int   cpu   =   get_cpu (); \n\n         __sched_fork ( clone_flags ,   p ); \n\n         p - state   =   TASK_NEW ; \n         ... \n         if   ( unlikely ( p - sched_reset_on_fork ))   { \n                 if   ( task_has_rt_policy ( p ))   { \n                         p - policy   =   SCHED_NORMAL ; \n                         p - static_prio   =   NICE_TO_PRIO ( 0 ); \n                         p - rt_priority   =   0 ; \n                 }   else   if   ( PRIO_TO_NICE ( p - static_prio )     0 ) \n                         p - static_prio   =   NICE_TO_PRIO ( 0 ); \n\n                 p - prio   =   p - normal_prio   =   __normal_prio ( p ); \n                 set_load_weight ( p ); \n                 ... \n         }     \n\n         if   ( rt_prio ( p - prio )) \n                 p - sched_class   =   rt_sched_class ; \n         else   { \n                 p - sched_class   =   fair_sched_class ; \n                 set_load_weight ( p ); \n         }     \n\n         __set_task_cpu ( p ,   cpu ); \n         if   ( p - sched_class - task_fork ) \n                 p - sched_class - task_fork ( p ); \n\n         ...  }", 
            "title": "setup_sched_fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#allocate-new-pid", 
            "text": "In both Lego and Linux, we don t allocate new pid for a new thread, if that thread is an  idle thread . So callers of  do_fork  needs to pass something to let  do_fork  know. In Linux, they use  struct pid, init_struct_pid  to check. In Lego, we introduce an new clone_flag  CLONE_IDLE_THREAD . If that flag is set,  do_fork()  will try to allocate a new pid for the new thread. Otherwise, it will be 0: 1\n2\n3\n4\n5\n6 /* clone idle thread, whose pid is 0 */  if   ( ! ( clone_flags     CLONE_IDLE_THREAD ))   { \n         pid   =   alloc_pid ( p ); \n         if   ( ! pid ) \n                 goto   out_cleanup_thread ;  }    So, only the  init_idle()  function can pass this  CLONE_IDLE_THREAD  down. All other usages are wrong and should be reported.  In order to avoid conflict with Linux clone_flag, we define it as: 1 #define CLONE_IDLE_THREAD       0x100000000", 
            "title": "Allocate new pid"
        }, 
        {
            "location": "/lego/syscall/fork/#settidcleartid", 
            "text": "These are some futex related stuff. I will cover these stuff in futex document: 1\n2\n3\n4\n5\n6\n7\n8\n9 p - set_child_tid   =   ( clone_flags     CLONE_CHILD_SETTID )   ?   child_tidptr   :   NULL ;  /*     * Clear TID on mm_release()?   */  p - clear_child_tid   =   ( clone_flags     CLONE_CHILD_CLEARTID )   ?   child_tidptr   :   NULL ;  #ifdef CONFIG_FUTEX  p - robust_list   =   NULL ;  #endif", 
            "title": "SETTID/CLEARTID"
        }, 
        {
            "location": "/lego/syscall/fork/#copy_thread_tls", 
            "text": "This is the most interesting function. Cover later.", 
            "title": "copy_thread_tls()"
        }, 
        {
            "location": "/lego/syscall/fork/#p2m_fork", 
            "text": "In order to track user activities, we need to know when user are going to create new process. Fork is the best time and the only time we kernel know. So, Lego adds this special hook to tell remote global monitor or memory manager that there is a new process going to be created. Upon receiving this message, remote monitor will update its bookkeeping for this specific user/vNode.  1\n2\n3\n4\n5\n6\n7\n8 /* Tell remote memory component */  #ifdef CONFIG_COMP_PROCESSOR  if   ( clone_flags     CLONE_GLOBAL_THREAD )   { \n         ... \n         p2m_fork ( p ,   clone_flags ); \n         ...  }     #endif    The  CLONE_GLOBAL_THREAD  should only be set, if the following cases happen:   fork()  vfork()  clone(), without  CLONE_THREAD  being set   In order to avoid conflict with Linux clone_flag, we define it as: 1 #define CLONE_GLOBAL_THREAD     0x200000000", 
            "title": "p2m_fork()"
        }, 
        {
            "location": "/lego/syscall/fork/#wake_up_new_task", 
            "text": "The last step of  do_fork  is waking up the new thread or process, which is performed by  wake_up_new_task()  function. The first question this function will ask is:  which cpu to land?  The answer comes from  select_task_rq() :  1\n2\n3\n4\n5\n6\n7\n8\n9 static   inline  int   select_task_rq ( struct   task_struct   * p ,   int   cpu ,   int   sd_flags ,   int   wake_flags )  { \n         if   ( p - nr_cpus_allowed     1 ) \n                 cpu   =   p - sched_class - select_task_rq ( p ,   cpu ,   sd_flags ,   wake_flags ); \n         else \n                 cpu   =   cpumask_any ( p - cpus_allowed ); \n         ...  }    Clearly, this is determined by  cpus_allowed , which is the same with its parent at this point. That being said, if the parent is only able to run on one specific CPU, then all its children will end up running on the same CPU when they wake up (they could change their affinity later). This is also the default on Linux:  A child created via fork(2) inherits its parent s CPU affinity mask. The affinity mask is preserved across an execve(2).  After landing CPU is selected, following operation is simple: just enqueue this task into landing CPU s runqueue, and we are done:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 void   wake_up_new_task ( struct   task_struct   * p )  { \n         ...  /* Select a CPU for new thread to run */  #ifdef CONFIG_SMP \n         /*              * Fork balancing, do it here and not earlier because:           *  - cpus_allowed can change in the fork path           *  - any previously selected cpu might disappear through hotplug           */ \n         set_task_cpu ( p ,   select_task_rq ( p ,   task_cpu ( p ),   SD_BALANCE_FORK ,   0 ));  #endif \n\n         rq   =   __task_rq_lock ( p ); \n         activate_task ( rq ,   p ,   0 ); \n         p - on_rq   =   TASK_ON_RQ_QUEUED ; \n         ...  }    \nYizhou Shan \nCreated: Feb 11, 2018 \nLast Updated: Feb 27, 2018", 
            "title": "wake_up_new_task()"
        }, 
        {
            "location": "/lego/syscall/getrusage/", 
            "text": "getrusage\n\n\nThe syscall \ngetrusage\n is used to get user program resource usage. It is a nice syscall. But only nice if kernel has all the nice bookkeeping. It is a luxury for us to have all the counting.\n\n\nThe syscall is added recently due to \nwait\n family syscalls, which use and bookkeep some of \nrusage\n.\n\n\nAs on the last updated date (Mar 7), the syscall in Lego only reports number of context switches and a few others.\n\n\n\nYizhou Shan\n\nCreated: Mar 7, 2018\n\nLast Updated: Mar 7, 2018", 
            "title": "getrusage()"
        }, 
        {
            "location": "/lego/syscall/getrusage/#getrusage", 
            "text": "The syscall  getrusage  is used to get user program resource usage. It is a nice syscall. But only nice if kernel has all the nice bookkeeping. It is a luxury for us to have all the counting.  The syscall is added recently due to  wait  family syscalls, which use and bookkeep some of  rusage .  As on the last updated date (Mar 7), the syscall in Lego only reports number of context switches and a few others.  \nYizhou Shan \nCreated: Mar 7, 2018 \nLast Updated: Mar 7, 2018", 
            "title": "getrusage"
        }, 
        {
            "location": "/lego/syscall/wait_and_exit/", 
            "text": "wait4(), waitid(), and exit()\n\n\nLego supports \nwait4()\n and \nwaitid()\n syscalls, and they are compatible with Linux programs. These two syscalls rely on \nexit_notify()\n function when a thread \nexit()\n. Basically, when a thread exit, it will notify its parent, and reparent\n3\n its children if necessary.\n\n\nFacts in Lego:\n\n\n\n\nLego does not have \nprocess group and session\n2\n concept. Each process is within its own process group and session.\n\n\nThis implies Lego will not have \nOrphaned Process Group\n1\n when a process exit.\n\n\nOrphan process\n3\n is adopted by init process (pid 1) if its father is a single-thread process, otherwise it will be adopted by other thread within its father\ns process. This is performed by function \nforget_original_parent()\n.\n\n\nwait, signal, exec, fork are close related.\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 8, 2018\n\nLast Updated: Mar 10, 2018\n\n\n\n\n\n\n\n\n\n\nOrphaned Process Groups\n\n\n\n\n\n\nProcess Group\n\n\n\n\n\n\nOrphan Process", 
            "title": "wait4 and exit()"
        }, 
        {
            "location": "/lego/syscall/wait_and_exit/#wait4-waitid-and-exit", 
            "text": "Lego supports  wait4()  and  waitid()  syscalls, and they are compatible with Linux programs. These two syscalls rely on  exit_notify()  function when a thread  exit() . Basically, when a thread exit, it will notify its parent, and reparent 3  its children if necessary.  Facts in Lego:   Lego does not have  process group and session 2  concept. Each process is within its own process group and session.  This implies Lego will not have  Orphaned Process Group 1  when a process exit.  Orphan process 3  is adopted by init process (pid 1) if its father is a single-thread process, otherwise it will be adopted by other thread within its father s process. This is performed by function  forget_original_parent() .  wait, signal, exec, fork are close related.   \nYizhou Shan \nCreated: Mar 8, 2018 \nLast Updated: Mar 10, 2018      Orphaned Process Groups    Process Group    Orphan Process", 
            "title": "wait4(), waitid(), and exit()"
        }, 
        {
            "location": "/lego/pcache/config/", 
            "text": "Pcache Configuration\n\n\nThis doc explains what configuration options pcache has, and how to config them properly. Pcache is only enabled in Lego\ns processor manager and currently it uses DRAM to emulate the last-level cache (or, L4).\n\n\nKconfig\n\n\nCONFIG_MEMMAP_MEMBLOCK_RESERVED\n\n\nDEFAULT: Y\n\n\nBy default, boot command line option \nmemmap $\n will reserve a range of physical memory.\nThis reserved memory will be marked reserved in e820 table, which\nmeans this range will not be registered into \nmemblock\n. Only memory that has been\nregistered into \nmemblock\n will be assigned \nstruct page\n with it (both \nmemblock.memory\n and \nmemblock.reserve\n will have). And do note that this part of reserved memory can be mapped as 1GB page at boot time.\n\n\nIn other words, by default (the linux semantic), users need to \nioremap\n\nthe \nmemmap $\n reserved physical memory, and use the returned kernel virtual address afterwards.\nAnd do note that the \nioremap()\n only support 4KB mapping.\n\n\nIn Lego, if this option is enabled, the memory marked by \nmemmap $\n will \nNOT\n be marked\nreserved into e820 table, instead, it will be pushed into \nmemblock\n, which means\nit is mapped into kernel direct mapping and has \nstruct page\n.\n\n\nFor those who have done DAX, or NVM related stuff, you must have struggled with\n\nmemmap $\n, and complained why it does not have \nstruct page\n, I guess? So here is\nthe simple code to do so:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nif\n \n(\n*\np\n \n==\n \n@\n)\n \n{\n\n        \nstart_at\n \n=\n \nmemparse\n(\np\n+\n1\n,\n \np\n);\n\n        \ne820_add_region\n(\nstart_at\n,\n \nmem_size\n,\n \nE820_RAM\n);\n\n\n}\n \nelse\n \nif\n \n(\n*\np\n \n==\n \n#\n)\n \n{\n\n        \nstart_at\n \n=\n \nmemparse\n(\np\n+\n1\n,\n \np\n);\n\n        \ne820_add_region\n(\nstart_at\n,\n \nmem_size\n,\n \nE820_ACPI\n);\n\n\n}\n \nelse\n \nif\n \n(\n*\np\n \n==\n \n$\n)\n \n{\n\n        \nstart_at\n \n=\n \nmemparse\n(\np\n+\n1\n,\n \np\n);\n\n\n\n#ifdef CONFIG_MEMMAP_MEMBLOCK_RESERVED\n\n        \nmemblock_reserve\n(\nstart_at\n,\n \nmem_size\n);\n\n\n#else\n\n        \ne820_add_region\n(\nstart_at\n,\n \nmem_size\n,\n \nE820_RESERVED\n);\n\n\n#endif\n\n\n\n\n\n\nBut why we are having this? Because I think the \ndirect 1GB mapping\n may have\nbetter performance: huge page mapping can truly save us a lot TLB misses. However, the real performance number is unknown.\n\n\nIf unsure, say \nY\n.\n\n\n\nYizhou Shan \n\nCreated: Feb 01, 2018\n\nLast Updated: Feb 01, 2018", 
            "title": "Config"
        }, 
        {
            "location": "/lego/pcache/config/#pcache-configuration", 
            "text": "This doc explains what configuration options pcache has, and how to config them properly. Pcache is only enabled in Lego s processor manager and currently it uses DRAM to emulate the last-level cache (or, L4).", 
            "title": "Pcache Configuration"
        }, 
        {
            "location": "/lego/pcache/config/#kconfig", 
            "text": "", 
            "title": "Kconfig"
        }, 
        {
            "location": "/lego/pcache/config/#config_memmap_memblock_reserved", 
            "text": "DEFAULT: Y  By default, boot command line option  memmap $  will reserve a range of physical memory.\nThis reserved memory will be marked reserved in e820 table, which\nmeans this range will not be registered into  memblock . Only memory that has been\nregistered into  memblock  will be assigned  struct page  with it (both  memblock.memory  and  memblock.reserve  will have). And do note that this part of reserved memory can be mapped as 1GB page at boot time.  In other words, by default (the linux semantic), users need to  ioremap \nthe  memmap $  reserved physical memory, and use the returned kernel virtual address afterwards.\nAnd do note that the  ioremap()  only support 4KB mapping.  In Lego, if this option is enabled, the memory marked by  memmap $  will  NOT  be marked\nreserved into e820 table, instead, it will be pushed into  memblock , which means\nit is mapped into kernel direct mapping and has  struct page .  For those who have done DAX, or NVM related stuff, you must have struggled with memmap $ , and complained why it does not have  struct page , I guess? So here is\nthe simple code to do so:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 if   ( * p   ==   @ )   { \n         start_at   =   memparse ( p + 1 ,   p ); \n         e820_add_region ( start_at ,   mem_size ,   E820_RAM );  }   else   if   ( * p   ==   # )   { \n         start_at   =   memparse ( p + 1 ,   p ); \n         e820_add_region ( start_at ,   mem_size ,   E820_ACPI );  }   else   if   ( * p   ==   $ )   { \n         start_at   =   memparse ( p + 1 ,   p );  #ifdef CONFIG_MEMMAP_MEMBLOCK_RESERVED \n         memblock_reserve ( start_at ,   mem_size );  #else \n         e820_add_region ( start_at ,   mem_size ,   E820_RESERVED );  #endif    But why we are having this? Because I think the  direct 1GB mapping  may have\nbetter performance: huge page mapping can truly save us a lot TLB misses. However, the real performance number is unknown.  If unsure, say  Y .  \nYizhou Shan  \nCreated: Feb 01, 2018 \nLast Updated: Feb 01, 2018", 
            "title": "CONFIG_MEMMAP_MEMBLOCK_RESERVED"
        }, 
        {
            "location": "/lego/pcache/sweep/", 
            "text": "Pcache Sweep\n\n\nSome notes while coding pcache sweep thread. The sweep thread wants to detect the hotness of pages, and then adjust LRU list accordingly.\n\n\nData Worth a Billion\n\n\nPcache-reclaim, or any other object reclaim, need some \ndata\n to algorithm about. So specific algorithm can select the \nbest\n candidate to reclaim. In reality, algorithms are designed quite well, but \nhow to get the data\n part becomes extremely hard. I think this applies to many different systems.\n\n\nFor example, to select the hot pages in x86 is notorious hard because x86 hardware only provides a \nReferenced\n bit for system software to reason about. To make it worse, \nReferenced\n bit is cached in TLB, which means CPU will \nNOT\n  set the \nReferenced\n bit even you reset in PTE, because CPU think the bit is already set. In order to get an \naccurate\n hot pages tracking, you probably need a TLB flush after reset \nReferenced\n bit.\u00a0But, are you kidding me, a TLB flush after each reset? We have to say NO here. The Linux code explains it well:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nstatic\n \ninline\n \nint\n \nptep_clear_flush_young\n(\npte_t\n \n*\nptep\n)\n\n\n{\n\n        \n/*\n\n\n         * On x86 CPUs, clearing the accessed bit without a TLB flush\n\n\n         * doesn\nt cause data corruption. [ It could cause incorrect\n\n\n         * page aging and the (mistaken) reclaim of hot pages, but the\n\n\n         * chance of that should be relatively low. ]\n\n\n         *\n\n\n         * So as a performance optimization don\nt flush the TLB when\n\n\n         * clearing the accessed bit, it will eventually be flushed by\n\n\n         * a context switch or a VM operation anyway. [ In the rare\n\n\n         * event of it not getting flushed for a long time the delay\n\n\n         * shouldn\nt really matter because there\ns no real memory\n\n\n         * pressure for swapout to react to. ]\n\n\n         */\n\n        \nreturn\n \nptep_test_and_clear_young\n(\nptep\n);\n\n\n}\n\n\n\n\n\n\nAggressiveness\n\n\nAn aggressive sweep algorithm will disturb the normal operations a lot. In Lego, there 4 main factors that define the aggressiveness:\n\n\n\n\nTime interval between each run\n\n\nNumber of sets to look at during each run\n\n\nSkip if it is not full\n\n\nSkip if it is under eviction\n\n\n\n\n\n\nNumber of lines to look at for each set\n\n\nSmaller or equal to associativity\n\n\n\n\n\n\nNumber of lines to adjust for each set\n\n\nSmaller or equal to lines to look at\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 18, 2018\n\nLast Updated: Mar 18, 2018", 
            "title": "Sweep"
        }, 
        {
            "location": "/lego/pcache/sweep/#pcache-sweep", 
            "text": "Some notes while coding pcache sweep thread. The sweep thread wants to detect the hotness of pages, and then adjust LRU list accordingly.", 
            "title": "Pcache Sweep"
        }, 
        {
            "location": "/lego/pcache/sweep/#data-worth-a-billion", 
            "text": "Pcache-reclaim, or any other object reclaim, need some  data  to algorithm about. So specific algorithm can select the  best  candidate to reclaim. In reality, algorithms are designed quite well, but  how to get the data  part becomes extremely hard. I think this applies to many different systems.  For example, to select the hot pages in x86 is notorious hard because x86 hardware only provides a  Referenced  bit for system software to reason about. To make it worse,  Referenced  bit is cached in TLB, which means CPU will  NOT   set the  Referenced  bit even you reset in PTE, because CPU think the bit is already set. In order to get an  accurate  hot pages tracking, you probably need a TLB flush after reset  Referenced  bit.\u00a0But, are you kidding me, a TLB flush after each reset? We have to say NO here. The Linux code explains it well:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 static   inline   int   ptep_clear_flush_young ( pte_t   * ptep )  { \n         /*           * On x86 CPUs, clearing the accessed bit without a TLB flush           * doesn t cause data corruption. [ It could cause incorrect           * page aging and the (mistaken) reclaim of hot pages, but the           * chance of that should be relatively low. ]           *           * So as a performance optimization don t flush the TLB when           * clearing the accessed bit, it will eventually be flushed by           * a context switch or a VM operation anyway. [ In the rare           * event of it not getting flushed for a long time the delay           * shouldn t really matter because there s no real memory           * pressure for swapout to react to. ]           */ \n         return   ptep_test_and_clear_young ( ptep );  }", 
            "title": "Data Worth a Billion"
        }, 
        {
            "location": "/lego/pcache/sweep/#aggressiveness", 
            "text": "An aggressive sweep algorithm will disturb the normal operations a lot. In Lego, there 4 main factors that define the aggressiveness:   Time interval between each run  Number of sets to look at during each run  Skip if it is not full  Skip if it is under eviction    Number of lines to look at for each set  Smaller or equal to associativity    Number of lines to adjust for each set  Smaller or equal to lines to look at     \nYizhou Shan \nCreated: Mar 18, 2018 \nLast Updated: Mar 18, 2018", 
            "title": "Aggressiveness"
        }, 
        {
            "location": "/lego/pcache/tlb/", 
            "text": "TLB Coherence\n\n\nx86 does not keep TLB coherent across cores, nor with in-memory page table. And that is why we need explicit TLB flush when some PTE modifications happen (e.g. downgrade RW to RO, clear PTE, etc.). Besides, TLB flush is very important and affect application correctness. I\nve had some really awful debugging experience which was eventually introduced by missed TLB flush. Below is a list of operations that should have TLB flush followed:\n\n\n\n\nmunmap\n (optional, can be optimized by holding the old VA range)\n\n\nmremap\n (required)\n\n\nfork (RW-\nRO)\n (required)\n\n\nCoW (RO-\nRW)\n (required)\n\n\nmprotect\n (required)\n\n\nmigration\n (required)\n\n\n\n\nUnfortunately, TLB flush is costly, especially if we need to shootdown TLB entries on remote core. TLB shootdown\n1\n2\n3\n is performed by sending IPI to remote core, and remote core will flush local TLB entries within its handler. Linux optimize this by batching TLB flush until context switch happens. Lego currently does not have this nice feature, we flush TLB one by one for each PTE change (listed as \nTODO\n).\n\n\n\nYizhou Shan\n\nCreated: Mar 19, 2018\n\nLast Updated: Mar 19, 2018\n\n\n\n\n\n\n\n\n\n\nOptimizing the TLB Shootdown Algorithm with Page Access Tracking, ATC\n18\n\n\n\n\n\n\nLATR: Lazy Translation Coherence, ASPLOS\n18\n\n\n\n\n\n\nHardware Translation Coherence for Virtualized Systems, ISCA\n17", 
            "title": "TLB Coherence"
        }, 
        {
            "location": "/lego/pcache/tlb/#tlb-coherence", 
            "text": "x86 does not keep TLB coherent across cores, nor with in-memory page table. And that is why we need explicit TLB flush when some PTE modifications happen (e.g. downgrade RW to RO, clear PTE, etc.). Besides, TLB flush is very important and affect application correctness. I ve had some really awful debugging experience which was eventually introduced by missed TLB flush. Below is a list of operations that should have TLB flush followed:   munmap  (optional, can be optimized by holding the old VA range)  mremap  (required)  fork (RW- RO)  (required)  CoW (RO- RW)  (required)  mprotect  (required)  migration  (required)   Unfortunately, TLB flush is costly, especially if we need to shootdown TLB entries on remote core. TLB shootdown 1 2 3  is performed by sending IPI to remote core, and remote core will flush local TLB entries within its handler. Linux optimize this by batching TLB flush until context switch happens. Lego currently does not have this nice feature, we flush TLB one by one for each PTE change (listed as  TODO ).  \nYizhou Shan \nCreated: Mar 19, 2018 \nLast Updated: Mar 19, 2018      Optimizing the TLB Shootdown Algorithm with Page Access Tracking, ATC 18    LATR: Lazy Translation Coherence, ASPLOS 18    Hardware Translation Coherence for Virtualized Systems, ISCA 17", 
            "title": "TLB Coherence"
        }, 
        {
            "location": "/lego/pcache/pgtable-lock/", 
            "text": "Fine-grain Page Table Lock\n\n\nIn old Linux or previous Lego, user page table operations, such as set, clear, are protected by \nmm-\npage_table_lock\n. This one single lock prohibits a lot parallelisms on big SMP machines. An ideal solution is to have finer-granularity locks, so that faults on different parts of the user address space can be handled with less contention.\n\n\nBut finer-granularity locks means you need more memory for the locks. This is a simple trade-off. Lego currently mimic the Linux x86 default setting\n1\n, where each PMD \nand\n PTE page table pages has their own lock. The lock is a spinlock embedded in the \nstruct page\n. As illustrated below:\n\n\n\n\nBoth Processor and Memory managers are using the same mechanism to increase parallelism. And it is something that can improve performance \na lot\n.\n\n\n\nYizhou Shan\n\nCreated: Mar 22, 2018\n\nLast Updated: April 13, 2018\n\n\n\n\n\n\n\n\n\n\nSplit page table locks", 
            "title": "PageTable Lock"
        }, 
        {
            "location": "/lego/pcache/pgtable-lock/#fine-grain-page-table-lock", 
            "text": "In old Linux or previous Lego, user page table operations, such as set, clear, are protected by  mm- page_table_lock . This one single lock prohibits a lot parallelisms on big SMP machines. An ideal solution is to have finer-granularity locks, so that faults on different parts of the user address space can be handled with less contention.  But finer-granularity locks means you need more memory for the locks. This is a simple trade-off. Lego currently mimic the Linux x86 default setting 1 , where each PMD  and  PTE page table pages has their own lock. The lock is a spinlock embedded in the  struct page . As illustrated below:   Both Processor and Memory managers are using the same mechanism to increase parallelism. And it is something that can improve performance  a lot .  \nYizhou Shan \nCreated: Mar 22, 2018 \nLast Updated: April 13, 2018      Split page table locks", 
            "title": "Fine-grain Page Table Lock"
        }, 
        {
            "location": "/lego/pcache/victim/", 
            "text": "Victim Cache\n\n\n\nYizhou Shan\n\nCreated: Mar 12, 2018\n\nLast Updated: Mar 12, 2018", 
            "title": "Victim Cache"
        }, 
        {
            "location": "/lego/pcache/victim/#victim-cache", 
            "text": "Yizhou Shan \nCreated: Mar 12, 2018 \nLast Updated: Mar 12, 2018", 
            "title": "Victim Cache"
        }, 
        {
            "location": "/lego/pcache/virtual_cache/", 
            "text": "Virtual Cache\n\n\n\n\nSynonymous\n\n\nimpact Cache coherence\n\n\nimpact TLB shootdown\n\n\nThe good thing is, synonymous actually happen very rare in really workload. But when OS is invoked, it actually creates a lot synonymous. Because the physical page is mapped both low user virtual address and high kernel virtual address.\n\n\nWhat is this?\n\n\nWhat is bad about this?\n\n\nWhen does this happen? (All cases: kernel, shared vma mapping)\n\n\nHow to solve this?\n\n\nSoftware solution: OS level detection, global virtual address, identical virtual address etc.\n\n\nHardware solution: detect and manage at runtime. Back pointer, Dynamic synonymous remapping, reverse mapping. Similar ideas.\n\n\nA nice summary can be found on \nA new perspective for efficient virtual-cache coherence, ISCA\n13\n.\n\n\n\n\n\n\n\n\n\n\n\n\nLet me share my reading list. I think I\nve collected most of the important virtual cache papers:\n\n\n\n\n\n\n\nTLB and cache line lifetime:\n\n\n\n\nEnigma, ISC\n10\n: For each case where valid data exists in the cache hierarchy without a corresponding valid translation entry, systems with physically-tagged caches have to resolve the translation miss. Only after the page table has been \u201cwalked\u201d and a valid translation entry installed can the already cache-resident data be provided to the processing core. Especially in the faster levels of cache, the additional page table walk can add significant latency to what otherwise would have been a low-latency cache hit.\n\n\n\nGPU virtual cache, ASPLOS\n18\n: We notice that the per-CU TLB miss ratio is high; however, many TLB misses hit in the GPU caches. Only 34% of references that miss in the 32-entry per-CU L1 TLB are also L2 cache misses and access main memory (blue bars). An average of 31% of total per-CU TLB misses find the corresponding data in private L1 caches (black bars), and an additional 35% of the total misses hit in a shared L2 virtual cache (red bars). These hits occur because blocks in the cache hierarchy are likely to reside longer than the lifetime of the corresponding per-CU TLB entries\n\n\n\n\n\n\n\n\n\nReading the \nGPU virtual cache ASPLOS\n18\n paper today. I mostly interested in how they handle synonymous and mremap issue.\n\n\n\n\nSynonymous:\n\n\nTheir solution for synonymous is quite simple (not sure if practical or effective): use a \nleading\n virtual address, which is the first VA that has the virtual cache miss. Subsequent misses that from \ndifferent\n VA will not have the their cache lines filled, instead, they will make subsequent VA forever miss, and fetch the content from the leading VA cache line (they call it replay). In all, synonymous is solved by only having one cache line, and does not fill other VA cache lines.\n\n\nmremap:\n\n\nThey did not mention mremap. But I guess they do not need to care this. When remap happens, the original PTE is invalidated first, and TLB shootdown follows, all they need to do is to invalidate the virtual cache line (need to be flushed back to memory if dirty). When the new VA mapping established and accessed, it will be a normal virtual cache miss\n\n\nOVC also does not need to care about this because they are doing a similar way (I guess).\n\n\nLego need to handle mremap differently. Because we don\nt want to flush the dirty line back to memory, to save 1) one clflush, 2) another pcache miss. This means Lego wants to keep the content in Pcache. So the set_index of new VA and old VA matters in our case.\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 28, 2018\n\nLast Updated: Mar 29, 2018", 
            "title": "Virtual Cache"
        }, 
        {
            "location": "/lego/pcache/virtual_cache/#virtual-cache", 
            "text": "Synonymous  impact Cache coherence  impact TLB shootdown  The good thing is, synonymous actually happen very rare in really workload. But when OS is invoked, it actually creates a lot synonymous. Because the physical page is mapped both low user virtual address and high kernel virtual address.  What is this?  What is bad about this?  When does this happen? (All cases: kernel, shared vma mapping)  How to solve this?  Software solution: OS level detection, global virtual address, identical virtual address etc.  Hardware solution: detect and manage at runtime. Back pointer, Dynamic synonymous remapping, reverse mapping. Similar ideas.  A nice summary can be found on  A new perspective for efficient virtual-cache coherence, ISCA 13 .       Let me share my reading list. I think I ve collected most of the important virtual cache papers:    TLB and cache line lifetime:   Enigma, ISC 10 : For each case where valid data exists in the cache hierarchy without a corresponding valid translation entry, systems with physically-tagged caches have to resolve the translation miss. Only after the page table has been \u201cwalked\u201d and a valid translation entry installed can the already cache-resident data be provided to the processing core. Especially in the faster levels of cache, the additional page table walk can add significant latency to what otherwise would have been a low-latency cache hit.  GPU virtual cache, ASPLOS 18 : We notice that the per-CU TLB miss ratio is high; however, many TLB misses hit in the GPU caches. Only 34% of references that miss in the 32-entry per-CU L1 TLB are also L2 cache misses and access main memory (blue bars). An average of 31% of total per-CU TLB misses find the corresponding data in private L1 caches (black bars), and an additional 35% of the total misses hit in a shared L2 virtual cache (red bars). These hits occur because blocks in the cache hierarchy are likely to reside longer than the lifetime of the corresponding per-CU TLB entries     Reading the  GPU virtual cache ASPLOS 18  paper today. I mostly interested in how they handle synonymous and mremap issue.   Synonymous:  Their solution for synonymous is quite simple (not sure if practical or effective): use a  leading  virtual address, which is the first VA that has the virtual cache miss. Subsequent misses that from  different  VA will not have the their cache lines filled, instead, they will make subsequent VA forever miss, and fetch the content from the leading VA cache line (they call it replay). In all, synonymous is solved by only having one cache line, and does not fill other VA cache lines.  mremap:  They did not mention mremap. But I guess they do not need to care this. When remap happens, the original PTE is invalidated first, and TLB shootdown follows, all they need to do is to invalidate the virtual cache line (need to be flushed back to memory if dirty). When the new VA mapping established and accessed, it will be a normal virtual cache miss  OVC also does not need to care about this because they are doing a similar way (I guess).  Lego need to handle mremap differently. Because we don t want to flush the dirty line back to memory, to save 1) one clflush, 2) another pcache miss. This means Lego wants to keep the content in Pcache. So the set_index of new VA and old VA matters in our case.     \nYizhou Shan \nCreated: Mar 28, 2018 \nLast Updated: Mar 29, 2018", 
            "title": "Virtual Cache"
        }, 
        {
            "location": "/lego/pcache/rmap/", 
            "text": "Reverse Mapping of Pcache\n\n\nThis document explains Lego\ns reverse mapping design for pcache. We also present Lego internal functions that eventually manipulate rmap data structures.\nFor readers who are not familiar with reverse mapping, I recommend you search \nwhat is rmap in Linux\n first.\n\n\nDesign\n\n\nThe reverse mapping, or rmap, of our pcache is implemented in a very basic and\nstraightforward way: pointing back to all page table entries (ptes) directly.\nShared pcache lines will have a list of ptes that point to this pcache line.\nWe also did this way in Hotpot.\n\n\nrmap is used by \n1)\n a bunch of syscalls, such as \nfork()\n, \nexecv()\n, \nmmap()\n,\n\nmunmap()\n, \nmremap()\n, \nbrk()\n. \n2)\n page reclaim, which needs to unmap all ptes for a\ngiven swapped page. Other than \nfork()\n and \nexecv()\n, other vm related syscalls\nare invoked very frequently for a typical datacenter application. Moreover, page\nreclaim and swap also run concurrently to gain exclusive access to rmap.\n\n\nSo, rmap operations have to be fast. Directly pointing to pte seems the best\nsolution here. However, this fine-granularity design will consume a lot memory\nfor the per-pte list.\nFurthermore, vma creation, deletion, split and merge happen frequently, the overhead\nto manage rmap is quite high. No wonder Linux choses another object-based way to do so,\nwhich leverages vma itself to take a longer path towards pte.\n\n\nThe important question is: \ndoes this naive solution fit \ncurrent\n Lego?\n\n\nYes, it fits, for several reasons. \n1)\n Current Lego run static-linked ELF binary only,\nthus there will not be any shared hot library pages, which implies rmap list maintenance\nis simplified. \n2)\n Our targeted applications\nmostly are single process. Even for multiple process ones, the number of processes\nstay stable and \nfork()\n happen at early init time. \n3)\n major users of rmap such\nas \nmremap()\n and \nmunmap()\n  perform rmap operation explicitly, \nmmap()\n perform\nrmap implicitly via pgfault (or pcache miss), \npcache reclaim\n perform sweep async.\nAll of them, combined with 1) and 2), most of the time will perform rmap operation\non a single pte.\n\n\nInternal\n\n\nThe following table describes different contexts that manipulate rmap data structures. Currently, rmap only has four possible operations. The context field describes the large context that trigger such rmap operation. The related functions and pcache callback field lists functions that actually did the dirty work.\n\n\n\n\n\n\n\n\nrmap operation\n\n\nContext\n\n\nRelated functions and pcache callback\n\n\n\n\n\n\n\n\n\n\nAdd\n\n\nfork()\n \npgfault\n\n\ncopy_pte_range()\n -\n \npcache_copy_pte()\n \n \npcache_add_rmap()\n\n\n\n\n\n\nRemove\n\n\nmunmap()\n \n \nexit_mmap()\n \n \nwrite_protected\n\n\nzap_pte_range()\n -\n \npcache_zap_pte()\n \n \npcache_do_wp\n -\n \npcache_remove_rmap\n\n\n\n\n\n\nUpdate\n\n\nmremap()\n\n\nmove_ptes()\n -\n \npcache_move_pte()\n\n\n\n\n\n\nLookup\n\n\npcache eviction sweep, etc.\n\n\npcache_referenced()\n, \npcache_wrprotect()\n \n \npcache_try_to_unmap()\n\n\n\n\n\n\n\n\nEach rmap holds one refcount of pcache. The refcount is increased after \npcache_add_rmap\n, and must be decreased after removing pcache rmap, can from \npcache_remove_rmap\n, \npcache_zap_pte\n or \npcache_move_pte_slowpath\n.\n\n\nThought\n\n\nOne function I personally love the most is \nrmap_walk()\n, whose name pretty much tells the story. To use \nrmap_walk()\n, caller passes a \nstruct rmap_walk_control\n, which including caller specific callback for each rmap. This function also isolates the specific data structures used by rmap from various callers. In Lego, a lot pcache functions are built upon \nrmap_walk()\n.\n\n\nstruct rmap_walk_control\n, or \nstruct scan_control\n, or \nstruct something_control\n are used a lot by Linux kernel. Personally I do love this way of doing data structure walk, or reuse functions. However, even this way can greatly reduce duplicated code size, it will make the code unnecessary complex. As a system developer, no more expects to see a function longer than 100 lines. People love saying: \nDo one thing and do it better\n, while it not always works that perfectly. Coding is nothing different life, it is all about trade-off.\n\n\n\nYizhou Shan \n\nCreated: Feb 02, 2018\n\nLast Updated: Mar 10, 2018", 
            "title": "Reverse Mapping"
        }, 
        {
            "location": "/lego/pcache/rmap/#reverse-mapping-of-pcache", 
            "text": "This document explains Lego s reverse mapping design for pcache. We also present Lego internal functions that eventually manipulate rmap data structures.\nFor readers who are not familiar with reverse mapping, I recommend you search  what is rmap in Linux  first.", 
            "title": "Reverse Mapping of Pcache"
        }, 
        {
            "location": "/lego/pcache/rmap/#design", 
            "text": "The reverse mapping, or rmap, of our pcache is implemented in a very basic and\nstraightforward way: pointing back to all page table entries (ptes) directly.\nShared pcache lines will have a list of ptes that point to this pcache line.\nWe also did this way in Hotpot.  rmap is used by  1)  a bunch of syscalls, such as  fork() ,  execv() ,  mmap() , munmap() ,  mremap() ,  brk() .  2)  page reclaim, which needs to unmap all ptes for a\ngiven swapped page. Other than  fork()  and  execv() , other vm related syscalls\nare invoked very frequently for a typical datacenter application. Moreover, page\nreclaim and swap also run concurrently to gain exclusive access to rmap.  So, rmap operations have to be fast. Directly pointing to pte seems the best\nsolution here. However, this fine-granularity design will consume a lot memory\nfor the per-pte list.\nFurthermore, vma creation, deletion, split and merge happen frequently, the overhead\nto manage rmap is quite high. No wonder Linux choses another object-based way to do so,\nwhich leverages vma itself to take a longer path towards pte.  The important question is:  does this naive solution fit  current  Lego?  Yes, it fits, for several reasons.  1)  Current Lego run static-linked ELF binary only,\nthus there will not be any shared hot library pages, which implies rmap list maintenance\nis simplified.  2)  Our targeted applications\nmostly are single process. Even for multiple process ones, the number of processes\nstay stable and  fork()  happen at early init time.  3)  major users of rmap such\nas  mremap()  and  munmap()   perform rmap operation explicitly,  mmap()  perform\nrmap implicitly via pgfault (or pcache miss),  pcache reclaim  perform sweep async.\nAll of them, combined with 1) and 2), most of the time will perform rmap operation\non a single pte.", 
            "title": "Design"
        }, 
        {
            "location": "/lego/pcache/rmap/#internal", 
            "text": "The following table describes different contexts that manipulate rmap data structures. Currently, rmap only has four possible operations. The context field describes the large context that trigger such rmap operation. The related functions and pcache callback field lists functions that actually did the dirty work.     rmap operation  Context  Related functions and pcache callback      Add  fork()   pgfault  copy_pte_range()  -   pcache_copy_pte()     pcache_add_rmap()    Remove  munmap()     exit_mmap()     write_protected  zap_pte_range()  -   pcache_zap_pte()     pcache_do_wp  -   pcache_remove_rmap    Update  mremap()  move_ptes()  -   pcache_move_pte()    Lookup  pcache eviction sweep, etc.  pcache_referenced() ,  pcache_wrprotect()     pcache_try_to_unmap()     Each rmap holds one refcount of pcache. The refcount is increased after  pcache_add_rmap , and must be decreased after removing pcache rmap, can from  pcache_remove_rmap ,  pcache_zap_pte  or  pcache_move_pte_slowpath .", 
            "title": "Internal"
        }, 
        {
            "location": "/lego/pcache/rmap/#thought", 
            "text": "One function I personally love the most is  rmap_walk() , whose name pretty much tells the story. To use  rmap_walk() , caller passes a  struct rmap_walk_control , which including caller specific callback for each rmap. This function also isolates the specific data structures used by rmap from various callers. In Lego, a lot pcache functions are built upon  rmap_walk() .  struct rmap_walk_control , or  struct scan_control , or  struct something_control  are used a lot by Linux kernel. Personally I do love this way of doing data structure walk, or reuse functions. However, even this way can greatly reduce duplicated code size, it will make the code unnecessary complex. As a system developer, no more expects to see a function longer than 100 lines. People love saying:  Do one thing and do it better , while it not always works that perfectly. Coding is nothing different life, it is all about trade-off.  \nYizhou Shan  \nCreated: Feb 02, 2018 \nLast Updated: Mar 10, 2018", 
            "title": "Thought"
        }, 
        {
            "location": "/lego/pcache/evict_and_ref/", 
            "text": "Mumble pcache eviction and refcount\n\n\nThis is about how Lego is doing eviction against live references of pcache. Unlike the \ngarbage collection\n where it only reclaims object that has no references, pcache eviction may try to evict a pcache that is currently being used by another thread. Both parties need to be very careful. A tricky business.\n\n\nTo describe the issue in a high-level, let us consider this case: the system now has two threads running on two different cores. The first thread try to evict a pcache line, and it truly find a candidate and prepare to evict. Meanwhile, the other thread is currently using this pcache line to do some operations such as \nzap_rmap()\n. If the first thread evict the pcache line without synchronization with the second thread, oops, the second thread is playing with a wrong pcache.\n\n\nThe textbook idea is adding refcount. However, this is not enough in C. Because:\n\n\n\n\nThere is no way to prevent the second thread from getting the \npointer\n to that pcm.\n\n\nA simple \ninc_refcount()\n from the second thread can happen anytime in the middle of first thread\ns eviction.\n\n\n\n\nSolutions:\n\n\n\n\nTo actually prevent the second thread from getting the pointer, we should think about \nhow\n it get the pointer? Luckily, in Lego, there is only one entry point, which is from \npte to pcm\n (aka. pcache_meta). So to synchronize pte change becomes very important. Luckily, we are doing pte_lock before getting the pcm. So this simple pte lock ensures the second thread a safe, will-not-be-evicted pcm (of course, with some other checkings). This idea can also be generalized to any data structures that need pointer references: \nprotect your pointer\n!\n\n\nRefcount checking is also necessary. In the eviction routine, we need to use  \natomic_xchg\n to reset the refcount. If this fails, it means someone else is using it. Do note, this \natomic_xchg\n is carried out with pcm locked. Thus the ordering of locking, get/put matters in the code.\n\n\n\n\nThe code itself tells a much more complete story, I strongly recommend you read the code if you are interested. Here I will list the most interesting part. For the other users except eviction, they need to do this:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\npcm\n \n=\n \npte_to_pcache_meta\n(\nptent\n);\n\n\n/*   \n\n\n * We have a strict lock ordering everyone should obey:\n\n\n *      lock pcache\n\n\n *      lock pte\n\n\n * The caller already locked pte, thus we should avoid deadlock here\n\n\n * by droping pte lock first and then acquire both of them in order.\n\n\n */\n\n\nif\n \n(\nunlikely\n(\n!\ntrylock_pcache\n(\npcm\n)))\n \n{\n\n    \n/* in case it got evicted and @pcm becomes invalid */\n\n    \nget_pcache\n(\npcm\n);\n\n\n    \n/*\n\n\n     * Once we release the pte lock, this pcm may be\n\n\n     * unmapped by another thread who is doing eviction.\n\n\n     * Since we have grabbed one extra ref above, so even\n\n\n     * it is unmapped, eviction thread will not fail to free it.\n\n\n     */\n\n    \nspin_unlock\n(\nptl\n);\n\n\n    \nlock_pcache\n(\npcm\n);\n\n    \nspin_lock\n(\nptl\n);\n\n\n    \n/*   \n\n\n     * Since we dropped the lock, the pcache line might\n\n\n     * be got evicted in the middle.\n\n\n     */\n\n\n    \nif\n \n(\n!\npte_same\n(\n*\npte\n,\n \nptent\n))\n \n{\n\n\n        \nunlock_pcache\n(\npcm\n);\n\n        \n/*   \n\n\n         * This put maybe decreases the ref to 0\n\n\n         * and eventually free the pcache line.\n\n\n         * This happens if the @pcm was selected\n\n\n         * to be evicted at the same time.\n\n\n         */\n\n        \nput_pcache\n(\npcm\n);\n\n        \nreturn\n \n-\nEAGAIN\n;\n\n    \n}\n    \n    \nput_pcache\n(\npcm\n);\n\n\n}\n\n\n\n\n\n\nAs for the eviction thread, it needs to make sure it is the last user using this pcm:\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n/*  \n\n\n * Each rmap counts one refcount, plus the one grabbed\n\n\n * during evict_find_line(), we should have (nr_mapped + 1)\n\n\n * here if there are no any other users.\n\n\n *\n\n\n * Furthurmore, others can not go from munmap/mremap/wp to\n\n\n * put_pcache() within pcache_zap_pte(), pcache_move_pte()\n\n\n * or pcache_do_wp_page(). Thus the refcount must larger or\n\n\n * equal to (nr_mapped + 1).\n\n\n *\n\n\n * But if there truly other users (refcount \n nr_mapped + 1),\n\n\n * then we should manually sub the refcount. The other users\n\n\n * which are currently holding the ref, will free the pcache\n\n\n * once it call put_pcache.\n\n\n */\n\n\nPCACHE_BUG_ON_PCM\n(\npcache_ref_count\n(\npcm\n)\n \n \nnr_mapped\n \n+\n \n1\n,\n \npcm\n);\n\n\nif\n \n(\nunlikely\n(\n!\npcache_ref_freeze\n(\npcm\n,\n \nnr_mapped\n \n+\n \n1\n)))\n \n{\n\n    \nif\n \n(\nunlikely\n(\npcache_ref_sub_and_test\n(\npcm\n,\n \nnr_mapped\n \n+\n \n1\n)))\n \n{\n\n        \npr_info\n(\nBUG: pcm refcount, nr_mapped: %d\n\\n\n,\n \nnr_mapped\n);\n\n        \ndump_pcache_meta\n(\npcm\n,\n \nref error\n);\n\n        \nBUG\n();\n\n    \n}\n   \n\n    \nClearPcacheReclaim\n(\npcm\n);\n\n    \nadd_to_lru_list\n(\npcm\n,\n \npset\n);\n\n    \nunlock_pcache\n(\npcm\n);\n\n\n    \ninc_pcache_event\n(\nPCACHE_EVICTION_EAGAIN_CONCURRENT\n);\n\n    \nreturn\n \nPCACHE_EVICT_EAGAIN_CONCURRENT\n;\n\n\n}\n\n\n\n\n\n\n\n\nMy personal thought: live eviction against live objects/references is very hard. You first need to use refcount to ensure a correct ordering. You also need to have a way to prevent others from using the going-to-be-evicted pointer, or have a way to detect a under-use pointer.  In this Lego pcache case, we use the combination of pte lock, pcache lock, and pcache refcount, to ensure everyone is safe. And all these is quite similar to Linux page operations. I learned a lot from its code. But I still not fully understand how it ensures the page is not used by others, it has way more parties than lego that can use the page at the same time of eviction. Magic kernel folks.\n\n\n\nYizhou Shan \n\nCreated: Mar 15, 2018\n\nLast Updated: Mar 16, 2018", 
            "title": "Evict and Refcount"
        }, 
        {
            "location": "/lego/pcache/evict_and_ref/#mumble-pcache-eviction-and-refcount", 
            "text": "This is about how Lego is doing eviction against live references of pcache. Unlike the  garbage collection  where it only reclaims object that has no references, pcache eviction may try to evict a pcache that is currently being used by another thread. Both parties need to be very careful. A tricky business.  To describe the issue in a high-level, let us consider this case: the system now has two threads running on two different cores. The first thread try to evict a pcache line, and it truly find a candidate and prepare to evict. Meanwhile, the other thread is currently using this pcache line to do some operations such as  zap_rmap() . If the first thread evict the pcache line without synchronization with the second thread, oops, the second thread is playing with a wrong pcache.  The textbook idea is adding refcount. However, this is not enough in C. Because:   There is no way to prevent the second thread from getting the  pointer  to that pcm.  A simple  inc_refcount()  from the second thread can happen anytime in the middle of first thread s eviction.   Solutions:   To actually prevent the second thread from getting the pointer, we should think about  how  it get the pointer? Luckily, in Lego, there is only one entry point, which is from  pte to pcm  (aka. pcache_meta). So to synchronize pte change becomes very important. Luckily, we are doing pte_lock before getting the pcm. So this simple pte lock ensures the second thread a safe, will-not-be-evicted pcm (of course, with some other checkings). This idea can also be generalized to any data structures that need pointer references:  protect your pointer !  Refcount checking is also necessary. In the eviction routine, we need to use   atomic_xchg  to reset the refcount. If this fails, it means someone else is using it. Do note, this  atomic_xchg  is carried out with pcm locked. Thus the ordering of locking, get/put matters in the code.   The code itself tells a much more complete story, I strongly recommend you read the code if you are interested. Here I will list the most interesting part. For the other users except eviction, they need to do this:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 pcm   =   pte_to_pcache_meta ( ptent );  /*      * We have a strict lock ordering everyone should obey:   *      lock pcache   *      lock pte   * The caller already locked pte, thus we should avoid deadlock here   * by droping pte lock first and then acquire both of them in order.   */  if   ( unlikely ( ! trylock_pcache ( pcm )))   { \n     /* in case it got evicted and @pcm becomes invalid */ \n     get_pcache ( pcm ); \n\n     /*       * Once we release the pte lock, this pcm may be       * unmapped by another thread who is doing eviction.       * Since we have grabbed one extra ref above, so even       * it is unmapped, eviction thread will not fail to free it.       */ \n     spin_unlock ( ptl ); \n\n     lock_pcache ( pcm ); \n     spin_lock ( ptl ); \n\n     /*          * Since we dropped the lock, the pcache line might       * be got evicted in the middle.       */       if   ( ! pte_same ( * pte ,   ptent ))   {           unlock_pcache ( pcm ); \n         /*              * This put maybe decreases the ref to 0           * and eventually free the pcache line.           * This happens if the @pcm was selected           * to be evicted at the same time.           */ \n         put_pcache ( pcm ); \n         return   - EAGAIN ; \n     }     \n     put_pcache ( pcm );  }    As for the eviction thread, it needs to make sure it is the last user using this pcm:  1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 /*     * Each rmap counts one refcount, plus the one grabbed   * during evict_find_line(), we should have (nr_mapped + 1)   * here if there are no any other users.   *   * Furthurmore, others can not go from munmap/mremap/wp to   * put_pcache() within pcache_zap_pte(), pcache_move_pte()   * or pcache_do_wp_page(). Thus the refcount must larger or   * equal to (nr_mapped + 1).   *   * But if there truly other users (refcount   nr_mapped + 1),   * then we should manually sub the refcount. The other users   * which are currently holding the ref, will free the pcache   * once it call put_pcache.   */  PCACHE_BUG_ON_PCM ( pcache_ref_count ( pcm )     nr_mapped   +   1 ,   pcm );  if   ( unlikely ( ! pcache_ref_freeze ( pcm ,   nr_mapped   +   1 )))   { \n     if   ( unlikely ( pcache_ref_sub_and_test ( pcm ,   nr_mapped   +   1 )))   { \n         pr_info ( BUG: pcm refcount, nr_mapped: %d \\n ,   nr_mapped ); \n         dump_pcache_meta ( pcm ,   ref error ); \n         BUG (); \n     }    \n\n     ClearPcacheReclaim ( pcm ); \n     add_to_lru_list ( pcm ,   pset ); \n     unlock_pcache ( pcm ); \n\n     inc_pcache_event ( PCACHE_EVICTION_EAGAIN_CONCURRENT ); \n     return   PCACHE_EVICT_EAGAIN_CONCURRENT ;  }     My personal thought: live eviction against live objects/references is very hard. You first need to use refcount to ensure a correct ordering. You also need to have a way to prevent others from using the going-to-be-evicted pointer, or have a way to detect a under-use pointer.  In this Lego pcache case, we use the combination of pte lock, pcache lock, and pcache refcount, to ensure everyone is safe. And all these is quite similar to Linux page operations. I learned a lot from its code. But I still not fully understand how it ensures the page is not used by others, it has way more parties than lego that can use the page at the same time of eviction. Magic kernel folks.  \nYizhou Shan  \nCreated: Mar 15, 2018 \nLast Updated: Mar 16, 2018", 
            "title": "Mumble pcache eviction and refcount"
        }, 
        {
            "location": "/lego/pcache/smp_design/", 
            "text": "SMP Design Thought\n\n\nCoding pcache is nothing different from coding mm code. It is the same with your familiar mixed pgfault, LRU, page cache and writeback code. Each pcache line can be involved with multiple activities at the same time. We have to use different states to synchronize among them. If you have ever read linux mm code, you will know that sometimes, comment is literally more than code. SMP pain in ass.\n\n\nI don\nt think this document is well written. It is just some random thoughts I wrote down while coding. Some of them might be wrong. But it is still worth looking back.\n\n\nPcache and Victim Cache Organization\n\n\nOur pcache and victim cache are allocated and arranged as a big array. As for\npcache we look at it in a \ncache set view\n, which means consecutive pcache lines\nare not relevant in natual. As for victim cache, we simply treat it as a big array\nand walk through it one by one.\n\n\nAllocation/Eviction SMP Consideration\n\n\nThe alloc/free of both pcache and victim cache are simple: each pcache line or\nvictim cache line has a \nAllocated\n bit to indicate if this line is free or not.\nThe \nAllocated\n bit is manipulated by atomic bit operations, thus SMP safe. This\nfurther implies that we do not need another spinlock to guard allocation.\n\n\nHowever, other activities such as explict eviction, background sweep may walk\nthrough the cache lines at the same time of cache allocation, a single \nAllocated\n\nbit is not enough. Because an allocated cache line will need some initial setup,\nsuch as reset refcount, clear flags (prep_new_pcache),\nthus there is a small time gap between Allocated bit being set and the cache line\nbeing truly safe to use. Other activities must wait the cache line to be usable,\nand then they can do further operations on this cache line.\n\n\nTo solve this race condition, there two possible solutions:\n1) Add another bit: \nUsable\n, which is set once initial setup is done.\n   In this case, functions excluding alloction code should always check if the \nUsable\n\n   bit is set or not. a) If it is set, this means the cache line is safe for further operations\n   b) If not, and \nAllocated\n bit is set, this means the cache line is under setup in another core,\n   We should skip it.\n   c) If not, and \nAllocated\n bit is not set, this means this cache line is simply free.\n   We should skip it.\n\n\n2) Add allocated cache lines to a list (such as LRU list), and functions excluding allocation\n   code will only look into cache lines within this list. In other words, others will only\n   look into surely usable cache lines.\n\n\nBoth solutions try to avoid others looking into \nun-mature\n cache lines in SMP envorinment.\nThe rule is simple: function should \nNOT\n look into data that is not supposed to be seen.\nThe cache line that has Allocated bit set but under setup is a typical case.\n\n\nAs an example, the physical page allocator, page reclaim, page cache in Linux are implemented with\nthe second solution. Pages freshly allocated will be added a LRU list or page cache own list.\nAnd page reclaim code will only look into pages within the LRU list, it will not go through all\nphysical pages to do so. The reason for Linux to do so is simple: kernel can not scan the whole\nphysical pages to find out pages to operate.\n\n\nPcache:\n When it comes to pcache, we use both.\nIn our envision, pcache will have high-associativity such as 64 or 128.\nIt will have very bad performance if our eviction algorithm or sweep thread need to go through every\ncache lines within a set to find out candidates, while there might be only 1 or 2 allocated lines.\nHowever, additional \nUsable\n bit is added for debug purpose.\n\n\nVictim Cache:\n When it comes to victim cache, the first solution seems a better choice.\nBecause victim cache only a few cache lines, e.g., 8 or 16. This means a whole victim cache line\nwalk is fast. While the list deletion and addition seem may introduce some unnecessary overhead.\nIt is all about trade-off.\n\n\nThese choices affect the usage of pcache and victim cache, mostly the eviction code.\n\n\nMore on above two solutions\n\n\nThe first solution is used if evict_random is configured. The second solution is used when\nevict_lru is configured.\n\n\nI do not have any doubt about second solution, it works, though with a lot SMP pain in ass.\nBut I do have more to say about the first solution, which is adding another usable bit.\nThe \nUsable\n bit \nonly\n ensures other threads will not use unmature pcache, but it can not\nprevent other threads seeing a going-to-be-freed pcache.\n\n\nWhat is this going-to-be-freed asshole? Let us consider this case: CPU0 is doing eviction\nand checked the \nUsable\n bit, which is set. Then CPU0 thought this cache line is all set,\nready to be torqued. Before doing all the dirty work, CPU0 will \nget_pcache_unless_zero()\n\nfirst to make sure the pcache will not go away in the middle. However, meanwhile, CPU1 did\na \nput_pcache()\n \nand\n a consecutive \npcache_alloc()\n right before CPU0 did called\n\nget_pcache_unless_zero()\n. Bang! CPU0 may use an mature pcache line, cause CPU1\ns \npcache_init_ref_count()\n\nmay come before CPU1\ns \nget_pcache_unless_zero()\n! How to solve this? CPU0 need to add\nadditional checking after \nget_pcache_unless_zero()\n.\n\n\nFor more details, please check the code in \npcache/evcit_random.c\n, which has more pretty explanation.\n\n\n\nYizhou Shan\n\nJan 31, 2018", 
            "title": "SMP Design"
        }, 
        {
            "location": "/lego/pcache/smp_design/#smp-design-thought", 
            "text": "Coding pcache is nothing different from coding mm code. It is the same with your familiar mixed pgfault, LRU, page cache and writeback code. Each pcache line can be involved with multiple activities at the same time. We have to use different states to synchronize among them. If you have ever read linux mm code, you will know that sometimes, comment is literally more than code. SMP pain in ass.  I don t think this document is well written. It is just some random thoughts I wrote down while coding. Some of them might be wrong. But it is still worth looking back.", 
            "title": "SMP Design Thought"
        }, 
        {
            "location": "/lego/pcache/smp_design/#pcache-and-victim-cache-organization", 
            "text": "Our pcache and victim cache are allocated and arranged as a big array. As for\npcache we look at it in a  cache set view , which means consecutive pcache lines\nare not relevant in natual. As for victim cache, we simply treat it as a big array\nand walk through it one by one.", 
            "title": "Pcache and Victim Cache Organization"
        }, 
        {
            "location": "/lego/pcache/smp_design/#allocationeviction-smp-consideration", 
            "text": "The alloc/free of both pcache and victim cache are simple: each pcache line or\nvictim cache line has a  Allocated  bit to indicate if this line is free or not.\nThe  Allocated  bit is manipulated by atomic bit operations, thus SMP safe. This\nfurther implies that we do not need another spinlock to guard allocation.  However, other activities such as explict eviction, background sweep may walk\nthrough the cache lines at the same time of cache allocation, a single  Allocated \nbit is not enough. Because an allocated cache line will need some initial setup,\nsuch as reset refcount, clear flags (prep_new_pcache),\nthus there is a small time gap between Allocated bit being set and the cache line\nbeing truly safe to use. Other activities must wait the cache line to be usable,\nand then they can do further operations on this cache line.  To solve this race condition, there two possible solutions:\n1) Add another bit:  Usable , which is set once initial setup is done.\n   In this case, functions excluding alloction code should always check if the  Usable \n   bit is set or not. a) If it is set, this means the cache line is safe for further operations\n   b) If not, and  Allocated  bit is set, this means the cache line is under setup in another core,\n   We should skip it.\n   c) If not, and  Allocated  bit is not set, this means this cache line is simply free.\n   We should skip it.  2) Add allocated cache lines to a list (such as LRU list), and functions excluding allocation\n   code will only look into cache lines within this list. In other words, others will only\n   look into surely usable cache lines.  Both solutions try to avoid others looking into  un-mature  cache lines in SMP envorinment.\nThe rule is simple: function should  NOT  look into data that is not supposed to be seen.\nThe cache line that has Allocated bit set but under setup is a typical case.  As an example, the physical page allocator, page reclaim, page cache in Linux are implemented with\nthe second solution. Pages freshly allocated will be added a LRU list or page cache own list.\nAnd page reclaim code will only look into pages within the LRU list, it will not go through all\nphysical pages to do so. The reason for Linux to do so is simple: kernel can not scan the whole\nphysical pages to find out pages to operate.  Pcache:  When it comes to pcache, we use both.\nIn our envision, pcache will have high-associativity such as 64 or 128.\nIt will have very bad performance if our eviction algorithm or sweep thread need to go through every\ncache lines within a set to find out candidates, while there might be only 1 or 2 allocated lines.\nHowever, additional  Usable  bit is added for debug purpose.  Victim Cache:  When it comes to victim cache, the first solution seems a better choice.\nBecause victim cache only a few cache lines, e.g., 8 or 16. This means a whole victim cache line\nwalk is fast. While the list deletion and addition seem may introduce some unnecessary overhead.\nIt is all about trade-off.  These choices affect the usage of pcache and victim cache, mostly the eviction code.", 
            "title": "Allocation/Eviction SMP Consideration"
        }, 
        {
            "location": "/lego/pcache/smp_design/#more-on-above-two-solutions", 
            "text": "The first solution is used if evict_random is configured. The second solution is used when\nevict_lru is configured.  I do not have any doubt about second solution, it works, though with a lot SMP pain in ass.\nBut I do have more to say about the first solution, which is adding another usable bit.\nThe  Usable  bit  only  ensures other threads will not use unmature pcache, but it can not\nprevent other threads seeing a going-to-be-freed pcache.  What is this going-to-be-freed asshole? Let us consider this case: CPU0 is doing eviction\nand checked the  Usable  bit, which is set. Then CPU0 thought this cache line is all set,\nready to be torqued. Before doing all the dirty work, CPU0 will  get_pcache_unless_zero() \nfirst to make sure the pcache will not go away in the middle. However, meanwhile, CPU1 did\na  put_pcache()   and  a consecutive  pcache_alloc()  right before CPU0 did called get_pcache_unless_zero() . Bang! CPU0 may use an mature pcache line, cause CPU1 s  pcache_init_ref_count() \nmay come before CPU1 s  get_pcache_unless_zero() ! How to solve this? CPU0 need to add\nadditional checking after  get_pcache_unless_zero() .  For more details, please check the code in  pcache/evcit_random.c , which has more pretty explanation.  \nYizhou Shan \nJan 31, 2018", 
            "title": "More on above two solutions"
        }, 
        {
            "location": "/lego/pcache/replication/", 
            "text": "Memory Replication\n\n\n\n\n\n\nKeep a single copy of each page in DRAM, with redundant copies on secondary storage such as disk or flash. This makes replication nearly free in terms of cost, and energy usage. But we should consider the extra network cost.\n\n\n\n\n\n\nRAMCloud has two components running on a single machine: \nmaster\n, and \nbackup\n. In lego, \nmaster\n is the handler running on \nMemory\n, \nbackup\n is the handler running on \nStorage\n.\n\n\n\n\n\n\nBecause of \ndual-Memory solution\n, we don\nt need a hash table from \npid, user_vaddr\n to objects in log: M1 has its own \nVA-PA\n mapping table, and it will not be updated on replication. M2 does not need to look up.\n\n\n\n\n\n\nRAMCloud use 8MB segment. Logs are first appended within each segment. Each log has different size, depends on the objects being written. Lego is different. Replication is triggered by pcache/victim flush, which means the data is always the size of a pcache line (4KB now). This make things somehow simpler. But other general rules still apply.\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 31, 2018\n\nLast Updated: Mar 31, 2018", 
            "title": "Replication"
        }, 
        {
            "location": "/lego/pcache/replication/#memory-replication", 
            "text": "Keep a single copy of each page in DRAM, with redundant copies on secondary storage such as disk or flash. This makes replication nearly free in terms of cost, and energy usage. But we should consider the extra network cost.    RAMCloud has two components running on a single machine:  master , and  backup . In lego,  master  is the handler running on  Memory ,  backup  is the handler running on  Storage .    Because of  dual-Memory solution , we don t need a hash table from  pid, user_vaddr  to objects in log: M1 has its own  VA-PA  mapping table, and it will not be updated on replication. M2 does not need to look up.    RAMCloud use 8MB segment. Logs are first appended within each segment. Each log has different size, depends on the objects being written. Lego is different. Replication is triggered by pcache/victim flush, which means the data is always the size of a pcache line (4KB now). This make things somehow simpler. But other general rules still apply.    \nYizhou Shan \nCreated: Mar 31, 2018 \nLast Updated: Mar 31, 2018", 
            "title": "Memory Replication"
        }, 
        {
            "location": "/lego/driver/pci/", 
            "text": "PCI Subsystem\n\n\nWhat we have ported so far\n\n\n\n\nPCI data structures such as \npci_dev\n, \npci_bus\n, and so on.\n\n\nMechanism to scan bus and build data structures during boot. Performed by \npci_scan_root_bus()\n, and most code is in \ndriver/pci/probe.c\n\n\n\n\nUnfinished business\n\n\n\n\nWays to go through all PCI device.\n\n\npci_init_capabilities()\n: for each PCI device\n\n\npci_fixup_device()\n: a lot quicks, maybe not useful\n\n\npcie_aspm_init_link_state()\n: PCIe link state\n\n\npci_iov_bus_range\n: all SR-IOV support\n\n\n\n\n\nYizhou Shan\n\nCreated: July 5, 2018\n\nLast Updated: July 5, 2018", 
            "title": "PCI"
        }, 
        {
            "location": "/lego/driver/pci/#pci-subsystem", 
            "text": "", 
            "title": "PCI Subsystem"
        }, 
        {
            "location": "/lego/driver/pci/#what-we-have-ported-so-far", 
            "text": "PCI data structures such as  pci_dev ,  pci_bus , and so on.  Mechanism to scan bus and build data structures during boot. Performed by  pci_scan_root_bus() , and most code is in  driver/pci/probe.c", 
            "title": "What we have ported so far"
        }, 
        {
            "location": "/lego/driver/pci/#unfinished-business", 
            "text": "Ways to go through all PCI device.  pci_init_capabilities() : for each PCI device  pci_fixup_device() : a lot quicks, maybe not useful  pcie_aspm_init_link_state() : PCIe link state  pci_iov_bus_range : all SR-IOV support   \nYizhou Shan \nCreated: July 5, 2018 \nLast Updated: July 5, 2018", 
            "title": "Unfinished business"
        }, 
        {
            "location": "/lego/driver/ib/", 
            "text": "Infiniband Subsystem\n\n\nCurrent Status\n\n\nLego\ns IB stack is ported based on \nlinux-3.11.1\n. We ported:\n\n\n\n\nib_core\n\n\nmlx4_ib\n\n\nmlx4_core\n\n\n\n\nLego does not support uverbs. At the time of writing, Lego IB stack has only been tested on \nMellanox Technologies MT27500 Family [ConnectX-3]\n.\n\n\nRandom summary\n\n\nThe stack is SUPER complex, a lot data structures and pointers fly all over. Good thing is the whole stack is layered clearly.\n\n\nTop down\n\n\nib_core\n\n\n\n\nIB core code is in \ndriver/infiniband/core\n, which exposes the major IB API to both user and kernel applications. Inside, it has two parts. The first part is function callback, that call back to underlying device-specific functions. The second part is the management stack, including communication manager (cm), management datagram (mad), and so on.\n\n\nIn IB, each port\ns QP0 and QP1 are reserved for management purpose. They will receive/send MAD from/to subnet manager, who typically runs on switch. All the IB management stuff is carried out by exchanging MAD.\n\n\nThere are several key data structures: ib_client, ib_device, and mad_agent. MAD, CM, and some others are ib_client, which means they use IB device, and will be called back whenever a device has been added. mad_agent is something that will be called back whenever a device received a MAD message from switch (see \nib_mad_completion_handler()\n). A lot layers, huh?\n\n\nib_mad_completion_handler()\n: we changed the behavior of it. we use busy polling instead of interrupt. Originally, it will be invoked by mlx4_core/eq.c\n\n\n\n\nmlx4_ib and mlx4_core\n\n\n\n\n\n\nmlx4_core is actually the Ethernet driver for Mellanox NIC device (drivers/net/ethernet/mellanox/hw/mlx4), which do the actual dirty work of talking with device. On the other hand, mlx4_ib is the glue code between ib_core and mlx4_core, who do the translation.\n\n\n\n\n\n\nA lot IB verbs are ultimately translated into \nfw.c __mlx4_cmd()\n, which actually send commands to device and get the result. There are two ways of getting result: 1) polling: after writing to device memory the command, the same thread keep polling. 2) sleep and wait for interrupt. By default, the interrupt way is used (obviously). But, at the time of writing (Aug 20, 2018), we don\nt really have a working IRQ subsystem, so we use polling instead. \nI\nm still a little concerned that without interrupt handler, we might lose some events and the NIC may behavave incorrectly if interrupts are not handled.\n\n\n\n\n\n\nInit Sequence\n\n\n\n\nInit PCI subsystem, build data structures\n\n\nCore IB layer register \nib_client\n\n\nmlx4_init()\n: register PCI driver, provide a callback\n\n\n__mlx4_init_one()\n: initialize the hardware itself, register interrupt handler.\n\n\nmlx4_ib_init()\n: allocate a ib_device, and register, which will callback through all \nib_client\n registered at step 1.\n\n\n\n\n\nYizhou Shan\n\nCreated: Aug 20, 2018\n\nLast Updated: Aug 20, 2018", 
            "title": "Infiniband"
        }, 
        {
            "location": "/lego/driver/ib/#infiniband-subsystem", 
            "text": "", 
            "title": "Infiniband Subsystem"
        }, 
        {
            "location": "/lego/driver/ib/#current-status", 
            "text": "Lego s IB stack is ported based on  linux-3.11.1 . We ported:   ib_core  mlx4_ib  mlx4_core   Lego does not support uverbs. At the time of writing, Lego IB stack has only been tested on  Mellanox Technologies MT27500 Family [ConnectX-3] .", 
            "title": "Current Status"
        }, 
        {
            "location": "/lego/driver/ib/#random-summary", 
            "text": "The stack is SUPER complex, a lot data structures and pointers fly all over. Good thing is the whole stack is layered clearly.  Top down", 
            "title": "Random summary"
        }, 
        {
            "location": "/lego/driver/ib/#ib_core", 
            "text": "IB core code is in  driver/infiniband/core , which exposes the major IB API to both user and kernel applications. Inside, it has two parts. The first part is function callback, that call back to underlying device-specific functions. The second part is the management stack, including communication manager (cm), management datagram (mad), and so on.  In IB, each port s QP0 and QP1 are reserved for management purpose. They will receive/send MAD from/to subnet manager, who typically runs on switch. All the IB management stuff is carried out by exchanging MAD.  There are several key data structures: ib_client, ib_device, and mad_agent. MAD, CM, and some others are ib_client, which means they use IB device, and will be called back whenever a device has been added. mad_agent is something that will be called back whenever a device received a MAD message from switch (see  ib_mad_completion_handler() ). A lot layers, huh?  ib_mad_completion_handler() : we changed the behavior of it. we use busy polling instead of interrupt. Originally, it will be invoked by mlx4_core/eq.c", 
            "title": "ib_core"
        }, 
        {
            "location": "/lego/driver/ib/#mlx4_ib-and-mlx4_core", 
            "text": "mlx4_core is actually the Ethernet driver for Mellanox NIC device (drivers/net/ethernet/mellanox/hw/mlx4), which do the actual dirty work of talking with device. On the other hand, mlx4_ib is the glue code between ib_core and mlx4_core, who do the translation.    A lot IB verbs are ultimately translated into  fw.c __mlx4_cmd() , which actually send commands to device and get the result. There are two ways of getting result: 1) polling: after writing to device memory the command, the same thread keep polling. 2) sleep and wait for interrupt. By default, the interrupt way is used (obviously). But, at the time of writing (Aug 20, 2018), we don t really have a working IRQ subsystem, so we use polling instead.  I m still a little concerned that without interrupt handler, we might lose some events and the NIC may behavave incorrectly if interrupts are not handled.", 
            "title": "mlx4_ib and mlx4_core"
        }, 
        {
            "location": "/lego/driver/ib/#init-sequence", 
            "text": "Init PCI subsystem, build data structures  Core IB layer register  ib_client  mlx4_init() : register PCI driver, provide a callback  __mlx4_init_one() : initialize the hardware itself, register interrupt handler.  mlx4_ib_init() : allocate a ib_device, and register, which will callback through all  ib_client  registered at step 1.   \nYizhou Shan \nCreated: Aug 20, 2018 \nLast Updated: Aug 20, 2018", 
            "title": "Init Sequence"
        }, 
        {
            "location": "/lego/paper/nmp/", 
            "text": "Near Memory Processing\n\n\n\n\nNMP: Near Memory Processing\n\n\n\n\nNDC: Near Data Computing\n\n\n\n\n\n\nPRIME\n:\n \nA\n \nNovel\n \nProcessing\n-\nin\n-\nmemory\n \nArchitecture\n \nfor\n \nNeural\n \nNetwork\nComputation\n \nin\n \nReRAM\n-\nbased\n \nMain\n \nMemory\n,\n \nISCA\n16\n\n\n\n\nHigh-performance\nacceleration of NN requires high memory bandwidth since\nthe \nPUs are hungry for fetching the synaptic weights [17]\n. To\naddress this challenge, recent special-purpose chip designs\nhave adopted large on-chip memory to store the synaptic\nweights. For example, DaDianNao [18] employed a large\non-chip eDRAM for both high bandwidth and data locality;\nTrueNorth utilized an SRAM crossbar memory for synapses\nin each core [19].\n\n\n\n\n\n\nDianNao\n and \nDaDianNao\n\n\n \nmemory bandwidth requirements\n of two important\nlayer types: convolutional layers with private kernels\n(used in DNNs) and classifier layers used in both CNNs and\nDNNs. For these types of layers, the total number of required\nsynapses can be massive, in the millions of parameters, or\neven tens or hundreds thereof.\n\n\nproviding sufficient eDRAM capacity to hold\nall \nsynapse\n on the combined eDRAM of all chips will\nsave on \noff-chip DRAM accesses\n, which are particularly\ncostly energy-wise\n\n\nSynapses\n. In a perceptron layer, all synapses are usually\nunique, and thus there is no reuse within the layer. On the\nother hand, the synapses are reused across network invocations,\ni.e., for each new input data (also called \u201cinput row\u201d)\npresented to the neural network. So a sufficiently large L2\ncould store all network synapses and take advantage of that\nlocality. For DNNs with private kernels, this is not possible\nas the total number of synapses are in the tens or hundreds\nof millions (the largest network to date has a billion\nsynapses [26]). However, for both CNNs and DNNs with\nshared kernels, the total number of synapses range in the\nmillions, which is within the reach of an L2 cache. In Figure\n6, see CLASS1 - Tiled+L2, we emulate the case where reuse\nacross network invocations is possible by considering only\nthe perceptron layer; as a result, the total bandwidth requirements\nare now drastically reduced.\n\n\nSo, ML workloads do need large memory bandwidth, and need a lot memory. But how about \ntemporary working set size\n? It\ns the best if it has a reasonable working set size that can fit the cache.\n\n\n\n\n\n\nTPU\n\n\nEach model needs between 5M and 100M weights (9\nth\n\ncolumn of Table 1), which can take a lot of time and energy to\naccess. To amortize the access costs, \nthe same weights are reused\nacross a batch of independent examples during inference or\ntraining\n, which improves performance.\n\n\nThe weights for the matrix unit are staged through an onchip\n\nWeight FIFO\n that reads from an \noff-chip 8 GiB DRAM\ncalled Weight Memory\n (for inference, weights are read-only; 8\nGiB supports many simultaneously active models). The weight\nFIFO is four tiles deep. The intermediate results are held in the \n24\nMiB on-chip Unified Buffer\n, which can serve as inputs to the Matrix Unit.\n\n\nIn virtual cache model, we actually can assign those weights to some designated sets, thus avoid conflicting with other data, which means we can sustain those weights in cache!\n\n\n\n\n\n\n\n\nTo conclude:\n\n\na)\n ML needs to use weight/synapses during computation, and those data will be reused repeatly across different stages. Besides, output from last stage serves the input of next stage, so buffering the \nintermediate data\n is important. Most ML accelerators use some kind of \non-chip memory\n (\nWeighted FIFO, Unified Cache in TPU\n) to buffer those data. This fits the \nHBM+Disaggregated Memory\n model: HBM is the on-chip memory, while disaggregated memory is the off-chip memory. \nb)\n Combined with virtual cache, we could assign special virtual addresses to weight data, so they stay in some designated cache sets. Kernel can avoid allocating conflict virtual addresses later. Thus we can retain these weight data in virtual cache easily.", 
            "title": "NMP"
        }, 
        {
            "location": "/lego/paper/nmp/#near-memory-processing", 
            "text": "NMP: Near Memory Processing   NDC: Near Data Computing    PRIME :   A   Novel   Processing - in - memory   Architecture   for   Neural   Network Computation   in   ReRAM - based   Main   Memory ,   ISCA 16   High-performance\nacceleration of NN requires high memory bandwidth since\nthe  PUs are hungry for fetching the synaptic weights [17] . To\naddress this challenge, recent special-purpose chip designs\nhave adopted large on-chip memory to store the synaptic\nweights. For example, DaDianNao [18] employed a large\non-chip eDRAM for both high bandwidth and data locality;\nTrueNorth utilized an SRAM crossbar memory for synapses\nin each core [19].    DianNao  and  DaDianNao    memory bandwidth requirements  of two important\nlayer types: convolutional layers with private kernels\n(used in DNNs) and classifier layers used in both CNNs and\nDNNs. For these types of layers, the total number of required\nsynapses can be massive, in the millions of parameters, or\neven tens or hundreds thereof.  providing sufficient eDRAM capacity to hold\nall  synapse  on the combined eDRAM of all chips will\nsave on  off-chip DRAM accesses , which are particularly\ncostly energy-wise  Synapses . In a perceptron layer, all synapses are usually\nunique, and thus there is no reuse within the layer. On the\nother hand, the synapses are reused across network invocations,\ni.e., for each new input data (also called \u201cinput row\u201d)\npresented to the neural network. So a sufficiently large L2\ncould store all network synapses and take advantage of that\nlocality. For DNNs with private kernels, this is not possible\nas the total number of synapses are in the tens or hundreds\nof millions (the largest network to date has a billion\nsynapses [26]). However, for both CNNs and DNNs with\nshared kernels, the total number of synapses range in the\nmillions, which is within the reach of an L2 cache. In Figure\n6, see CLASS1 - Tiled+L2, we emulate the case where reuse\nacross network invocations is possible by considering only\nthe perceptron layer; as a result, the total bandwidth requirements\nare now drastically reduced.  So, ML workloads do need large memory bandwidth, and need a lot memory. But how about  temporary working set size ? It s the best if it has a reasonable working set size that can fit the cache.    TPU  Each model needs between 5M and 100M weights (9 th \ncolumn of Table 1), which can take a lot of time and energy to\naccess. To amortize the access costs,  the same weights are reused\nacross a batch of independent examples during inference or\ntraining , which improves performance.  The weights for the matrix unit are staged through an onchip Weight FIFO  that reads from an  off-chip 8 GiB DRAM\ncalled Weight Memory  (for inference, weights are read-only; 8\nGiB supports many simultaneously active models). The weight\nFIFO is four tiles deep. The intermediate results are held in the  24\nMiB on-chip Unified Buffer , which can serve as inputs to the Matrix Unit.  In virtual cache model, we actually can assign those weights to some designated sets, thus avoid conflicting with other data, which means we can sustain those weights in cache!     To conclude:  a)  ML needs to use weight/synapses during computation, and those data will be reused repeatly across different stages. Besides, output from last stage serves the input of next stage, so buffering the  intermediate data  is important. Most ML accelerators use some kind of  on-chip memory  ( Weighted FIFO, Unified Cache in TPU ) to buffer those data. This fits the  HBM+Disaggregated Memory  model: HBM is the on-chip memory, while disaggregated memory is the off-chip memory.  b)  Combined with virtual cache, we could assign special virtual addresses to weight data, so they stay in some designated cache sets. Kernel can avoid allocating conflict virtual addresses later. Thus we can retain these weight data in virtual cache easily.", 
            "title": "Near Memory Processing"
        }, 
        {
            "location": "/lego/paper/processor_oom/", 
            "text": "Process/Memory Kernel Memory\n\n\nThis document is based on discussion with Yiying, about how to deal with processor or memory component\ns out-of-kernel-memory situation. It mainly bothers processor component, which has a small kernel memory while needs to support all running user threads.\n\n\nProcess\ns local kernel memory is limited by design. There are several major users:\n\n\n\n\n1) pcache\ns rmap, which is propotional to pcache size.\n\n\n2) IB, which depends on concurrent outgoing messages.\n\n\n3) running threads. For each thread at processor, Lego needs to allocate some kernel memory for it, e.g, \nkernel stack\n, \ntask_strcut\n, and so on.\n\n\n\n\nBoth 1) and 2) are fine, they can be easily controlled. However we can not limit how many threads user can create, thus 3) becomes the critical criminal of oom.\n\n\nWhen processor is running out of kernel memory, Lego needs to deal with it. Currently, we propose three different solutions:\n\n\n\n\ns1) \nSwap\n kernel memory to remote memory component\n\n\ns2) \nKill\n some threads to have some usable memory (OOM killer)\n\n\ns3) \nMigrate\n, or \ncheckpoint\n, threads to processors that have usable kernel memory\n\n\n\n\nFor solution 3), there is a case where \nall\n processors are running out of memory. Then we have to use solution 1) or 2).\n\n\n\nYizhou Shan\n\nFeb 17, 2018", 
            "title": "Processor OOM"
        }, 
        {
            "location": "/lego/paper/processor_oom/#processmemory-kernel-memory", 
            "text": "This document is based on discussion with Yiying, about how to deal with processor or memory component s out-of-kernel-memory situation. It mainly bothers processor component, which has a small kernel memory while needs to support all running user threads.  Process s local kernel memory is limited by design. There are several major users:   1) pcache s rmap, which is propotional to pcache size.  2) IB, which depends on concurrent outgoing messages.  3) running threads. For each thread at processor, Lego needs to allocate some kernel memory for it, e.g,  kernel stack ,  task_strcut , and so on.   Both 1) and 2) are fine, they can be easily controlled. However we can not limit how many threads user can create, thus 3) becomes the critical criminal of oom.  When processor is running out of kernel memory, Lego needs to deal with it. Currently, we propose three different solutions:   s1)  Swap  kernel memory to remote memory component  s2)  Kill  some threads to have some usable memory (OOM killer)  s3)  Migrate , or  checkpoint , threads to processors that have usable kernel memory   For solution 3), there is a case where  all  processors are running out of memory. Then we have to use solution 1) or 2).  \nYizhou Shan \nFeb 17, 2018", 
            "title": "Process/Memory Kernel Memory"
        }, 
        {
            "location": "/lego/paper/genz/", 
            "text": "Interconnect Technology Comparison\n\n\n\n\n\n\n\n\nInterconnect Technology\n\n\nProducts or Vendor\n\n\nPhysical Domain\n\n\nCache Coherent\n\n\nAccess Semantic\n\n\nMaximum Bandwidth\n\n\nMedium Latency\n\n\n\n\n\n\n\n\n\n\nGen-Z\n7\n8\n\n\nN/A\n\n\nCross components\n\n\n\n\nMemory\n\n\n32 GBps ~ 400+ GBps \n \nUnidirectional\n\n\n100ns\n\n\n\n\n\n\nOpenCAPI\n7\n\n\nIBM Power9\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n50 GBps per lane \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nCCIX\n7\n\n\nN/A\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n32/40/50 GBps/lane \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nOmniPath\n9\n10\n\n\nIntel KnightsLanding\n\n\nCross networrk\n\n\n\n\nNetwork\n\n\n25 GBps/port \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nPCIe 3.0\n\n\nA Lot\n\n\nMotherboard\n\n\n\n\nPCIe\n\n\n~1GBps/lane\n12\n\n\n4B Read ~756ns\n11\n\n\n\n\n\n\nPCIe 4.0\n\n\nSoon\n\n\nMotherboard\n\n\n\n\nPCIe\n\n\n~2GBps/lane\n\n\n?\n\n\n\n\n\n\nIB EDR\n\n\nMellanox ConnectX4,X5\n\n\nCross network\n\n\n\n\nNetwork\n\n\n100Gbps\n\n\n0.5us\n\n\n\n\n\n\nIB HDR\n\n\nMellanox ConnectX6\n\n\nCross network\n\n\n\n\nNetwork\n\n\n200Gbps\n\n\n0.5us\n\n\n\n\n\n\nHyperTransport\n4\n\n\nAMD\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n51.2 GBps per link \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nNVLink\n2\n\n\nNVIDIA V100 \n IBM Power9\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n50GBps per link \n \nBidirectional\n\n\n?\n\n\n\n\n\n\nQPI\n5\n6\n\n\nIntel\n\n\nMotherboard\n\n\n\n\nMemory\n\n\n?\n\n\n?\n\n\n\n\n\n\nIntel Main Memory Bus\n\n\nIntel\n\n\nProcessor\n\n\n\n\nMemory\n\n\nE7-8894 v4 \n85 GB/s\n \n E5-2620 v3 \n59 GB/s\n\n\n?\n\n\n\n\n\n\nEthernet\n3\n\n\nA Lot\n\n\nMotherboard\n\n\n\n\nNetwork\n\n\nMellanox \n200Gbps\n \n Cisco ASR \n100 Gbps\n1\n\n\n?\n\n\n\n\n\n\n\n\n\n\nPOWER9, NVLink 2.0, 300GB/s\n\n\n\n\n\nCreated: Feb 28, 2018\n\nLast Updated: March 01, 2018\n\n\n\n\n\n\n\n\n\n\nEthernet Cisco ASR 9000 Series 4-Port 100-Gigabit Ethernet\n\n\n\n\n\n\nTerabit Ethernet\n\n \nhttps://en.wikipedia.org/wiki/NVLink\n\n\n\n\n\n\nNVLink\n\n\n\n\n\n\nHyperTransport\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect\n\n\n\n\n\n\nhttps://communities.intel.com/thread/21872\n\n\n\n\n\n\nhttps://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf\n\n\n\n\n\n\nGen-Z Overview\n\n\n\n\n\n\nhttp://www.hoti.org/hoti23/slides/rimmer.pdf\n\n\n\n\n\n\nhttps://www.intel.com/content/www/us/en/products/network-io/high-performance-fabrics/omni-path-edge-switch-100-series.html\n\n\n\n\n\n\nhttps://forum.stanford.edu/events/posterslides/LowLatencyNetworkInterfaces.pdf\n\n\n\n\n\n\nhttps://www.xilinx.com/support/documentation/white_papers/wp350.pdf", 
            "title": "Gen-Z"
        }, 
        {
            "location": "/lego/paper/genz/#interconnect-technology-comparison", 
            "text": "Interconnect Technology  Products or Vendor  Physical Domain  Cache Coherent  Access Semantic  Maximum Bandwidth  Medium Latency      Gen-Z 7 8  N/A  Cross components   Memory  32 GBps ~ 400+ GBps    Unidirectional  100ns    OpenCAPI 7  IBM Power9  Motherboard   Memory  50 GBps per lane    Bidirectional  ?    CCIX 7  N/A  Motherboard   Memory  32/40/50 GBps/lane    Bidirectional  ?    OmniPath 9 10  Intel KnightsLanding  Cross networrk   Network  25 GBps/port    Bidirectional  ?    PCIe 3.0  A Lot  Motherboard   PCIe  ~1GBps/lane 12  4B Read ~756ns 11    PCIe 4.0  Soon  Motherboard   PCIe  ~2GBps/lane  ?    IB EDR  Mellanox ConnectX4,X5  Cross network   Network  100Gbps  0.5us    IB HDR  Mellanox ConnectX6  Cross network   Network  200Gbps  0.5us    HyperTransport 4  AMD  Motherboard   Memory  51.2 GBps per link    Bidirectional  ?    NVLink 2  NVIDIA V100   IBM Power9  Motherboard   Memory  50GBps per link    Bidirectional  ?    QPI 5 6  Intel  Motherboard   Memory  ?  ?    Intel Main Memory Bus  Intel  Processor   Memory  E7-8894 v4  85 GB/s    E5-2620 v3  59 GB/s  ?    Ethernet 3  A Lot  Motherboard   Network  Mellanox  200Gbps    Cisco ASR  100 Gbps 1  ?      POWER9, NVLink 2.0, 300GB/s   \nCreated: Feb 28, 2018 \nLast Updated: March 01, 2018      Ethernet Cisco ASR 9000 Series 4-Port 100-Gigabit Ethernet    Terabit Ethernet \n  https://en.wikipedia.org/wiki/NVLink    NVLink    HyperTransport    https://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect    https://communities.intel.com/thread/21872    https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf    Gen-Z Overview    http://www.hoti.org/hoti23/slides/rimmer.pdf    https://www.intel.com/content/www/us/en/products/network-io/high-performance-fabrics/omni-path-edge-switch-100-series.html    https://forum.stanford.edu/events/posterslides/LowLatencyNetworkInterfaces.pdf    https://www.xilinx.com/support/documentation/white_papers/wp350.pdf", 
            "title": "Interconnect Technology Comparison"
        }, 
        {
            "location": "/lego/paper/replication/", 
            "text": "Replication, Checkpoint, Logging, and Recovery\n\n\nDiscussion\n\n\n\n\n\n\n03/25/18:\n\n\n\n\nRevisit RAMCloud, which has a very similar goal with Lego. It keeps a full copy of data in DRAM, use disk to ensure crash consistency. The key assumption of RAMCloud is the battery-backed DRAM or PM on its disk side.\n\n\nWe don\nt need to provide a 100% recoverable model. Our goal here is to reduce the failure probabilities introduced by more components. Let us say Lego do the persist in a batching fashion, instead of per-page. We are not able to recover if and only if failure happen \nwhile\n we do the batch persist. But we are safe if failure happen between batched persist.\n\n\nThat actually also means we need to checkpoint process state in Processor side. We have to save all the process context along with the persisted memory log! Otherwise, the memory content is useless, we don\nt know the exact IP and other things.\n\n\nI\nm wrong. :-)\n\n\n\n\n\n\n\n\n03/20/18: when memory is enough, use pessimistic replication, when demand is high, use optimistic to save memory components.\n\n\n\n\n\n\nReplication\n\n\nBefore started, I spent some time recap, and found Wiki pages\n1\n2\n3\n are actually very good.\n\n\nTwo main approaches:\n\n\n\n\nOptimistic (Lazy, Passive) Replication\n \n4\n, in which replicas are allowed to diverge\n\n\nEventual consistency\n5\n6\n7\n, meaning that replicas are guaranteed to converge only when the system has been quiesced for a period of time\n\n\n\n\n\n\nPessimistic (Active, Multi-master\n8\n) Replication\n, tries to guarantee from the beginning that all of the replicas are identical to each other, as if there was only a single copy of the data all along.\n\n\n\n\nLego is more towards memory replication, not storage replication. We may want to conduct some ideas from DSM replication (MRSW, MRMW), or in-memory DB such as RAMCloud, VoltDB?\n\n\nCheckpointing\n\n\nSome nice reading\n9\n.\n\n\nApplication types:\n\n\n\n\nLong-running v.s. Short-lived\n\n\nBuilt-in checkpoint/journaling v.s. no built-in checkpoint/journaling\n\n\n\n\nTwo main approaches:\n\n\n\n\nCoordinated\n\n\n2PC\n\n\n\n\n\n\nUn-coordinated\n\n\nDomino effect\n\n\n\n\n\n\n\n\nWe should favor \n[Long-running \n no built-in checkpoint/journaling]\n applications. Normally they are not distributed systems, right? Even it is, it might be running as a single-node version. Based on this, I think we should favor coordinated checkpointing.\n\n\nHPC community\n10\n11\n12\n has a lot publications on checkpoint/recovery (e.g., Lawrence National Laboratory).\n\n\nMISC\n\n\nSome other interesting topics:\n\n\n\n\nErasure Coding\n\n\nLess space overhead\n\n\nParity Calculation is CPU-intensive\n\n\nIncreased latency\n\n\n\n\n\n\n\n\n\nYizhou Shan\n\nCreated: Mar 19, 2018\n\nLast Updated: Mar 19, 2018\n\n\n\n\n\n\n\n\n\n\nWiki: Replication\n\n\n\n\n\n\nWiki: High-availability_cluster\n\n\n\n\n\n\nWiki: Virtual synchrony\n\n\n\n\n\n\nWiki: Optimistic Replication\n\n\n\n\n\n\nWiki: Quiesce\n\n\n\n\n\n\nWiki: Eventual Consistency\n\n\n\n\n\n\nWiki: CAP Theorem\n\n\n\n\n\n\nWiki: Multi-master replication\n\n\n\n\n\n\nWiki: Application Checkpointing\n\n\n\n\n\n\nPaper: A Survey of Checkpoint/Restart Implementations \n\n\n\n\n\n\nPaper: The Design and Implementation of Berkeley Lab\u2019s Linux\nCheckpoint/Restart\n\n\n\n\n\n\nBerkeley Lab Checkpoint/Restart (BLCR) for LINUX", 
            "title": "Replication"
        }, 
        {
            "location": "/lego/paper/replication/#replication-checkpoint-logging-and-recovery", 
            "text": "", 
            "title": "Replication, Checkpoint, Logging, and Recovery"
        }, 
        {
            "location": "/lego/paper/replication/#discussion", 
            "text": "03/25/18:   Revisit RAMCloud, which has a very similar goal with Lego. It keeps a full copy of data in DRAM, use disk to ensure crash consistency. The key assumption of RAMCloud is the battery-backed DRAM or PM on its disk side.  We don t need to provide a 100% recoverable model. Our goal here is to reduce the failure probabilities introduced by more components. Let us say Lego do the persist in a batching fashion, instead of per-page. We are not able to recover if and only if failure happen  while  we do the batch persist. But we are safe if failure happen between batched persist.  That actually also means we need to checkpoint process state in Processor side. We have to save all the process context along with the persisted memory log! Otherwise, the memory content is useless, we don t know the exact IP and other things.  I m wrong. :-)     03/20/18: when memory is enough, use pessimistic replication, when demand is high, use optimistic to save memory components.", 
            "title": "Discussion"
        }, 
        {
            "location": "/lego/paper/replication/#replication", 
            "text": "Before started, I spent some time recap, and found Wiki pages 1 2 3  are actually very good.  Two main approaches:   Optimistic (Lazy, Passive) Replication   4 , in which replicas are allowed to diverge  Eventual consistency 5 6 7 , meaning that replicas are guaranteed to converge only when the system has been quiesced for a period of time    Pessimistic (Active, Multi-master 8 ) Replication , tries to guarantee from the beginning that all of the replicas are identical to each other, as if there was only a single copy of the data all along.   Lego is more towards memory replication, not storage replication. We may want to conduct some ideas from DSM replication (MRSW, MRMW), or in-memory DB such as RAMCloud, VoltDB?", 
            "title": "Replication"
        }, 
        {
            "location": "/lego/paper/replication/#checkpointing", 
            "text": "Some nice reading 9 .  Application types:   Long-running v.s. Short-lived  Built-in checkpoint/journaling v.s. no built-in checkpoint/journaling   Two main approaches:   Coordinated  2PC    Un-coordinated  Domino effect     We should favor  [Long-running   no built-in checkpoint/journaling]  applications. Normally they are not distributed systems, right? Even it is, it might be running as a single-node version. Based on this, I think we should favor coordinated checkpointing.  HPC community 10 11 12  has a lot publications on checkpoint/recovery (e.g., Lawrence National Laboratory).", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/lego/paper/replication/#misc", 
            "text": "Some other interesting topics:   Erasure Coding  Less space overhead  Parity Calculation is CPU-intensive  Increased latency     \nYizhou Shan \nCreated: Mar 19, 2018 \nLast Updated: Mar 19, 2018      Wiki: Replication    Wiki: High-availability_cluster    Wiki: Virtual synchrony    Wiki: Optimistic Replication    Wiki: Quiesce    Wiki: Eventual Consistency    Wiki: CAP Theorem    Wiki: Multi-master replication    Wiki: Application Checkpointing    Paper: A Survey of Checkpoint/Restart Implementations     Paper: The Design and Implementation of Berkeley Lab\u2019s Linux\nCheckpoint/Restart    Berkeley Lab Checkpoint/Restart (BLCR) for LINUX", 
            "title": "MISC"
        }, 
        {
            "location": "/lego/paper/related/", 
            "text": "dRedBox\n\n\n\n\nnews\n\n\nIBM Advancing cloud with memory disaggregation\n\n\n[Slides: Open Source Cloud Ecosystem for Next-Gen Disaggregated Datacenters] (\nhttps://schd.ws/hosted_files/osseu17/60/dReDBox.CloudOpen2017.talk.pdf\n)\n\n\nSlides: Demo", 
            "title": "Related"
        }, 
        {
            "location": "/lego/paper/related/#dredbox", 
            "text": "news  IBM Advancing cloud with memory disaggregation  [Slides: Open Source Cloud Ecosystem for Next-Gen Disaggregated Datacenters] ( https://schd.ws/hosted_files/osseu17/60/dReDBox.CloudOpen2017.talk.pdf )  Slides: Demo", 
            "title": "dRedBox"
        }
    ]
}