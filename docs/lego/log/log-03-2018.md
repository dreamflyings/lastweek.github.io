# March 2018

---
## 03/08 Thur
Took several days off. This morning finished the porting of `wait4` and `waitid`, which actually has a lot code change. The concept and mechanism is fairly simple, but the legacy UNIX tradition make the implementation quite complex.

Now, look back to finish debugging the pcache issue. It must be fixed this week.

### python
Tried `python hello_world.py`, the program runs for a while and crashes at a deterministic point:
```c hl_lines="4 8 39"
wuklab13 and wuklab15, ~/ttyS1
[419097.929969] __pcache_do_fill_page(): O pid:32 tgid:32 address:0x7ffff7a4b008 flags:0x50 ret:0(OKAY)
[419098.039145] __pcache_do_fill_page(): I pid:32 tgid:32 address:0x7ffff7a4c010 flags:0x50
[419098.306537] __pcache_do_fill_page(): O pid:32 tgid:32 address:0x7ffff7a4c010 flags:0x50 ret:0(OKAY)
[419098.413756] CPU5 PID32 sys_mprotect+0x0/0x90
[419098.465753] SYSC_mprotect() cpu(5) tsk(32/32/python) user-ip:0x7ffff7df3d27
[419098.549990]     start:0x7ffff7d8c000,len:0x2000,prot:0x1
[419098.614469] BUG: unable to handle kernel paging request at ffff9001801ff000
[419098.698703] IP: [<ffffffff8102f7a9>] pcache_handle_fault+0x69/0x6c0
[419098.774621] PGD 0
[419098.799579] Oops: 0000 [#1] SMP PROCESSOR
[419098.848457] CPU: 5 PID: 32 Comm: python 4.0.0-lego-ys+ #312
[419098.916054] RIP: 0010:[<ffffffff8102f7a9>]  [<ffffffff8102f7a9>] pcache_handle_fault+0x69/0x6c0
[419099.021089] RSP: 0000:ffff88107e857ed8  EFLAGS: 00010286
[419099.085567] RAX: ffff9001801ff000 RBX: ffff9001801ff000 RCX: 00003ffffffff000
[419099.171884] RDX: 00000801801ff000 RSI: 0000000000601008 RDI: ffff88107e83d648
[419099.258199] RBP: ffff88107e857f18 R08: 00007ffff7fe3000 R09: 00007ffff7fe3000
[419099.344516] R10: 0000000000000000 R11: 0000000000000206 R12: 0000000000601008
[419099.430832] R13: ffff88107e83d648 R14: 0000000000000050 R15: 00007ffff7ffe150
[419099.517149] FS:  00007ffff7fdf740(0000) GS:ffff88207fc40000(0000) knlGS:0000000000000000
[419099.614905] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[419099.684582] CR2: ffff9001801ff000 CR3: 000000207fccf000 CR4: 00000000000406a0
[419099.770899] Stack:
[419099.795858] 00007ffff7d8c000 0000000000002000 0000000000000001 0000000000000004
[419099.884254] 0000000000601008 ffff88107e857f58 0000000000000000 00007ffff7ffe150
[419099.972650] ffff88107e857f48 ffffffff81010082 0000000000000000 0000000000000001
[419100.061047] 000392c29c720ba2 0000000000000000 00007fffffffdc40 ffffffff8100d91f
[419100.149442] 00007ffff7ffe150 0000000000000000 000392c29c720ba2 0000000000000001
[419100.237839] Call Trace:
[419100.267998] <TSK>
[419100.291917] [<ffffffff81010082>] do_page_fault+0xa2/0x1a0
[419100.357434] [<ffffffff8100d91f>] page_fault+0x1f/0x30
[419100.418792] <EOT>


M:
...
[419142.163396] handle_p2m_pcache_miss() cpu 4 I nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7a4c010
[419142.268460] handle_p2m_pcache_miss() cpu 4 O nid:0 pid:32 tgid:32 flags:50 vaddr:0x7ffff7a4c010
(Last Message)
```

Dig deeper:
```c hl_lines="4 5"
int pcache_handle_fault(struct mm_struct *mm,
                        unsigned long address, unsigned long flags)
{
        ..
        pgd = pgd_offset(mm, address);
        pr_info("    addr: %#lx, pgd: %p\n", address, pgd);
        pud = pud_alloc(mm, pgd, address);
        pr_info("    addr: %#lx, pgd: %p pud %p\n", address, pgd, pud);
        if (!pud)
                return VM_FAULT_OOM;
        pmd = pmd_alloc(mm, pud, address);
        if (!pmd)
..
}

[21130.503314] strace__mprotect cpu5 start=0x7ffff7d8c000, len=0x2000, prot(0x1)=PROT_READ
[21130.598994] SYSC_mprotect() cpu(5) tsk(32/32/python) user-ip:0x7ffff7df3d27
[21130.682193]     start:0x7ffff7d8c000,len:0x2000,prot:0x1
[21130.745635]     addr: 0x601008, pgd: ffff88207fccf000
[21130.805954]     addr: 0x601008, pgd: ffff88207fccf000 pud ffff9001801ff000
[21130.888116] BUG: unable to handle kernel paging request at ffff9001801ff000
[21130.971314] IP: [<ffffffff8102fa11>] pcache_handle_fault+0x91/0x6f0
```

Print pgd and pud info, these three messages are related and the last one leads to panic:
```c hl_lines="4 8 12"
wuklab13 ~/ys/0308-6
[  479.375498] addr: 0x400040, pgd: ffff88207fccf000
[  479.435819] pud_alloc_one(): addr: 0x400040, pud: ffff88207fc6f000
[  479.511739] pud_alloc(): addr: 0x400040 pgd ffff88207fccf000, pgd.cont_va ffff88207fc6f000, pud_index=0x0 pud: ffff88207fc6f000
[  479.649021] addr: 0x400040, pgd: ffff88207fccf000 pud ffff88207fc6f000

[  480.016381] addr: 0x600dd8, pgd: ffff88207fccf000
[  480.076701] pud_alloc(): addr: 0x600dd8 pgd ffff88207fccf000, pgd.cont_va ffff88207fc6f000, pud_index=0x0 pud: ffff88207fc6f000
[  480.213982] addr: 0x600dd8, pgd: ffff88207fccf000 pud ffff88207fc6f000

[  680.072819] addr: 0x601008, pgd: ffff88207fccf000
[  680.133138] pud_alloc(): addr: 0x601008 pgd ffff88207fccf000, pgd.cont_va ffff90107e834000, pud_index=0x0 pud: ffff90107e834000
[  680.270422] addr: 0x601008, pgd: ffff88207fccf000 pud ffff90107e834000

[  680.352583] BUG: unable to handle kernel paging request at ffff90107e834000
[  680.435783] IP: [<ffffffff8102fc43>] pcache_handle_fault+0xb3/0x770
[  680.510664] PGD 0

```

I need to check what happens between 480s to 680s. Something in between corrupted pgtable. I doubt it can be:

- copy_to_user related syscalls
- pcache establish mapping, mempcy
- all other memcpy strcpy etc stuff

---
## 03/02 Fri

TODO:

- {---add vsyscall---}
- {---pcache_exit_process: free rmap, free cacheline, etc. When rmap is NULL, we clearly should free this pcache.---}
- pcache_exit_thread? I don't think we need this. All pcache related activities should relate to mm, or thread group leader, not one particular thread.
- check python bug
- use omnigraffle to draw the whole workflow of pcache.

Phoenix, word_count-seq, 4G dataset, 4GB pcache:
```
[  273.268853] Processor: Processor manager is running.
[  573.272479] page:ffffea0071bb9660 count:0 mapcount:-128
[  573.332903] flags: 0x200000000000300(slab|slob_free)
[  573.392182] page dumped because: VM_BUG_ON_PAGE(page_ref_count(page) == 0)
[  573.474340] ------------[ cut here ]------------
[  573.529459] BUG: failure at ./include/lego/mm.h:251/put_page_testzero()!
[  573.609537] Kernel Panic - not syncing: BUG!
[  573.660496] CPU: 4 PID: 13 Comm: kvictim_flushd 4.0.0-lego+ #18
[  573.731212] Stack:
[  573.755132] ffff88207e4bfe10 ffffffff81023644 0000000000000008 ffff88207e4bfe20
[  573.842490] ffff88207e4bfdd8 0000000021475542 0000000000000000 0000000000000000
[  573.929848] 0000000000000000 0000000000000000 0000000000000000 0000000000000000
[  574.017205] 0000000000000000 0000000000000000 0000000000000000 0000000000000000
[  574.104563] 0000000000000000 0000000000000000 0000000000000000 0000000000000000
[  574.191921] Call Trace:
[  574.221039] <TSK>
[  574.243919] [<ffffffff81023650>] panic+0xc2/0xeb
[  574.299038] [<ffffffff8105a35a>] ? client_internal_poll_sendcq+0x2a/0x80
[  574.379115] [<ffffffff8105a4fd>] ? client_send_message_with_rdma_write_with_imm_request+0x14d/0x360
[  574.487273] [<ffffffff8101ac3c>] ? task_tick_rt+0x2c/0xd0
[  574.551751] [<ffffffff81018395>] ? scheduler_tick+0x55/0x60
[  574.618308] [<ffffffff81015a45>] ? tick_handle_periodic+0x45/0x70
[  574.691107] [<ffffffff810064c4>] ? apic_timer_interrupt+0x54/0x90
[  574.763905] [<ffffffff8100dbaa>] ? smp__apic_timer_interrupt+0x6a/0x70
[  574.841903] [<ffffffff8101198d>] ? printk+0x11d/0x1b0
[  574.902222] [<ffffffff81025c00>] __free_pages+0x2e0/0x3c0
[  574.966699] [<ffffffff81028472>] kfree+0x62/0x480
[  575.022858] [<ffffffff8102e6be>] victim_flush_func+0x15e/0x1e0
[  575.092536] [<ffffffff8102e560>] ? victim_try_fill_pcache+0x390/0x390
[  575.169494] [<ffffffff8101e446>] kthread+0xf6/0x120
[  575.227733] [<ffffffff8101e350>] ? __kthread_parkme+0x70/0x70
[  575.296371] [<ffffffff8100de32>] ret_from_fork+0x22/0x30
[  575.359810] <EOT>
```

---
## 03/01 Thur

Weird.
```
[43181.388400] p2m_fork(cpu5): I cur:24-word_count-seq new:25
[43181.435341] p2m_fork(cpu5): O succeed cur:24-word_count-seq new:25
[43181.436013] __pcache_do_fill_page(): I pid:24 tgid:24 address:0x4158d0 flags:0x150
[43181.439246] __pcache_do_fill_page(): O pid:24 tgid:24 address:0x4158d0 flags:0x150 ret:0(OKAY) csum:0x9e8f028e

[43181.510534] __pcache_do_fill_page(): I pid:25 tgid:25 address:0x415000 flags:0x150
[43181.517729] __pcache_do_fill_page(): O pid:25 tgid:25 address:0x415000 flags:0x150 ret:0(OKAY) csum:0xffff88029e8f028e
```

After all, it is TLB issue. I forgot to flush tlb after making the original pte read-only during fork. So the parent will be also to continue RW some pages, which should be process-private.

Lego's current TLB flush is very native, we do tlbflush after each pte changes. This will have worse performance compared to linux's batch flush.

Today's case is flush tlb after making pte read-only. And this really has to be performed one by one
