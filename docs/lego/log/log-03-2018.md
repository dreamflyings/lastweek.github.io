# March 2018

---
## 03/02 Fri

TODO:

- {---add vsyscall---}
- {---pcache_exit_process: free rmap, free cacheline, etc. When rmap is NULL, we clearly should free this pcache.---}
- pcache_exit_thread? I don't think we need this. All pcache related activities should relate to mm, or thread group leader, not one particular thread.
- check python bug
- use omnigraffle to draw the whole workflow of pcache.

Phoenix, word_count-seq, 4G dataset, 4GB pcache:
```
[  273.268853] Processor: Processor manager is running.
[  573.272479] page:ffffea0071bb9660 count:0 mapcount:-128
[  573.332903] flags: 0x200000000000300(slab|slob_free)
[  573.392182] page dumped because: VM_BUG_ON_PAGE(page_ref_count(page) == 0)
[  573.474340] ------------[ cut here ]------------
[  573.529459] BUG: failure at ./include/lego/mm.h:251/put_page_testzero()!
[  573.609537] Kernel Panic - not syncing: BUG!
[  573.660496] CPU: 4 PID: 13 Comm: kvictim_flushd 4.0.0-lego+ #18
[  573.731212] Stack:
[  573.755132] ffff88207e4bfe10 ffffffff81023644 0000000000000008 ffff88207e4bfe20
[  573.842490] ffff88207e4bfdd8 0000000021475542 0000000000000000 0000000000000000
[  573.929848] 0000000000000000 0000000000000000 0000000000000000 0000000000000000
[  574.017205] 0000000000000000 0000000000000000 0000000000000000 0000000000000000
[  574.104563] 0000000000000000 0000000000000000 0000000000000000 0000000000000000
[  574.191921] Call Trace:
[  574.221039] <TSK>
[  574.243919] [<ffffffff81023650>] panic+0xc2/0xeb
[  574.299038] [<ffffffff8105a35a>] ? client_internal_poll_sendcq+0x2a/0x80
[  574.379115] [<ffffffff8105a4fd>] ? client_send_message_with_rdma_write_with_imm_request+0x14d/0x360
[  574.487273] [<ffffffff8101ac3c>] ? task_tick_rt+0x2c/0xd0
[  574.551751] [<ffffffff81018395>] ? scheduler_tick+0x55/0x60
[  574.618308] [<ffffffff81015a45>] ? tick_handle_periodic+0x45/0x70
[  574.691107] [<ffffffff810064c4>] ? apic_timer_interrupt+0x54/0x90
[  574.763905] [<ffffffff8100dbaa>] ? smp__apic_timer_interrupt+0x6a/0x70
[  574.841903] [<ffffffff8101198d>] ? printk+0x11d/0x1b0
[  574.902222] [<ffffffff81025c00>] __free_pages+0x2e0/0x3c0
[  574.966699] [<ffffffff81028472>] kfree+0x62/0x480
[  575.022858] [<ffffffff8102e6be>] victim_flush_func+0x15e/0x1e0
[  575.092536] [<ffffffff8102e560>] ? victim_try_fill_pcache+0x390/0x390
[  575.169494] [<ffffffff8101e446>] kthread+0xf6/0x120
[  575.227733] [<ffffffff8101e350>] ? __kthread_parkme+0x70/0x70
[  575.296371] [<ffffffff8100de32>] ret_from_fork+0x22/0x30
[  575.359810] <EOT>
```

---
## 03/01 Thur

Weird.
```
[43181.388400] p2m_fork(cpu5): I cur:24-word_count-seq new:25
[43181.435341] p2m_fork(cpu5): O succeed cur:24-word_count-seq new:25
[43181.436013] __pcache_do_fill_page(): I pid:24 tgid:24 address:0x4158d0 flags:0x150
[43181.439246] __pcache_do_fill_page(): O pid:24 tgid:24 address:0x4158d0 flags:0x150 ret:0(OKAY) csum:0x9e8f028e

[43181.510534] __pcache_do_fill_page(): I pid:25 tgid:25 address:0x415000 flags:0x150
[43181.517729] __pcache_do_fill_page(): O pid:25 tgid:25 address:0x415000 flags:0x150 ret:0(OKAY) csum:0xffff88029e8f028e
```

After all, it is TLB issue. I forgot to flush tlb after making the original pte read-only during fork. So the parent will be also to continue RW some pages, which should be process-private.

Lego's current TLB flush is very native, we do tlbflush after each pte changes. This will have worse performance compared to linux's batch flush.

Today's case is flush tlb after making pte read-only. And this really has to be performed one by one
