# April 2018

## 04/07 Sat
Well, now we finished all the profiling stuff. Continue on other work.

Now I like listening to Jazz while coding. Amazing Jazz, really good.

## 04/06 Fri

Well.
Now we have in-kernel strace, in-kernel readprofile. Yummy.

## 04/05 Thur

Discussion with Yilun.
1. munmap+nr_pgfault figure: count number of pgfaults between munmap, it should be an interesting figure.
2. track number of pgfault at: since there is no eviction, so any mmaped area at M should only have exactly one pcache fetch.
3. I probably want to use per-cpu counter.

Anyway, continue strace work first. Finished.

## 04/04 Wed

### STRACE Performance

TF has very bad performance. It is either due to the syscall or pcache. Now I'm adding facilities to track syscall activities, including average latency, total time.

Basic utilities of strace are done. But I somehow need to change the design of multithread strace. Previously, I naively make the thread group keep some info, and let all other threads use that info to do bookkeeping.

But this is really hard and not accurate. We first need to make sure we are running on a non-preemptable kernel, so the per-cpu time tracking will be accurate. Besides, we also need to make sure threads do not migrate because of syscalls such as sched_setaffinity.

Oh, well, so I though I have to use per-thread strace_info. The first design I thought is: accumulating the counter of one thread to its thread group leader, when it exit. But this is slightly complex, and will affect the thread group leader runtime.

So the second solution I came up is let all threads within a process, chain their straec_info together. And normal thread does not need to accumulate the counter. It can just exit. While the thread group leader exit, it walk through the chain to accumulate the counters. This is simpler. Besides, the strace_info of dead thread is safe. No one will touch it.

Yeh! Let us do this tomorrow. We will have a robust kernel version strace.

### SM Heartbeat
Continue run some experiments on yesterday's case.

One we sure is SM will keep sending requests to HCA. And it looks like it does not send in a very deterministic interval:
```c
[ 1224.034898] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 15
[ 1224.130616] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 15
[ 1224.222189] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 16
[ 1224.417181] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 16

[ 1393.159845] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 17
[ 1393.255546] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 17
[ 1393.347132] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 18
[ 1393.538972] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 18

[ 1449.437542] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 19
[ 1449.533248] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 19
[ 1449.624833] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 20
[ 1449.722512] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 20

[ 4322.423624] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 21
[ 4322.519328] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 21
[ 4322.610914] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 22
[ 4322.708594] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 22
[ 4350.750574] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 23
[ 4350.846278] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 23
[ 4350.937863] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 24
[ 4351.035543] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 24

[ 4519.690559] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 25
[ 4519.786262] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 25
[ 4519.877848] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 26
[ 4519.975527] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 26

[ 4576.396279] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 27
[ 4576.491979] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 27
[ 4576.583565] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 28
[ 4576.681245] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 28

[ 4942.886820] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 29
[ 4942.982523] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 29
[ 4943.074108] ib_mad_completion_handler 2344 got successful recv cq op 128 mad_got_one 30
[ 4943.171789] ib_mad_completion_handler 2338 got successful send cq op 0 mad_got_one 30
```

## 04/03 Tue

### BUG BUG BUG
Finished basic replication mechanism last night.

Today merged several patches. And both Yilun and I think there is something wrong with `ib_mad_completion_handler`. It seems it will break things behind our back.

This is one bug catched today:

#### ib_mad_completion_handler
```c
At very early stage:

[ 1174.406177] newpid: 20 home:1 replica: 1
[ 1174.452983] p2m_fork(cpu10): I cur:20-exe.o new:21
[ 1177.462795] ib_mad_completion_handler 2324 got successful recv cq op 128 mad_got_one 22
[ 1177.556502] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020
[ 1177.650101] IP: [<ffffffff81059104>] ib_mad_completion_handler+0xb4/0x8a0

./scripts/faddr2line vmImage  ib_mad_completion_handler+0xb4
ib_mad_completion_handler+0xb4/0x899:
ib_mad_recv_done_handler at drivers/infiniband/core/mad.c:1899
 (inlined by) ib_mad_completion_handler at drivers/infiniband/core/mad.c:2325

ib_mad_recv_done_handler():
1899: qp_info = mad_list->mad_queue->qp_info;

```

A more scared one after I changed ib_mad_completion_handler. Note that recvcq is the only thread running on cpu4:
```c
[  863.887705] p2m_fork(cpu10): I cur:20-exe.o new:21
[  868.478424] p2m_fork(cpu10): O succeed cur:20-exe.o new:21
[  868.541991] BUG: unable to handle kernel NULL pointer dereference at 0000000000000008
[  868.635569] IP: [<ffffffff810656d4>] __schedule+0x94/0x1e0
[  868.701090] PGD 0
[  868.725010] general protection fault: 0000 [#1] SMP PROCESSOR
[  868.793651] CPU: 4 PID: 17 Comm: recvpollcq 4.0.0-lego-ys+ #737

Source:
clear_tsk_need_resched(prev);
```

Even this one for Phoenix:
```c
[  763.442043] BUG: unable to handle kernel NULL pointer dereference at 0000000000000010
[  763.535636] IP: [<ffffffff81018d6f>] task_curr+0xf/0x30
[  763.598035] PGD 103e956067 PUD 103e964067 PMD 0
[  763.653154] Oops: 0000 [#1] SMP PROCESSOR
[  763.700992] CPU: 12 PID: 21 Comm: word_count-pthr 4.0.0-lego-ys+ #740
[  763.777950] RIP: 0010:[<ffffffff81018d6f>]  [<ffffffff81018d6f>] task_curr+0xf/0x30
```

This NEVER happen before. And this part of code should be correct. We've ran a
lot things.. I doubt if recent IB merge corrupt things.


#### fit_poll_cq
Another one:
```c
[  690.401626] stat: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_1GB.txt
[  690.507742] SYSC_close() CPU12 PID:21 [fd: 4] -> [/sys/devices/system/cpu/online]
[  713.899884] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 21
[  713.995606] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 21
[  714.087185] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 22
[  714.184871] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 22
[  742.078102] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 23
[  742.173810] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 23
[  742.265399] ib_mad_completion_handler 2337 got successful recv cq op 128 mad_got_one 24
[  742.363085] ib_mad_completion_handler 2331 got successful send cq op 0 mad_got_one 24
[  847.063372] mlx4_ib_handle_error_cqe syndrome 21
[  847.116511] mlx4_ib_handle_error_cqe syndrome 5
[  847.170590] send request failed at connection 7 as 12
[  847.230909] mlx4_ib_handle_error_cqe syndrome 5
[  847.284988] mlx4_ib_handle_error_cqe syndrome 5
[  847.339067] mlx4_ib_handle_error_cqe syndrome 5
[  847.393146] fit_poll_cq: failed status (5) for wr_id 1832
[  847.457624] fit_poll_cq: failed status (5) for wr_id 1833
[  847.522103] fit_poll_cq: connection 7 Recv weird event as -1
[  847.589701] fit_poll_cq: failed status (5) for wr_id 1834
[  847.654179] fit_poll_cq: connection 7 Recv weird event as -30704
[  847.725938] fit_poll_cq: failed status (5) for wr_id 1835
[  847.790416] fit_poll_cq: connection 7 Recv weird event as -30704
[  847.862174] mlx4_ib_handle_error_cqe syndrome 5
[  847.916252] mlx4_ib_handle_error_cqe syndrome 5
[  847.970331] mlx4_ib_handle_error_cqe syndrome 5
[  848.024410] mlx4_ib_handle_error_cqe syndrome 5
[  848.078490] fit_poll_cq: failed status (5) for wr_id 1836
[  848.142967] fit_poll_cq: failed status (5) for wr_id 1837
[  848.207446] fit_poll_cq: connection 7 Recv weird event as -1
[  848.275044] fit_poll_cq: failed status (5) for wr_id 1838
[  848.339523] fit_poll_cq: connection 7 Recv weird event as -30704
[  848.411281] fit_poll_cq: failed status (5) for wr_id 1839
[  848.475760] fit_poll_cq: connection 7 Recv weird event as -30704
[  848.547517] mlx4_ib_handle_error_cqe syndrome 5
[  848.601596] mlx4_ib_handle_error_cqe syndrome 5
[  848.655675] mlx4_ib_handle_error_cqe syndrome 5
[  848.709753] mlx4_ib_handle_error_cqe syndrome 5
[  848.763832] fit_poll_cq: failed status (5) for wr_id 1840

[  848.828313] BUG: unable to handle kernel NULL pointer dereference at           (null)
[  848.921908] IP: [<ffffffff8106346d>] fit_poll_cq+0x4ad/0x510
[  848.989507] PGD 0
[  849.013426] Oops: 0002 [#1] SMP PROCESSOR
[  849.061265] CPU: 4 PID: 17 Comm: recvpollcq 4.0.0-lego-ys+ #744
[  849.131983] RIP: 0010:[<ffffffff8106346d>]  [<ffffffff8106346d>] fit_poll_cq+0x4ad/0x510
[  849.228700] RSP: 0000:ffff88103e813d88  EFLAGS: 00010246
[  849.292139] RAX: 0000000000001008 RBX: ffff88103effbad0 RCX: 0000000000000000
[  849.377418] RDX: 0000000000000000 RSI: ffffffff811d46e0 RDI: ffffffff811dbc08
[  849.462695] RBP: ffff88103e813ea8 R08: 0000000000000000 R09: 0000000000000000
[  849.547973] R10: 0000000000000002 R11: 0000000000000004 R12: 0000000000000000
[  849.633251] R13: ffff88103e801008 R14: 0000000000000004 R15: ffff88103e813da0
[  849.718529] FS:  0000000000000000(0000) GS:ffff88107fc40000(0000) knlGS:0000000000000000
[  849.815246] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  849.883884] CR2: 0000000000000000 CR3: 000000000113d000 CR4: 00000000000406a0
[  849.969163] Stack:
[  849.993082] ffffffff81003299 000001b03e813da0 0000000000000004 0000000000000730
[  850.080440] 0000008100000005 00001008000000f9 ffff88103eff8c50 002c222040000000
[  850.167798] 0010004000000002 ffff88107fc20000 0000000000000731 ffffffff00000005
[  850.255156] ffff8810000000f9 ffff88103eff8c50 0000000000000000 ffff88103e813e38
[  850.342513] ffffffff81019854 0000000000000732 ffff881000000005 ffff8810000000f9
[  850.429871] Call Trace:
[  850.458992] <TSK>
[  850.481870] [<ffffffff81003299>] ? native_smp_send_reschedule+0x39/0x50
[  850.560909] [<ffffffff81019854>] ? try_to_wake_up+0xe4/0x1f0
[  850.628506] [<ffffffff81065708>] ? __schedule+0xf8/0x1e0
[  850.691945] [<ffffffff810634d0>] ? fit_poll_cq+0x510/0x510
[  850.757464] [<ffffffff810634e4>] fit_poll_cq_pass+0x14/0x30
[  850.824021] [<ffffffff81020636>] kthread+0xf6/0x120
[  850.882260] [<ffffffff81020540>] ? __kthread_parkme+0x70/0x70
[  850.950898] [<ffffffff8100e572>] ret_from_fork+0x22/0x30

/* handle normal reply */
...
memcpy((void *)ctx->reply_ready_indicators[reply_indicator_index], &length, sizeof(int));
...
(This is a bad memcpy: reply_indicator_index, ctx, etc should be checked.)
```

### IB Spec: QP, CQE, WQE, SEND

The channel adapter detects the WQE posting and accesses the WQE.
The channel adapter interprets the command, validates the WQE’s virtual 12
addresses, translates it to physical addresses, and accesses the data.
The outgoing message buffer is split into one or more packets. To each packet the channel adapter adds a transport header (sequence numbers, opcode, etc.). If the destination resides on a remote subnet the channel adapter adds a network header (source & destination GIDs). The channel adapter then adds the local route header and calculates both the variant
and invariant checksums.

For a Send operation, the QP retrieves the address of
the receive buffer from the next WQE on its receive queue, translates it to physical addresses, and accesses memory writing the data. If this is not
the last packet of the message, the QP saves the current write location in 38 its context and waits for the next packet at which time it continues writing
the receive buffer until it receives a packet that indicates it is the last packet of the operation. It then updates the receive WQE, retires it, and sends an acknowledge message to the originator.

When the originator receives an acknowledgment, it creates a CQE on the 5
CQ and retires the WQE from the send queue.

A QP can have multiple outstanding messages at any one time but the 8
target always acknowledges in the order sent, thus WQEs are retired in the order that they are posted.

## 04/02 Mon

Patching storage replica handler, able to finish today.

## 04/01 Sun

Anyway. Summary of the day: replication at M almost done. Only flush part left. Storage also need a handler. But we still need code to recover.

I'm tired. :-( A month to go.

Record a IB error. Using wuklab12 (P) and wuklab14(M+RAMFS), running usr/pcache_conflic.o:
```c
P
[30801.296160] ibapi_send_reply() CPU:8 PID:19 timeout (30010 ms), caller: clflush_one+0x1c9/0x370
[30938.564843] mlx4_ib_handle_error_cqe syndrome 21
[30938.617988] mlx4_ib_handle_error_cqe syndrome 5
[30938.672068] send request failed at connection 6 as 12
[30938.732389] mlx4_ib_handle_error_cqe syndrome 5
[30938.786470] mlx4_ib_handle_error_cqe syndrome 5
[30938.840551] mlx4_ib_handle_error_cqe syndrome 5
[30938.894632] fit_poll_cq: failed status (5) for wr_id 1584
[30938.959112] fit_poll_cq: failed status (5) for wr_id 1585
[30939.023593] fit_poll_cq: connection 6 Recv weird event as -1
[30939.091194] fit_poll_cq: failed status (5) for wr_id 1586
[30939.155676] fit_poll_cq: connection 6 Recv weird event as -30704
[30939.227436] fit_poll_cq: failed status (5) for wr_id 1587
[30939.291917] fit_poll_cq: connection 6 Recv weird event as -30704
[30939.363678] mlx4_ib_handle_error_cqe syndrome 5
[30939.417759] mlx4_ib_handle_error_cqe syndrome 5
[30939.471839] mlx4_ib_handle_error_cqe syndrome 5
[30939.525921] mlx4_ib_handle_error_cqe syndrome 5
[30939.580002] fit_poll_cq: failed status (5) for wr_id 1588
[30939.644483] BUG: unable to handle kernel NULL pointer dereference at           (null)
[30939.738083] IP: [<ffffffff81062fcd>] fit_poll_cq+0x4ad/0x510
[30939.805684] PGD 0
[30939.829604] Oops: 0002 [#1] SMP PROCESSOR
[30939.877445] CPU: 4 PID: 17 Comm: recvpollcq 4.0.0-lego-ys+ #715
[30939.948166] RIP: 0010:[<ffffffff81062fcd>]  [<ffffffff81062fcd>] fit_poll_cq+0x4ad/0x510

fit_poll_cq at net/lego/fit_internal.c:1734
memcpy((void *)ctx->reply_ready_indicators[reply_indicator_index], &length, sizeof(int));

M
[30913.642698] mlx4_ib_handle_error_cqe syndrome 21
[30913.695839] mlx4_ib_handle_error_cqe syndrome 5
[30913.749919] send request failed at connection 1 as 12
[30913.810236] mlx4_ib_handle_error_cqe syndrome 5
[30913.864315] mlx4_ib_handle_error_cqe syndrome 5
[30913.918395] mlx4_ib_handle_error_cqe syndrome 5
[30913.972474] fit_poll_cq: failed status (5) for wr_id 305
[30914.035912] fit_poll_cq: failed status (5) for wr_id 306
```
