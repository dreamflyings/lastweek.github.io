# Feb 2018

---
## 02/14 Wed Rainy

---
## 02/13 Tue Sunny
Checking our SLOB allocator today. So I found Yutong's code is using `set_page_private` when slob get a new page from buddy. This private field is only intended to be used by buddy to record the `order`. This mixed usage will confuse buddy and create bug.

Even though I removed the `:::c set_page_private(page, 0)` after `free_page`, word_count-pthread still fails. Damn.

---
## 02/12 Mon Cloudy
Add this commit `4cb3a8b6a943c90714fd9bb5e5465ee315f0aa30`:
```
    memory: Use kzalloc instead of kmalloc in __bprm_mm_init (loader)

    This was an potentionl bug that was not triggered previously.
    It is simply because kmalloc'ed vma contains some garbage area,
    while later in the pgfault code, we use
            if (vma->vm_ops && vma->vm_ops->fault)
                    ...
    to check if it is an file-backed fault.

    Fortunately the vma->vm_ops happens to have some leftover value.
    So this bug was triggered.

    This actually reminds me that this is a series of potential bugs!
    Even though before I've added things like force GFP_ZERO in all
    physical page allocation, I missed the kmalloc's case!
```

The story is:

I patched the stop_machine code today, and tried to run code with P+M on VM, everything works fine. However, when I tried to run the new code with P+M+S on physical machine, M crashed at a very weird point:
``` hl_lines="7  12 13 14 15 16"
[ 7791.998168] handle_p2m_execve(): pid:81,argc:2,envc:2,file:/root/ys/phoenix/phoenix-2.0/tests/word_count/word_count-pthread
[ 7792.129312] BUG: unable to handle kernel NULL pointer dereference at 0000000000000031
[ 7792.222889] IP: [<ffffffff8102c180>] handle_lego_mm_fault+0x160/0x4b0
[ 7792.299842] PGD 0
[ 7792.323760] Oops: 0000 [#1] PREEMPT SMP MEMORY
[ 7792.376794] CPU: 4 PID: 79 Comm: mc-manager 4.0.0-lego+ #29
[ 7792.443349] RIP: .. [<ffffffff8102c180>] handle_lego_mm_fault+0x160/0x4b0
......
....
[ 7793.750506] Call Trace:
[ 7793.779623] <TSK>
[ 7793.802501] [<ffffffff810053f4>] ? apic_timer_interrupt+0x54/0x90
[ 7793.875295] [<ffffffff8102e469>] faultin_page+0x9/0x70
[ 7793.936649] [<ffffffff8102ef01>] copy_strings.isra.1+0xe1/0x130
[ 7794.007362] [<ffffffff8102f11e>] exec_loader+0x1ce/0x340
[ 7794.070796] [<ffffffff81027def>] handle_p2m_execve+0x12f/0x200
[ 7794.140469] [<ffffffff810274fb>] mc_manager+0x1ab/0x2b0
[ 7794.202864] [<ffffffff81027350>] ? bitmap_fill+0x33/0x33
[ 7794.266298] [<ffffffff8101c6b7>] kthread+0x107/0x130
[ 7794.325572] [<ffffffff8101c5b0>] ? __kthread_parkme+0x90/0x90
[ 7794.394205] [<ffffffff8100b462>] ret_from_fork+0x22/0x30
```

So faulting source code is:
``` c hl_lines="5 6"
static int handle_pte_fault(struct vm_area_struct *vma, unsigned long address,
                            unsigned int flags, pte_t *pte, pmd_t *pmd)
{
	....
        if (vma->vm_ops && vma->vm_ops->fault)
                return do_linear_fault(vma, address, flags,
                                       pte, pmd, entry)
	....
```

Something wrong with `vma`? At this loader stage, this vma is a temporaty stack vma created for saving `argv` and `envp`. So I look back into the code that created this vma:
``` c hl_lines="8"
managers/memory/loader/core.c:
static int __bprm_mm_init(struct lego_binprm *bprm)
{
        int err;
        struct vm_area_struct *vma = NULL;
        struct lego_mm_struct *mm = bprm->mm;

        bprm->vma = vma = kmalloc(sizeof(*vma), GFP_KERNEL);
        if (!vma)
                return -ENOMEM;
```

The code after this does NOT do necessary cleanup. The `vm_ops` happens to have some garbage value from last user. So it is not 0, so the above `vma->vm_ops` is true, and it will try to read `vma->vm_ops->fault`. And that, my friend, is where garbage turns into crash.

This presents a series of potential bugs. Ugh, `memory safety`!

---
## `02/09 Fri Cloudy`
Tried to modify Phoneix code: replace `realloc` with `malloc+mempcy`. Thus the `mremap` syscall is avoided, but it still has general protection fault. Same with yesterday, corrupted at `__strcmp_sse42`, with corrupted `RSI` or `RDI`. So I guess it is not about `mremap` itself at all. I will follow yesterday's checking list.

---
## `02/08 Thur Cloudy`

```
00000000004272d0 <__strcmp_sse42>:

  4272d0:       89 f1                   mov    %esi,%ecx
  4272d2:       89 f8                   mov    %edi,%eax
  4272d4:       48 83 e1 3f             and    $0x3f,%rcx
  4272d8:       48 83 e0 3f             and    $0x3f,%rax
  4272dc:       83 f9 30                cmp    $0x30,%ecx
  4272df:       77 3f                   ja     427320 <__strcmp_sse42+0x50>
  4272e1:       83 f8 30                cmp    $0x30,%eax
  4272e4:       77 3a                   ja     427320 <__strcmp_sse42+0x50>
  4272e6:       f3 0f 6f 0f             movdqu (%rdi),%xmm1
* 4272ea:       f3 0f 6f 16             movdqu (%rsi),%xmm2
  4272ee:       66 0f ef c0             pxor   %xmm0,%xmm0
  4272f2:       66 0f 74 c1             pcmpeqb %xmm1,%xmm0
  4272f6:       66 0f 74 ca             pcmpeqb %xmm2,%xmm1
  4272fa:       66 0f f8 c8             psubb  %xmm0,%xmm1
  4272fe:       66 0f d7 d1             pmovmskb %xmm1,%edx
  427302:       81 ea ff ff 00 00       sub    $0xffff,%edx
  427308:       0f 85 42 0d 00 00       jne    428050 <__strcmp_sse42+0xd80>
  42730e:       48 83 c6 10             add    $0x10,%rsi
  427312:       48 83 c7 10             add    $0x10,%rdi
  427316:       66 2e 0f 1f 84 00 00    nopw   %cs:0x0(%rax,%rax,1)
  42731d:       00 00 00  
  427320:       48 83 e6 f0             and    $0xfffffffffffffff0,%rsi
  427324:       48 83 e7 f0             and    $0xfffffffffffffff0,%rdi
  427328:       ba ff ff 00 00          mov    $0xffff,%edx
  42732d:       45 31 c0                xor    %r8d,%r8d
  427330:       83 e1 0f                and    $0xf,%ecx
  427333:       83 e0 0f                and    $0xf,%eax
  427336:       66 0f ef c0             pxor   %xmm0,%xmm0
  42733a:       39 c1                   cmp    %eax,%ecx
  42733c:       74 32                   je     427370 <__strcmp_sse42+0xa0>
  42733e:       77 07                   ja     427347 <__strcmp_sse42+0x77>
  427340:       41 89 d0                mov    %edx,%r8d
  427343:       91                      xchg   %eax,%ecx
  427344:       48 87 f7                xchg   %rsi,%rdi
* 427347:       66 0f 6f 17             movdqa (%rdi),%xmm2
  (RDI: 0000000000000000)
```

Frustrating! What is wrong with multithread program? Because of broken FPU-switch code? of inappropriate TLB flush? of IB corrupts memory? of what? ugh?

I'm done with this random guess and frustrated general protection or segfault, I need to first make sure underlying kernel is 100%  percent correct, this is a checking list:

- fpu save/restore
    - always fail at some XMM instruction
    - always with corrupted RDI or RSI
- switch_to_asm
    - %gs and %fs
    - switch_mm (pgd)
    - stack frame
- set_arch_tls (%fs)
    - glibc's way of using per thread data
- some cpu may miss tlb flush
- kernel entry/exit assembly
    - current_task macro
    - stack_stratch
    - per-cpu data in entry.S
- futex
     - clear_tid
     - set_tid
     - shared mm
     - robust list
- interrupts
     - vector array
     - APIC setup
     - IO-APIC
     - timer interrupt
- cpu_init and Trampoline
- faked kernel version
- P side pgfault handling code (SMP)
- and M side pgfault handling (SMP)
- mremap, munmap
    - check pgtable boundary
- In all, check SMP implications

Is there any code, that is solely used to test if the underlying kernel has appropriate behaviors? Like glibc test code?

How to protect kernel virtual memory? Any existing solutions in Linux?

What is the implication of multiple CPU entering kernel at the same time? How can it corrupt user pages? Maybe: kernel entry code, per-cpu data in entry code, fpu code, switch_to, scheduler.

Why it always fail at those FPU code i.e. the strcmp function? I failed to compile without those sse, any solution? How it hurt performance?

---
## `02/07 Wed Cloudy`
`20:07`  
Pushed a small patch on mremap issue. Hope it will work. mremap really makes the whole thing very interesting, will be a very good research finding on combing virtual cache and operating system. Need to go gym with a friend, will be back on debugging late tonight.

`9:30`  
Have two meetings to do today, and an security class, won't have too much time coding during daytime.

---
## `02/06 Tue Sunny`
Well. We've ruled out both `smp_call_function` and `workqueue` yesterday with Yiying's help. But the multi-thread word-count still fails `:-(` Single thread word-count just finished 4GB dataset (with 8GB pcache). So what could be still wrong with multithread one????

 - chill
 - check exit code
 - `(Checked)` check pcache's usage of task_struct, should always use the group_leader
 - check cpu boot code and check the switch code again
 - I believe pinpoint the issue in multithread word-count can solve a lot issues, it must be some thread creation, removal, schedule things.
 - How about adding a lock for ibapi, make it sequential? Sweet, I tried, finally it is `a bug that we are able to debug`.

`22:39`  
Done for today. I'm trying to patch `move_pte` and `pcache_move_pte`. Although in theory we defenitly need to patch it, I keep thinking the code before should not trigger any serious bus or memory corruption. Ugh. Maybe it is concurrent `mremap` that one of them remap from A to B, while another one remap from C to A. It is possible. But my dead brain can not think of this anymore. I'm going to hit the gym and do some squats.

`17:01`  
Criminal found: `mremap()` and `virtual cache` did the crime. Interesting, I have not seen any research paper, tech-reports, writeup, code about this, not even the OVC paper, which, by the way, I think they must consider this case. Otherwise, a mremap will simply crash its virtual cache. Many thanks went to my smoke-and-think time.

`15:14`  
Something new came up! After adding a spinlock for ibapi, this showed up (I tried one more time after this, which does not show up). We are lucky to catch this. At least I know where to look at. Also, this is defenitly triggered by `mremap`. It is seems it is overlapped `mremap()`. One thing I did not know is which thread trigger this bug, the sweep thread? Cause mremap related pcache rmap functions do not use `rmap_get_locked_pte`.

```
[ 3826.048774] normal_p2s_open(): f_name: word_100MB.txt, mode: 04400, flags: 0
[ 3827.891622] SYSC_mremap(cpu18): move: [0x7fffe5788000 - 0x7fffe5806000] -> [0x7fffe531b000 - 0x7fffe5399000]
[ 3828.178643] SYSC_mremap(cpu14): move: [0x7fffe5941000 - 0x7fffe5980000] -> [0x7fffe57c7000 - 0x7fffe5806000]

****    ERROR: mismatched PTE and rmap
****    rmap->owner_process: word_count-pthr uva: 0x7fffe57c8000 ptep: ffff88107efe0e40, rmap->page_table: ffff88107efe0e40
****    pcache_pfn: 0x1257c8, pte_pfn: 0x125942
```

`14:00`   
`word_count-pthread`: 100MB dataset
`pcache`: 8GB, 8-way
`victim`: 8 entries
```
[ 1294.845313] STDOUT: ---[
Wordcount: Running...
]---
[ 1294.903661] STDOUT: ---[

o;
]---
[ 1294.946301] normal_p2s_open(): f_name: /root/ys/phoenix/phoenix-2.0/tests/word_count/word_count_datafiles/word_100MB.txt, mode: 04400, flags: 0
[ 1295.100517] SYSC_close(): [4] -> [/sys/devices/system/cpu/online]
[ 1295.594658] word_count-pthr[59] general protection ip:4272ea sp:7ffff1b8ed28 error:0
[ 1295.685236] CPU: 10 PID: 59 Comm: word_count-pthr 4.0.0-lego+ #113
[ 1295.759070] RIP: 0033:[<00000000004272ea>]  [<00000000004272ea>] 0x4272ea
[ 1295.840184] RSP: 002b:00007ffff1b8ed28  EFLAGS: 00010283
[ 1295.903621] RAX: 000000000000000f RBX: 00007fffe5a3d010 RCX: 0000000000000001
[ 1295.988893] RDX: 0000000000000000 RSI: 4854005942004441 RDI: 00007ffff1c1e80f
[ 1296.074166] RBP: 00007ffff1c1e80f R08: 0000000000000000 R09: 0000000000000010
[ 1296.211435] R10: 0000000000427ce0 R11: 00007ffff1bbb3ba R12: 0000000000001de4
[ 1296.296711] R13: 00000000006e4a80 R14: 0000000000001d9e R15: 0000000000001dc1
[ 1296.433978] FS:  00007ffff1b8f700(0000) GS:ffff88107fca0000(0000) knlGS:0000000000000000
[ 1296.582686] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[ 1296.963297] CR2: 00007ffff1c1e000 CR3: 000000207fd8a000 CR4: 00000000000406a0
```
So what is this `ip:4272ea`, let us objdump the binary:
```
0000000000425e60 <strcmp>:
  425e60:       48 8d 05 69 14 00 00    lea    0x1469(%rip),%rax        # 4272d0 <__strcmp_sse42>
  425e67:       f7 05 5f b8 2b 00 00    testl  $0x100000,0x2bb85f(%rip)        # 6e16d0 <_dl_x86_cpu_features+0x10>
  425e6e:       00 10 00
  425e71:       75 1a                   jne    425e8d <strcmp+0x2d>
  425e73:       48 8d 05 46 b0 00 00    lea    0xb046(%rip),%rax        # 430ec0 <__strcmp_ssse3>
  425e7a:       f7 05 4c b8 2b 00 00    testl  $0x200,0x2bb84c(%rip)        # 6e16d0 <_dl_x86_cpu_features+0x10>
  425e81:       02 00 00
  425e84:       75 07                   jne    425e8d <strcmp+0x2d>
  425e86:       48 8d 05 03 00 00 00    lea    0x3(%rip),%rax        # 425e90 <__GI_strcmp>
  425e8d:       c3                      retq
  425e8e:       66 90                   xchg   %ax,%ax
 .. ..
 .. ..
00000000004272d0 <__strcmp_sse42>:
  4272d0:       89 f1                   mov    %esi,%ecx
  4272d2:       89 f8                   mov    %edi,%eax
  4272d4:       48 83 e1 3f             and    $0x3f,%rcx
  4272d8:       48 83 e0 3f             and    $0x3f,%rax
  4272dc:       83 f9 30                cmp    $0x30,%ecx
  4272df:       77 3f                   ja     427320 <__strcmp_sse42+0x50>
  4272e1:       83 f8 30                cmp    $0x30,%eax
  4272e4:       77 3a                   ja     427320 <__strcmp_sse42+0x50>
  4272e6:       f3 0f 6f 0f             movdqu (%rdi),%xmm1
* 4272ea:       f3 0f 6f 16             movdqu (%rsi),%xmm2
  4272ee:       66 0f ef c0             pxor   %xmm0,%xmm0
```
You can see `%rsi` has some garbage value `RSI: 4854005942004441`. Something went wrong. Will it be our FPU? I'm not quite sure. If FPU code has error, why single-thread one succeed? Why it only shows up at multithread ones?


---
## `02/05 Mon Sunny`
From yesterday's testing of Phoenix, it looks like something is wrong in `smp_call_functions()`. They are invoked through `tlb flush`, which was further invoked by `mremap`, or `munmap`. The warning from smp is:

```C
[ 1260.586696] WARNING: CPU: 0 PID: 73 at kernel/smp.c:129 generic_smp_call_function_single_interrupt+0xb8/0x160
[ 1260.705251] CPU: 0 PID: 73 Comm: word_count-pthr 4.0.0-lego+ #99
[ 1260.777008] Stack:
[ 1260.800927] ffff88207fdffef8 ffffffff8100ec67 ffff88107fc00000 ffff88107fc00000
[ 1260.888283] ffffffff8100d410 ffff88207fe23df0 ffff88207fdfff08 ffffffff8100ed5f
[ 1260.975639] ffff88207fdfff38 ffffffff8100fe68 00007fffe58c3010 0000000000000f96
[ 1261.062995] 000000000000f960 0000000000000f95 ffff88207fdfff48 ffffffff810020dd
[ 1261.150351] 00007ffff58869c1 ffffffff8100b2e9 0000000000000f96 0000000000000f95
[ 1261.237707] Call Trace:
[ 1261.266825] <TSK>
[ 1261.289704] [<ffffffff8100ec76>] __warn.constprop.0+0xa6/0x100
[ 1261.359381] [<ffffffff8100d410>] ? pgd_free+0x90/0x90
[ 1261.419699] [<ffffffff8100ed5f>] warn_slowpath_null+0xf/0x20
[ 1261.487295] [<ffffffff8100fe68>] generic_smp_call_function_single_interrupt+0xb8/0x160
[ 1261.581931] [<ffffffff810020dd>] call_function_interrupt+0x1d/0x20
[ 1261.655767] [<ffffffff8100b2e9>] smp__call_function_interrupt+0x69/0x70
```

So I decided to look into smp.c a little bit to find out if there is something wrong (I wrote it long time ago). The warning itself is true, it means some inconsistent behavior.. I saw `alloc_percpu` stuff during `call_function_init`, hence probably I also need to check percpu code a little code cause I'm not sure if I port all the functionalities.

In all, today's task, check `percpu` and `smp_call_function` code. Esp, `percpu` code, they are crucial and very hard to relate real bugs to it.

Well... things changed. I found a more serious bug: something about `cpuhotplug`, even though lego is not using it. `cpuhotplug` is a set of implict callbacks to all different subsystems who want to do some initialization work on each `offline->online` cpu.

Let us dig into how secondary cpu boots:
```C
Trampoline.. setup 64bit mode
start_secondary()
  smp_callin()
        notify_cpu_starting()
              ...
              while (st->state < target) {
                      st->state++;
                      cpuhp_invoke_callback(cpu, st->state, true, NULL);
              }
          cpuhp_invoke_callback()
```

See? There will be some callbacks! What are those callbacks exactly? Well, they are predefined at the `kernel/cpu.c`. To save the trouble of reading code, I just print what functions are executed, the log is:
```
[    0.118235] cpuhp_invoke_callback(): 136  CPU:0  page_writeback_cpu_online+0x0/0x20

[    0.368478] cpuhp_invoke_callback(): 136  CPU:1  smpboot_create_threads+0x0/0x90
[    0.370196] cpuhp_invoke_callback(): 136  CPU:1  perf_event_init_cpu+0x0/0xa0
[    0.370403] cpuhp_invoke_callback(): 136  CPU:1  workqueue_prepare_cpu+0x0/0x80
[    0.371112] cpuhp_invoke_callback(): 136  CPU:1  hrtimers_prepare_cpu+0x0/0x60
[    0.371339] cpuhp_invoke_callback(): 136  CPU:1  smpcfd_prepare_cpu+0x0/0x80
[    0.371584] cpuhp_invoke_callback(): 136  CPU:1  relay_prepare_cpu+0x0/0xe0
[    0.371794] cpuhp_invoke_callback(): 136  CPU:1  rcutree_prepare_cpu+0x0/0x170
[    0.372333] cpuhp_invoke_callback(): 136  CPU:1  notify_prepare+0x0/0xa0
[    0.372744] cpuhp_invoke_callback(): 136  CPU:1  bringup_cpu+0x0/0x100
[    0.008000] cpuhp_invoke_callback(): 136  CPU:1  sched_cpu_starting+0x0/0x60
[    0.926124] cpuhp_invoke_callback(): 136  CPU:1  smpboot_unpark_threads+0x0/0x90
[    0.926124] cpuhp_invoke_callback(): 136  CPU:1  perf_event_init_cpu+0x0/0xa0
[    0.927028] cpuhp_invoke_callback(): 136  CPU:1  workqueue_online_cpu+0x0/0x2a0
[    0.927768] cpuhp_invoke_callback(): 136  CPU:1  rcutree_online_cpu+0x0/0x70
[    0.928045] cpuhp_invoke_callback(): 136  CPU:1  notify_online+0x0/0x20
[    0.928256] cpuhp_invoke_callback(): 136  CPU:1  page_writeback_cpu_online+0x0/0x20
[    0.928527] cpuhp_invoke_callback(): 136  CPU:1  sched_cpu_activate+0x0/0x190

[    0.929084] cpuhp_invoke_callback(): 136  CPU:2  smpboot_create_threads+0x0/0x90
[    0.930240] cpuhp_invoke_callback(): 136  CPU:2  perf_event_init_cpu+0x0/0xa0
[    0.930434] cpuhp_invoke_callback(): 136  CPU:2  workqueue_prepare_cpu+0x0/0x80
[    0.931070] cpuhp_invoke_callback(): 136  CPU:2  hrtimers_prepare_cpu+0x0/0x60
[    0.931264] cpuhp_invoke_callback(): 136  CPU:2  smpcfd_prepare_cpu+0x0/0x80
[    0.931464] cpuhp_invoke_callback(): 136  CPU:2  relay_prepare_cpu+0x0/0xe0
[    0.931649] cpuhp_invoke_callback(): 136  CPU:2  rcutree_prepare_cpu+0x0/0x170
[    0.932245] cpuhp_invoke_callback(): 136  CPU:2  notify_prepare+0x0/0xa0
[    0.932475] cpuhp_invoke_callback(): 136  CPU:2  bringup_cpu+0x0/0x100
[    0.008000] cpuhp_invoke_callback(): 136  CPU:2  sched_cpu_starting+0x0/0x60
[    1.005023] cpuhp_invoke_callback(): 136  CPU:2  smpboot_unpark_threads+0x0/0x90
[    1.005065] cpuhp_invoke_callback(): 136  CPU:2  perf_event_init_cpu+0x0/0xa0
[    1.005408] cpuhp_invoke_callback(): 136  CPU:2  workqueue_online_cpu+0x0/0x2a0
[    1.005729] cpuhp_invoke_callback(): 136  CPU:2  rcutree_online_cpu+0x0/0x70
[    1.006029] cpuhp_invoke_callback(): 136  CPU:2  notify_online+0x0/0x20
[    1.006206] cpuhp_invoke_callback(): 136  CPU:2  page_writeback_cpu_online+0x0/0x20
[    1.006549] cpuhp_invoke_callback(): 136  CPU:2  sched_cpu_activate+0x0/0x190
```

Interesting! Currently, Lego need to add the `smpboot_create_threads()`, `workqueue_prepare_cpu()`, `workqueue_prepare_cpu()`, `bringup_cpu()`, `smpboot_unpark_threads()`, `workqueue_online_cpu()`.

This hidden things is really hard to find and not easy to track during boot. Especially during boot, they should do something like `for_each_online_cpu` and init one by one. But I guess, after adding support of cpu hotplug, code kind of merged. Some stuff will be executed whenever a cpu has been teardown or bought up. And bang, why not use the same set of hotplug during boot, right?
Well.

---
