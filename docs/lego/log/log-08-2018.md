# Aug 2018

## Aug 26
Oh well. I saw the same damn lost packet issue again. The issue can be desribed as: P use lite rpc to send a request to M. M processed the handled, and called rpc reply to sent back to P. M need to poll send_cq to poll completion. But M fail to get the CQE for the should-be-sent-out WQE.

This is tested with M's `CONFIG_FIT_NOWAIT` optimization, which is basically an optimization that M will not poll cq every time a reply was sent out, instead, do batch polling.

The following stack dump was reported by M side watchdog. It is not necessary mlx4_poll_cq's issue, since there is a while (1) loop at fit code. Oh well.
```
Log name: 0826-w9-1

[187736.669027] watchdog: worker[0] CPU10 stucked
[187736.673972] watchdog:  common_header [op=0x30000000 src_nid:0]
[187736.680566] CPU: 10 PID: 20 Comm: thpool-worker0 4.0.0-lego+ #26
[187736.687351] RIP: 0010:[<ffffffff810522c3>]  [<ffffffff810522c3>] mlx4_ib_poll_cq+0x1d3/0x850
[187736.696854] RSP: 0000:ffff88103ef3f750  EFLAGS: 00000286
[187736.702865] RAX: 00000000fffffff5 RBX: 0000000000000000 RCX: ffff88103ed6b050
[187736.710913] RDX: 0000000080630000 RSI: 0000000000000001 RDI: ffff88103edb0bf0
[187736.718961] RBP: ffff88103ef3f7b8 R08: 0000000000000020 R09: 0000000000000002
[187736.727007] R10: 0000000ffc53fddc R11: 0000000040bf1040 R12: ffff88103ef3f7c8
[187736.735055] R13: 0000000000000000 R14: 0000000000000000 R15: ffff88103edb0bf0
[187736.743104] FS:  0000000000000000(0000) GS:ffff88107fca0000(0000) knlGS:0000000000000000
[187736.752218] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[187736.758714] CR2: 0000000000000000 CR3: 000000000116a000 CR4: 00000000000406a0
[187736.766762] Stack:
[187736.769089] 0000000ffc53fddc 0000000000000002 0000000000000020 ffff88103edb0c98
[187736.777331] 0000000000000286 0000000080630000 ffff88103ef3f7d0 0000638000000018
[187736.785572] ffff88103edb0bf0 0000000000000001 ffff88103ef25008 0000000000000003
[187736.793813] 000000000000000c ffff88103ef3fd30 ffffffff8106920c ffff88103ef3fd54
[187736.802054] 0000000100000000 0000000100000000 ffff88103edb07b0 ffff88103e81b008
[187736.810296] Call Trace:
[187736.813108] <TSK>
[187736.815338] [<ffffffff8106920c>] fit_internal_poll_sendcq+0x6c/0xe0
[187736.822416] [<ffffffff8106ab2f>] ? fit_send_reply_with_rdma_write_with_imm+0x25f/0x3a0
[187736.831336] [<ffffffff81033ff0>] ? _lego_copy_to_user+0x110/0x250
[187736.838220] [<ffffffff81028d65>] ? __free_pages+0x25/0x30
[187736.844329] [<ffffffff8102e981>] ? __storage_read+0xf1/0x120
[187736.850728] [<ffffffff81019865>] ? scheduler_tick+0x55/0x60
[187736.857031] [<ffffffff810693d2>] ? fit_send_message_with_rdma_write_with_imm_request+0x152/0x350
[187736.866920] [<ffffffff810693d2>] ? fit_send_message_with_rdma_write_with_imm_request+0x152/0x350
[187736.876810] [<ffffffff8103043f>] ? __vma_adjust+0x38f/0x550
[187736.883113] [<ffffffff81030944>] ? vma_merge+0x1a4/0x280
[187736.889123] [<ffffffff81030f20>] ? arch_get_unmapped_area_topdown+0xe0/0x220
[187736.897075] [<ffffffff810693d2>] fit_send_message_with_rdma_write_with_imm_request+0x152/0x350
[187736.906771] [<ffffffff81069ab5>] fit_ack_reply_callback+0x185/0x1e0
[187736.913848] [<ffffffff8102f129>] ? handle_p2m_flush_one+0x69/0x160
[187736.920830] [<ffffffff8102bde0>] thpool_worker_func+0xe0/0x3a0
[187736.927424] [<ffffffff8102bd00>] ? handle_bad_request+0x40/0x40
[187736.934113] [<ffffffff81020ca6>] kthread+0xf6/0x120
[187736.939639] [<ffffffff81020bb0>] ? __kthread_parkme+0x70/0x70
[187736.946137] [<ffffffff8100e632>] ret_from_fork+0x22/0x30
```

## Aug 22

Damn it!!! After so much effort verifying we had a solid IB stack, we still has memory corruption and deadlock issues. Fuck!

One thing at a time, simple stuff first. Okay, tomorrow first add DEBUG_SPINLOCK to detect possible deadlocks. This, could help to identify some buggy code. After this, I will spend some time looking into the LITE, it's fucking HEAVY. I do found a lot issues during summer.

Personally, I'm not feeling good this days. I treat someone with love and respect, but there is not too much in return. Yeahyeahyeah, I know how this works. It's just sad that sometimes you just have a BAD timing. I've went through too much things in 2018, good and bad. I care sooo much about the people I love, family and others. I feel this is good, of course. Anyway, it is supposed to be a Lego dump, that no one probably interested in.
