# Practical Cache Coherence Implementation

??? note "Version History"
	|Date|Description|
	|:---|-----------|
	|Feb 24, 2020| Kobe and Gigi. Add Intel CCIP. |
	|Oct 3, 2019| Add FPGA related discussion|
	|Jun 28, 2019| Initial draft|

:sake:

- [Practical Cache Coherence](#practical-cache-coherence)
	- [Summary and Thoughs](#summary-and-thoughs)
	- [Readings](#readings)
	- [Misc Facts](#misc-facts)
	- [Case Study](#case-study)
		- [Intel](#intel)
		- [AMD](#amd)
		- [ARM](#arm)
		- [OpenCAPI and CCIX](#opencapi-and-ccix)
		- [OpenPiton](#openpiton)
		- [FPGA](#fpga)

A general collection of resources on cache coherence.
I started this when I was having a hard time optimizing lock delegation.
This note is not about acadamic new ideas, but rather for
a concrete understanding of current cache coherence implementations.

## Summary and Thoughs

- The textbooks tough us the basic concept of MESI. And realizations
  like snoop and directory. But what usually missing is the implementation
  details when it comes to: 1) conflicts, 2) no single shared bus.
- Modern processors have Network-on-Chip (NoC). Cores, cache slices,
  and memory controllers are connected via an on-chip network.
  The model is no different from a datacenter cluster connected by real network.
- Cache requests generated by MESI protocols should appear _atomic_ to cores.
  Given the distributed nature of all resources, those cache requests
  will have to be implemented like __distributed transactions__.
- For example, the MESIF is the cache coherence protocol used by Intel.
  When a read is made to an invalid line, the corresponding cache
  will perform a _cache read transaction_ to read the data from
  either other caches or memory. This transaction consists multiple
  steps such as: send requests, collect responses, send ACKs.
- Those transactions will conflict if multiple reads and writes
  happen at the same time. Someone has to resolve it.
  It can be resolved by different cache controllers, or by a single
  serialization point like home agent.
- Just like you can have many ways to implement transactions
  for distributed systems, there are also many ways to do
  cache coherence transactions. And there are many.
- Atomic Read-Modify-Write (RMW) instructions will make cache coherence
  implementations even more complex. Those instructions include
  `read-and-inc`, `test-and-set`, and `lock; `-prefixed.
  I think, there will some "lock the bus", or "locked state" at the
  home agent per cache line. Having atomic RMW instructions
  will add more complexity to the overall transaction design.
- While reading Intel related cache coherence diagrams/transactions,
  you might find many different descriptions. Don't panic. They are
  just different implementations proposed by Intel. Different
  implementations will have different trade-offs and performance,
  you can check [Frank's post](https://frankdenneman.nl/2016/07/11/numa-deep-dive-part-3-cache-coherency/)
  for more details.
- Directory-based cache coherence protocol and implementation will
  be the future for multicore machines. Because it incurs much less
  coherence traffic than snoop-based ones, thus more scalable.
  The trend is confirmed by recent Intel UPI directory-based approach.
  Related readings:
    - [1]: [Why On-Chip Cache Coherence Is Here to Stay](http://www.cis.upenn.edu/acg/papers/cacm12_why_coherence.pdf)
    - [2]: [QPI 1.1 Invovled](https://www.realworldtech.com/qpi-evolved/3/)
    - [3]: [Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA '99](http://research.cs.wisc.edu/multifacet/papers/isca99_multicast_talk_pdf.pdf)
    - [4]: [Paper: Using Destination-Set Prediction to Improve the Latency/Bandwidth Tradeoff in Shared-Memory Multiprocessors, ISCA'03](https://www.cis.upenn.edu/~milom/papers/isca03_destination_set_prediction.pdf)
    - [5]: The trade-off: ![img_1](cache_coherence_img1.png)

Left questions:
- Do cache coherence implementations ensure __fairness__ among cores?

## Readings

- [The Architecture of the Nehalem Processor and Nehalem-EP SMP Platforms](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.455.4198&rep=rep1&type=pdf), chapter 5.2 Cache-Coherence Protocol for Multi-Processors.
- [Intel: Performance Analysis Guide for Intel® Core™ i7 Processor and Intel® Xeon™ 5500 processors](https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf)
- [Blog: NUMA Deep Dive Part 3: Cache Coherency](https://frankdenneman.nl/2016/07/11/numa-deep-dive-part-3-cache-coherency/)
    - By far the BEST blog I've seen on the topic of `Intel snoop models`. Frank's other articles are also amazing.
    - Intel is using MESIF cache coherence protocol, but it has multiple cache coherence implementations.
      The first one is `Source Snoop` (or `Early Snoop`), which is more like a traditional snoop-based
      cache coherence implementation. Upon miss, the caching agent will broadcast to other agents.
      The second one is `Home Snoop`, which is more like a directory-based cache coherence implementation.
      Upon miss, the caching agent will contact home agent, and then the home agent will send requests
      to other caching agents who have the requested cache line.
      There are other implementations like Cluster-on-Die.
      Intel UPI gets rid of all this complexity, it is only using directory-based, in the hope to reduce
      cache coherence traffic, which make sense.
    - Related: [Broadwell EP Snoop Models](https://software.intel.com/en-us/articles/intel-xeon-processor-e5-2600-v4-product-family-technical-overview)
    - Related: [Skylay UPI](https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview)
- [Paper: MESIF: A Two-Hop Cache Coherency Protocol for Point-to-Point Interconnects (2009)__](https://researchspace.auckland.ac.nz/bitstream/handle/2292/11594/MESIF-2009.pdf?sequence=6)
    - A MUST read.
    - This paper has the most extensive description of the MESIF protocol implementation.
      It has many `timing diagrams` than describe how cache requests actually proceed.
      Those diagrams can help us understand what is needed to finish a cache request.
    - Their [slides](https://parlab.eecs.berkeley.edu/sites/all/parlab/files/20091029-goodman-ssccp.pdf)
      has more timing diagrams.
    - But do note: the implementation described by this paper is different from
      what [Intel QPI](https://www.intel.ca/content/dam/doc/white-paper/quick-path-interconnect-introduction-paper.pdf)
      has in products. The difference is discussed at chapter 4. MESIF and QPI, namely,
      other caching agents will send responses to Home agent rather than to requesting agent.
      QPI relies on Home agent to solve conflict.
    - Also note: this is just one of the possible implementations to realize MESIF protocol.
      There could be many other ways, e.g., QPI source snooping, QPI home snooping.
      But all of them share the essential and general concepts and ideas.
- [Appendix I: Large-Scale Multiprocessors and Scientific Applications](https://www.elsevier.com/books-and-journals/book-companion/9780128119051),
  chapter 7 Implementing Cache Coherence.
    - This is probably some most insightful discussion about real implementation of cache coherence.
      With the distributed nature and Network-on-Chip, implementing cache coherence in modern
      processors is no different than implementing a distributed transaction protocol.
    - Cache activities like read miss or write miss have multi-step operations, but they
      need to appear as "atomic" to users. Put in another way, misses are like transactions,
      they have multiple steps but they must be atomic. They can be retried.
    - Having directory for cache coherence will make implementation easier. Because
      the place (e.g., L3) where directory resides can serve as the serialization point.
      They can solve write races.
    - `Home directory controller` and `cache controller` will exchange messages like a set of distributed machines.
      In fact, with NoC, they are actually distributed system.
- [Intel: An Introduction to the Intel® QuickPath Interconnect](https://www.intel.ca/content/dam/doc/white-paper/quick-path-interconnect-introduction-paper.pdf),
  page 15 MESIF.
      - [HotChips slide](https://www.hotchips.org/wp-content/uploads/hc_archives/hc21/1_sun/HC21.23.1.SystemInterconnectTutorial-Epub/HC21.23.120.Safranek-Intel-QPI.pdf), has timing diagrams.
      - It explains the `Home Snoop` and `Source Snoop` used by Intel.
      - Based on their explanation, it seems both `Home Snoop` and `Source Snoop` are using a combination of
        snoop and directory. The Processor#4 (pg 17 and 18) maintains the directory.
      - And this is a perfect demonstration of the details described in [Appendix I: Large-Scale Multiprocessors and Scientific Applications](https://www.elsevier.com/books-and-journals/book-companion/9780128119051).
      - Related patent: [Extending a cache coherency snoop broadcast protocol with directory information](https://patents.google.com/patent/US20150081977)
- [Paper: Multicast Snooping: A New Coherence Method Using a Multicast Address Network, ISCA '99](http://research.cs.wisc.edu/multifacet/papers/isca99_multicast_talk_pdf.pdf)
    - A hybrid snoop and directory cache coherence implementation. The insight is snoop
      cause too much bandwidth, directory incurs longer latency.
    - So this paper proposed `Multicast snoop`, where it multicasts coherence transactions
      to selected processors, lowering the address bandwidth required for snooping.
- [Paper: Why On-Chip Cache Coherence Is Here to Stay, Communications of ACM'02](http://www.cis.upenn.edu/acg/papers/cacm12_why_coherence.pdf)
    - This paper discusses why cache coherence can scale. A nice read.
    - R1: Coherence’s interconnection network traffic per miss scales
          when precisely tracking sharers. (Okay increased directory bits,
	  what about those storage cost? See R2).
    - R2: Hierarchy combined with inclusion enables efficient scaling
          of the storage cost for exact encoding of sharers.
    - R3: private evictions should send explicit messages to shared cache
          to enable precise tracking. Thus the recall (_back invalidation_) traffic can be
	  reduced when shared cache is evicting (assume inclusion cache).
    - R4: Latencies of cache request can be amortized.
- [Book: Parallel Computer Organization and Design](https://www.amazon.com/Parallel-Computer-Organization-Design-Professor/dp/0521886759), Chapter 7.
    - Links coherence and consistency together. This chapter uses detailed graphs to show
      how different cache coherence implementations affect consistency.
- [Book: A Primer on Memory Consistency and Cache Coherence](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.9278&rep=rep1&type=pdf)
    - Best book for this topic.
- [Dr.Bandwidth on explaining core-to-core communication transactions!](https://software.intel.com/en-us/forums/intel-moderncode-for-parallel-architectures/topic/700477)
    - Seriously, it's so good!
    - Although, I just feel there are so many unpublished details about the exact coherence transactions.
      Dr.Bandwidth himself used a lot "maybe", and listed possible actions.
- [Transactional Memory Coherence and Consistency, ISCA'04](http://www.mavam.com/lance/publications/tcc_ISCA04.pdf)
- [Programming with Transactional Coherence and Consistency (TCC)](http://www.mavam.com/lance/publications/tcc_ASPLOS04.pdf)
    - [Slide1](http://www.mavam.com/lance/publications/tcc_ASPLOS04Talk.pdf)
    - Awarded the most influential paper at ISCA 2019. I took a read today (Jul 21, 2019).
    - I feels like it's using the "batch" optimization for all time. The TCC design,
      kind of combines both cache coherence and memory consistency: how transactions
      commit or orders, determins the coherence and consistency.
    - It seems the load/store speculative execution used in their context is so similar
      to what Dr.Bandwidth said about Intel's implementation. Basically, the processor
      might read some data from L1/L2 and continue execution, but there is a chance,
      that the data is modifed by others, and the L3 caching agent or home agent
      could decide to revoke it. Once receiving such revoke message,
      the processor must cancel all executions that use the speculatively read data. 
    - It mentions couple Thread-Level Speculation papers, I think they should on this topic.

## Misc Facts

- Intel Caching Agent (Cbox) is per core (or per LLC slice). Intel Home Agent is per memory controller.
    - "The LLC coherence engine (CBo) manages the interface between the core and the last
level cache (LLC). All core transactions that access the LLC are directed from the core
to a CBo via the ring interconnect. The CBo is responsible for managing data delivery
from the LLC to the requesting core. It is also responsible for maintaining coherence
between the cores within the socket that share the LLC; generating snoops and
collecting snoop responses from the local cores when the MESIF protocol requires it."
    - "Every physical memory address in the system is uniquely associated with a single Cbox
      instance via a proprietary hashing algorithm that is designed to keep the distribution of
      traffic across the CBox instances relatively uniform for a wide range of possible address patterns."
    - Read more [here, chapter 2.3](https://www.intel.com/content/dam/www/public/us/en/documents/manuals/xeon-e5-2600-v2-uncore-manual.pdf).
    - Starting from Intel UPI, Caching Agent and Home Agent are combined as CHA.
- A good [discussion](https://www.realworldtech.com/qpi-evolved/3/) about why QPI gradually drop `Source Snoop` and solely use `Home Snoop`.
    - The motivation is scalability. It turns out the new UPI only supports directory-based protocol.
    - This makes sense because 1) inter socket bandwidth is precious, 2) snoop will consume a lot bandwidth.
- Intel UPI is using directory-based home snoop coherency protocol
    - [Intel® Xeon® Processor Scalable Family Technical Overview](https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview)
- To provide sufficient bandwidth, shared caches are typically interleaved
  by addresses with banks physically distributed across the chip.

## Case Study

### Intel

Intel does not disclose too much details about their cache coherence implementations.
The most valuable information is extracted from uncore PMU manuals, and discussions
from Dr. Bandwidth. According to Dr. Bandwidth, the Intel CPU could dynamically
adapt its coherence strategy during runtime according to workload. There won't
be one fixed cache coherence implementation, there will be many. It depends on
workload which one is used at runtime.

List below might not be completely true. Just my understanding.

- Physical addresses are uniquely hashed into L3 slices. That means each individual
  physical address belongs to a L3 slice, and also belongs to a home agent.
- Upon L2 miss, it will send requests to corresponding L3 slice. If the L3 slice
  is in the local socket, the request can be delievered within the same socket.
  If the L3 slice belongs to another remote socket, the L2 miss request will
  be sent over QPI/UPI. Also note that the L2 controller will not send snoop requests.
  (This is answering the question of "why using local memory is faster than remote"
   from the cache coherence perspective.)
- At L3, when received the request from a L2,
    - If it's in source snoop model, it will send `snoop messages` to other sockets.
    - If it's in home snoop model, it will send `read message` to other sockets.
      The another socket will generate snoop and collect responses. (R3QPI or home?)
    - Quote Dr. Bandwidth: Maintaining consistency is easier if the data is sent
      to the L3 first, and then to the requesting core, but it is also possible to
      send to both at the same time (e.g., "Direct2Core"). In recent processors,
      these return paths are chosen dynamically based on undocumented states and
      settings of the processor.
    - I'm not sure who will ACK L2 at last. L3 or home agent? Both are possible.
- I think both L3 and home agent have directory information. They know where
  to send snoop/read messages. And both of them can serialize coherence transactions!
  It's just undocumented who is doing what.
- In generall, we need to bear the fact that we cannot just figure out how Intel
  cache coherence works underlying. We maybe just need to "vaguely" know the fact that:
    - Both directory and snoop will be used in combination.
    - L3/home agent will serialize conflicting transactions
    - L3/home agent will send data to requesting core
    - L3/home agent will send final ACK to requesting L2
    - A coherence transaction is a multi-step distributed transaction.
      It involes sending requests, serialize conflicts, receiving responses/ACKs.

When in doubt, read the [discussion](https://software.intel.com/en-us/forums/intel-moderncode-for-parallel-architectures/topic/700477) posted by Dr. Bandwidth.

### AMD

- AMD HyperTransport Assit for Cache Coherence
    - [Slide](https://www.hotchips.org/wp-content/uploads/hc_archives/hc14/3_Tue/28_AMD_Hammer_MP_HC_v8.pdf)
    - [Slide](http://www.hotchips.org/wp-content/uploads/hc_archives/hc21/2_mon/HC21.24.100.ServerSystemsI-Epub/HC21.24.110.Conway-AMD-Magny-Cours.pdf)

### ARM

- [AMBA CHI Specifications](https://developer.arm.com/architectures/system-architectures/amba/documentation?_ga=2.147594999.1797165765.1562530195-129127748.1561485892)
    - This is probabaly the most comprehensive document I've ever seen about cache coherence.
      Although terms used by ARM differs from the ones used by Intel, still, you can map them.
      Chapter 5 Interconnect Protocol Flows has a lot timing diagrams regarding read/write/atomic
      coherence transactions.
    - It's a good reference to know, but it would be hard to actually understand the details.

### OpenCAPI and CCIX

- [CCIX White Paper](https://docs.wixstatic.com/ugd/0c1418_c6d7ec2210ae47f99f58042df0006c3d.pdf)
- [OpenCAPI]()

### OpenPiton

- [OpenPiton Microarchitecture Specification](https://parallel.princeton.edu/openpiton/docs/micro_arch.pdf)
    - Directory-based MESI
    - This spec has detailed coherence message packet format and type. Unfortunately,
      it does not say anything about how they deal with coherence transaction conflicts.
      E.g., some timeline diagrams like Figrue 2/3 in this [paper](https://researchspace.auckland.ac.nz/bitstream/handle/2292/11594/MESIF-2009.pdf?sequence=6).

### FPGA

- Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA Device, FPL'19
- LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories, FCCM'14.
    - This work is built on their earlier work, which basically add the data caching
      concept to FPGA: using BRAM as L1, on-board DRAM as L2, host or remote DRAM as L3.
    - In their earlier work, each FPGA application (or bitstream) has a private L1 cache.
    - In this work, the add MESI coherence to these private L1 caches, as in they can make
      multiple L1 cache cache-coherent.
    - The techniques and protocols from this paper are similar to the exisiting ones. For example,
      1) they use a global serializing point to serialize transactions, 2) they designed a lot
      messaging types such as INV, RESP and so on.
- [VMware Research Project PBerry](https://research.vmware.com/publications/project-pberry-fpga-acceleration-for-remote-memory)
    - A very interesting and promising project.
- [Intel FPGA PAC](https://www.intel.com/content/www/us/en/programmable/documentation/bfr1522087299048.html)
    - Intel itself is building a FPGA-CPU cache coherent setting. They use the Intel UPI interconnect
      to natually the spectrum. The FPGA shell has some modules to handle this.
- [Intel FPGA CCIP](https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/manual/mnl-ias-ccip.pdf)
    - Maybe the ASPLOS'20 Optimus paper is uing CCIP-related research platform?

- Also some pointer chasing related stuff
	- [A Study of Pointer-Chasing Performance on Shared-Memory Processor-FPGA Systems, FPGA'16](https://users.ece.cmu.edu/~jhoe/distribution/2016/fpga16.pdf)


### Formal Verification
TODO
FIll me in.
